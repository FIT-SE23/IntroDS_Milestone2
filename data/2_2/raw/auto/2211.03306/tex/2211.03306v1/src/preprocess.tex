\section{Preprocessing}
In this section, we present a simple, scalable preprocessing algorithm, which often reduces the size of the input networks significantly and results in a substantial speed-up. 
Specifically, the algorithm first computes an approximate solution by solving an LP, 
which is much smaller than LP~\eqref{LP:general} in practice, 
and then removes vertices from the original network using the information of the approximate solution obtained. 
We assume that $S^*_i$ for all $i\in [k]$ are known in advance
because they can be computed efficiently  using Charikar's LP-based algorithm~\cite{Charikar2000}
together with the preprocessing algorithm introduced by Balalau et al.~\cite{Balalau+15}.
%Consequently, we can decrease the number of essential variables in LP~\eqref{LP:general}. 
%This preprocess is necessary to deal with a large graph in practice. 
To describe our algorithm, we introduce some notations. 
For $S\subseteq V$, $v\in S$, and $i\in[k]$, let $d_i(S,v)$ denote the weighted degree of $v$ in the subgraph induced by $S$ in layer $i$, 
i.e., $d_i(S,v)\coloneqq\sum_{e\in E_i[S]:\,v\in e}w_i(e)$. When $S=V$, we simply write $d_i(v)$.
%In addition, let $d_i(S)\coloneqq d_i(V,v)$.


\begin{comment}
We first describe an algorithm for computing an approximate solution for $(\bm{\alpha}, \bm{\beta})$-\dens. 
Specifically, our algorithm iteratively removes a vertex with the minimum degree in the currently remaining graph in the currently \emph{worst} layer to obtain a sequence of vertex subsets from $V$ to a singleton, 
and then returns a probability distribution $p\in \Delta(2^V)$ corresponding to the best subset over the iteration. 
\begin{algorithm}[t] 
\caption{Compute an approximate solution}\label{alg:peeling}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{\ $(V,E;w_1,\dots,w_k)$}
\Output{\ $p\in \Delta(2^V)$}
%\Output{\ Lower bound $\ell^*$}
$T_n\ot V$\;
\For{$j\ot n,\dots,2$}{
  Let $i^*\in\argmin_{i\in[k]}\left[\alpha_i\frac{w_i(T_j)}{|T_j|}+\beta_i\right]$\;
  Find $v_j\in \argmin_{v\in T_j} d_{i^*}(T_j,v)$ and $T_{j-1}\ot T_j\setminus\{v_j\}$\;
}
Pick $S\in \argmax_{S\in \{T_n,\dots, T_2\}}\min_{i\in[k]}\left[\alpha_i\frac{w_i(T_j)}{|T_j|}+\beta_i\right]$\;
\Return $p\in \Delta(2^V)$ such that $p_S=1$
%\Return $\max_{j\in \{n,\dots, 2\}}\min_{i\in[k]}\left[\alpha_i\frac{w_i(T_j)}{|T_j|}+\beta_i\right]$\;
\end{algorithm}
The procedure is detailed in Algorithm~\ref{alg:peeling}. 
This algorithm can be implemented to run in $O(k|E|+k|V|\log |V|)$ time.
Note that this algorithm is a generalization of the greedy peeling algorithm introduced by Jethava and Beerenwinkel~\cite{JB2015} for the densest common subgraph problem. They showed that the algorithm is a $1/2$-approximation algorithm for the problem. 
\end{comment}

We first describe a fast algorithm for finding an approximate solution for $(\bm{\alpha}, \bm{\beta})$-\dens. 
%\memo{We first compute $S^*_i$ for all $i\in [k]$? Then...} 
Specifically, we compute a probability distribution $q\in\Delta(2^V)$ that maximizes the $(\bm{\alpha}, \bm{\beta})$-density 
under the constraint that $q_S=0$ for all $S\in 2^V\setminus \{S_1^*,\dots,S_k^*\}$.
The distribution can be found by solving the following LP:
\begin{align}
\begin{array}{rll}
\text{max.}       & t &\\
\text{s.t.}& \displaystyle t\leq \alpha_i \sum_{j\in[k]} \frac{w_{i}(S_j^*)}{|S_j^*|} q_j+\beta_i  &(\forall i\in [k]),\\
           & \displaystyle \sum_{j\in[k]}q_j = 1,  &\\
           & q_j\ge 0                &(\forall j\in[k]).
\end{array}\label{LP:lower}
\end{align}
Note that this LP has $k+1$ variables and $2k+1$ constraints. 
As $k$ is usually much smaller than $|V|$ and $|E|$, this LP is much smaller than LP~\eqref{LP:general} in practice. 
%Obviously, the optimal value of this LP is a lower bound on the optimal value of $(\bm{\alpha},\bm{\beta})$-\dens. 

% Next we describe an algorithm for removing vertices using the information of the output solution of Algorithm~\ref{alg:peeling}. 
% Our algorithm first computes the objective function value of the output of Algorithm~\ref{alg:peeling}, which is clearly a lower bound on the optimal value of $(\bm{\alpha},\bm{\beta})$-\dens. 
Next we describe an algorithm for removing vertices using the information of the above approximate solution $q$. 
Let $\ell^*$ be the $(\bm{\alpha},\bm{\beta})$-density of $q$, i.e., the optimal value of LP~\eqref{LP:lower}. 
Note that this is a lower bound on the optimal value of $(\bm{\alpha},\bm{\beta})$-\dens. 
Our algorithm iteratively removes any vertex $v^*$ that satisfies $\max_{i\in[k]}[\alpha_i\cdot d_i(V',v^*)+\beta_i]< \ell^*$, where $V'$ is a remaining vertex set (initially $V'=V$), as long as there exists such a vertex. 
For reference, we describe the procedure in Algorithm~\ref{alg:remove}. 
%Note that this is a generalization of the above procedure, which may accept not only the output of Algorithm~\ref{alg:peeling} but also any $p\in \Delta(2^V)$. 
This algorithm can be implemented to run in $O(k|E|+|V|\log |V|)$ time.
\begin{algorithm}[t] 
\caption{Remove useless vertices}\label{alg:remove}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
%\Input{\ $(V,E;w_1,\dots,w_k)$ and $p\in \Delta(2^V)$}
\Input{\ $(V,(E_i)_{i\in [k]})$ with $w_1,\dots,w_k$, and $\ell^*\in \mathbb{R}$}
\Output{\ $(V',(E_i[V'])_{i\in [k]})$} % ;w_1,\dots, w_k
%\Output{\ $(V',E_1[V'])$,\dots,$(V',E_k[V'])$} % ;w_1,\dots, w_k
%$\ell^*\ot \min_{i\in [k]} \mathbb{E}_{S\sim p}\left[\alpha_i\frac{w_i(S)}{|S|}+\beta_i\right]$\;
$V'\ot V$\;
\While{\texttt{True}}{
  Let $v^*\in\argmin_{v\in V'}\max_{i\in[k]}[\alpha_i\cdot d_i(V',v)+\beta_i]$\;
  \If{$\max_{i\in[k]}[\alpha_i\cdot d_i(V',v^*)+\beta_i]\ge \ell^*$}{
      \Return $(V',(E_i[V'])_{i\in [k]})$. 
  }
  \lElse{$V'\ot V'\setminus\{v^*\}$}
}
\end{algorithm}



From now on, we demonstrate that the algorithm does not remove any vertex that is contained in a subset in $\supp(p)$, where $p$ is an arbitrary 
optimal solution to $(\bm{\alpha},\bm{\beta})$-\dens. 
The following is a key lemma in our analysis. 
% If we have the following lemma, the proof of the above theorem is immediate. 
% \memo{$p\in\Delta(2^V)$から$(x,y,t)$への変換が必要}
% By~\eqref{eq:transform}, the theorem is equivalent to the following lemma, 
% and hence it is sufficient to prove the lemma.
\begin{lemma}\label{lemma:6.2}
Let $\ell^*$ be a lower bound on the optimal value of $(\bm{\alpha},\bm{\beta})$-\dens.
If $\max_{i\in[k]}[\alpha_i\cdot d_i(v^*)+\beta_i]<\ell^*$, 
then $y_{v^*}=0$ for any optimal solution to LP~\eqref{LP:general}.
\end{lemma}
\begin{proof}
We prove the lemma by contradiction.
We denote by $((\hat{x}_e)_{e\in E},(\hat{y}_v)_{v\in V},\hat{t})$ an optimal solution to LP~\eqref{LP:general}
and let $v^*\in V$ be a vertex that satisfies $\alpha_i\cdot d_i(v^*)+\beta_i<\ell^*$ for all $i\in[k]$.
Suppose for contradiction that $\hat{y}_{v^*}>0$.

We construct a solution $((x_e)_{e\in E},(y_v)_{v\in V},t)$ of LP~\eqref{LP:general} as follows: 
\begin{align*}
x_e&=\begin{cases}
\frac{1}{1-\hat{y}_{v^*}}\cdot\hat{x}_e&(e\not\ni v^*),\\
0        &(e\ni v^*),
\end{cases}\quad
y_v=\begin{cases}
\frac{1}{1-\hat{y}_{v^*}}\cdot\hat{y}_v&(v\ne v^*),\\
0        &(v= v^*),
\end{cases}\\
t&=\min_{i\in[k]}\Bigl[\alpha_i\sum_{e\in E_i}w_i(e)x_e+\beta_i\Bigr].
\end{align*}
It is easy to see that $((x_e)_{e\in E},(y_v)_{v\in V},t)$ is a feasible solution of LP~\eqref{LP:general}.
Moreover, we have
\begin{align*}
t
&=\min_{i\in[k]}\Bigl[\alpha_i\sum_{e\in E_i}w_i(e)x_e+\beta_i\Bigr]\\
&=\min_{i\in[k]}\Bigl[\alpha_i\frac{\sum_{e\in E_i:\,v^*\not\in e}w_i(e)\hat{x}_e}{1-\hat{y}_{v^*}}+\beta_i\Bigr]\\
&=\min_{i\in[k]}\Bigl[\alpha_i\frac{\sum_{e\in E_i}w_i(e)\hat{x}_e-\sum_{e\in E_i:\, v^*\in e}w_i(e)\hat{x}_{e}}{1-\hat{y}_{v^*}}+\beta_i\Bigr]\\
&\ge \min_{i\in[k]}\Bigl[\alpha_i\frac{\sum_{e\in E_i}w_i(e)\hat{x}_e-\sum_{e\in E_i:\, v^*\in e}w_i(e) \hat{y}_{v^*}}{1-\hat{y}_{v^*}}+\beta_i\Bigr]\\
&=\min_{i\in[k]}\Bigl[\alpha_i\frac{\sum_{e\in E_i}w_i(e)\hat{x}_e-d_i(v^*) \hat{y}_{v^*}}{1-\hat{y}_{v^*}}+\beta_i\Bigr]\\
&=\min_{i\in[k]}\frac{(\alpha_i\sum_{e\in E_i}w_i(e)\hat{x}_e+\beta_i)- (\alpha_i\cdot d_i(v^*)+\beta_i)\cdot \hat{y}_{v^*}}{1-\hat{y}_{v^*}}\\
&>\min_{i\in[k]}\frac{(\alpha_i\sum_{e\in E_i}w_i(e)\hat{x}_e+\beta_i)-\ell^*\cdot \hat{y}_{v^*}}{1-\hat{y}_{v^*}}\\
&=\frac{\hat{t}-\ell^*\cdot \hat{y}_{v^*}}{1-\hat{y}_{v^*}}
\ge \frac{\hat{t}-\hat{t}\cdot \hat{y}_{v^*}}{1-\hat{y}_{v^*}}
=\hat{t},
\end{align*}
%\begin{align*}
%t
%&=\min_{i\in[k]}\left[\alpha_i\sum_{e\in E}w_i(e)x_e+\beta_i\right]\\
%&=\min_{i\in[k]}\left[\alpha_i\frac{\sum_{e\in E:\,v^*\not\in e}w_i(e)\hat{x}_e}{1-\hat{y}_{v^*}}+\beta_i\right]\\
%&=\min_{i\in[k]}\left[\alpha_i\frac{\sum_{e\in E}w_i(e)\hat{x}_e-\sum_{e':\, v^*\in e'\in E}w_i(e')\hat{x}_{e'}}{1-\hat{y}_{v^*}}+\beta_i\right]\\
%&\ge \min_{i\in[k]}\left[\alpha_i\frac{\sum_{e\in E}w_i(e)\hat{x}_e-\sum_{e':\, v^*\in e'\in E}w_i(e') \hat{y}_{v^*}}{1-\hat{y}_{v^*}}+\beta_i\right]\\
%&=\min_{i\in[k]}\left[\alpha_i\frac{\sum_{e\in E}w_i(e)\hat{x}_e-d_i(v^*) \hat{y}_{v^*}}{1-\hat{y}_{v^*}}+\beta_i\right]\\
%&=\min_{i\in[k]}\frac{(\alpha_i\sum_{e\in E}w_i(e)\hat{x}_e+\beta_i)- \hat{y}_{v^*}(\alpha_id_i(v^*)+\beta_i)}{1-\hat{y}_{v^*}}\\
%&>\min_{i\in[k]}\frac{(\alpha_i\sum_{e\in E}w_i(e)\hat{x}_e+\beta_i)-\ell^*\cdot \hat{y}_{v^*}}{1-\hat{y}_{v^*}}\\
%&=\frac{\hat{t}-\ell^*\cdot \hat{y}_{v^*}}{1-\hat{y}_{v^*}}
%\ge \frac{\hat{t}-\hat{t}\cdot \hat{y}_{v^*}}{1-\hat{y}_{v^*}}
%=\hat{t},
%\end{align*}
where the first inequality follows from $\hat{x}_{e} \leq \hat{y}_{v^*}$ for each $e \ni v^*$, 
the second inequality follows from the assumptions $\alpha_i\cdot d_i(v^*)+\beta_i< \ell^*$ and $\hat{y}_{v^*}>0$, and the third inequality follows from Lemma~\ref{lemma:general_lower}. 
%where the first inequality holds because $\hat{x}_{e'} \leq r_{v^*}$ for each $e' \not\ni v^*$, the second inequality holds by the assumption $\alpha_id_i(v^*)+\beta_i< \ell^*$, and the third inequality holds by $\ell^* \leq \hat{t}$. 
This contradicts the optimality of $((\hat{x}_e)_{e\in E},(\hat{y}_v)_{v\in V},\hat{t})$. 
\end{proof}


%We have the following theorem.
\begin{theorem}
Let $\ell^*$ be a lower bound on the optimal value of $(\bm{\alpha},\bm{\beta})$-\dens.
Then, any vertex $v^*$ that satisfies $\max_{i\in[k]}[\alpha_i\cdot d_i(v^*)+\beta_i]<\ell^*$ 
%Then, any vertex $v^*$ with weighted degree smaller than $\ell^*$ for all $i\in [k]$ (i.e., $\max_{i\in[k]}[\alpha_i\cdot d_i(v^*)+\beta_i]<\ell^*$) 
is not contained in any subset in the support of any optimal solution to $(\bm{\alpha},\bm{\beta})$-\dens.
\end{theorem}
\begin{proof}
Let $v^*$ be any vertex with $\max_{i\in[k]}[\alpha_i\cdot d_i(v^*)+\beta_i]<\ell^*$. 
Let $\hat{p}$ be any optimal solution to $(\bm{\alpha},\bm{\beta})$-\dens.
Construct $((\hat{x}_e)_{e\in E},(\hat{y}_v)_{v\in V},\hat{t})$ from $\hat{p}$ as in \eqref{eq:transform}. 
From Lemma~\ref{lemma:expect} and the proof of Lemma~\ref{lemma:general_lower}, 
we see that $((\hat{x}_e)_{e\in E},(\hat{y}_v)_{v\in V},\hat{t})$ is an optimal solution to LP~\eqref{LP:general}. 
Thus, by Lemma~\ref{lemma:6.2}, we have $\hat{y}_{v^*} = 0$.
By the construction of $\hat{y}_{v^*}$ in \eqref{eq:transform}, $\hat{p}_S=0$ for all $S\subseteq V$ containing $v^*$. 
%Therefore, the theorem holds. 
\end{proof}

This theorem indicates that Algorithm~\ref{alg:remove} does not remove any vertex that is contained in a subset in the support of any optimal solution to $(\bm{\alpha},\bm{\beta})$-\dens.



