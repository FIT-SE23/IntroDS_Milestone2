
\section{Experimental Settings}
\label{app:exp}

\subsection{MPE}
\label{app:mpe}
The three tasks are built on the origin MPE \citep{MADDPG} (MIT license) and are originally used in \citet{transfer} (MIT license). The objective in these three tasks are listed as follows:

\begin{itemize}
    \item \textbf{Simple Spread:} There are $N$ agents who need to occupy the locations of $N$ landmarks. 
    \item \textbf{Line Control:} There are $N$ agents who need to line up between 2 landmarks.
    \item \textbf{Circle Control:} There are $N$ agents who need to form a circle around a landmark.
\end{itemize}

The reward in these tasks is the distance between all the agents and their target locations. We set the number of agents $N = 5$ for these three tasks in our experiment. 

\subsection{Multi-Agent MuJoCo}
Multi-agent MuJoCo \citep{mamujoco} (Apache-2.0 license) is a robotic locomotion task with continuous action space for multi-agent settings. The robot could be divided into several parts and each part contains several joints. Agents in this environment control a part of the robot which could be different varieties. So the type of the robot and the assignment of the joints decide a task. For example, the task `HalfCheetah-3$\times$2' means dividing the robot `HalfCheetah' into three parts for three agents and each part contains 2 joints. 

The details about our experiment settings in multi-agent Mujoco are listed in Table \ref{tab:mujoco}.  The configuration defines the number of agents and the joints of each agent. The `agent obsk' defines the number of nearest agents an agent can observe. %So all these tasks are actually partial observable.
\begin{table}[h]
    \centering
    \caption{The task settings of multi-agent MuJoCo}
    \label{tab:mujoco}
    %\vspace{2mm}
    \begin{tabular}{c|c|c}
    \toprule
         task & configuration & agent obsk \\
         \midrule
         HalfCheetah & 3$\times$2 & 2 \\
         Hopper & 3$\times$1 & 2 \\
         Walker2d & 3$\times$2 & 2 \\
         Ant & 4$\times$2 & 2 \\
    \bottomrule
\end{tabular}
    
\end{table}

\section{Training Details}
\label{app:train}

Our code is based on the open-source code\footnote{https://github.com/marlbenchmark/on-policy} of MAPPO \citep{MAPPO} (MIT license). We modify the code for individual parameters and ban the tricks used by MAPPO for SMAC. The network architectures and base hyperparameters of DPO and IPPO are the same for all the tasks in all the environments. We use 3-layer MLPs for the actor and the critic and use ReLU as non-linearities. The number of the hidden units of the MLP is 128.  We train all the networks with an Adam optimizer. The learning rates of the actor and critic are both 5e-4. The number of epochs for every batch of samples is 15 which is the recommended value in \citet{MAPPO}. For IPPO, the clip parameter is 0.2 which is the same as \citet{PPO}. For DPO, the initial values of the coefficient $\beta^i_1$ and $\beta^i_2$ are 0.01. The value of $d_{\operatorname{target}}$ is 0.1 for the cooperative stochastic game, 0.01 for MPE, 0.001 for multi-agent MuJoCo, and 0.02 for SMAC.  

The version of the game StarCraft2 in SMAC is 4.10 for our experiments in all the SMAC tasks. 
We set the episode length of all the multi-agent MuJoCo tasks as 1000 in all of our multi-agent MuJoCo experiments. 
We perform the whole experiment with a total of four NVIDIA A100 GPUs.
We have summarized the hyperparameters in Table \ref{tab:hyperparameter}.

\begin{table}[h]
    \centering
    \caption{Hyperparameters for all the experiments}
    \label{tab:hyperparameter}
    %\vspace{2mm}
    \begin{tabular}{cc}
    \toprule
         hyperparameter & value\\
         \midrule
         MLP layers & 3 \\
         hidden size & 128 \\
         non-linear & ReLU \\
         optimizer & Adam \\
         actor\_lr & 5e-4  \\
         critic\_lr & 5e-4 \\
         numbers of epochs & 15 \\
         initial $\beta^i_1$ & 0.01 \\
         initial $\beta^i_2$ & 0.01 \\
         $\delta$ & 1.5  \\
         $\omega$ & 2   \\
         $d_{\on{target}}$ & different for environments as aforementioned  \\ 
         clip parameter for IPPO  & 0.2 \\
    \bottomrule
    \end{tabular}
\end{table}
\label{app:hyper}

\section{Additional Results}
\label{app:more}




\citet{PPO} actually proposed two versions of PPO. The first version, which is also the most popular version, is with the clip trick. The second version is directly optimizing the penalty formula with adaptive coefficients and we refer to this algorithm as PPO-KL. IPPO \citep{IPPO} is actually extended from the first version, while the practical algorithm of DPO is similar to the second version. The main difference between DPO and PPO-KL is the term of the square root of the KL-divergence in the policy loss. We modify IPPO by making each agent learn with PPO-KL to obtain IPPO-KL. 
On the other hand, independent Q-learning (IQL) \citep{IQL} is a classic independent learning algorithm. IQL could obtain good performance in some tasks but it is not with any theoretical guarantee, to the best of our knowledge. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/matrix_game_ablation.pdf}
    \caption{Learning curve of DPO compared with IPPO, IPPO-KL, IQL, and the global optimum in the cooperative stochastic game, where x-axis is environment steps. }
    \label{fig:matrix2}
\end{figure}

For the completeness of our experiments, we test the performance of IPPO-KL and IQL in the cooperative stochastic game and the empirical result is illustrated in Figure~\ref{fig:matrix2}. We find that the performance of IPPO-KL is close to DPO but a little bit lower and more unstable. This can be explained as the policy loss of IPPO-KL is actually a biased approximation of DPO, which omits the square root term. The theoretical guarantee of DPO may also help IPPO-KL perform better than IPPO in this game, while the bias in the policy loss of IPPO-KL makes it perform worse than DPO. As for IQL, its performance is lower than IPPO though it converges faster. Since IQL performs worse than IPPO in such a didactic environment and IQL is a value-based algorithm and is out of the scope of our discussion, we do not take IQL as a baseline in other environments. Moreover, we think the essential relations among IPPO, IPPO-KL, and DPO can be clearly witnessed from the empirical results in the cooperative stochastic game. For other tasks, we focus on the mainstream version of IPPO as in \citet{IPPO,MAPPO,benchmark}.

Besides our empirical results, we would like to share our views on the difference between DPO and IPPO and give some intuitive ideas. KL regularization and ratio clipping are similar in the single-agent setting, but they are not supposed to be similar in multi-agent settings. The `correct' ratio clipping in multi-agent setting according to the theory of PPO should clip over the joint policy ratio $\frac{ \boldsymbol{\pi_{\operatorname{new}}} (\boldsymbol{a}|s) }{\boldsymbol{\pi_{\operatorname{old}} } (\boldsymbol{a}|s)}$. IPPO just clips individual policy ratio $\frac{\pi^i_{\operatorname{new}}(a_i|s)}{\pi^i_{\operatorname{old}}(a_i|s)}$ for each agent $i$ which may not be enough to realize the `correct' ratio clipping.  We could find more discussion about this in the CoPPO \citep{COPPO} paper. So IPPO is not supposed to enjoy the theoretical results of DPO.

We could rewrite the objective of IPPO for agent $i$ with a similar formulation in HPO \citep{HPO} as follows:
$$
\mathcal{L}^{i,\operatorname{IPPO}}_{\boldsymbol{\pi}_{\operatorname{old}}}(\pi^i_{\operatorname{new}}) = \sum_{s} \boldsymbol{\rho}_{\operatorname{old}}(s) \sum_{a_i} \pi^i_{\operatorname{new}}(a_i|s) |A^i_{\operatorname{old}}(s,a_i)|l\left( \operatorname{sign}(A^i_{\operatorname{old}}(s,a_i)), u_i(s,a_i)- 1, \epsilon \right),
$$
where $l(y,x,\epsilon) = \operatorname{max}\{0,\epsilon - y\times x\}$  is the hinge loss and $u_i(s,a_i) = \frac{\pi^i_{\operatorname{new}}(a_i|s)}{\pi^i_{\operatorname{old}}(a_i|s)}$ is the ratio.

If we follow the same idea as PPO, then IPPO is the `correct' ratio clipping version for the surrogate of DPO. However, the effectiveness of this ratio clipping formulation in theory is still open in decentralized learning since there is not any convergence guarantee for IPPO, to the best of our knowledge. 

Though the effectiveness of IPPO in theory is beyond the scope of our paper, we could provide an intuitive explanation for the fact that the performance of DPO can surpass IPPO from this formulation and the analysis in HPO. In the proof of HPO, there is a critical assumption that the sign of the estimated advantage is the same as that of the true advantage (Assumption 4 in Section 2.3 in \citet{HPO}). And HPO also shows that the sign of the advantage is more important than the value for this formulation of PPO-clip. In decentralized learning, both DPO and IPPO are facing the difficulty of learning the individual advantage function as there may be noise in the individual value function. However, the objective of DPO is continuous and the objective of IPPO  is discrete for $\operatorname{sign}(A^i_{\operatorname{old}}(s,a_i))$. So the impact of the noise in the value function may be larger on IPPO than DPO. 


\section{Discussion}

In the paper, we derive a novel lower bound that can be naturally divided into independent surrogate \eqref{eq:de-obj} for each agent. By each agent optimizing this surrogate, the monotonic improvement of the joint policy can be guaranteed in fully decentralized settings. However, the practical algorithm of DPO takes the formula of \eqref{eq:adaptive-obj} with several approximations. How to solve the optimization of \eqref{eq:de-obj} more precisely is left as future work.

Moreover, we expect our work could provide some insights for future studies on fully decentralized multi-agent reinforcement learning, since current methods still have a gap from the optimum as shown in Figure~\ref{fig:matrix2}.

%As aforementioned, IPPO-KL is similar to the practical algorithm of DPO, but is a biased approximation to DPO (without the square root term). Thus, we argue that in fully decentralized settings DPO is a better choice than IPPO-KL while IPPO-KL is a better choice than IPPO, 