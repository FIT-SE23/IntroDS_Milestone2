\section{Introduction}

  The use of automated, formal methods has become indispensable for analysing complex security protocols, such as those for authentication, key exchange and secure channel establishment.
  Nowadays there exist mature, fully automated such analysers; among others \avispa \cite{ABB05}, \proverif \cite{B16}, \scyther \cite{C08}, \tamarin \cite{SMC13} or Maude-NPA \cite{SEM14}.
  These tools are able to automatically verify full fledged models of widely deployed protocols and standards, such as the TLS protocol for secure connexion \cite{BBK17,CHH17}, the Signal messaging protocol \cite{KBB17,CCG18}, authentication protocols of the 5G standard \cite{BDH18}, or deployed multi-factor authentication protocols \cite{JK18}.
  Theory-wise, the tools operate in so-called \emph{symbolic} models, rooted in the seminal work by Dolev and Yao \cite{DY81}: 
  the attacker has full control over the communication network, unbounded computational power, but cryptography is idealised.
  This model is well suited for finding attacks in the protocol logic, and tools have indeed been extremely effective in discovering this kind of flaw or proving their absence.

  While most works investigate \emph{reachability} properties, a later trend consists in adapting the tools---and the underlying theory---to the more complex \emph{indistinguishability} properties.
  Such properties are generally modelled as a behavioural equivalence (bisimulation or trace equivalence) in a dedicated process calculus such as the spi calculus \cite{AG99} or the applied pi calculus \cite{ABF17}.
  A typical example is real-or-random secrecy: after interacting with a protocol, an adversary is unable to distinguish the \emph{real} secret used in the protocol from a \emph{random} value.
  Privacy-type properties can also be expressed as such:
  anonymity may be modelled as the adversary's inability to distinguish two instances of a protocol executed by different agents;
  vote privacy \cite{DKR09} has been expressed as indistinguishability of the situations where the votes of two agents have been swapped or not;
  unlinkability \cite{ACR10} is seen as indistinguishability of two sessions, either both executed by the same agent \(A\), or by two different agents \(A\) and \(B\).

  \subsection*{Contributions}
    We significantly improve the theoretical understanding
    % , in terms of computational complexity,
    and the practical verification of equivalences when the number of protocol sessions is bounded.
    We emphasise that even in this setting, the system under study has an infinite state space due to the term algebra modelling cryptographic primitives.
    Our work targets the wide class of cryptographic primitives that can be represented by a subterm convergent rewriting system.
    Concretely, we provide:

    \begin{enumerate}
      \item tight complexity results for several equivalence relations:
      static equivalence, trace equivalence and labelled bisimilarity.
      In addition to the conference paper~\cite{CKR18}, we showcase the generality of our approach by providing, with a negligible proof overhead, a tight analysis of other security relations, namely similarity, simulation, and trace inclusion;
      \item a novel procedure deciding all of the above mentioned security relations for a bounded number of sessions, for the class of cryptographic primitives modelled by a destructor subterm convergent rewrite system;
      \item an implementation of our procedure for trace equivalence in a tool called \deepsec (DEciding Equivalence Properties for SECurity protocols), improved compared to its initial presentation in the conference paper~\cite{CKR18}.
    \end{enumerate}

    \noindent
    We detail the three contributions below.

    % \medskip

  \paragraph{Complexity} 
    We provide the first complexity results for deciding trace equivalence and labelled bisimilarity in the applied pi calculus, without any syntactic or semantic restriction on the class of protocols (other than bounding the number of sessions), and for a large class of cryptographic primitives modelled as rewrite rules. 
    As mentioned above, our results extend to several other security relations such as simulation.
    Let us also highlight one small, yet substantial difference with existing work: we do not consider cryptographic primitives (rewrite systems) as constants of the problem.
    As most modern verification tools allow for user-specified primitives \cite{manual-proverif,SMC13,SEM14,CCC16}, our approach seems to better fit this reality.
    Typically, all existing procedures for static equivalence can only be claimed \ptime because of this difference and are actually exponential in the sizes of the signature or equational theory.
    Our complexity results are summarised in Figure~\ref{fig:summary}.
    All our lower bounds hold for subterm convergent rewrite systems and even for the positive fragment (without \(\ElseP\) branches).
    \textit{En passant}, we present results for the pi calculus:
    although investigated in \cite{BT00}, complexity was unknown when restricted to a bounded number of sessions.
    Still, our main result is the co\nexp completeness (and in particular, the decidability) of trace equivalence and labelled bisimilarity for destructor subterm convergent rewrite systems.

    \begin{figure}[ht]
      \centering
      \begin{tabular}{|c|c|c|}
        \hline
        & \multirow{2}{*}{Pure pi calculus} & Applied pi calculus\\
        & & \small with destr. subterm convergent theory\\
        \hline
        static equivalence & \logspace & co\np complete\\
        \hline
        trace equivalence & \polyh 2 complete & co\nexp complete \\
        \hline
        labelled (bi)similarity & \pspace complete & co\nexp complete \\
        \hline
      \end{tabular}
      % \Description{Summary of complexity results}
      \caption{Summary of complexity results}
      \label{fig:summary}
    \end{figure}

  \paragraph{Decision procedure} 
    We present a novel procedure based on a symbolic semantics and constraint solving.
    Unlike most other work, our procedure decides equivalences \emph{exactly}, i.e. without approximations.
    Moreover, it does not restrict the class of processes (except for replication), nor the use of \(\ElseP\) branches, and is correct for any cryptographic primitives that can be modelled by a subterm convergent destructor rewrite system (see Section~\ref{sec:model} for more details).
    The design of the procedure did greatly benefit from our complexity study, and was developed in order to obtain tight complexity upper bounds.
    The theory is also more mature compared to the initial conference paper \cite{CKR18} which allowed some significant optimisations of the constraint solving procedure.

    % Moreover, we introduce the new notions of \emph{partition tree} and \emph{most general solutions} of constraint systems, which may be of more general interest for deciding equivalences and resolution of constraint systems.

    % \medskip

  \paragraph{Tool implementation}
    We implemented our procedure for trace equivalence in a tool, \deepsec.
    Its prototype has initially been presented in the conference paper \cite{CKR18} (and some implementation details in a tool paper \cite{CKR18b}), but has significantly matured since then.
    In addition of the improvements at the level of the theoretical procedure, the low level implementation has been more carefully engineered data-structure-wise.
    All in all, the \deepsec 2.0.0 release includes the following new features:
    \begin{itemize}
      \item A significantly \emph{reduced verification time} (several orders of magnitude on some examples).
      \item An \emph{optional procedure exploiting the symmetries} that often arise in practical verification.
      When used, this further reduces the verification time by orders of magnitude albeit for occasionally introducing false attacks.
      In this paper we rather focus on the main procedure;
      details about this feature can be found in \cite{CKR19}.
      \item An \emph{improved user experience}.
      The html based pretty-print of the original prototype has been upgraded into a standalone graphical user interface.
      Verification queries and options can be managed directly from the interface and a simulator displays interactively equivalence proofs or attacks to better visualise the outcome of the analysis.
    \end{itemize}

    \noindent
    Naturally \deepsec still integrates already-present features such as multicore distribution and the partial order reductions presented in \cite{BDH15}.
    All in all this makes the tool more user friendly and scale well despite the high theoretical complexity of the problem (co\nexp).
    Installation guidelines can be found in the official website \cite{website} together with a manual and a tutorial.

    % While still a prototype, \deepsec was carefully engineered.
    % The tool output is available in pretty printed $\mathsf{html}$ format and allows to step through an attack, if any is found.
    % \deepsec can also distribute the computation, thus exploiting multicore architectures or clusters of computers to their fullest.
    % Finally, we integrated several classical optimisations for trace-equivalence analysis, e.g. \emph{partial order reductions} (POR) \cite{BDH15}.
    % This has appeared to reduce the search space dramatically, making the tool scale well in practice despite the high theoretical complexity (co\nexp).

    % \medskip

    Through extensive benchmarks, we compare \deepsec to other tools limited to a bounded number of protocol sessions: \apte, \spec, \akiss, \satequiv and our previous prototype (as presented in \cite{CKR18}).
    This prior version was already more efficient---by several orders of magnitude---than \apte, \spec and \akiss, even though \deepsec covers a strictly larger class of protocols than \apte and \spec.
    Besides, its performances were comparable to \satequiv, which still outperforms \deepsec when the number of parallel processes significantly increase.
    This gap in performance seems unavoidable as \deepsec operates on a much larger class of protocols (more primitives, \(\ElseP\) branches, no limitation to simple processes, termination guaranteed).
    % Besides, as mentioned earlier, the current implementation (\deepsec 2.0.0) outperforms \deepsec 1.0.2.
    %
    Part of the benchmarks consists of classical authentication protocols and focuses on demonstrating scalability of the tool when augmenting the number of parallel protocol sessions.
    The other examples include more complex protocols, such as Abadi and Fournet's anonymous authentication protocol \cite{AF04}, the protocols implemented in the European passport \cite{P04}, a model (without XOR) of the AKA protocol used in 3G mobile telephony, as well as the Pr\^et-\`a-Voter \cite{RS06} and the Helios \cite{A08} e-voting protocols.

  \subsection*{Related Work}
    The problem of analysing security protocols is undecidable in general but several decidable subclasses have been identified.
    While many complexity results are known for trace properties \cite{DLM04,RT03}, the case of behavioural equivalences remains mostly open.
    When the attacker is an eavesdropper and cannot interact with the protocol, the indistinguishability problem---\emph{static equivalence}---has been shown \ptime for large classes of cryptographic primitives \cite{AC06,CDK12,CBC11}.
    For active attackers, bounding the number of protocol sessions is often sufficient to obtain decidability \cite{RT03} and is of practical interest: most real-life attacks indeed only require a small number of sessions.
    In this context Baudet \cite{B05}, and later Chevalier and Rusinowtich \cite{CR10}, showed that real-or-random secrecy was co\np for cryptographic primitives that can be modelled as subterm convergent rewrite systems, by checking whether two constraint systems admit the same set of solutions.
    These procedures do however not allow for \(\ElseP\) branches, nor do they verify trace equivalence in full generality.
    In \cite{CCD13}, Cheval et al. have used Baudet's procedure as a black box to verify trace equivalence of \emph{determinate} processes.
    This class of processes is however insufficient for most anonymity properties.
    Finally, decidability results for an unbounded number of sessions exist \cite{CCD15,CCD15b}, but with severe restrictions on processes and equational theories.

    Tool support also exists for verifying equivalence properties.
    We start discussing tools that are limited to a bounded number of sessions.
    The \spec tool \cite{TD10, TNH16} verifies a sound symbolic bisimulation, but is restricted to particular cryptographic primitives (pairing, encryption, signatures and hash functions) and does not allow for \(\ElseP\) branches.
    The \apte tool \cite{C14} covers the same primitives but allows \(\ElseP\) branches and decides trace equivalence exactly.
    On the contrary, the \akiss tool \cite{CCC16} allows for user-defined cryptographic primitives.
    The procedure of this tool is correct for primitives modelled by an arbitrary convergent rewrite system that has the finite variant property \cite{CD05}, and termination is additionally guaranteed for subterm convergent rewrite systems.
    However, \akiss does only decide trace equivalence for a class of determinate processes; for other processes trace equivalence can be both over- and under-approximated.
    The recent \satequiv tool \cite{CDD17} uses a different approach: it relies on Graph Planning and SAT solving to verify trace equivalence, rather than a dedicated procedure.
    The tool is extremely efficient and several orders of magnitude faster than other tools.
    It does however not guarantee termination and is currently restricted to pairing and symmetric encryption and only considers a class of \emph{simple processes} (a subclass of determinate processes) that satisfy a type-compliance condition.
    These restrictions severely limit its scope.

    Other tools support verification of equivalence properties, even for an unbounded number of sessions.
    This is the case of \proverif \cite{BAF08}, \tamarin \cite{BDS15} and Maude NPA \cite{SEM14} which all allow for user-defined cryptographic primitives.
    However, given that the underlying problem is undecidable, these tools may not terminate.
    Moreover, they only approximate trace equivalence by verifying the stronger \emph{diff-equivalence}.
    This equivalence is too strong on many examples.
    While some recent improvements on \proverif \cite{CB13,BS16} help covering more protocols, general verification of trace equivalence is still out of scope.
    For instance, the verification by Arapinis et al. \cite{AMR12} of unlinkability in the 3G mobile phone protocols required some ``tricks'' and approximations of the protocol to avoid false attacks.
    In \cite{CGL17}, Cortier et al. develop a type system and automated type checker for verifying equivalences.
    While extremely efficient, this tool only covers a fixed set of cryptographic primitives (the same as \spec and \apte) and verifies an approximated equivalence, similar to diff-equivalence.
    A different approach has been taken by Hirschi et al. \cite{HBD16}, identifying sufficient conditions provable by \proverif for verifying unlinkability properties, implemented in the tool \ukano, a front-end to the \proverif tool.
    \ukano does however not verify equivalence properties in general.
    % In comparison, the results of our paper allow to verify equivalences precisely, albeit for a bounded number of sessions.


  \newpage
  \tableofcontents
  \newpage
