We introduce related work along four main directions: Lagrangian flow-based representations, methods for efficiently computing flow maps for FTLE, deep learning as applied to flow visualization, and more broadly relevant research in machine learning.

\textbf{Lagrangian Particle Interpolation.} The Lagrangian representation of unsteady flow fields stores data in the form of trajectories of massless particles. Agranovsky et al.~\cite{agranovsky2014improved} showed that, for exploratory analysis, in-situ trajectory computation and post-hoc interpolation is more storage-efficient than compared to traditional Eulerian representations. Several works to improve the accuracy of post-hoc Lagrangian particle interpolation have been proposed since~\cite{chandler2014interpolation, agranovsky2015multi, bujack2015lagrangian, sane2019interpolation, rapp2019void}. Bujack and Joy~\cite{bujack2015lagrangian} proposed a method for representing trajectories as parametric curves for a more accurate post-hoc interpolation, and additionally, they performed an error estimation of the proposed Lagrangian representation. In a similar way, several works focused on theoretical/empirical error analysis of Lagrangian interpolation~\cite{chandler2016analysis, hummel2016error, sane2018revisiting}. Even though these techniques do not require expensive numerical integration during post-hoc analysis, they are still expensive because of the number of steps required to compute the full trajectory. In the recent work by Li et al.~\cite{li2022efficient} they represented the trajectories as B-spline curves and improve the computation time of new trajectories by interpolation between the B-spline control points. Lagrangian representations of flow are attractive as a kind of data reduction, and assuming a sufficiently-dense sampling of trajectories, can often be quite accurate. Nevertheless, this representation can come at a steep computational cost for interactive analysis, as a common bottleneck is repeatedly performing spatial queries over irregularly-sampled particles in space-time.

%The goals of their work aligns with the goals of our work i.e. improving the computation time of flow maps, however, the type of data considered by both the techniques are different.

\textbf{Fast FTLE computation.} Computation of FTLE and its applications have received significant attention in the literature. Haller et al.~\cite{haller2000finding, haller2000lagrangian} showed that Lagrangian coherent structures can be extracted as the ridges of FTLE. Following this pioneering work many researchers focused on improving the computation time of FTLE. Garth et al.~\cite{garth2007efficient} proposed an incremental flow map approximation technique for improving the computation time of FTLE. Sadlo et al.~\cite{sadlo2007efficient} introduced a technique for FTLE ridge extraction using adaptive mesh refinement. Their proposed approach provides a speed-up in FTLE computation by avoiding integration of seed particles where no ridges are present. Kasten et al.~\cite{kasten2009localized} constructed a localized FTLE and additionally, a faster way to compute it by reusing the separation values from previous time steps. Lipinski et al.~\cite{lipinski2010ridge} proposed a ridge tracking algorithm that approximates ridges in the FTLE for each time step and then approximates the ridge location in subsequent times. Brunton et al.~\cite{brunton2010fast} proposed a fast FTLE computation technique taking advantage of flow map composition for longer flow map approximations. Hlawatsch et al.~\cite{hlawatsch2010hierarchical} introduced a hierarchical line integration scheme taking advantage of spatial and temporal coherence to improve the computation time of a dense set of particles. This work focuses on projection of particles based on the short pre-computed integral curves and thus has an accuracy trade-off. Sadlo et al.~\cite{sadlo2011time} proposed a grid advection technique for efficient FTLE computation taking advantage of temporal coherence. All these techniques are specifically targeted towards improving the computation time of a specific downstream task i.e. either FTLE or extraction of LCS from FTLE. In this work, we focus on improving the computation time of flow map itself allowing for a improved computation time for a wide range of downstream tasks.

\textbf{Deep learning for Flow Visualization.} In the recent years, numerous deep learning based techniques have been proposed in relation to flow visualization. Han et al.~\cite{han2019flow} introduced FlowNet an encoder-decoder deep learning framework for clustering, filtering and selection of streamlines and stream surfaces. Gu et al.~\cite{gu2021reconstructing} proposed a two-stage deep learning framework for flow field reconstruction using selected streamlines. Guo et. al~\cite{guo2020ssr} and Sahoo et al.~\cite{sahoo2021integration} proposed a deep learning based vector field super resolution using novel loss functions. Most relevant to our method is the flow map super resolution technique proposed by Jakob et al.~\cite{jakob2020fluid}, wherein the proposed technique requires a low resolution flow map as input and outputs higher resolution (4x) flow map. Even though the inference of the high resolution flow map is fast, the technique still the requires computation of a low resolution flow map making it less efficient in terms of overall computation time. Moreover, the flow maps are limited only to the grid locations. Our technique on the other hand is able to generate arbitrary flow map samples at any given space-time location. Another relevant work to our method is the recent work by Han et al.~\cite{han2021exploratory} where the authors proposed a deep learning technique which takes in a space-time coordinate and time span as input, and outputs the evaluation of the flow map. Our approach is similar to the technique proposed by Han et al. in scope, e.g. learning the full flow map of a corresponding vector field, however we differ in network architecture, optimization scheme, and data requirements. Specifically, our technique does not require flow map samples for supervision, and instead can learn a flow map representation only using a provided vector field.

\textbf{Neural Differential Equations} Neural ordinary differential equation (Neural ODE), a technique to solve initial-value ODE problems proposed by Chen et al.~\cite{chen2018neuralode} has been extended~\cite{liu2021second, norcliffe2021neural} and applied to various different research domains~\cite{han2021temporal, portwood2019turbulence}. Theoretically, since, flow maps are solution to an initial-value ODE, neural ODE should naturally extend to solve the problem. However, learning a flow map representation using neural ODE has not been studied yet. Our work is closely related to the work by Bilo{\v{s}} et al.~\cite{bilovs2021neural}, wherein they approximate the solution directly in a single step instead of integrating within a latent representation space. We draw inspiration from their work in the way we model the flow map prediction, however, the main distinction between our approach is in the network architecture design and the proposed optimization scheme. Our method of optimizing for flow map derivatives further draws inspiration from gradient-based learning methods, e.g. modeling shapes with gradient fields~\cite{cai2020learning}, and accelerating the volume-rendering integral through learning antiderivative networks (AutoInt) ~\cite{lindell2021autoint}. \new{A key difference between our method and AutoInt is that our novel network design allows us to forgo the requirement of computing integrals that are ultimately used for supervision in optimizing a gradient network, making our method of optimization more computationally efficient}.