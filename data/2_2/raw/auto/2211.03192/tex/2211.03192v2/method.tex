In this section, we first discuss the preliminary concepts required to understand our approach followed by the details of our approach.

\subsection{Background: Flow Map}
Flow map is an important mathematical tool that is utilized by numerous flow visualization techniques. To understand the concept of flow map let us consider a time-dependent flow field $f : \mathbb{R}^n \times \mathbb{R} \rightarrow \mathbb{R}^n$, where $f(\mathbf{x}(t), t)$ describes the vector at the spatial position $\mathbf{x} \in \mathbb{R}^n$ at time $t$. Then, the trajectory of a massless particles advected under the influence of the flow field $f$ is governed by the following ordinary differential equation:
\begin{equation}
\frac{d \mathbf{x}(t)}{d t} = f(\mathbf{x}(t),t) , \quad \mathbf{x}(t_0) = \mathbf{x_0},
\end{equation}
where $\mathbf{x_0}$ represents the initial position of particle at starting time $t_0$. A flow map is used to encode these trajectories. More specifically, a flow map encodes the final position of a massless particle advected under the influence of a flow field for a finite-time span T. Formally, a flow map for the flow field $f(\textbf{x}(t), t)$ is given by:

\begin{equation}
    \phi(\mathbf{x_0}, t_0, \tau): \mathbb{R}^n \times \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}^n = \mathbf{x_0} + \int_{t_0}^{t_0+\tau} f(\mathbf{x}(t),t) dt,  
\end{equation}

where $\phi$ encodes the position of a particle seeded at spatial position $\mathbf{x_0}$ and time $t_0$ after being advected under the flow for a duration of $\tau$. In practice, flow maps are computed using numerical integration scheme making it a computationally expensive process. Thus, motivating the need for an alternative representations of flow map that can eliminate the need for the expensive numerical integration process.

\subsubsection{Flow Map Properties}
\label{sec:flowmap_properties}
In fluid dynamics, there are some basic properties that a flow map satisfies when the integration duration $\tau = 0$. We list these properties below :-
\begin{equation}
    \label{eq:identity}
    \phi(\mathbf{x}, t, 0) = \mathbf{x}
\end{equation}

\begin{equation}
    \label{eq:zero_tau_deriv}
    \frac{\partial \phi(\mathbf{x}, t, 0)}{\partial \tau} = f(\mathbf{x}, t)
\end{equation}
Eq.~\ref{eq:identity} refers to the identity property of flow map, which describes that any particle seeded at spatial position $\mathbf{x}$ and time $t$ must map to the same spatial position $\mathbf{x}$ when the integration duration $\tau = 0$. Additionally, Eq.~\ref{eq:zero_tau_deriv} describes that the partial derivative of the flow map when $\tau = 0$ is equivalent to the vector at the seed position. We argue that both Eq.~\ref{eq:identity} and \ref{eq:zero_tau_deriv} are essential properties of a flow map and any alternative flow map representation should exhibit these properties to be a valid representation.

\subsection{Background: Implicit Neural Representations}
INRs are a class of neural networks that accepts the positional coordinates of a given scalar/vector field as input and predicts the scalar/vector at the input position. In the recent years, INRs have been used in various field as an alternative representation. In this section, we discuss about the two most widely used variants of INRs.

\textbf{SIREN}
These are the class of neural networks that uses sinusoidal activation functions in a multi layer perceptron (MLP) network architecture. More specifically, for a given input position $x \in \mathbb{R}^n$, SIREN performs the following operations:-
\begin{equation}
    z_0(\mathbf{x}) = sin(W_0\mathbf{x}),
\end{equation}
\begin{equation}
    z_i(\mathbf{x}) = sin(W_i z_{i-1}(\mathbf{x})),
\end{equation}
\begin{equation}
    f_\theta(\mathbf{x}) = W_l z_{l-1}
\end{equation}
where, $z_0$ represents the positional encoding, $z_i$ is the output at the $i^{th}$ layer of the $l$ layer MLP and $W_i$ is the weight matrix that comprises of the parameters of the network.

\textbf{Localized Grid-SIREN}
A major drawback of a single MLP INR is its inability to scale to larger dataset. To overcome these challenges, another variant of INRs have been proposed in the literature that utilizes a coarse spatial grid of learnable frequencies to generate the positional encoding. These variants are designed to scale to larger dataset meanwhile significantly improving the training and inference times as compared to their MLP counterparts. More specifically, for a given input $\mathbf{x} \in \mathbb{R}^n$, the positional encoding can be generated by interpolating within the grid using the following operation:-
\begin{equation}
    z_0(\mathbf{x}) = sin(interp(\mathbf{x}, Z)),
\end{equation}
where interp corresponds to an interpolation of the frequencies defined at the locations of Z.

\subsection{Naive Neural Flow Map Representation}
SIRENs/Localized Grid-SIRENs with their powerful representational capabilities can be readily used as an alternative representation for flow maps. However, such naive adaptation would require us to tack-on the integration duration as a coordinate. Consequently, resulting in scalability issues in terms of model size and the amount of training data required because of the necessity to optimize over a $(n+2)$-dimensional space, where $n$ represents the spatial dimension of the flow field. Additionally, the default architectures do not satisfy the basic properties of identity mapping described in Eq.~\ref{eq:identity} by design. Thus, one has to enforce the identity mapping by introducing a regularization term during optimization. However, at best, this results in an identity approximation and not an exact identity mapping. Moreover, the resulting partial derivative of the neural flow map with respect to $\tau$ is also non-trivial. Thus, motivating the need for a better neural flow map architectural design. 

\subsection{Neural Flow Map Representation}
\label{sec:neural_flow_map}
In this section, we give the basic idea behind of our approach and how our proposed network architecture overcomes the short-comings of a naive INR network design. We aim to satisfy the following goals for our network design (1)
exhibit the basic properties of flow maps (c.f~\ref{sec:flowmap_properties}) and (2) allow the model to learn efficiently without requiring tremendous amount of data.

\subsubsection{Satisfying flow map properties}
\label{sec:learning_vf}
\begin{figure}[!t]
  \centering
 	\includegraphics[width=\linewidth]{figures/approach-overview.pdf}
	\caption{TODO}
    \label{fig:approach-overview}
\end{figure}
As previously discussed, treating the integration duration $\tau$ as coordinate is inefficient and makes the learning process more difficult. Thus, proper treatment of the $\tau$ variable is a necessity. To this end, we propose an architecture design that allows to satisfy the identity property by design and at the same time yields a very simple gradient network when $\tau = 0$. Let us consider an arbitrary INR:-
\begin{equation}
    f_{\theta}(\mathbf{x}, t): \mathbb{R}^n \times \mathbb{R} \rightarrow \mathbb{R}^d,
\end{equation}
where $x, t$ are the spatial position and time respectively, $n$ represents the spatial dimension and $d$ represent some latent dimension. Then our proposed architecture uses the following set of operations to satisfy the flow map properties :-
\begin{equation}
\label{eq:pe}
    z_0 = f_{\theta}(\mathbf{x}, t)
\end{equation}
\begin{equation}
\label{eq:residual}
    z_{i+1} = z_{i} + \tau\cdot\psi(W_i z_{i}) 
\end{equation}
\begin{equation}
\label{eq:output}
    \phi_{\theta}(\mathbf{x}, t, \tau) = \mathbf{x} + (W_l z_{l-1}) ,
\end{equation}
where $\psi$ represents an arbitrary activation function which evaluates to 0 when the input is 0 (for example $relu$ activation function). $W_i \in \mathbb{R}^{d \times d}$ for $i < l$ represents the weight matrix of a shallow MLP and $W_l \in \mathbb{R}^{d \times n}$ is the weight matrix for the output layer of the MLP. Clearly, when $\tau = 0$, then the network $\phi_{\theta}(\mathbf{x},t,0) = x$, thereby satisfying the identity property. Additionally,$\frac{\partial\phi_{\theta}(\mathbf{x},t,\tau)}{\partial\tau}$ evaluates to $W_l f_{\theta}(\mathbf{x},t)$ when $\tau = 0$ (Please refer the appendix for the detailed evaluation of the partial derivative). This allows us to learn the vector field parameterized by $f_\theta$, assuming access to ground truth vectors for supervision.

\subsubsection{Flow map self-supervision for efficient learning}
\label{sec:self_supervision}
Learning a flow map representation using flow map samples as supervision requires generation of tremendous amount of ground truth flow map samples. It is unclear how many samples are sufficient for efficient learning and will indeed vary based the on the size and complexity of the dataset. However, optimizing for the vectors is a more feasible option. To this end, we utilize our proposed network architecture which allows us to learn the vector field as a first step using the gradient network $f_\theta$ resulting from $\frac{\partial \phi_\theta}{\partial\tau}$ when $\tau = 0$. What remains next is to learn flow maps when $\tau > 0$. To achieve this, we take advantage of the flow map compositions. To understand flow map composition, let us consider a finite time-span $T \in [t_0,t_n]$. Now, dividing the time-span into 2 discrete intervals give us $[t_0, t_1, t_n]$ such that $t_1-t_0 = t_n-t_{1} = t_s$. We can now define flow map composition as follows:-
\begin{equation}
    \phi(\mathbf{x}, t_0, t_n-t_0) = \phi(\phi(\mathbf{x}, t_0, t_s), t_1, t_s))
\end{equation}
Please Note that, the right hand side of the above equation can be composed using any number of intervals, also, the intervals does not necessarily needs to be equally spaced. We take advantage of this specific composition property of flow maps to learn neural flow maps $\phi_\theta(x, t, \tau)$ for $\tau > 0$ in the form of self-supervision. The basic idea behind this approach is that, we first learn a good approximation of the vector field as a consequence of Eq.~\ref{eq:zero_tau_deriv} using $f_\theta$. We assume that, for small non-zero $\tau$ value we can obtain good approximation for the instantaneous motion as a result of the inductive bias of the neural network. Thereby, we employ the following self-supervised composition loss to learn the flow maps at $\tau > 0$:-
\begin{equation}
\label{eq:composition}
    || \frac{\partial \phi_{\theta}(\mathbf{x}, t, \tau_n)}{\partial \tau} - \frac{\partial\phi_\theta(\phi_\theta(\phi_\theta(x, t, \tau_{n1}), t+ \tau_{n1}, \tau_{n2}), t+ \tau_n, 0)}{\partial \tau}||_2,
\end{equation}
where, $n1 + n2 = n$. Note in practice, multiple numbers of composition steps are taken to get a more accurate flow map prediction. Since, we have only optimized for the instantaneous vectors, it is unreasonable to expect good prediction of flow map at $\tau = \tau_n$, however, by taking multiple composition steps with sufficiently small $\tau$ values we can expect reasonably good approximation for the right hand side of the equation with increasing optimization steps.


\section{Network Architecture}
\begin{figure*}[!t]
  \centering
 	\includegraphics[width=\linewidth]{figures/illustrative_network.pdf}
	\caption{TODO}
    \label{fig:illustrative_network}
\end{figure*}
In this section, we describe network architecture in details and provide justification for the choices we made.

Our overall architectural design follows the set of operations described in Sec.~\ref{sec:neural_flow_map}. More specifically, we use the localized Grid-SIREN similar to the one's used in previous works ~\cite{} as our choice of $f_\theta$. However, we want to learn the flow maps in a two-stage process (1) learning the vector field at $\tau=0$ as described in Sec.~\ref{sec:learning_vf} and (2) learning the flow maps for $\tau>0$ as described in Sec.~\ref{sec:self_supervision}. Thus, having the same set of learnable frequencies in the form of a single feature grid responsible for both learning the vector field as well as flow maps for $\tau > 0$ can possibly impact the optimization process negatively. To this end, we propose a decoupled split architecture wherein $f_\theta$ is designed to be comprised of two separate components, enabling us to learn the vector field and the flow maps when $\tau > 0$ without any entanglement. 

\textbf{Decoupled Split Architecture} Let us denote the two decoupled components of $f_\theta(\mathbf{x}, t, \tau)$ as $g_\theta(\mathbf{x}, t, \tau)$ and $h_\theta(\mathbf{x}, t, \tau)$ respectively. We use the localized feature grids as positional encoding for both $g_\theta$ and $h_\theta$, whose combined size depends on the input dimensionality $d_i$ of the time-varying vector field. For examples, for $d_i = 3$ i.e. a 2-d time-varying vector field the combined size of the feature grids amounts to $Z \subset \mathbb{R}^{D \times w \times h \times t}$, where $D$ denotes the feature dimensions (the number of learnable frequencies), $w, h,$ and $t$ denotes the width, height and temporal dimension of the discretized vector field. We split the total size of $Z$ into two equal parts resulting in two feature grids $Z_g$ and $Z_h$. We then form $g_\theta$ using $Z_g$ as the positional encoding, followed by a linear layer mapping and an non-linear activation to get a latent vector $l_g$. We then scale the latent vector by the input $\tau$ value multiplied by a learnable scalar $\tau_{pe}$. To summarize we obtain the latent vector $l_g$ as output of the first component $g_\theta$ using the following operations:-
\begin{equation}
    l_g = (\tau * \tau_{pe})\cdot \psi(W_0\ interp(\mathbf{x}, t, Z_g)),
\end{equation}
where, $\psi$ represents non-linear activation function. In practice, we use swish activation function unless otherwise specified.
We obtain another latent vector $l_h$ as the output of $h_\theta$ which corresponds to the second component of $f_\theta$. We form $h_\theta$ by using $Z_h$ as positional encoding, followed by a linear layer mapping. The latent vector $l_h$ is obtained through the following operation:
\begin{equation}
    l_h = \hat{W}\ interp(\mathbf{x}, t, Z_h)
\end{equation}
We then form $f_\theta$ as a combination of $g_\theta$ and $h_\theta$ using the following operation:-
\begin{equation}
    f_\theta = \psi(W_f\ \tanh(l_g) * l_h)
\end{equation}
After $f_\theta$ is obtained we use a shallow 3-layer MLP and follow the same set of operations described in Eq.~\ref{eq:residual} and~\ref{eq:output}. Using out decoupled split architecture we can efficiently learn the vector field in the first stage of optimization by utilizing $g_\theta$ and during the second stage learn the flow maps for $\tau > 0$ using $h_\theta$.

\section{Optimization}
\begin{figure}
  \centering
 	\includegraphics[width=0.7\linewidth]{figures/optimization.pdf}
	\caption{TODO}
    \label{fig:optim}
\end{figure}
We employ a two-stage optimization process to efficiently learn the neural flow map representation without requiring ground truth flow map samples as supervision. In the first stage of optimization we aim to optimize for the vector field by minimizing the following loss function:-
\begin{equation}
    \frac{1}{|\mathbb{T}|} \sum || \frac{\partial \phi_\theta(\mathbf{x}, t, 0)}{\partial \tau} - f(\mathbf{x},t) ||_2
\end{equation}
where, $\mathbb{T}$ represents the total number of ground truth vectors. Please note that, $\frac{\partial \phi_\theta(\mathbf{x}, t, 0)}{\partial \tau}$ corresponds to following set of operation in our architecture:-

\begin{equation}
    z_0 = \tau_{pe}\cdot \psi(W_0\ interp(\mathbf{x}, t, Z_g))    
\end{equation}

\begin{equation}
    z_{out} = z_0\ W_f^T    
\end{equation}
As a consequence of this optimization process, the feature grid $Z_g$ where the bulk of the network parameters for $f_\theta$ exists captures the instantaneous vectors. After optmizing for the vector field, in the second stage of optimization we aim to learn the flow maps for $\tau > 0$.
Assuming, we were able to learn a good approximation of the vector field, we utilize the self-supervision loss as described in Sec.~\ref{sec:self_supervision} to learn flow maps for $\tau > 0$. Please note that at the beginning of the second stage of optimization, we have good approximation of instantaneous vectors, however, for larger $\tau$ value the network is expected to be inaccurate. Thus, composing flow maps by taking multiple steps with sufficiently small $\tau$ values can provide a good approximation and act as a supervision for the single step flow map (left hand side of Eq.~\ref{eq:composition}). However, this limits us from optimizing for extremely large $\tau$ values. Thus, for the second stage of optimization we choose a $\tau_{max}$ and sample $\tau_m < \tau_{max}$ uniformly at random for optimization. Additionally, during the second stage of training we use a different optimizer for $f_\theta$ and set a low learning rate to avoid significant changes to the already learned vector field and only fine-tune the parameters.
