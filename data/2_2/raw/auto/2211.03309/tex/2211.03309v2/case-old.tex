%\input{figs/case-core}
%\input{figs/case-network}
%\input{figs/case-mem}
%\input{figs/case-core-net}


\section{Case Studies}
Here we will show how we use our model to project the impact of logic, network and memory technology scaling on overall performance. 
\name is a very expressive model that not only can be used to predict performance but also to predict hardware counters (number of accesses to different levels of memory hierarchy for different operations, number of bytes transferred for each operation under each parallelism strategy, etc.) which we use to understand the observed pattern.  

\subsection{Methodology}
For all the experiments below we are assuming a system with 512 GPU nodes which can be used to parallelize a model in $35$ different ways.
For example, $64$ GPUs can be used to exploit 64-way data-parallel (\texttt{R1\_C1\_D64\_L1}) strategy, 
or can be used to get 8-way data-parallel and for each data shard use 8-way model parallel strategy (\texttt{CR8\_D8\_L1}).

One thing to note here is that not all parallelism strategies are feasible under a given design configuration (memory capacity limit). 
Different parallelism strategies have different memory capacity requirements per data shard per model shards.
Parallelism strategies with no bars refers to infeasible cases (Figure~\ref{fig:case-core}, \ref{fig:case-net} and \ref{fig:case:case-mem}).

\subsection{Logic Technology Scaling}
As technology node scales down, the number of cores on chip grows which results in improved computational throughput. 
Similarly, cache bandwidth and cache capacity at different levels of memory hierarchy improve. 
Overall, the expectation is to see a large performance improvement from one technology node to the next. 
Figure~\ref{fig:case-core} shows the impact of technology scaling on overall performance across different parallelism strategies.
Contrary to the common belief, the technology scaling shows a negligible improvement in performance.
We use \name to find the sources of bottleneck. Our analysis suggests that execution time is mostly dominated by the data transfer time for the reduction operation which is abundant in parallelism strategies which does not scale with technology nodes. Therefore, we conclude technology scaling would not be sufficient if network technology does not scale hand-in-hand. Indeed, without network technology scaling, it seems techniques as simple as changing parallelism strategies would have more prominent impact on performance than technology scaling. Next we show how improving network technology along with technology scaling can improve performance.

\subsection{Network Scaling}
Next, we study the overall impact of network technology scaling on performance and then see how technology scaling would be different in presence of improved interconnection network.
As shown in Figure~\ref{fig:case-net} network bandwidth improvement has a significant impact on overall performance. For better visibility, we are only showing the feasible parallelism strategies. Note here that the network values refer to the peer-2-peer bandwidth between GPUs and not the aggregate bandwidth. 
Today, we are at 50 GB/s between two GPUs given technologies like NVlink. However, we can see that there is room for performance improvement if we can scale network bandwidth to 400 GB/s. Beyond that, we would see a diminishing return because . 

Figure~\ref{fig:case-core-net} shows the impact of logic technology scaling on performance if interconnection bandwidth would have been 30$\times$ faster than today's SOTA technology (1395 GB/s vs. 48 GB/s).
Unlike Figure~\ref{fig:case-core}, we see a clear trend of performance improvement while improving technology nodes. With interconnection bandwidth bottleneck alleviated, the performance is more or less bottlenecked on core and memory throughput and so we see clear improvement transitioning from technology node 14 to 10 to even 5 nm, but beyond that we see a diminishing return as memory bandwidth would become a new bottleneck.


%case_study/perf_model/energy_per_flop/
\subsection{Memory Technology Scaling} As memory technology improves, we will have more memory capacity and more memory bandwidth at hand. However, our analysis suggests that memory scaling will not improve performance per parallelism strategy but still results in dramatic performance improvements ($8 \times$) as it unlocks new parallelism strategies that were dimmed infeasible before due to memory capacity constraints. Within the realm of feasible parallelism strategies, improving memory bandwidth does not result in any performance improvement as the computation part of the problem is bounded on L2 cache bandwidth. 

%case_study/perf_model/energy_per_membit/


\subsection{Architecture Design Space Exploration}
So far, we studied cases where the hardware configuration is assumed fixed and we explore the impact of technology, network and bandwidth scaling on overall performance. In this subsection, we assume the underlying hardware architecture can also change as we explore the impact of other parameters.
There are many studies that can be conducted having a design space exploration framework. Due to space limitation, we showcase only one such use case.
%
Specifically, we are trying to answer the question of whether or not technology node scaling could be successful assuming we re-design the hardware architecture as we transition to a different technology node -- as is customary. 
Recall from Figure~\ref{fig:core} that technology scaling is not effective if the underlying hardware is fixed.  
We 