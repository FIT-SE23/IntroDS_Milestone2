@article{KunleScaledML,
      title={Accelerating Software 2.0}, 
      author={Kunle Olukotun},
      journal={ScaledML},
      year={2020}
}

@misc{aws,
      title = "{Achieve 12x higher throughput and lowest latency for PyTorch Natural Language Processing applications out-of-the-box on AWS Inferentia}",
      author={Amazon AWS Inferentia},
      howpublished=
      {\url{https://tinyurl.com/3mbuetmr}},
      year = "{(accessed Sep 10, 2021)}"}  
      
@misc{openai,
      title = "{AI and Compute}",
      author={OpenAI},
      howpublished=
      {\url{https://openai.com/blog/ai-and-compute/}},
      }  
      
@article{jia2018beyond,
  title={Beyond data and model parallelism for deep neural networks},
  author={Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  journal={arXiv preprint arXiv:1807.05358},
  year={2018}
}

@INPROCEEDINGS{timeloop,
  author={Parashar, Angshuman and Raina, Priyanka and Shao, Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A. and Mukkara, Anurag and Venkatesan, Rangharajan and Khailany, Brucek and Keckler, Stephen W. and Emer, Joel},
  booktitle={2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={Timeloop: A Systematic Approach to DNN Accelerator Evaluation}, 
  year={2019},
  volume={},
  number={},
  pages={304-315},
  doi={10.1109/ISPASS.2019.00042}}
  
 @ARTICLE{eyeriss,
  author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S. and Sze, Vivienne},
  journal={IEEE Journal of Solid-State Circuits}, 
  title={Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks}, 
  year={2017},
  volume={52},
  number={1},
  pages={127-138},
  doi={10.1109/JSSC.2016.2616357}}
  
  
  @inproceedings{hestness2019beyond,
  title={Beyond human-level accuracy: Computational challenges in deep learning},
  author={Hestness, Joel and Ardalani, Newsha and Diamos, Gregory},
  booktitle={Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
  pages={1--14},
  year={2019}
}

@INPROCEEDINGS{mcm-gpu,
  author={Arunkumar, Akhil and Bolotin, Evgeny and Cho, Benjamin and Milic, Ugljesa and Ebrahimi, Eiman and Villa, Oreste and Jaleel, Aamer and Wu, Carole-Jean and Nellans, David},
  booktitle={2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={MCM-GPU: Multi-chip-module GPUs for continued performance scalability}, 
  year={2017},
  volume={},
  number={},
  pages={320-332},
  doi={10.1145/3079856.3080231}}
  
  @INPROCEEDINGS{ws-gpu,
  author={Pal, Saptadeep and Petrisko, Daniel and Tomei, Matthew and Gupta, Puneet and Iyer, Subramanian S. and Kumar, Rakesh},
  booktitle={2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={Architecting Waferscale Processors - A GPU Case Study}, 
  year={2019},
  pages={250-263},
  doi={10.1109/HPCA.2019.00042}
  }
  
  @misc{tesla-dojo,
  title = {{Tesla Dojo}},
  howpublished = {\url{https://www.nextplatform.com/2022/08/23/inside-teslas-innovative-and-homegrown-dojo-ai-supercomputer/}},
  note = {Accessed: 2022-10-15}
  }

  @misc{Wikichip_technode,
  title = {{Wikichip: Technology Node}},
  howpublished = {\url{https://en.wikichip.org/wiki/\\technology\_node}},
  note = {Accessed: 2021-10-15}
  }
  
  @misc{HBM3,
  title = {{HBM3: Big Impact On Chip Design}},
  howpublished = {\url{https://semiengineering.com/hbm3s-impact-on-chip-design/}},
  note = {Accessed: 2021-10-15}
  }
  
  @misc{dlcost,
  title = {{Deep Learning's Diminishing Returns}},
  howpublished = {\url{https://spectrum.ieee.org/deep-learning-computational-cost}},
  note = {Accessed: 2021-10-15}
  }
  
  @article{stillmaker,
title = {{Scaling equations for the accurate prediction of CMOS device performance from 180nm to 7nm}},
journal = {Integration},
volume = {58},
pages = {74-81},
year = {2017},
issn = {0167-9260},
doi = {https://doi.org/10.1016/j.vlsi.2017.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167926017300755},
author = {Aaron Stillmaker and Bevan Baas},
keywords = {Transistor scaling, Deep submicron performance, VLSI design, CMOS device},
abstract = {Classical scaling equations which estimate parameters such as circuit delay and energy per operation across technology generations have been extremely useful for predicting performance metrics as well as for comparing designs across fabrication technologies. Unfortunately in the CMOS deep-submicron era, the classical scaling equations are becoming increasingly less accurate and new practical scaling methods are needed. We curve fit second and third-order polynomials to circuit delay, energy, and power dissipation results based on HSpice simulations utilizing the Predictive Technology Model (PTM) and International Technology Roadmap for Semiconductors (ITRS) models. While the classical scaling equations give differences as much as 83×from the predictions of PTM and ITRS models, our predictive polynomial models with table-based coefficients yield a coefficient of determination, or R2, value of greater than 0.95.}
}

@INPROCEEDINGS{flexflow,
  author={Lu, Wenyan and Yan, Guihai and Li, Jiajun and Gong, Shijun and Han, Yinhe and Li, Xiaowei},
  booktitle={2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks}, 
  year={2017},
  volume={},
  number={},
  pages={553-564},
  doi={10.1109/HPCA.2017.29}}
  
  @ARTICLE{maestro,
  author={Kwon, Hyoukjun and Chatarasi, Prasanth and Sarkar, Vivek and Krishna, Tushar and Pellauer, Michael and Parashar, Angshuman},
  journal={IEEE Micro}, 
  title={MAESTRO: A Data-Centric Approach to Understand Reuse, Performance, and Hardware Cost of DNN Mappings}, 
  year={2020},
  volume={40},
  number={3},
  pages={20-29},
  doi={10.1109/MM.2020.2985963}}

@inproceedings{mindmapping,
author = {Hegde, Kartik and Tsai, Po-An and Huang, Sitao and Chandra, Vikas and Parashar, Angshuman and Fletcher, Christopher W.},
title = {Mind Mappings: Enabling Efficient Algorithm-Accelerator Mapping Space Search},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446762},
doi = {10.1145/3445814.3446762},
abstract = {Modern day computing increasingly relies on specialization to satiate growing performance
and efficiency requirements. A core challenge in designing such specialized hardware
architectures is how to perform mapping space search, i.e., search for an optimal
mapping from algorithm to hardware. Prior work shows that choosing an inefficient
mapping can lead to multiplicative-factor efficiency overheads. Additionally, the
search space is not only large but also non-convex and non-smooth, precluding advanced
search techniques. As a result, previous works are forced to implement mapping space
search using expert choices or sub-optimal search heuristics. This work proposes Mind
Mappings, a novel gradient-based search method for algorithm-accelerator mapping space
search. The key idea is to derive a smooth, differentiable approximation to the otherwise
non-smooth, non-convex search space. With a smooth, differentiable approximation,
we can leverage efficient gradient-based search algorithms to find high-quality mappings.
We extensively compare Mind Mappings to black-box optimization schemes used in prior
work. When tasked to find mappings for two important workloads (CNN and MTTKRP), Mind
Mapping finds mappings that achieve an average 1.40\texttimes{}, 1.76\texttimes{}, and 1.29\texttimes{}&nbsp;(when run for
a fixed number of steps) and 3.16\texttimes{}, 4.19\texttimes{}, and 2.90\texttimes{}&nbsp;(when run for a fixed amount
of time) better energy-delay product (EDP) relative to Simulated Annealing, Genetic
Algorithms and Reinforcement Learning, respectively. Meanwhile, Mind Mappings returns
mappings with only 5.32\texttimes{} higher EDP than a possibly unachievable theoretical lower-bound,
indicating proximity to the global optima.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {943–958},
numpages = {16},
keywords = {gradient-based search, mapping space search, programmable domain-specific accelerators},
location = {Virtual, USA},
series = {ASPLOS 2021}
}


@article{roofline,
author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
title = {Roofline: An Insightful Visual Performance Model for Multicore Architectures},
year = {2009},
issue_date = {April 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/1498765.1498785},
doi = {10.1145/1498765.1498785},
abstract = {The Roofline model offers insight on how to improve the performance of software and
hardware.},
journal = {Commun. ACM},
month = apr,
pages = {65–76},
numpages = {12}
}

%%%MLSys Review paper references to include
@inproceedings{zhu2020daydream,
  title={Daydream: Accurately Estimating the Efficacy of Optimizations for $\{$DNN$\}$ Training},
  author={Zhu, Hongyu and Phanishayee, Amar and Pekhimenko, Gennady},
  booktitle={2020 USENIX Annual Technical Conference (USENIX ATC 20)},
  pages={337--352},
  year={2020}
}

@inproceedings{geoffrey2021habitat,
  title={Habitat: A $\{$Runtime-Based$\}$ Computational Performance Predictor for Deep Neural Network Training},
  author={Geoffrey, X Yu and Gao, Yubo and Golikov, Pavel and Pekhimenko, Gennady},
  booktitle={2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages={503--521},
  year={2021}
}

@inproceedings{C111,
address = {New York, NY, USA},
author = {Pal, Saptadeep and Gupta, Puneet},
booktitle = {{System-Level Interconnect - Problems and Pathfinding Workshop}},
keywords = {wsi},
location = {San Diego, California},
month = {November},
numpages = {8},
publisher = {ACM},
series = {SLIP '20},
title = {{Pathfinding for 2.5D Interconnect Technologies}},
year = {2020}
}


@inproceedings{google-optical-1,
author = {Poutievski, Leon and Mashayekhi, Omid and Ong, Joon and Singh, Arjun and Tariq, Mukarram and Wang, Rui and Zhang, Jianan and Beauregard, Virginia and Conner, Patrick and Gribble, Steve and Kapoor, Rishi and Kratzer, Stephen and Li, Nanfang and Liu, Hong and Nagaraj, Karthik and Ornstein, Jason and Sawhney, Samir and Urata, Ryohei and Vicisano, Lorenzo and Yasumura, Kevin and Zhang, Shidong and Zhou, Junlan and Vahdat, Amin},
title = {Jupiter Evolving: Transforming Google's Datacenter Network via Optical Circuit Switches and Software-Defined Networking},
year = {2022},
isbn = {9781450394208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544216.3544265},
doi = {10.1145/3544216.3544265},
abstract = {We present a decade of evolution and production experience with Jupiter datacenter network fabrics. In this period Jupiter has delivered 5x higher speed and capacity, 30% reduction in capex, 41% reduction in power, incremental deployment and technology refresh all while serving live production traffic. A key enabler for these improvements is evolving Jupiter from a Clos to a direct-connect topology among the machine aggregation blocks. Critical architectural changes for this include: A datacenter interconnection layer employing Micro-Electro-Mechanical Systems (MEMS) based Optical Circuit Switches (OCSes) to enable dynamic topology reconfiguration, centralized Software-Defined Networking (SDN) control for traffic engineering, and automated network operations for incremental capacity delivery and topology engineering. We show that the combination of traffic and topology engineering on direct-connect fabrics achieves similar throughput as Clos fabrics for our production traffic patterns. We also optimize for path lengths: 60% of the traffic takes direct path from source to destination aggregation blocks, while the remaining transits one additional block, achieving an average block-level path length of 1.4 in our fleet today. OCS also achieves 3x faster fabric reconfiguration compared to pre-evolution Clos fabrics that used a patch panel based interconnect.},
booktitle = {Proceedings of the ACM SIGCOMM 2022 Conference},
pages = {66–85},
numpages = {20},
keywords = {datacenter network, traffic engineering, topology engineering, software-defined networking, optical circuit switches},
location = {Amsterdam, Netherlands},
series = {SIGCOMM '22}
}


@misc{google-optical-2,
  doi = {10.48550/ARXIV.2208.10041},
  
  url = {https://arxiv.org/abs/2208.10041},
  
  author = {Urata, Ryohei and Liu, Hong and Yasumura, Kevin and Mao, Erji and Berger, Jill and Zhou, Xiang and Lam, Cedric and Bannon, Roy and Hutchinson, Darren and Nelson, Daniel and Poutievski, Leon and Singh, Arjun and Ong, Joon and Vahdat, Amin},
  
  keywords = {Networking and Internet Architecture (cs.NI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Mission Apollo: Landing Optical Circuit Switching at Datacenter Scale},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{NVLINK,
      author = "{NVIDIA}",
      title  = "{NVLink and NVSwitch}",
      howpublished = {\url{https://www.nvidia.com/en-us/data-center/nvlink/}},
       urldate   = "(accessed Oct. 31, 2022)",
       year = {2022},
    }
     
     
@INPROCEEDINGS{astra-sim,  author={Rashidi, Saeed and Sridharan, Srinivas and Srinivasan, Sudarshan and Krishna, Tushar},  booktitle={2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},   title={ASTRA-SIM: Enabling SW/HW Co-Design Exploration for Distributed DL Training Platforms},   year={2020},  volume={},  number={},  pages={81-92},  doi={10.1109/ISPASS48437.2020.00018}}

