\section{Micro-Architecture Generator Engine}
The micro-architecture generator engine, AGE,  takes three sets of inputs: (1) A technology components library, where the characteristics of each component such as cores, different types of memories, network interfaces, etc. are defined, (2) Architecture template, where the overall high-level chip and system organization (such as compute and memory hierarchies) is provided, (3) Hardware resource allocation, where area, power, and chip perimeter budgets are provided for the different components of the system. Using this information, the AGE generates the final micro-architecture parameters (such as overall compute throughput, memory bandwidths at different memory levels, network bandwidth) as shown in Figure~\ref{fig:overview}. 

%In this section, we will first provide details of each of the three inputs and then describe how the uArchitecture generator engine uses these inputs to generate the final uArchitectural parameters of the system.

\subsection{Technology Components Library}

A system is generally composed of many primitive components or building blocks such as the compute units, SRAM banks, DRAM, interconnect network components (on-chip and off-chip), etc. A library of these components and their associated technology parameters are provided as input to the tool through a $tech\_config$ YAML file. We classify these components into three primary categories: compute, memory and network. 
%Next we describe the different hardware attributes we assign to the components in each category.
%\vspace{-0.1in}
\subsubsection{Compute}  
Attributes for the minimal compute components such as matrix-multiplier units, vector-matrix multiply units, or a dataflow architecture unit like systolic array are specified under this category. When a compute component is added to the library, the compute attributes listed in Table~\ref{tab:component-parameter} will have to be defined for that component. The tool user can add any type of compute component in the library ranging from a simple scalar unit to a complex unit comprising of a bundle of systolic arrays and capture the micro-architectural characteristics in the final architecture template file.
%\vspace{-0.1in}
\subsubsection{Memory}
The memory components in a system can be built out of different technologies (e.g., SRAM, DRAM, MRAM, RRAM, 3D-XPoint). Also, these memory components can be used in two ways: on-chip memory and off-chip memory. A library of fine-grained memory components can be created and stored under this category which is utilized to construct different levels of the memory hierarchy. The characteristics of the on-chip components are described at the granularity of a bank because the smallest on-chip memory unit available to a system designer is usually a memory bank. The parameters of a memory bank such as capacity, bit area, periphery overhead etc. are taken as inputs. 
On the other hand, we model the off-chip memory components such as DRAM, or 3D-XPoint at device level granularity, e.g., an HBM stack. This is because the off-chip components are usually obtained at a device level granularity. For off-chip memories, other parameters such as memory controller area, I/O bus width per device, etc. need to be defined. This information is then used to precisely model the capacity and throughput of different levels of the memory hierarchy under the given area and power constraints.
%\vspace{-0.1in}
\subsubsection{Network}
The inter-chip network component is  either intra-node or inter-node communication link. In the case of a multi-chip module (MCM) where multiple compute dies and memory devices are integrated on a 2.5D integration substrate within the same package, the inter-die communication is done using high density and energy-efficient links on the 2.5D substrate. These links are considered as intra-node links. On the other hand, the off-package communication links between nodes are considered as inter-node links. The attributes that need to be defined for inter and intra-die communication network components are provided in Table~\ref{tab:component-parameter}. In case of a waferscale system, the entire wafer could be considered as a single node.


\begin{table}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|l|l|}
\hline
\multirow{4}{*}{\textbf{Compute}} & Technology Node & Nominal Area \\ \cline{2-3} 
 & Nominal Voltage & Threshold Voltage \\ \cline{2-3} 
 & Nominal Frequency & Minimum Voltage \\ \cline{2-3} 
 & Nominal OP rate & Maximum Voltage \\ \hline
\multirow{4}{*}{\textbf{On-chip Memory}} & Technology & Latency \\ \cline{2-3} 
 & Dynamic energy per bit & Static energy per bit \\ \cline{2-3} 
 & Area per bit and total area overhead & Bank Capacity \\ \cline{2-3} 
 & Controller area overhead per bank & Controller power overhead per bank \\ \hline
\multirow{6}{*}{\textbf{Off-chip Memory}} & Technology & Number of links per device \\ \cline{2-3} 
 & Dynamic energy per bit & Nominal Voltage \\ \cline{2-3} 
 & Static power per bit & Nominal Frequency \\ \cline{2-3} 
 & Device Capacity & Minimum Voltage \\ \cline{2-3} 
 & Device Area & Maximum Voltage \\ \cline{2-3} 
 & Memory Controller and I/O Area & Access Latency \\ \hline
\multirow{4}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Network (intra-node \\      and inter-node)\end{tabular}}} & Nominal Voltage & Number of links per mm \\ \cline{2-3} 
 & Nominal Frequency & Threshold Voltage \\ \cline{2-3} 
 & Nominal Energy per Link & Minimum Voltage \\ \cline{2-3} 
 & Nominal Area per Link & Link Latency \\ \hline
\end{tabular}%
}
\caption{Different technology components.} \label{tab:component-parameter}
%\vspace{-0.2cm}
\end{table}



\ignore{
\begin{table}[]
\tiny
\centering
\caption{Different architectural components and their associated technology parameters.}
\label{tab:component-parameter}
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|l|}
\hline
\multirow{9}{*}{\textbf{Compute}} & Technology Node \\ \cline{2-2} 
 & Nominal Voltage \\ \cline{2-2} 
 & Nominal Frequency \\ \cline{2-2} 
 & Nominal Flop rate \\ \cline{2-2} 
 & Nominal Power \\ \cline{2-2} 
 & Nominal Area \\ \cline{2-2} 
 & Threshold Voltage \\ \cline{2-2} 
 & Minimum Voltage \\ \cline{2-2} 
 & Maximum Voltage \\ \hline
\multirow{9}{*}{\textbf{On-chip Memory}} & Technology \\ \cline{2-2} 
 & Dynamic energy per bit \\ \cline{2-2} 
 & Static power per bit \\ \cline{2-2} 
 & Area per bit \\ \cline{2-2} 
 & Bank capacity \\ \cline{2-2} 
 & Area overhead \\ \cline{2-2} 
 & Controller area overhead per bank \\ \cline{2-2} 
 & Controller power overhead per bank \\ \cline{2-2} 
 & Latency \\ \hline
\multirow{12}{*}{\textbf{Off-chip Memory}} & Technology \\ \cline{2-2} 
 & Dynamic energy per bit \\ \cline{2-2} 
 & Static power per bit \\ \cline{2-2} 
 & Device Capacity \\ \cline{2-2} 
 & Device Area \\ \cline{2-2} 
 & Memory Controller and I/O Area \\ \cline{2-2} 
 & Number of links per device \\ \cline{2-2} 
 & Nominal Voltage \\ \cline{2-2} 
 & Nominal Frequency \\ \cline{2-2} 
 & Minimum Voltage \\ \cline{2-2} 
 & Maximum Voltage \\ \cline{2-2} 
 & Access Latency \\ \hline
\multirow{8}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Network (intra-node \\      and inter-node)\end{tabular}}} & Nominal Voltage \\ \cline{2-2} 
 & Nominal Frequency \\ \cline{2-2} 
 & Nominal Energy per Link \\ \cline{2-2} 
 & Nominal Area per Link \\ \cline{2-2} 
 & Number of links per mm \\ \cline{2-2} 
 & Threshold Voltage \\ \cline{2-2} 
 & Minimum Voltage \\ \cline{2-2} 
 & Link Latency \\ \hline
\end{tabular}%
}
\end{table}






\begin{table*}[h!]
    \centering
   \begin{tabular}{|>{\centering\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{14cm}|}
         \hline
         \multicolumn{2}{|c|}{\textbf{Compute}} \\
         \hline
         \textbf{Technology Node} & The technology node in which the core is implemented \\
         \hline
         \textbf{Nominal Voltage} & The nominal voltage of operation corresponding to the technology node \\
         \hline
         \textbf{Nominal Frequency} & The frequency at which the core can be run when operating at nominal voltage. This number would usually be obtained from a synthesis tool or silicon implementation \\
         \hline
         \textbf{Nominal OP rate} & Number of operation the compute element performs per cycle. E.g., V100 tensor cores performs 128 operations per cycle \\
         \hline
         \textbf{Nominal Power} & The power of the core when run at nominal voltage and frequency \\
         \hline
         \textbf{Nominal Area} & The area of the core implemented in the technology node at the nominal voltage and frequency \\
         \hline
         \textbf{Threshold Voltage} & Transistor threshold voltage ($V_{th}$) for the core. This is used in the voltage and frequency scaling models \\
         \hline
         \textbf{Minimum Voltage} & The minimum voltage at which the core can operate reliably \\
         \hline
         \textbf{Maximum Voltage} & The maximum voltage at which the core can operate reliably \\
         \hline
         \multicolumn{2}{|c|}{\textbf{On-chip Memory}} \\
         \hline
         \textbf{Technology} & The technology of the memory component \\
         \hline
         \textbf{Dynamic energy per bit} & The average energy per bit of access to the memory component \\
         \hline
         \textbf{Static power per bit} & The leakage power per bit \\
         \hline
         \textbf{Area per bit} & The area of a single bit of on-chip memories \\
         \hline
         \textbf{Bank capacity} & The number of bits in a bank of this memory component \\
         \hline
         \textbf{Area overhead} & The area overhead of the peripheral circuitry per bank of this memory component \\
         \hline
         \textbf{Controller area overhead per bank} & The area of the control circuitry per bank. We assume that multiple banks share a control unit (bank select, crossbar interconnect, arbiters etc.) and the overhead grows with the number of banks. For our experiments, we obtain number from ORION2.0~\cite{} \\
         \hline
         \textbf{Controller power overhead per bank} & Similar to the area overhead, we account for the controller's power overhead as well \\
         \hline
         \textbf{Latency} & Average time to access for reads and writes. In our experiments, we calculated this number from CACTI6.0~\cite{} \\
         \hline
         \multicolumn{2}{|c|}{\textbf{Off-chip Memory}} \\
         \hline
         \textbf{Dynamic energy per bit} & The average energy per bit of access of an off-chip memory component is defined here. This includes accessing the memory array and energy to transfer the data, address and commands between the processor and the memory device \\
         \hline
         \textbf{Static power per bit} &  The static power consumed per bit by the memory component. For DRAMs, this is often determined by the average refresh power \\
         \hline
         \textbf{Device Capacity} & The number of bits in the entire memory component. E.g., for high bandwidth memory (HBM) based systems, this is the capacity per 3D-stacked HBM device \\
         \hline
         \textbf{Device Area} & The area of the device that it consumes on the integration substrate such as interposer~\cite{} or waferscale substrate~\cite{}. Therefore, only tightly integrated memory solutions such as HBM's area need to be accounted for in this case \\
         \hline
         \textbf{Memory Controller and I/O Area} & The area overhead of the memory controller and the I/O PHY circuitry on the compute chip per device \\ 
         \hline
         \textbf{Number of links per device} & The width of the data bus connecting the compute die and the memory device \\
         \hline
         \textbf{Nominal Voltage} & The nominal voltage at which the device is operated \\
         \hline
         \textbf{Nominal Frequency} & The frequency of the data bus \\
         \hline
         \textbf{Minimum Voltage} & The minimum voltage at which the device can operate reliably \\
         \hline
         \textbf{Maximum Voltage} & The maximum voltage at which the device can operate reliably \\
         \hline
         \textbf{Access Latency} & Average latency of accessing the memory device \\
         \hline
         \multicolumn{2}{|c|}{\textbf{Network (intra-node and inter-node)}} \\
         \hline
         \textbf{Nominal Voltage} & The nominal voltage at which the inter-chip interconnects operate \\
         \hline
         \textbf{Nominal Frequency} & The nominal frequency at which the inter-chip interconnects operate \\
         \textbf{Nominal Energy per Link} & The energy per bit of the interconnect when operating at nominal voltage and frequency \\
         \hline
         \textbf{Nominal Area per Link} & The amount of area the I/O transceiver circuitry consumes per I/O link \\
         \hline
         \textbf{Number of links per mm} & The number of inter-chip links that the interconnect substrate can accommodate per mm of die edge \\
         \hline
         \textbf{Threshold Voltage} & Transistor threshold voltage ($V_{th}$) for the I/O transceiver circuitry. This is used in the voltage and frequency scaling models \\
         \hline
         \textbf{Minimum Voltage} & The minimum voltage at which the transceiver circuitry can operate reliably \\
         \hline
         \textbf{Link Latency} & The latency to communicate between two chips (includes the router latency)\\
         \hline
         
         
    \end{tabular}
    \caption{Caption}
    \label{tab:my_label}
\end{table*}

}


\input{figs/hardware-system-overview}
\subsection{Architecture Template}
Once all  system components are instantiated from the technology library, the next step is to hierarchically organize one or multiple components from each category to construct the overall system.  Distributed machine learning training is done on scale-out multi-node system, as shown in Figure~\ref{fig:hardware-system-overview}. Such a system consists of multiple individually packaged nodes which communicate through off-package interconnects (such as NVLink, Infiniband etc.) that form the inter-node network. Inside each package, there can be multiple different accelerator nodes connected using an intra-node network. Each accelerator within the package typically consists of one accelerator die that is connected to its own off-chip main memory components (such as HBM, as shown in the figure). Each accelerator die itself can be composed of smaller compute units.

\name  provides a rich template that can be used to specify the overall architectural organization of such an accelerator system. 
%The template is used to specify the high-level micro-architectural organization of the compute units, organization of the memory hierarchy, inter-chip network topology (both intra- and inter- node) and the system hierarchy. 
Next we describe in detail how the template is organized and how different system configurations can be achieved using this template.

\subsubsection{\textbf{Compute unit}}\label{subsec:mcu} As shown in the accelerator die architecture in Figure~\ref{fig:hardware-system-overview}, compute units are often organized in hierarchies. E.g., in an NVIDIA GPU, multiple tensor cores are bundled in a streaming multi-processor (SM) and the SM as a whole interacts with the cache hierarchy. In \name one can express such hierarchy by defining \textit{minimal compute units (MCUs)} and
\textit{MCU bundle}. MCU is the smallest compute unit that we expose to the tool user.  It defines the dataflow model and layout (e.g. MCU can be a systolic array that its height and width are configurable as input) and interacts with the first level of memory hierarchy. Meanwhile, MCU bundle defines the number of MCUs that are bundled together and are exposed to the second level of memory hierarchy. 

In dataflow architectures such as Eyeriss, TPU etc., data can flow directly between different cores. Hence, the tool allows one to define the type of dataflow within an MCU. Currently the performance model supports three types of dataflow: \textit{weight stationary, activation stationary and output stationary}. The tool can also find the best dataflow strategy among the three for any given kernel.% if the dataflow mode is selected as `best'.

Software runtime, scheduling overheads and the architecture of the cores often restrict the maximum compute utilization. For example, the tensor-cores in NVIDIA V100 incurs fill-drain related under-utilization during tensor loading from the registers and therefore achieves a maximum utilization of 85\%. To account for such overheads, a maximum utilization value can be defined which derates the core throughput by that factor. 

\subsubsection{\textbf{Memory Hierarchy and Scope}} The memory hierarchy is defined by initializing multiple memory levels from the highest to the lowest level (i.e., registers to the main memory) as shown in Figure~\ref{fig:hardware-system-overview}. Each level of memory has two attributes: (1) \underline{Memory technology} component from the technology component library which defines the physical attributes of the memory as outlined in Table~\ref{tab:component-parameter}, and (2) \underline{Scope} defines 
the set of components from the next level of memory hierarchy that are accessible from this level of memory hierarchy.
%which micro-architectural components can access that level of memory. 
For example, the `global' scope indicates that the memory level is accessible to all the components.

\subsubsection{\textbf{Network Topology}} In \name, we support two levels of network hierarchy: intra-package and inter-package. For each level, a different topology (e.g. mesh, torus, crossbar) can be defined. 


\subsection{Hardware Resource Allocation}
 %
 Hardware design under a limited area and power budget is a fine art of finding the right balance (breakdown of resources) across different micro-architectural components. 
 %
 The area and power allocation for each micro-architectural component, as well as the perimeter allocation for certain components derive the design and specification of that component. 
 
We define resource (area, power, perimeter) distribution across different components of the compute chip, as input parameters.
%
The input definition also includes the total area and power budgets for the entire compute node. The total perimeter is inferred from area. 
%
The area budget is usually dictated by packaging constraints. For example, if the compute and memory dies are assembled on a 2.5D silicon interposer-based interconnect substrate, the total area of the node will be limited by the maximum size of the interconnect substrate that can be fabricated. 
%
Compare this to a waferscale system which houses an entire node on a wafer where the total area budget can be as large as 70,000 $mm^2$. 
A node's power budget is determined by the cooling infrastructure that extract heat from the node and the power delivery constraints.

We define budget distribution across different components of the compute graph as a percentage breakdown.
As shown in the YAML snippet
in Figure~\ref{fig:area-breakdown}, 
fractions of the total area is distributed across cores, levels of memory hierarchy and network
components. Similarly, the fraction of the compute chip's power and perimeter gets devoted to different hardware components.
\begin{figure}
\begin{minted}[
    breaklines,
    frame=single,
    fontsize=\tiny
  ]{yaml}
area_breakdown:
    node_area_budget: 1230 #mm2
    proc_chip_area_budget: 815 #mm2
    core: 0.35
    L2: 0.14
    L1: 0.1 
    L0: 0.2
    DRAM: 0.05
    network:
      intra_package: 0.06
      inter_package: 0.1
\end{minted}
\caption{Resource breakdown example: This example is showing the area budget allocation and breakdown across all micro-architectural components.}
\label{fig:area-breakdown}
%\vspace{-0.3cm}
\end{figure}

Given the overall resource allocation and distribution, the AGE performs a series of optimizations (voltage-frequency scaling) to find an optimal parameter settings for each micro-architectural component. An optimal parameter setting is one that utilizes the most of the allocated budget. Note that an unbalanced resource allocation may leave some of the budget under-utilized. While we allow users to provide a manual breakdown of resources as input, we highly recommend to use SOE (Search and Optimization Engine) to automatically find the best setting which maximizes the overall resource utilization. 
%then calculates the micro-architectural parameters of various chip components which would be used by the PPR for performance prediction. 
%Examples of such output parameters include compute throughput, capacity and bandwidth to the different memory levels and the intra- and inter-node network bandwidths. 

%Often at early stages of an accelerator design, designers have a rough idea of resource budget for each u-architectural component but not necessarily the final parameters.
%This stage mimics the real-world process of hardware design.
%Next, we elaborate the micro-architectural paramter generation for each of these components in detail.

%optimally allocate resources corresponding to each component provided in the architecture template. 
%Besides, the perimeter of the compute die determines the total number of wires that can escape out of the compute chip. Hence, the tool also takes as input the breakdown of the perimeter of the compute die to allocate portions of die perimeter to different off-chip links such as off-chip memory links, intra-node and inter-node links.

\subsection{Micro-architectural Parameter Generation}
%\textcolor{blue}{Can we potentially move this to appendix if crunch for space?}
Next, the tool generates the micro-architectural parameters for each component of the architecture. Given the architecture template, alongside the resource breakdown across the different components, and the technology parameters, we find the maximum throughput for each component. E.g., We find the maximum number of cores that can fit in the given area allocation and find the voltage-frequency points to maximize compute throughput under the power budget. Similarly, for on-chip caches, we find the memory capacity and memory bandwidth at each level that can fit in the area budget while taking the network and controller overhead into account. For off-chip memories and network interfaces, we use the energy per bit information along with the physical I/O transceiver  area, bump pitch as well interconnect wiring pitch to determine the maximum bandwidth that can be realized on the chip (using a model similar to \cite{C111}).

These architectural parameters, throughput, bandwidth, capacity etc., are then provided as input to the performance prediction engine. Next, we discuss in detail how we model and calculate these parameters.

%More details about how we model and calculate these parameters have been explained in Appendix~\ref{detailed_uarch}


\subsubsection{Core} For deep learning models, the kernels are usually highly parallel in nature and therefore, our goal is to maximize total compute throughput under the area and power budgets allocated for compute. Given the area budget, we first compute the maximum number of MCUs (minimal compute units, introduced in Section~\ref{subsec:mcu}) that can fit within the area allocated. 
%Then, we compute the voltage and frequency of the compute units such that the total peak power is within the power budget allocated. 
The nominal frequency and voltage for each MCU is an input to the model, therefore the nominal power for each MCU and the entire core can be derived very easily. 
%
If the nominal power exceeds the power budget, we scale down the frequency and voltage. If we hit the minimum voltage limit set in the component description, we reduce the number of MCUs till we satisfy the total power budget allocated to the compute units.
%
This explains a case where the core design is power-bound and not area-bound.

Once we determine the total number of cores and the frequency of operation, we compute the compute throughput by appropriately scaling the nominal flop rate, as shown in equation~\ref{eqn:compute_throughput}.
%\begin{comment}
\begin{equation}
    \texttt{Throughput} = N \times \texttt{flop}_{nominal}\times {\frac{f_{op}}{f_{nominal}}}
    \label{eqn:compute_throughput}
\end{equation}

where $N$ is the total number of cores, $\texttt{flop}_{nominal}$ is the nominal flop rate of each core, $f_{nominal}$ is the nominal frequency corresponding to the technology node of the core and $f_{op}$ is the final optimal operating frequency. We use standard Voltage-Frequency-Power scaling methodology to obtain the operating voltage and frequency.
%\end{comment}
%\vspace{-0.1cm}
\subsubsection{Register and Cache Memory} The total area and power budgets allocated to each level of on-chip memory is split between the memory banks and 
the network circuitry that connects the memory banks at each level to micro-architectural components at the next level that are under its scope.
%the on-chip network used to communicate between the microarchitectural components under the scope of that level. 
We assume this interconnect to have a crossbar topology. The total number of components under its scope and the number of banks in that memory level determine the area and power overheads of the network. We iteratively determine the total number of  banks possible at each level of memory hierarchy such that the total area of the banks and the network at every level satisfies the area budget allocation. 
Once we determine the number of memory banks, we calculate total static power of all the banks (Equation~\ref{eqn:static_power_cache}) and we allocate the remaining power budget to dynamic access energy. The available dynamic energy budget determines the maximum achievable throughput as shown in Equation~\ref{eqn:throughput_memory}.
%\begin{comment}


\begin{equation}
    P_{static} = P_{static-per-bit} \times N_{banks} \times \texttt{Capacity}_{bank}
    \label{eqn:static_power_cache}
\end{equation}

\begin{equation}
    \texttt{Throughput} = \frac {P_{on-chip-mem} - P_{static}}{\texttt{Energy}_{dyn-per-bit}}
    \label{eqn:throughput_memory}
\end{equation}
%\end{comment}

%\vspace{-0.2cm}
\subsubsection{Main Memory} 
Main memory has two major components that collectively control the overall capacity and bandwidth but are housed in two different places. Memory controller which is placed on the compute chip, and the memory devices are placed outside the compute die within the same package.
The area allocation to each component determines the maximum number of memory devices that can be supported, which in turn determines the total memory capacity (see Equation~\ref{eqn:num_mem_devices}). 

\begin{equation}
\small
\begin{split}
    \#Devices = min(&\frac{Node\ Area - Processor\ Chip\ Area}{Device Area}, \\ & \frac{Area\ budget\ for\ Memory\ Controllers}{Memory\ Controller\ Area},
    \\ & \frac{Perimeter \times \#Links\ per\ mm}{\#Links\ per\ device})
    \label{eqn:num_mem_devices}
\end{split}
\end{equation}

Meanwhile power and perimeter allocation dictates the number of links (that can fit along the compute die), and the frequency of each link which collectively determine the overall off-chip memory bandwidth. 

%The number of main memory devices that can fit in a node is determined by three factors:(1) Main memory area budget within the node. E.g., in an interposer or multi-chip module setup, the number of memory devices that can be accommodated in a package is determined by the size of the integration substrate available after the compute/processor chip is placed on the substrate; (2) The number of memory controllers that can fit in the area available in the processor chip. (3) The number of memory interfaces/links that can fit along the available perimeter budget of the compute die. Note that this is dictated by the integration technology's interconnect density. In this framework, we assume that the off-chip memory devices would use standard interfaces and therefore the interface width per device would be fixed and is an input to the tool. 
%The maximum number of memory devices is determined by the most limiting of the three above-mentioned factors. Once the maximum number of memory devices is determined, AGE calculates the total capacity based on the capacity per device. The overall throughput then is calculated based on the frequency and the total interface data-bus width, similar to the case of caches.
%Next, the total static power of the off-chip memory subsystem is computed based on the static power per-bit data. Given the off-chip memory power budget and the static power known, the next step is to calculate the total memory bandwidth based on the remaining power budget. To do so, the frequency of operation of the memory is calculated using the standard voltage-frequency scaling methodology. The overall throughput then is calculated based on the frequency and the total interface data-bus width, similar to Eq.~\ref{eqn:throughput_memory}.


\begin{comment}



%the tool calculates the bus width between the compute and the memory die using Equation~\ref{eqn:bus_width}, where $BusWidth$ is the number of links possible between the compute die and each memory device, $perimeter_dram$ is the perimeter of the compute die that is allocated for off-chip memory communication, $N_{devices}$ is the total number of memory devices, $\#Links_{device}$ is the maximum bus width that each memory device can support and $\#Links_{mm}$ is the total number of inter-chip links that the interconnect substrate can accommodate.

\begin{equation}
\small
\begin{split}
    \#Devices = min(&\frac{Node\ Area - Processor\ Chip\ Area}{Device Area}, \\ & \frac{Area\ budget\ for\ Memory\ Controllers}{Memory\ Controller\ Area},
    \\ & \frac{Perimeter \times \#Links\ per\ mm}{\#Links\ per\ device})
    \label{eqn:num_mem_devices}
\end{split}
\end{equation}
\end{comment}
%Once the bus width per memory device is known, the tool calculates the link frequency based on the main memory budget using the standard voltage-frequency scaling methodology. The tool ensures that the operating voltage satisfies the maximum and the minimum voltage limits set in the technology library. Finally the throughput is calculated similarly as provided in Equation~\ref{eqn:throughput_memory}.
%\vspace{-0.2cm}
\subsubsection{Network} 
The off-chip network links (intra and inter-package) consume both power and area on the compute die. Moreover, the wires need to escape the periphery of the die which gets determined by the interconnect density and the available chip perimeter. The maximum number of links that can be accommodated in the compute die is limited either by the area available to fit in the link I/O cells or the amount of perimeter available for the links to escape the die periphery. Therefore, the tool uses the area per link, the available area budget, wiring density and the die perimeter budget to find the maximum number of links that can fit in the chip. Next, the tool uses the standard voltage-frequency scaling methodology to find the operating point for each link such that the total network-related power is within the power budget allocated. The network bandwidth is then calculated by multiplying the total number of links and the operating frequency of each link. We perform this step for the intra-node network and inter-node network separately.