%\input{figs/validation}
\section{Design Space Exploration Engine}\label{sec:dse}
%\textbf{Parameters} Input parameters, $W$ are grouped into three different categories: area ($A$), power ($P$) and perimeter ($R$). Parameters within each group capture the percentage of the overall area, power or perimeter allocated to each resource.
%\begin{equation}
%    W & = \{A, P, R\}
%\end{equation}
We denote the set of hardware \textbf{\textit{parameters}} to explore as $W = \{\{A_i\}_0^{H-1}, \{P_i\}_0^{H-1}, \{R_i\}_0^{H-1}\}$, 
where $H$ is the number of micro-architectural components in the hardware accelerator node, and $A_i$, $P_i$ and $R_i$ capture the percentage of the overall area, power and perimeter allocated to each component, respectively. 

Our \textbf{objective} is to find the optimal $W^*$ that minimizes the total run time, $f(W)$,
such that $\sum_{i=0}^{H-1} A_i \leq 1$, $\sum_{i=0}^{H-1} P_i \leq 1$, and $\sum_{i=0}^{H-1} R_i \leq 1$.
The objective function $f$ does not have a closed form, but we can calculate  it by  querying the performance model (CrossFlow).
This problem is an example of a \textit{constrained black-box continuous} optimization.
Since the objective function evaluation (i.e. querying CrossFlow) is considerably cheap (milliseconds), 
we use a variation of projected gradient descent (GD) optimization to solve for $W^*$ (see~\ref{eqn:gd_search}).
Empirically, we found that GD with exponential averaging in the parameter space (rather than gradients) works the best for our problem. %, which we refer to as value-momentum.
%We found that GD search to be very slow and out-of-the-box accelerated optimizers like Momentum, Adam, and Ada-Grad to perform poorly with projection.
%Empirically, we found that projected gradient descent works great with the value-momentum. 
%In value-momentum, we perform the moving average on weights rather than the gradient vectors:

%where $f$ is a complicated function (with thousands of lines of code that predicts execution time) with many nested if-else statements. 

%\textbf{Objective} 
%Our \textbf{objective} is to find the best parameter configuration that minimize time, such that the sum of parameters belonging to each group be less than or equal to 1. These constraints imply that we won't use more than allocated resources.
%\begin{equation}
%    \begin{aligned}
%    \max_{A,P,R} \quad & \textrm{Perf (A, P, R)} \\
%        \textrm{s.t.} \quad C_A:\quad & \sum_{i=1}^{N} A_i \leq 1  \quad\quad\quad \forall i \quad 0 \leq A_i \leq 1 \\
%        C_P:\quad &\sum_{i=1}^{N} P_i \leq 1  \quad\quad\quad \forall i \quad 0 \leq P_i \leq 1 \\ 
%       C_R:\quad &\sum_{i=1}^{N} R_i \leq 1  \quad\quad\quad \forall i \quad 0 \leq R_i \leq 1 \\ 
%    \end{aligned}
%\end{equation}

%As can be seen, we are dealing with a constrained, non-linear optimization problem. The objective function \texttt{Perf} is a complicated function (thousands of lines of code to explain performance in terms of A, P and R) with many nested if statements. 
%Overall, it is not clear how to formulate \texttt{Perf} in an algebraic form, and even if we can formulate it, it would be intractable to conventional optimization software (for instance, due to discontinuities, non-smoothness, or excessive computational cost of a function evaluation). From optimization perspective, we are dealing with a ``black box" optimization problem where the black box function can be queried through invocation of our execution engine which provides a system output for specified values of system inputs.
%While there are many approaches to black-box optimization, including Baysian optimization and , we found numeric gradient estimation to be sufficient for our problem. For one, the function \texttt{perf} is cheap to evaluate for any set of inputs, hence we use numeric gradient estimation to guide the search. To be more precise, we are using projected gradient descent algorithm.

%\textbf{Gradient Descent Optimization} comes in many forms, including Adam, Adamx, Nadam, and AMSGrad. Empirically, we found that all these optimization strategies work poorly in the context of projected gradient descent. 
%In projected gradient descent, after each update, we project the new point in the space to the constrained space. We found projected gradient descent works great with the value-momentum. In value-momentum, we perform the moving average on weights rather than the gradient vectors:

\small
\begin{equation}
\vspace{-0.5cm}
\begin{split}
    W_t & = W_{t-1} - \eta \mathrm{g_t} \qquad     \hat{W_t} = \frac{W_{t}}{||W_{t}||} \\
    M_t & = \beta M_{t-1} + (1 - \beta) \hat{W_t} \\ 
    W_t & = \textrm{Project}(M_t) \quad \textrm{onto} \quad C_A, C_P, C_R
\end{split}
\end{equation}
\label{eqn:gd_search}
\normalsize

Where $W_t$ and $\mathrm{g_t}$ are the input parameters and gradients at time step $t$, $\eta$ is the learning rate and $\beta$ is the discounting factor. 
%The intuition behind using momentum over values rather than gradient is to ...
We repeat the update steps shown above until convergence or the maximum number of steps ($T$), whichever conditions happens earlier. 
The final result is very sensitive to initialization. We repeat the steps above from $S$ different starting points and return the best result.
Empirically, we found that $T=100$ and $S=10$ are sufficient to find a near optimal solution.  
