\vspace{-0cm}
\section{Conclusion}

%This paper is the first effort to explore the cross-stack impact of technology scaling, model scaling and architecture innovations from a holistic perspective, and at the same time considering real-world design constraints like area and power budget for deep learning training.
We proposed DeepFlow, a performance modeling framework that enables a cross-stack analysis for hardware-software-technology co-design at-scale. We envision DeepFlow to be used by \textit{ML practitioners} (to decide what hardware to use to maximize their utilization, or simply predict their hypothetical model architecture performance which might not be realizable in today's hardware for many reasons including capacity limitation), by \textit{system designers} (to decide what hardware accelerators they need to acquire or build from scratch to meet their application needs, what new technologies to invest in, etc.), and finally by \textit{ technology experts} (to guide future technology development by assessing its impact  all the way across the stack, at scale). Our future work plans to extend DeepFlow modeling to other applications beyond language models and GEMM kernels. 

%and, at the same time, considering .... Furthermore, we capture ... and present an end-to-end analysis for \textit{what} and \textit{how} hardware-software design and at-scale optimization can help improve system utilization. We share the key challenges and chart out important directions across all dimensions of AI---data, algorithms, systems, metrics, standards, and best experimentation practices.