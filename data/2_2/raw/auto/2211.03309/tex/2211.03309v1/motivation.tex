\input{figs/motivation}
\vspace{-0.1cm}
\section{Motivation}
High-level algorithmic design decisions such as batch size, parallelism strategy and degrees of parallelism stress the underlying hardware components in different ways. %To maximize the underlying hardware utilization, different hardware designs are required. 
One important metric that guides a balanced compute-memory design is computation intensity. Computation intensity is a workload property defined as the ratio of the number of computation flops to number of accesses to main memory.
%This ratio dictates the optimal ratio of computation throughput to memory bandwidth in the underlying hardware accelerator.

Figure~\ref{fig:compint} (left) shows the computation intensity distribution across different number of GPUs. We performed this analysis for a GEMM problem of size $(64K, 64K, 64K)$ distributed across many GPUs. Depending on the parallelism strategy and number of available GPUs, each GPU gets a non-regular matrix shards for compute.
Each boxplot shows the spread of computation intensity for different number of GPUs.
For each level of parallelism, we see a large spread of compute intensities, particularly for lower parallelism degrees. This is the result of different parallelization strategies as well as different tiling strategies.
It is clear from this figure that computation intensity is much smaller at higher degrees of parallelism, implying the need for a different design point. 

%Besides the parallelism degree, the choice of parallelization strategy has a direct impact on the utilization of the underlying hardware.
There are a myriad of ways to parallelize a model across a large multi-node system.
Figure~\ref{fig:compint}(right) shows the distribution of computation intensity across different parallelization strategies for a fixed level of parallelism (64K GPUs). 
On the X-axis, we show various parallelization strategies across 64K GPUs. 
RC or CR refers to Row-Column or Column-Row distributed GEMM (a.k.a kernel parallelism, more details in Section~\ref{subsec:par_strategy}).
%It is clear from the figure that computation intensity is different across different parallelization strategies,  implying 
As shown, optimal design point is different for different parallelization strategies.
%%High-level parallelism not only controls the design of the hardware nodes but also the network connecting them together. 
%One metric that guides the optimal design point at network level is the ratio of the number of bytes transferred to far memory over network to near memory. 
%We refer to this metric as communication intensity. Figure~\ref{fig:comint}(bottom) shows the distribution of communication intensity across different degrees of parallelism.
%
%Parallelization not only influences the design of the accelerator node but also the network infrastructure that connects them together. 
%This includes network topology and network bandwidth.
%For a given network topology, the metric that guides the bandwidth decision is the amount of data bytes transferred from one node to the next. 
%The data bytes transferred over the network from one node to the next vary by levels of parallelism and the type of parallelism strategy (RC vs CR).
%
%Therefore, the optimal hardware architecture strongly depends on high-level algorithmic and software decisions. Hence, an exploration framework is needed that would allow one to obtain and analyze the various possible optimal system and algorithm design combinations. Furthermore, 
%
%These results indicate dependency between high-level algorithmic design decisions and low-level hardware design, hence the need for a cross-stack co-design. 

Large training workloads are rapidly becoming the applications driving massive investments in semiconductor technology development all the way down to fabrication equipment, making such a cross-layer pathfinding framework immensely valuable to ML engineers, system architects and technology developers alike. 
