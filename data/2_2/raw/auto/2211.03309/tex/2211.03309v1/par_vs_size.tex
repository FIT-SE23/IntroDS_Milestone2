Large number of options for compute node exists for programmers. 
Different cloud providers have different hardware. It is hard to figure out which one to go with for distributed training.

Programmers use high level programming libraries and platforms such as TF/Pytorch to implement ML models on these compute nodes. 
Beneath the high level platforms, the hardware manufacturers provides optimized implementation of various ML kernels. As a result the programmers often would not need to spend time and resources in optimizing for the given hardware and would just use the library components to stitch together a model. 

The most common ideology of programming using platforms like TF and PyTorch is that a programmer writes the ML model's DFG. Then based on the amount of memory available per node, the programmers fix the batch size such that all the overall memory requirement is less than that of the memory available in the hardware. Then, if distributed training is needed, programmers often rely on libraries for implementation of data parallelism, model parallelism and/or pipeline parallelism. Examples of such libraries include MeshTF, GPipe, Horovod etc. 

Choosing the right parallelism strategy for a given model and hardware however is not trivial. For example, a hardware node can be a NVIDIA A100 GPU with 320 TFLOPs of compute and 80GB memory or it can be a Cerbras waferscale engine which has XXX PFLOPs of compute and 40GB of memory. Given that the characteristics of the hardware node itself can vary wildly, how to use multiple of these nodes for distributed computing where the inter-node network also plays a big role in determining the overall performance of training is a very challenging task. In this work, we ask the question: can we create simple rules to guide the parallelism strategy in a distributed computing environment which takes in to account the characteristics of the underlying hardware node? 

We also ask the question: Is it worth for system builders to provide larger black-box nodes to programmers and hide away all the details inside the node from programmer? Does that ideology hurt resource utilization in a distributed training environment? What is the right level of granularity that the programmers should be exposed to?