
\section{Introduction}

Over the last decade, the demand on compute and memory resources for AI workloads has grown by multiple orders of magnitude~\cite{openai}.
%Over the last decade, the amount of compute and memory for AI has grown by multiple orders of magnitude~\cite{openai}. 
%The exponential growth in computation demand coupled with the slowdown of Moore's law
%have shifted the AI hardware landscape towards specialization and distributed scale-out computation.
As AI models grow in size along with the volume of training data, distributed training on cutting-edge scale-out systems composed of a large number of accelerators and processors has become the norm.
%However, there is a growing concern over low-utilization of such large-scale systems.
However, it has often been noticed that large scale AI training suffers from poor resource utilization. E.g., recent analysis reveals 5-20\% utilization across 1000s of GPUs~\cite{KunleScaledML}. Such poor utilization of resources is becoming a source of major concern.
Inefficiencies across different layers of the compute stack~\cite{jia2018beyond,aws} (from hardware micro-architecture to software parallelization strategies) and the design imbalance across different layers are among a few factors resulting in such low system utilization.
Different layers of the stack, technology nodes, hardware architecture, network topology, model architecture, parallelism strategy are designed across different organizations and retrofitted into the large-scale systems. The distributed nature of the design makes cross-layer optimization challenging if not impossible. 
%
For example, high-level design decisions like batch size, model architecture, and parallelism strategy exploited at algorithmic level
stress underlying hardware components (network, memory bandwidth or compute units) in different ways which call for different architectural designs, network topologies, memory technologies and technology nodes to ensure high system utilization.
%

Despite this, the distributed AI training hardware landscape often focuses on just a small set of parallelism strategies for a fixed hardware design~\cite{jia2018beyond}. Exploring the trade-offs between parallelization strategy (e.g. data parallelism and model parallelism) and performance (run-time) is often done in an ad-hoc manner. 
There is no methodical framework or research that explores the trade-offs between low-level hardware technology details and high-level algorithmic design (such as model architecture, parallelism strategy and batch size) on over performance and utilization of compute and memory resources. % under a fixed area and power budget.
%
%In this work, our goal is to develop a framework that enables cross-stack optimization/exploration analysis.
As a result, we set out to develop a framework that could enable across-the-stack analysis and allow us to look at the optimal points in the vast technology, system and algorithm design space. Towards that goal, we develop \textbf{CrossFlow}, a performance modeling framework that enables ``what-if'' analysis across different layers of the stack, and \textbf{DeepFlow} that builds on top of \textit{CrossFlow} and uses machine-learning based techniques to automate the design space search. %\footnote{We will open-source and publicly release the tool.}
CrossFlow is an end-to-end performance modeling tool based on an analytical model which takes the entire system-architecture into account and is more sophisticated than a simple Roofline analysis and less time-consuming than simulation. 
The framework provides a templatized interface for defining technology (minimum operating voltage, bitcell area, etc), chip (compute cores, memory hierarchy, etc.) and system-level architecture (node-level organization, intra-node network, and inter-node network), machine-learning model's compute graph, and parallelization strategies and predicts run-time per iteration step. Key contributions of this work include:
\begin{itemize}
    \item We conduct a variety of case studies looking at impact of a variety of high-cost technology innovations on eventual performance of distributed  DL training. We show that future logic technology nodes alone would provide minimal performance gains, and advancement in HBM and inter-node network technologies is needed to provide the next leap in performance. Also, optimal parallelism strategy selection could provide more performance gains than using naive parallelism strategies on next generation hardware (Section~\ref{sec:case-study}).
    \item We develop the first open-source, full-stack pathfinding framework, \href{https://github.com/nanocad-lab/DeepFlow}{DeepFlow}\footnote{\url{https://github.com/nanocad-lab/DeepFlow}}, for large distributed deep  learning (DL) training: the driving workload for most future technology, hardware and software development (Section~\ref{sec:overview}-~\ref{sec:dse}).
    \item We validate CrossFlow performance prediction against measurements on real commercial hardware (NVIDIA P4, V100 and DGX-1) running kernels and DL application in both single and distributed settings, observing near perfect correlation and 10\% - 16\% error.
    %and show that many of them may not deliver on the promise. 
    Next we show that large multi-chip  integration and waferscale technologies would not be worthy investments for large scale language models (Section~\ref{sec:validation}).
\end{itemize}

CrossFlow and DeepFlow can be used to bridge researchers across different layers of the stack (often spanning across different industries) to communicate their needs. 
%For example, ML practitioners, system designers and technology experts can use DeepFlow to strategize model design, hardware design and investment in new technologies, while being mindful of how their design choices trickles up or down the stack.
%We show a sample use case of DeepFlow, exploring the cross-stack impact of technology scaling, architecture innovations and model parallelism at scale from a holistic perspective, while considering real-world design constraints like area and power budget.

%We use CrossFlow to perform a cross-stack analysis, finding the relationship between parallelism strategy exploited at high-level and technology scaling at low levels.
%\textcolor{blue}{We show that carefully choosing the parallelization strategy can have as much impact on performance improvement as investing in futuristic (and often costly) technologies.}
 



\if 0
Recent studies show that the compute requirement of deep learning applications is doubling every 3 months~\cite{}.
Comparing this to Moore's law, where the number of transistors per chip only doubles every 3 years, 
the only path forward is to build scale-out multi-chip solutions for training deep learning networks.  There is a need to increase not only the computational and memory capacity per AI accelerator chip but also the scale of the system.
From an algorithmic standpoint, this implies exploiting all forms of parallelism strategies, such as data, model, pipeline and hybrid parallelism, to just name a few.
State-of-the-art deep learning problems are currently using parallelism across more than 1000s of GPUs and/or TPUs. 
However, these outrageous scaling attempts come at cost of severe under-utilization  -- 20\% efficiency across 400 GPUs~\cite{} and 6\% efficiency across 1000 GPUs. 

Designing a multi-accelerator system is more challenging than a single-accelerator design. For one, the design space would grow in size, including both per-accelerator architectural parameters as well as inter-accelerator parameters. Second, there is a complex interplay between the features of the two groups. Third, the inter-accelerator design choices are often dictated by the parallelism strategies exposed at algorithmic-level,  while the best parallelism strategy itself is dictated by the underlying system design. Existing approaches either focus on per-accelerator design or the inter-accelerator design. A large body of work \cite{} focuses on designing the best accelerator for a specific set of applications or domains. On the other hand, researchers are proposing methodologies for mapping their ever-growing applications onto multi-accelerator systems, 
assuming the underlying system is given and unchangeable \cite{}.

Designing a scale-out system for such large-scale machine learning training  problems requires careful co-optimization of accelerator architecture, memory subsystem, inter-chip network and algorithmic parallelization approaches. Though, there have been some efforts to standardize benchmarking of machine learning hardware \cite{}, what is needed is "full-stack" pathfinding of  accelerators. This is even more critical given that machine learning has emerged as the primary driving workload for future algorithms, architectures, circuits and semiconductor technology. 
%The exact resource allocation to each accelerator and the interconnection network across accelerators depend heavily on the operation intensity of each parallel shard and their communication pattern, which in turn depends on the parallelism strategy exploited at higher level. 
%Different forms of parallelism strategies stress different components of the hardware differently and there is no one single parallelism strategy that works the best across the board. The choice of best parallelism strategy also depends on the application and its configuration.

%Meanwhile, machine learning problems are changing rapidly and researchers, in quest for a better accuracy, are relying on techniques like model architecture search to find the best model configuration~\cite{}.
%Model architecture search is a reinforcement-learning-based technique that explores millions of model configurations. This implies we would stress hardware components in a different way for every model configuration.
%Meanwhile, there are many hardware companies offering widely different solutions, which puts the burden on customers to understand which one is better.
%Some great effort like MLPerf tries to address these challenges by encouraging hardware companies to provide evaluation numbers on most important existing applications in the field~\cite{}.
%However, hardware research community needs to look further than what is important today to decide how to make their next generation hardware.
%From the intellectual perspective, we are curious to understand if we can answer questions like this for model configurations that donot yet exist -- mostly for the lack of support for hardware community -- but are on trajectory to emerge soon in near future~\cite{}. 

%For example, as we show later for an outrageously large NLP, an example of which we are already seeing,
%data parallelism stresses the memory bandwidth and memory capacity more than kernel parallelism but kernel parallelism stresses the interconnection bandwidth more.


%The main reasons for such under-utilization can be summarized as 
%(1) AI accelerator chips are usually optimized for deployment at small scale. 
%(2) When partitioning model and batch size into smaller shards, the operational intensity 
%per model shard/data shard would be different depending on how partitioning is performed. 
%This indicates that there is no right design decision after all for designing an AI accelerator a-priori,
%but have to co-design it along with parallelism strategies at system-level. 

%One implication is that we need to design our accelerators to be flexible enough to adapt to new algorithms and parallelism strategies.
%For the sake of this paper, we are assuming/hoping that someone from our community can design such flexible AI accelerator,
%the goal of this paper is to provide insight into how to configure such flexible hardware to get high utilization and low running cost.
%Note that the design decisions would be different under different trends of technology scaling, data scaling, model scaling and algorithmic optimizations. 
%so our goal is to picture different landscapes under different scaling scenarios. 

%these trends imply that we cannot rely anymore on one single AI chip to meet the growing demands of the future AI applications.
%Owing to the fast growth of data and computing resources, deep learning has seen a tremendous success in myriad application domains. 
%However, with the slowing of technology scaling and the upper-bound on the size of the wafer, 
%the only way to sustain this trend under a fixed energy budget and time-to-train constraints 
%We should not only increase the computational and memory capacity per accelerator but also scale the system to thousands. 

%Designing a multi-accelerator system is more challenging than a single-accelerator design. 
%For one, the design space would grow in size, including both per-accelerator architectural parameters as well as
%inter-accelerator parameters. Second, there is a complex interplay between the features of the two groups. 
%Third, the inter-accelerator design choices are often dictated by the parallelism strategies exposed at algorithmic-level, 
%while the best parallelism strategy itself is dictated by the underlying system design. 

%In the literature, the existing approaches either focus on per-accelerator design or the inter-accelerator design. 
%On one side, a large body of work focuses on designing the best accelerator for a specific set of applications or domains.
%On the other hand, researchers are proposing methodologies for mapping their ever-growing applications onto multi-accelerator systems, 
%assuming the underlying system is given and unchangeable.
%Not having a holistic view results in designs where computing resources are underutilized within and across accelerators.
%Recent reports have shown more than XXX\% of GPU cores and XXX\% of bandwidth in datacenters are underutilized.
%Our goal is to design a multi-accelerator system with high-utilization under user-defined time-to-train and power budget constraints.
%To enable this analysis, we have designed \name to evaluate each system configuration study the interplay between time-to-train and technology parameters (e.g., energy per 
To enable such analysis, we have developed a  model and pathfinding tool, \name, which captures the interplay between technology parameters (e.g., energy per
flop, energy per bit access to different levels of memory hierarchy), AI accelerator parameters (e.g., compute throughput, memory bandwidth, memory capacity), cross-accelerator parameters (e.g., network bandwidth and network topology), model architecture parameters (e.g., computation graph, width, depth, sequence length), parallelism strategy (model parallelism, data parallelism, hybrid parallelism) and power budget as input and predicts performance (time-to-train) as output. In addition to accurately modeling technology, hardware and algorithm behavior, Key contributions of this paper as follows.
\begin{itemize}
    \item We develop detailed and self-consistent models of logic, memory, interconnect and packaging technology for scale-out machine learning accelerators.
    \item We implement optimized dataflows for a variety of parallelization strategies on abstracted but accurate architecture models. 
    \item We show that \name models performance accurately by comparing its predictions to existing 1, 2, 4 and 8 GPU systems. \name 's predictions have less than xx\% error on an average across a range of parallelization strategies.  
    \item We develop the first computationally efficient micro-architecture design-space exploration approach for scale-out machine learning systems.
    \item We use \name for several technology and architecture pathfinding studies drawing several interesting observations. For instance, XXX
\end{itemize}
We believe \name can identify key bottlenecks for computational scaling of future machine learning workloads and help guide technology as well as architecture development. 

\fi