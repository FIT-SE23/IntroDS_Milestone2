\begin{figure*}[ht]
\centering
\includegraphics[width=0.9\linewidth]{figs/figs/parallelism-updated.pdf}
\caption{\small \textbf{ An Example of a Compute Graph Transformation, Device Mapping and Routing, and End-to-End Time Estimation: (top)} Cross-edges are shown in red. To preserve readability, we only show a subset of cross-edges for kernel parallelism. 
Blue solid borderlines indicates separate hardware nodes. At every parallelization stage, we use black hashed lines to show graph replication along that dimension. A replica is a graph with a similar structure, however the kernel size and/or data size could be different for each replica.
For simplicity, the original graph is a simple 3-layer feed-forward neural network that is divided into two sub-graphs (P2).
Then for each pipeline stage, batch size is distributed across three workers (D3).
Then for each data shard of each pipeline stage, the kernels are distributed in a row-column fashion across a 4$\times$2 torus (RC-K4-K2). \textbf{(middle)} Mapping a 4-D hyper-cube into a 2-D mesh: a greedy layout mapped in the following order: kernel(R), kernel(C), pipeline and data. The bolded black edge in G4 is mapped onto a 4-hop path in the system graph. \textbf{(bottom)} backward pass time estimation. }
%\textcolor{blue}{FIXME: draw a box around the the top row, mark it as graph transformation, draw another box around the bottom row mark it as end-to-end time estimation. Show cross-edges in red.}
\label{fig:transformation}
\vspace{-0.2cm}
\end{figure*}
%gemm_val.tex