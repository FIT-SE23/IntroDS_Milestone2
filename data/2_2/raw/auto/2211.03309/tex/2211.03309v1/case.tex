%\input{figs/case}
%\vspace{-0.2cm}
\section{Case Studies}\label{sec:case-study}
%Here we will show how we use CrossFlow to project the impact of logic, network and memory technology scaling on the overall performance. 
%We also show the impact of co-optimizing parallelism strategy and the architecture at different technology nodes.
%\name is a very expressive model that not only can be used to predict performance but also to predict hardware counters (number of accesses to different levels of memory hierarchy for different operations, number of bytes transferred for each operation under each parallelism strategy, etc.) which we use to understand the observed pattern.  

%\subsection{Methodology}
DeepFlow is a pathfinding framework with studies and use cases spanning semiconductor technology development, micro-architecture, neural network models, and algorithmic parallelization techniques. In this section, we give few example case studies  for a large-scale language model 
(hidden dim: 16K, global batch size: 16K, vocab size: 800K, number of layers: 2, sequence length: 20) distributed across 512 hardware nodes. For future technology exploration, we study 7 consecutive \textbf{logic} technology nodes (from 12nm (N12) to 1nm (N1). Based on the recent scaling trends for logic technologies~\cite{stillmaker,Wikichip_technode}, we assume area and power scale by 1.8$\times$ and 1.3$\times$ from one node to the next for iso-performance), 4 different \textbf{memory} technologies (HBM2 (1 TB/s), HBM2e (2 TB/s), HBM3 (projected 2.6 TB/s~\cite{HBM3}), and HBM4 (projected 3.3 TB/s)) and 3 different \textbf{network} technologies (Infiniband-NDR-x8 (100 GB/s), XDR-x8 (200GB/s) and GDR-x8 (3.3 TB/s)). The caveat to these results (as with any pathfinding study with DeepFlow) is that if the system architecture or dataflow or neural network is radically different (e.g., this study assumes that same node is homogeneously replicated within the package),  the conclusions may change. 
 
%. Similarly, for memory and network technologies, we assumm that the architecture and signalling technologies will improve signalling bandwidth and the current and proposed scaling trends in each domain will hold in the future.
 

\subsection{Impact of Technology Scaling}
\begin{figure}[h]
  
\centerline{\includegraphics[width=0.9\linewidth]{figs/figs/case-tech}}
  \caption{\textbf{Technology Scaling:} scaling logic, memory and network technology}
  \label{fig:case1}
  \end{figure}
%\textcolor{blue}{Here we asked the question, scaling which technology would provide the maximum end-to-end performance benefit?} 
The first question we seek to answer is where the performance bottlenecks are across the stack and which technology could provide the maximum end-to-end performance benefit? Semiconductor technology development decisions are increasingly driven by machine learning as the workload. Many of these decisions trigger massive, multi-year investments. Figure~\ref{fig:case1} shows the impact of scaling logic, memory, and network technology for a large-scale language model using data-parallelism.
For these experiments, we assume that power/node = 300W and area/chip = 850 $mm^2$. %, and the fraction of power and area devoted to each u-architectural component is fixed. %Also, we fix the parallelization strategy at data parallelism.

%As shown, on the X-axis, we improve logic technology nodes from 12nm to 20\AA, and on the Y-axis we report execution time. The top four lines correspond to four recent and upcoming memory technologies, and the bottom three lines correspond to new networking technologies that improve bandwidth. Within each line, we observe a saturating trend, indicating that logic scaling alone is not a sustainable approach: 
Logic scaling improves compute throughput, and also caching capacity and bandwidth, but only to a smaller extent. Going from N12 to N7, we observe a jump in performance irrespective of memory technology. This is because at N12, the performance of a significant number of kernels are L2 bandwidth bound. At N7, the L2 bandwidth and capacity improve enough for HBM bandwidth to become the new bottleneck. Therefore, with improvement in HBM bandwidth, the balance can shift back again to caches and saturation point can be further improved with logic scaling, hence saturation point shifts further to the right. This trend continues up to N3. Beyond N3, even at very high memory bandwidth (3.3 TB/s) and network bandwidth (400 GB/s) performance stays unchanged as cache capacity and bandwidth are the main bottlenecks. Since the on-chip network connecting MCUs to cache and the cache controller overhead scale along with number of cache banks and the number of MCUs (which scale at  $\sim1.8\times$ per technology node), the cache capacity as well as bandwidth increase only marginally at N2 and N1. These trends are well inline with commercial examples from NVIDIA and AMD, where jump to N7 node provided large performance benefits and then, multiple high-end SKUs of the GPUs with higher bandwidth HBM memories have been released for further performance improvements.

\textbf{Network technology} scaling is another big factor that determines overall end-to-end performance of a distributed deep learning system. As logic and memory technologies scale alongside the size of the models, more inter-node bandwidth is needed to accelerate the inter-node communication collectives. Our analysis (Figure~\ref{fig:case1}) shows that beyond N3, scaling networking technology will provide much larger performance gains as opposed to logic scaling. This trend also aligns with the recent efforts in the industry to push high bandwidth and low energy networking technologies and architectures for inter-node and intra-node communication, targeted towards deep learning systems~\cite{google-optical-1, google-optical-2, NVLINK}.

%but logic scaling cannot sufficiently improve cache capacity/bandwidth:
%
%
%However, we see that beyond N3, even 3.3 TB/s bandwidth wouldn't be sufficient to provide further performance improvement. 
%
%This lead us to find the cause behind why would increased cache capacity from advanced nodes don't provide performance benefits?
%This is because the overhead of the on-chip network connecting the MCUs to the cache %, and the cache controller circuitry overhead 
%scales up with the number of MCU cores and the number of banks. 
%Moving from one node to the next will increase the number of MCUs on chip by $\sim1.8\times$.
%As a result, the cache capacity as well as bandwidth increases only marginally.
%and as a result bottlenecks performance under fixed chip area and power budgets.

%Our results also suggest that as logic technology improves, the bottleneck shifts from compute to memory and as memory improves, the bottleneck shifts from memory to network. But beyond Infiband-GDR, we see that the improvement from network would be marginal. This is because the communication portion becomes a small fraction of the end-to-end time.
%Understanding where the bottlenecks are is of our paramount importance as it directly influence our strategy for scaling. CrossFlow enables such bottleneck analysis from technology-levels all the way to algorithmic levels.
%it is important to understand if %we are compute-bound, memory-bound or network-bound, as it demands different scaling solutions.


%\vspace{-0.1cm}
\subsection{Co-optimizing Technology, Parallelism Strategy and Hardware Architecture Design}
\begin{figure}[h]
    \centerline{\includegraphics[width=0.9\linewidth]{figs/figs/case-all}}
  \caption{Co-optimizing parallelism strategy and hardware architecture design.} 
  \label{fig:case2}
  
\end{figure}

% We argue that technology and architecture exploration should be co-optimized along with parallelism strategy.
Figure~\ref{fig:case2} shows the importance of co-optimizing technology with parallelism and hardware design in an incremental fashion.
%The top-line is the baseline with no architecture or parallelism strategy exploration (same as the green dashed line from Figure~\ref{fig:case1}). 
%
%The next line shows model's performance with the optimal parallelism strategy at each technology point. 
%Finally, the bottom line shows model's performance when co-optimizing parallelism strategy and hardware architecture together.
%
%As technology nodes get more advanced and compute throughput per node improves (with cache bandwidth and capacity to a smaller extent), the best parallelism strategy would also change. For example, at N12, the best parallelism strategy is $\texttt{R2-C4-D64-P1}$, while at N1, it is $\texttt{CR4-D128-P1}$. 
%
%Finally the last line shows model's performance when co-optimizing technology,  parallelism strategy and hardware architecture together.
%
As shown: 
(1) Parallelism strategy optimization alone can offer $\sim2\times$ performance improvement.
(2) Co-optimizing architecture and parallelism strategy offers meaningful benefits for mature technology (12nm and 7nm) nodes. But for more advanced technology nodes, only marginal benefits (20\%-30\%) can be gained on top of parallelism strategy optimization. 
(3) For current and near-future technology nodes, co-optimizing for model architecture can provide as much benefit as scaling technology nodes (by almost two generations). 

%\vspace{-0.15cm}
\subsection{Effect of Multi-Node Package}
\begin{figure}[t!]
   
\centerline{\includegraphics[width=0.8\linewidth]{figs/figs/Perf_vs_package}}
  \caption{Performance improvement from multi-node package} 
  \label{fig:multi-node_package}
%\caption{Best case execution time improvement from multi-node packages}
\end{figure}


Next, we evaluate the performance improvement that multi-node packaged systems (e.g., MCM-GPU~\cite{mcm-gpu}, waferscale-GPU~\cite{ws-gpu}, Tesla Dojo~\cite{tesla-dojo}) can provide in a distributed training setup (see Fig.~\ref{fig:multi-node_package}).
We assumed 2TB/s link bandwidth for the intra-package links and performed both parallelism and architecture search for each case.

%\begin{table}[]
%\centering
%\footnotesize
%\begin{tabular}{|c|c|}
%\hline
%\#Nodes in Package & Normalized Execution Time \\
%\hline
%1                  & 1                         \\
%\hline
%4                  & 0.802397143               \\
%\hline
%16                 & 0.761674856               \\
%\hline
%64                 & 0.755620015              \\
%\hline
%\end{tabular}
%\caption{Best case execution time improvement from multi-node packages}
%\end{table}
Couple of key takeaways from these experiments were: (1)  Increasing the number of nodes in a package improves overall performance by roughly 32\% at best. (2) Beyond 4-nodes per package, performance improvement is marginal. Since ultra-large packages or waferscale integration dramatically worsens cost, we believe that such technologies may not be worthy investments for scaling large language model training. These conclusions hold across multiple different batch sizes, hidden dimension sizes and intra-node link bandwidths.
   

%To conclude, we argue that building optimized architectures at relatively mature nodes (N12/N7) can provide as much benefits as building architectures at the more costly advanced nodes. On the other hand, support for different parallelization strategies and algorithms to guide parallelism choice need to be enabled so that ML practitioners can derive optimal performance out of a given hardware. 

%We use that as a baseline where no architecture exploration conducted.
%We then show how parallelism exploration 
%Figure~\ref{fig:case2} (the dark green line) is same as the green dashed line from Figure~\ref{fig:case1}. 
