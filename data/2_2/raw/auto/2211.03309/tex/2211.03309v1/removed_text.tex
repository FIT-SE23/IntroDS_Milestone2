We use hierarchical roofline analysis to predict time for each node within the graph.
To predict performance of an application on a given hardware, roofline model calculates operational intensity (OI) to find if application is memory-bound or compute-bound.
Hierarchical roofline model much like roofline analysis relies on operation intensity estimation to measure performance.
Note here that operational intensity would be different for different levels of memory hierarchy, controlled by the number of accesses to each level. Hence, our performance estimation very much depends on accurate estimation of number of accesses to each levels of memory hierarchy. Once the roofline estimations for each level of memory hierarchy is in place, the overall performance is dictated by the slowest performing component. 

%%%%%%%%%%%
\subsection{Parallelism Modeling} 
Once the parallelism strategy is decided, we \textit{transform} our graph to reflect the impact of different forms of parallelism strategies, such as data, model and pipeline parallelism. Figure~\ref{fig:} shows an example of such transformation.

%
Data parallelism, would results in replicating the graph and connecting corresponding nodes in a fashion consistent with the reduction algorithm. For example, in case of ring-all-reduce, the corresponding nodes would be connected in a ring fashion. Note that the new edges that will be added to the graph will be marked as \textit{cross-edge} to capture the fact that they connect nodes belonging to different accelerators.

%
Model parallelism would results in replacing each node with sub-nodes and adjusting the problem size within each node and connecting the sub-components in a manner consistent with the parallelism strategy and the reduction algorithm.  
Much similar to data parallelism, the new edges would be marked as \textit{cross-edge}.

%
Pipeline parallelism, unlike other forms, does not result in replication or replacement of graph nodes. It simply modifies the edge type between nodes across layers that are mapped to different pipeline stages to capture their cross-connection nature.  
% 


%%%%%%%%%%%
\subsection{Network Modeling} 
So far we discussed how performance prediction engine predicts the time for each node within the compute graph. Here, we look deeply into how to model performance for each edge.
%
Recall that depending on the choice of parallelism strategy, the transformed compute graph can be a 5D/4D/3D hyper-cube (refer to Figure~\ref{fig:}).
%
However, 5D/4D/3D hyper-cube is not realizable in hardware. Physically speaking, the interconnection network across several nodes is usually structured hierarchically. Compute nodes that sit on a wafer are usually connected through one network topology and nodes across wafers are connected through another network topology. 
%
For example Nvidia DGX box consists of 8 GPUs, with one GPU on each wafer and wafers organized in a 3D hyper-cube on a ... substrate. 
%
In this paper, we are interested in wafer-scale integration, hence we study cases where we can fit more than one GPU/accelerator on one wafer. For connecting nodes on one wafer,  we assume a 2D mesh network topology, and for nodes across wafers, we assume a 2-D torus. This is similar to what recent wafer-scale hardware accelerators like Cerebras are adopting.

Note the mismatch between physical and logical network topology (5D/4D/3D vs. 2D). This indicates that multiple edge in logical graph can be mapped into one edge on physical layout. The number of logical edges that are mapped into a physical edge is an important metric as it captures the number of virtual channels required to simulate the connection in logical graph. The number of virtual channels per edge dictates the factor by which we have to derate the physical connection to capture the actual bandwidth. Overall bandwidth is dictated by the slowest link. So our goal is to find an optimal mapping that minimizes the maximum number of edges that is mapped into each edge across all edges. We use greedy heuristics to find such mappings. Our heuristic works as follow: starting with one axis, we try to map the nodes along that axis as close as possible, before moving to elements along the second axis and we repeat this until we map all axes. We repeat this for different permutations of axes. Figure~\ref{fig:} shows an example of a 4D to 2D mapping.