\section{Style Backdoor Attack}
\label{sec:threat}

\subsection{Threat Model}

\textbf{Attacker's Capabilities.}
We assume a gray-box threat model where the adversary has access to a small portion of the training data, which can be altered without restriction, but has no knowledge of the training algorithm and the model's architecture. 
Such capabilities are realistic as modern datasets are based on crowdsourcing~\cite{common-voice-dataset}, and malicious data may evade security checks~\cite{large-dataset-pyrrhic-win}. 

\textbf{Attacker's Aim.}
The attacker's purpose is to insert a secret functionality into the deployed model, which is activated when the trigger is present in the model's input. Usually, this functionality causes targeted misclassifications with a very high probability. Furthermore, the model should behave normally for any other input to avoid raising suspicions. 

\subsection{Attack Formulation}
Prior studies in backdoor attacks mainly focus on \textit{static triggers}, where the attack trivially inserts a trigger $\epsilon$ in victim samples.
Such addition is sample-independent.
Let $x$ be a sample, $\epsilon$ a trigger, the backdoor $\mathcal{F}$ can be described as: 
\begin{equation}\label{eq.backdoor_static}
\setlength\abovedisplayskip{0pt}
\setlength\belowdisplayskip{0pt}
    \mathcal{F}(x, \epsilon) = x + \epsilon.  
\end{equation}
In BadNets~\cite{gu2017badnets} (CV), the trigger is a patch at a pixel level that replaces a part of the original sample. In audio, a static trigger could be a tone superimposed on the original sample~\cite{can-you-hear-it}.
Static triggers can thus be considered as a \textit{constant} parameter of the attack, as a batch of benign samples $\{x_0, \ldots, x_n\}$ is transformed in nothing more than $\{x_0 + \epsilon, \ldots, x_n + \epsilon\}$. 
Thus, the victim's model $\mathbf{C}_{back}$ learns to associate the static pattern $\epsilon$ to a target label $y*$.

In this work, we approach the backdoor attack from an orthogonal perspective: can the trigger be something that the sample \textit{is} rather than something the sample \textit{has}? 
Static backdoors answer the \textit{has} proposition: the poisoned sample contains a specific (and constant) pattern that $\mathbf{C}_{back}$ associates with the target class.
Answering the \textit{is} is more complex: this is how a sample is presented, and we can think of it as a latent variable globally describing that sample. 
In our study, global descriptors are defined by \textit{stylistic properties}.
Examples are image exposure (CV), writing complexity (text), and the signal's pitch (audio).
Once the stylistic trigger $\epsilon$ is identified, we need a function $\mathcal{S}_\epsilon$ that embeds such a style to a given sample $x$. Formally, Eq.~\eqref{eq.backdoor_static} changes to:
\begin{equation}\label{eq.backdoor_style}
\setlength\abovedisplayskip{0pt}
\setlength\belowdisplayskip{0pt}
    \mathcal{F}(x, \mathcal{S}_\epsilon) = \mathcal{S}_\epsilon(x).  
\end{equation}
We explore different $\mathcal{S}_\epsilon$ for speech recognition in the following sections. 
With the formulation given in Eq.~\eqref{eq.backdoor_style}, the batch of adversarial samples$\{\mathcal{S}_\epsilon (x_0), \ldots,$ $\mathcal{S}_\epsilon (x_n)\}$ now contains dynamic triggers since the outcome of the backdoor generation varies based on the inputted sample. 
Now, suppose that $\mathcal{S}_\epsilon$ exists.
The stylistic backdoor is effective if the model $\mathbf{C}_{back}$ learns an association between $\mathcal{S}_\epsilon$ and the target class $y*$.
This problem can be formulated with two sub-questions:
\begin{compactenum}
    \item Can $\mathbf{C}$ learn to recognize the presence of $\mathcal{S}_\epsilon$? We need proof or at least an indication that stylistic properties can be learned by the victim's model.  
    \item Can we let $\mathbf{C}$ learn the association between $\mathcal{S}_\epsilon$ and $y*$? If the previous question is positively answered, we need to understand further how to create the backdoor and what are the possible obstacles at this stage.  
\end{compactenum}
Geirhos et al.~\cite{geirhos2018imagenettrained} answered the first question, showing that CNNs focus more on textures rather than object shapes.
For example, an image with `cat' shapes and `elephant' texture is classified as an elephant. 
Similarly, styles can be learned in the speech domain~\cite{grinstein2018audio, AlBadawy2020}.

Finally, we need to understand what are the conditions to create the stylistic backdoor in $\mathbf{C}$.
In source-agnostic backdoors, the trigger is present only in one of the classes of the training data. 
This condition is easily satisfied by the classic backdoor attacks injecting artifacts as triggers.
Conversely, with stylistic backdoors, such conditions might not always be met. 
For example, high exposure (image domain) and low-frequency tunes (audio domain) might be present in many classes of clean data. 
We conclude that the choice of $\mathcal{S}_\epsilon$ impacts attack success. 
In our experiments, we use stylistic functions that are unlikely to be present in the training data. %\todo{maybe we can discard this sentence}

