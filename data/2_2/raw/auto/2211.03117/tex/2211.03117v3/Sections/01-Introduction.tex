\section{Introduction}
\label{sec:introduction}
\blfootnote{$^*$Equal Contribution}
Backdoor attacks are a class of machine learning threats where the attacker embeds a secret functionality into the victim's model, which can be triggered at the testing time from malicious inputs~\cite{gu2017badnets, zhai2021backdoor}.
Backdoor triggers can be grouped into two major families: \textit{static}, when the trigger is a fixed pattern attached to the poisoned sample~\cite{gu2017badnets}, and \textit{dynamic} when the trigger's properties vary for each poisoned sample~\cite{dynamic-backdoor-attacks-against-ml-models,dynamic-backdoors-with-gap}. Dynamic triggers are generally stronger as they can be effective under different conditions and potentially bypass state-of-the-art countermeasures~\cite{dynamic-backdoor-attacks-against-ml-models}.
Recently, a new way of creating dynamic triggers has emerged: stylistic triggers.
For instance, paintings and writing styles can be used as triggers in computer vision (CV)~\cite{deep-feature-space-trojan-attack-of-nns-by-controlled-detoxification} and text domain~\cite{mind-the-style-of-text}.

\textbf{Motivation.}
Stylistic backdoors are powerful but not yet studied in the audio domain. Such backdoors highly depend on the targeted application resulting in domain-specific challenges. In audio, the most important challenge is to create a style that alters the original signal in a way that is distinguishable by the trained models but also keeps the audio quality at an acceptable level. To our best knowledge, no work has explored stylistic backdoors in audio, and we aim to fill this gap.
We investigate the effect of six stylistic triggers in a speech classification task in both clean and dirty-label settings.
Triggers are generated through electric guitar effects that do not require training of complex generative models~\cite{deep-feature-space-trojan-attack-of-nns-by-controlled-detoxification,mind-the-style-of-text}, making our attack easy to deploy.
Our contributions are:
\begin{compactitem}
    \item We propose and demonstrate the feasibility of stylistic backdoor attacks (denoted JingleBack) in the audio domain through electric guitar effects. For our experiments, we trained 444 models, and JingleBack reached up to 96\% attack success rate.
    \item We are the first to formally describe stylistic backdoor attacks and establish a domain-agnostic framework that can be used for stylistic backdoors in any application.
\end{compactitem}
