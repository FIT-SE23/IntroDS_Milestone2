\section{Background}
\label{sec:background}

\textbf{Evasion Attack.}
The attacker aims to find a small perturbation $\epsilon$ for a sample $x$ to produce a misclassification in a target classifier $\mathbf{C}$.
The definition of $\epsilon$ is task-dependent~\cite{evasion2004}.
For images, $\epsilon$ is usually an additive noise at a pixel level computed via a gradient-based procedure~\cite{fgsm}.
For audio, $\epsilon$ is a small waveform computed through an optimization process~\cite{audio-adversarial-examples-targeted-attacks-on-speech-to-text}.


\textbf{Poisoning Attack.}
Attackers able to manipulate victims' training data $\mathcal{D}^{tr}$ can inject adversarial samples to increase $\mathbf{C}$'s testing error and decrease the model's performance~\cite{biggio_poisoning}.
Data manipulation is a concrete threat because building datasets often involves untrusted sources, and complex model training is outsourced to third parties.

\textbf{Backdoor Attack.}
Data poisoning leads to \textit{backdooor attacks}. The attackers insert backdoors in the model resulting in targeted misclassifications~\cite{gu2017badnets}.
Most backdoors are source-agnostic, as the trigger can be applied to any dataset's class. There are two different approaches to creating such a backdoor: \textit{dirty-label}~\cite{gu2017badnets}, where the adversary adds the trigger to some training samples but also alters their labels, and \textit{clean-label}~\cite{clean-label-backdoor-attacks}, where the attacker only poisons samples from the target class.
The dirty-label attack produces stronger backdoors and requires less poisoned data, but the clean-label attack is a more realistic threat as poisoned samples cannot be easily identified by manual inspection~\cite{clean-label-backdoor-attacks}.
For example, crowdsourced datasets like Mozilla's Common Voice~\cite{common-voice-dataset} use volunteers to validate each audio file and its transcription.
When the attack is dirty-label, the volunteer can easily spot inconsistencies between the provided audio and its transcription. However, in a clean-label attack, the trigger will remain unnoticed because the audio and its transcription will be correct.
This work explores the effects of both approaches on a style-based backdoor attack in speech classification.
