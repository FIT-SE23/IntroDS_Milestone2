\section{Background}
\label{sec:background}

% In this section, we discuss evasion and poisoning attacks, with a special emphasis on backdoor attacks. Finally, we discuss the concept of stylistic backdoors.

% \subsection{Adversarial Machine Learning}
% \label{sub.aml}
Adversarial Machine Learning (AML) studies threats that undermine ML applications.
% The attack surface usually depends on the attacker's \textit{knowledge} (e.g., what the attacker knows about the target system) and \textit{capabilities} (e.g., interaction level with the target system or the type of sample perturbation). 
We can consider evasion and poisoning attacks as two main types of attacks~\cite{barreno2010security}.

\textbf{Evasion Attack.} 
The attacker aims to find a small perturbation $\epsilon$ to a sample $x$ to produce a misclassification in a target classifier $\mathbf{C}$. 
The definition of $\epsilon$ is task-dependent~\cite{evasion2004}. 
For images, $\epsilon$ is usually an additive noise at a pixel level computed via a gradient-based procedure (e.g., FGSM~\cite{fgsm}). 
For audio, the perturbation is a small waveform computed through an optimization process~\cite{audio-adversarial-examples-targeted-attacks-on-speech-to-text}.
% For text, the perturbation consists of word replacement (or sentence paraphrasing) and typos, computed via optimization processes like FGSM~\cite{text_adv}. \todo{why image and text and not sound? try to connect with our work}


\textbf{Poisoning Attack.} 
If attackers can manipulate the training data $\mathcal{D}^{tr}$ of a victim model, they can inject crafted adversarial samples to increase $\mathbf{C}$'s testing error and decrease the model's performance~\cite{biggio_poisoning}.
%\todo{but this is only data poisoning? what about code? stef: We didn't put the code and weight poisoning as it would require 2 more citations}
Since building datasets is an expensive process, often involving untrusted parties, and training complex models frequently involves external parties, data manipulation is a concrete threat.
% This is a concrete threat, especially when considering that building datasets is expensive and often involves untrusted parties, and that the training of complex models often involve external parties.   

% Building a dataset from scratch often involves data collection from untrusted sources and third parties. 
% Additionally, training a well-performing model might not be possible due to computational limitations or problem complexity. Current solutions involve pre-trained models or outsourced training (e.g., Machine-Learning-as-a-Service). 
% Both trends open the door to adversaries, making it possible to affect models' performance.

% This is a realistic threat as modern datasets are often crowdsourced or collected from untrusted sources. Additionally, limitations in the computational resources lead to a new trend; Machine Learning as a Service (MLaaS), where a model is trained by a third part.

\textbf{Backdoor Attack.} 
Data poisoning can lead to \textit{backdooor attacks}. The attackers insert backdoors in the model that can be activated at testing time, resulting in targeted misclassifications~\cite{gu2017badnets}.
Most backdoors in the literature are source-agnostic, meaning that the trigger can be applied to any dataset's class. There are two different approaches to creating such a backdoor: \textit{dirty-label}~\cite{gu2017badnets}, where the adversary adds the trigger to some training samples but also alters their labels, and \textit{clean-label}~\cite{clean-label-backdoor-attacks}, where the attacker only poisons samples from the target class.
%hoping the trained model perceives the trigger as a feature of the target class.
The dirty-label attack produces stronger backdoors and requires less poisoned data, but the clean-label attack is a more realistic threat as poisoned samples cannot be easily identified by manual inspection~\cite{clean-label-backdoor-attacks}. For example, crowdsourced datasets like Mozilla's Common Voice~\cite{common-voice-dataset} use volunteers to validate each provided training sample, which is a pair of an audio file and its transcription. 
When the attack is dirty-label, the volunteer can easily spot inconsistencies between the provided audio and its transcription. However, in a clean-label attack, the trigger will remain unnoticed because the audio and its transcription will be correct. In this work, we explore the effects of both approaches on a style-based backdoor attack in speech classification.


