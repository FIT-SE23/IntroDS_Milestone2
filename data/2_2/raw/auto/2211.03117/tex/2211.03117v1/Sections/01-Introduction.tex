\section{Introduction}
\label{sec:introduction}

Machine learning became popular in the last decade with applications in critical domains like autonomous vehicles and financial predictions. 
It is thus essential to ensure its security, which led to the field of adversarial machine learning.
% Adversarial machine learning studies the security of machine learning applications .
%For example, \textit{evasion} attack manipulates input samples to produce misclassifications~\cite{evasion2004}.
In this work, we focus on \textit{backdoor} attacks, where the attacker embeds a secret functionality into the victim's model, which can be triggered at the testing time from specially crafted inputs~\cite{gu2017badnets}. These inputs include an attacker-chosen predefined property (\textit{trigger}) that activates the backdoor at testing time, resulting in a targeted misclassification. 
Usually, backdoors are inserted through a \textit{poisoning} attack, where the attacker controls a part of the training data~\cite{gu2017badnets}.

% Two poisoning strategies can be distinguished: \textit{dirty-label} and \textit{clean-label}. The former dictates a modification of the labels of the poisoned training samples, while the latter preserves their true labels. Despite the fact that the dirty-label attack produces stronger backdoors and requires less poisoned data, the clean-label poisoning is stealthier and more realistic as it can easily bypass manual inspection techniques~\cite{clean-label-backdoor-attacks}. 
% For example, crowdsourced datasets like Mozilla's Common Voice~\cite{common-voice-dataset} use volunteers to validate each provided training sample which is a pair of an audio file and its transcription. 
% When the attack is dirty-label, the volunteer can easily spot inconsistencies between the provided audio and its transcription. However, if the attack is clean-label, the trigger will remain unnoticed because the audio and its transcription will be correct.

% \textbf{Research Motivations.}
Backdoor triggers can be grouped into two major families: \textit{static}, when the trigger is a fixed pattern attached to the poisoned sample~\cite{gu2017badnets}, and \textit{dynamic} when the trigger's properties vary for each poisoned sample~\cite{dynamic-backdoor-attacks-against-ml-models,dynamic-backdoors-with-gap}. In general, dynamic triggers are stronger as they can be effective under different conditions and can potentially bypass state-of-the-art countermeasures~\cite{dynamic-backdoor-attacks-against-ml-models}.
Recently, a new way of creating dynamic triggers has emerged in the literature: stylistic triggers.
%Currently, only a few works successfully proposed dynamic trigger, and such triggers are the result of stylistic transformations. 
For instance, in computer vision, StyleGAN generates poisoned samples with styles based on a style image (e.g., Van Gogh painting)~\cite{deep-feature-space-trojan-attack-of-nns-by-controlled-detoxification}. On the other hand, in the text domain, the writing style is used as a
trigger~\cite{mind-the-style-of-text}.
%trigger~\cite{li2021hidden,mind-the-style-of-text,hidden-trigger-backdoor-attack-on-nlp-models-via-linguistic-style-manipulation}.\todo{do we even need to mention text domain and especially with 3 citations?}

\textbf{Motivation.}
Stylistic backdoors are powerful but not yet studied in the audio domain. Such backdoors highly depend on the targeted application resulting in domain-specific challenges. In audio, the most important challenge is to create a style that alters the original signal in a way that is distinguishable by the trained models but also keeps the audio quality at an acceptable level. To our best knowledge, no work has explored stylistic backdoors in audio, and we aim to fill this gap. 
We investigate the effect of six stylistic triggers in a speech classification task in both clean and dirty-label settings. 
Triggers are generated through electric guitar effects that do not require training of complex generative models~\cite{deep-feature-space-trojan-attack-of-nns-by-controlled-detoxification,mind-the-style-of-text}, making our attack easy to deploy.
Our contributions are:
\begin{compactitem}
    \item We propose and demonstrate the feasibility of stylistic backdoor attacks (denoted JingleBack) in the audio domain through electric guitar effects. For our experiments, we trained 444 models, and JingleBack reached up to 96\% attack success rate. 
    \item We are the first to formally describe stylistic backdoor attacks and establish a domain-agnostic framework that can be used for stylistic backdoors in any application.
\end{compactitem}


%%%==========================old========
% Data-driven approaches like Machine Learning (ML) have been confirmed as state of the art in many disciplines in the 2010s. 
% Their success is uncountable, as we can find their application in many industries like automotive~\cite{Wu_2017_CVPR_Workshops} and healthcare~\cite{dilsizian2014artificial}. 
% The market behind this technology is growing as well: as Reported by Statista\footnote{\url{www.statista.com/statistics/694638/worldwide-cognitive-and-artificial-intelligence-revenues/}}, this market was worldwide valued $\$300$ billion in 2021, with an expectation of $\$500$ billion in 2023. 
% Nevertheless, ML applications might be threatened by adversaries aiming to affect or control their execution. 
% For example, an adversary might manipulate the environment to produce misclassifications. This attack is called \textit{evasion}~\cite{evasion2004}.
% The area that studies the security of machine learning is called \textit{adversarial machine learning}.
% Since ML can be used in critical applications (e.g., autonomous vehicles), it is fundamental to identify new potential threats that might undermine the correct workflow of these algorithms. 

% In this work, we focus on \textit{backdoor} attacks. In this threat, the attacker embeds a secret functionality into the victim's model which can be triggered at testing time from specially crafted inputs. These inputs include an attacker-chosen predefined property, i.e., the trigger, which activates the backdoor. 


% The first backdoor attacks exploited two common ML deployment conditions~\cite{gu2017badnets}; dataset creation from untrusted sources and third party training (e.g. outsourced training and transfer learning).
%\begin{enumerate}
%    \item Complex ML models require mammoth datasets. The creation of such datasets is difficult in a controlled environment and data should be retrieved from untrusted parties.
%    \item Complex ML models require non-trivial computational resources. As a consequence, many companies uses pre-trained models, or they outsource their training to third parties. In both cases, the third party that produces the model might be colluded. 
%\end{enumerate}
% Two poisoning strategies can be distinguished: \textit{dirty-label} and \textit{clean-label} attacks. The first involves changing the labels of the poisoned training samples. In contrast, in the latter case, the adversary preserves the true labels of the poisoned samples. Despite the fact that the dirty-label attack produces stronger attacks and requires less poisoned data, the clean poisoning is stealthier and more realistic~\cite{clean-label-backdoor-attacks} as it can easily bypass manual inspection techniques. For example, crowdsourced datasets like Mozilla's Common Voice~\cite{common-voice-dataset} use volunteers to validate each provided training sample. When the attack is dirty label, the volunteer can easily identify inconsistencies between the provided audio file and the corresponding text. If the attack is clean label though, the trigger will remain unnoticed because the audio and the corresponding text will be correct.

% Similarly, we can distinguish two trigger families. \textit{Static}, when the trigger is a fixed pattern attached to the poisoned sample~\cite{gu2017badnets}, and \textit{dynamic} when the trigger's properties vary for each poisoned sample~\cite{dynamic-backdoor-attacks-against-ml-models,dynamic-backdoors-with-gap}. In general dynamic triggers are stronger as they can be effective even under different conditions and can potentially bypass state-of-the-art countermeasures~\cite{dynamic-backdoor-attacks-against-ml-models}.

% For this reason a new approach for trigger generation has been recently emerged. These triggers are based on style transformations that exploit a property on the model's input instead of a static feature like a pixel pattern in a fixed location.
% In~\cite{deep-feature-space-trojan-attack-of-nns-by-controlled-detoxification} the authors used StyleGAN to poison images with a specific style and through ``controlled detoxification'' ensured that the trained model learnt the style instead of just a shallow feature like the color. However, they assumed white-box access to the model's training and that the adversary is able to train a GAN which is not always a realistic scenario.
% \cite{mind-the-style-of-text} introduces textual backdoors and adversarial attacks based on linguistic styles. In this work only the dirty-label backdoor attack was applied in models trained from scratch. This approach has been extended in~\cite{hidden-trigger-backdoor-attack-on-nlp-models-via-linguistic-style-manipulation} with style-aware injection algorithms that amplified the model's perception of trigger style. Apart from backdoor attacks, style transfer techniques have been also used for evasion attacks in video classification~\cite{stylefool} and computer vision~\cite{adversarial-camouflage-hiding-physical-world-attacks-with-natural-styles}.

% In this work we use electric guitar effects as our triggers to create the first (to the best of our knowledge) style-based audio backdoor. Additionally, we investigated the backdoor's behavior for both clean-label~\cite{clean-label-backdoor-attacks} and dirty-label~\cite{gu2017badnets} setups. In particular, our contributions are: \todo{fill a bullet list with contributions once we are done}. 

% Mind the style of text
%They generated both adversarial and backdoor attacks based on style.
%Through an iterative process they found a text style that is clearly distinguished from the style of the training samples.

% Controlled detoxification Notes.
%\begin{itemize}
%    \item Similariites:
%    \begin{itemize}
%        \item They claim that the pixel patterns are easily detectable because they may seem unnatural. Thus, such triggers are not stealthy, and a manual inspection could identify them. By using a function that embeds a property to the input samples, we can create more difficult to detect trojans.
%        \item  They also mention that style triggers (or, as they call it feature space triggers) are dynamic triggers as they depend on each input sample. And what does this mean???
%        \item They checked how a clean model classifies poisoned inputs to measure the trigger's stealthiness. If the trigger is stealthy, the clean model should recognize the features of the poisoned input's original class and classify it correctly. 
%    \end{itemize}
%    \item Differences:
%    \begin{itemize}
%        \item The threat model they use is different. They use a white box threat model as they assume the attacker has access to both the model and the training dataset, and can control the training process (for the detoxification).
%        \item They did not use a clean-label attack as we did. This difference may lead to stealthier triggers that will not require the detoxification process to avoid defenses.
%        \item They trained a CycleGAN that transforms images from domain A (clean) to B (poisoned with a different style) and from B to A based on [3]. We did not use a GAN, which can be a more realistic attack scenario as the attacker may not have the resources or the knowledge to train a GAN in the outsourced training or MLaaS paradigm.
%    \end{itemize}
%    \item We can also borrow the following ideas:
%    \begin{itemize}
%        \item They said that in general, it is challenging to control what a model learns through data-poisoning-based backdoor attack. It would be nice to try to check somehow what the model learns from our attack.
%        \item They provide a nice definition of stealthy and robust backdoors. We can use this definition in our paper and experiments too.
%    \end{itemize}
%\end{itemize}



