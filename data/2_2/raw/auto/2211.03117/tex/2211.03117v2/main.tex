%\documentclass[letterpaper,twocolumn,10pt]{article}
\documentclass{article}             
\input{preamble}


%%% -------------------------------------------------------------------------

\title{Going in Style: Audio Backdoors through Stylistic Transformations}
 
\name{anonymous submission}
\address{}
\name{Stefanos Koffas$^{1,*}$, Luca Pajola$^{2,*}$, Stjepan Picek$^{3,1}$, Mauro Conti$^{2,1}$}    
 %TODO: Should we add our email addresses here?
 \address{$^1$Cybersecurity Group, Delft University of Technology, The Netherlands\\
          $^2$Security and Privacy Research Group, University of Padua, Italy\\
          $^3$Digital Security Group, Radboud University, The Netherlands
          %$^*$Equal contribution
          }
\begin{document}
\maketitle

\begin{abstract}
A backdoor attack places triggers in victims' deep learning models to enable a targeted misclassification at testing time. 
In general, triggers are fixed artifacts attached to samples, making backdoor attacks easy to spot.
Only recently, a new trigger generation harder to detect has been proposed: the \textit{stylistic} triggers that apply stylistic transformations to the input samples (e.g., a specific writing style).
\par
Currently, stylistic backdoor literature lacks a proper formalization of the attack, which is established in this paper. 
Moreover, most studies of stylistic triggers focus on text and images, while there is no understanding of whether they can work in sound. This work fills this gap. 
We propose JingleBack, the first stylistic backdoor attack based on audio transformations such as chorus and gain.
% We examine audio effect transformations to demonstrate their feasibility in the sound domain.
Using 444 models in a speech classification task, we confirm the feasibility of stylistic triggers in audio, achieving 96\% attack success.


%%% =========== OLD
% Backdoor attacks are a recent threat against deep learning models activated through specially crafted inputs (triggers) aiming at targeted misclassifications. 
% Most of the literature's triggers are static patterns applied on malicious samples\todo{this somehow implies that what we do is not applied on malicious samples}, while only recently, dynamic triggers have been proposed to make attacks stealthier and harder to defend. 
% Dynamic triggers can be generated in various ways, such as stylistic transformations. \todo{shall we say what it is}
% Stylistic triggers have been demonstrated to be powerful in domains like text and images, while there are no works investigating their feasibility on speech.

% This work proposes six stylistic triggers in the speech domain, consisting of distinct guitar effects. 
% We demonstrate the feasibility of our attack in a practical use case: the speech classification task. \todo{do we need this sentence? nothing new from the previous one}
% Our experiments analyze three distinct baseline models, four different poisoning rates, and two attack settings (clean and dirty labels). Our most effective style yielded an ASR larger than 84\% for the clean-label attack and more than 96\% for the dirty-label attack with only 1\% poisoned training samples.
% %Each experiment is repeated four times, for  total of 576 tested models. 
% \todo{add the outcomes}

% ==================== USENIX
% Data poisoning is a class of machine learning threats where an attacker injects malicious samples to affect the victim's model training. An attacker can leverage data poisoning to insert a backdoor in a deployed model that is activated when the model's input has a specific property (trigger). In general, backdoor attacks can be mounted with or without the \textit{label flipping} of the poisoned samples. \textit{Label flipping} or \textit{dirty-label attack}, requires that the attacker not only changes a few training samples but their labels too. On the contrary, \textit{clean-label attack} assumes that the adversary has no access to the labels.  While the first is more effective and requires less poisoned data, the second is stealthier and more realistic. Current state-of-the-art mainly focuses on producing backdoors with `static triggers', where malicious samples are crafted by adding a unique `signature' (e.g., inserting a patch into an image). While static triggers are effective, they can be easily detected and mitigated even through manual inspection of the dataset. 

% In this work, we propose ``Style Backdoor'', a novel dynamic backdoor for audio based on stylistic transformations. We use electric guitar sound effects as our triggers and compare the performance of clean-label and dirty-label attacks. 
% \todo{complete when the paper is done}
% \todo{Mention that our code is public.}
\end{abstract}

\input{Sections/01-Introduction}
\input{Sections/02-Background}
\input{Sections/03-threat_model}
%\input{Sections/04-Case study: Image Classification}
% \input{Sections/05-Case Study:CV Multiclass}
\input{Sections/06-case_study_speech_classification}
%\input{Sections/07-Defense}
\input{Sections/08-Conclusions}

% Usenix
%\bibliographystyle{plain}
% ICASSP
% \bibliographystyle{IEEEbib}
\bibliographystyle{ieeetr}
\bibliography{main}

% \appendix
% \input{Sections/00-Appendix}

\end{document}
