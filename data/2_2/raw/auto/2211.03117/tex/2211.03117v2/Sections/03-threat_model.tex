\section{Style Backdoor Attack}
\label{sec:threat}

% \subsection{Attacker's Capabilities}
\subsection{Threat Model}

\textbf{Attacker's Capabilities.}
This work assumes a gray-box threat model where the adversary has access to a small portion of the training data, which can be altered without restriction, but has no knowledge of the training algorithm and the model's architecture. 
Such capabilities are realistic as modern datasets are based on crowdsourcing~\cite{common-voice-dataset} and malicious data may evade security checks~\cite{large-dataset-pyrrhic-win}. 

\textbf{Attacker's Aim.}
The attacker's purpose is to insert a secret functionality into the deployed model, which is activated when the trigger is present in the model's input. Usually, this functionality causes targeted misclassifications with a very high probability. Furthermore, the model should behave normally for any other input to avoid raising suspicions. 

\subsection{Attack Formulation}
Prior studies in backdoor attacks mainly focus on \textit{static triggers}, where the backdoor generation procedure trivially inserts a trigger $\epsilon$ in a part of training samples. 
Such addition is sample-independent.
Formally, let $x$ be a sample, $\epsilon$ a trigger, and the backdoor generative function $\mathcal{F}$ can be described as:
\begin{equation}\label{eq.backdoor_static}
    \mathcal{F}(x, \epsilon) = x + \epsilon.  
\end{equation}
In BadNets~\cite{gu2017badnets} (computer vision domain), the trigger is a patch at a pixel level that replaces a part of the original sample. In audio, a static trigger could be a tone superimposed on the original sample~\cite{can-you-hear-it}.
Static triggers can thus be considered as a \textit{constant} parameter of the attack, as a batch of benign samples $\{x_0, ..., x_n\}$ is transformed in nothing more than $\{x_0 + \epsilon, ..., x_n + \epsilon\}$. 
Thus, the victim's model $\mathbf{C}_{back}$ learns to associate the static pattern $\epsilon$ to a target label $y*$.

In this work, we approach the backdoor attack from an orthogonal perspective: can the trigger be something that the sample \textit{is} rather than something the sample \textit{has}? 
Static backdoors answer the \textit{has} proposition: the poisoned sample contains a specific (and constant) pattern that $\mathbf{C}_{back}$ associates with the target class.
Answering the \textit{is} is more complex: this is how a sample is presented, and we can think of it as a latent variable globally describing that sample. 
In our study, global descriptors are defined by \textit{stylistic properties}. Such properties can be the image exposure or the saturation level (computer vision), the writing complexity (text), or the signal's pitch  (audio).
%For the image domain, a stylistic property might be the image exposure or the saturation level. In the text domain, stylistic properties are the sentiment (e.g., positive or negative) or the writing complexity. 
%In the sound domain, the signal's pitch could potentially be the trigger.
%
% why am I writing this complex thing? Well, we want to somehow demonstrate 
% why a stylistic property can become a backdoor. Otherwise 
%
Once the stylistic trigger $\epsilon$ is identified, we need a function $\mathcal{S}_\epsilon$ that allows embedding such a style to a given sample $x$. Formally, Eq.~\eqref{eq.backdoor_static} changes to:
\begin{equation}\label{eq.backdoor_style}
    \mathcal{F}(x, \mathcal{S}_\epsilon) = \mathcal{S}_\epsilon(x).  
\end{equation}
We explore different $\mathcal{S}_\epsilon$ for speech recognition in the following sections. 
With the formulation given in Eq.~\eqref{eq.backdoor_style}, the batch of adversarial samples$\{\mathcal{S}_\epsilon (x_0), ...,$ $\mathcal{S}_\epsilon (x_n)\}$ now contains dynamic triggers since the outcome of the backdoor generation varies based on inputted sample. 
Now, suppose that $\mathcal{S}_\epsilon$ exists.
The stylistic backdoor is effective if the model $\mathbf{C}_{back}$ learns an association between $\mathcal{S}_\epsilon$ and the target class $y*$.
This problem can be formulated with two sub-questions:
\begin{compactenum}
    \item Can $\mathbf{C}$ learn to recognize the presence of $\mathcal{S}_\epsilon$? We need proof or at least an indication that stylistic properties can be learned by the victim's model.  
    \item Can we let $\mathbf{C}$ learn the association between $\mathcal{S}_\epsilon$ and $y*$? If the previous question is positively answered, we need to understand further how to create the backdoor and what are the possible obstacles at this stage.  
\end{compactenum}
Geirhos et al.~\cite{geirhos2018imagenettrained} answered the first question, showing that CNNs focus more on textures rather than object shapes.
For example, an image with `cat' shapes and `elephant' texture is classified as an elephant. 
Similarly, styles can be learned in the speech domain~\cite{grinstein2018audio, AlBadawy2020}.
% We thus can affirm that a DNN can learn stylistic properties. 

Finally, we need to understand what are the conditions to create the stylistic backdoor in $\mathbf{C}$.
% The poisoning process follows the standard methodology described in Section~\ref{sec:background}. 
In source-agnostic backdoors, the trigger is present only in one of the classes of the training data. 
This condition is easily satisfied by the classic backdoor attacks injecting artifacts as triggers.
Conversely, with stylistic backdoors, such conditions might not always be met. 
For example, high exposure (image domain) and low-frequency tunes (audio domain) might be present in many classes of clean data. 
% For example, in the image domain, a high exposure level might not be an appropriate stylistic trigger since this might be present in many classes of clean data. \todo{why not talk about sound? It would motivate this work better}
% Let us consider the image domain and the high exposure level as the stylistic trigger: it is likely that `high exposure' pictures are a common trait of multiple classes. \todo{so what}
We conclude that the choice of $\mathcal{S}_\epsilon$ impacts attack success. 
In our experiments, we use stylistic functions that are unlikely to be present in the training data.

