\section{Evaluation}

\begin{table*}[h]
    \centering
    \begin{tabular}{ ccccccc|c } 
    
    \hline
    Algorithm   &  A$\rightarrow$W &  A$\rightarrow$D &  D$\rightarrow$W &  D$\rightarrow$A &  W$\rightarrow$A &  W$\rightarrow$D  & Avg    \\
    \hline
    ResNet-50 \cite{he2016deep}    &   68.4\%&            68.9\%&             96.7\%&         62.5\%&         60.7\%&   99.3\%&           76.1\%\\
    DANN \cite{ganin2015unsupervised}         &   82.0\%&            79.7\%&             96.9\%&         68.2\%&         67.4\%&   99.1\%&           82.2\%\\
    MSTN \cite{xie2018learning}        &   91.3\%&            90.4\%&             98.9\%&         72.7\%&         65.6\%&   \textbf{100\%}&   86.5\%\\
    CDAN+E \cite{long2018conditional}      &   94.1\%&            92.9\%&             98.6\%&         71.0\%&         69.3\%&   \textbf{100\%}&   87.7\%\\
    DMRL \cite{wu2020dual}         &   90.8\%&            93.4\%&             99.0\%&         73.0\%&         71.2\%&   \textbf{100\%}&   87.9\%\\
    SymNets \cite{zhang2019domain}      &   90.8\%&            93.9\%&             98.8\%&         74.6\%&         72.5\%&   \textbf{100\%}&   88.4\%\\
    GSDA \cite{hu2020unsupervised}         &   95.7\%&            94.8\%&             99.1\%&         73.5\%&         74.9\%&   \textbf{100\%}&   89.7\%\\
    CAN \cite{kang2019contrastive}         &   94.5\%&            95.0\%&             99.1\%&         78.0\%&         77.0\%&   99.8\%&           90.6\%\\
    SRDC \cite{tang2020unsupervised}        &   95.7\%&            \textbf{95.8\%}&    99.2\%&         76.7\%&         77.1\%&   \textbf{100\%}&   90.8\%\\
    RSDA-MSTN \cite{gu2020spherical}    &   \textbf{96.1\%}&   \textbf{95.8\%}&    99.3\%&         77.4\%&         78.9\%&   \textbf{100\%}&   91.1\%\\
    FixBi \cite{na2021fixbi}       &   \textbf{96.1\%}&   95.0\%&             99.3\%&         78.7\%&         79.4\%&   \textbf{100\%}&   91.4\%\\
    \hline
    MiddleGAN    &   92.4\%&            94.1\%&             \textbf{100\%}&   \textbf{84.9\%}&   \textbf{83.5\%}&   \textbf{100\%}&   \textbf{92.4\%}\\


    \hline
    \\
    \end{tabular}
    \caption{The results on the domain adaptation tasks among the three domains in the dataset Office-31. The metric is accuracy.}

    \label{table:eval_office_31}
\end{table*}

\begin{table*}[h]
    \centering
    \scalebox{0.65}{
    \begin{tabular}{ ccccccccccccc|c } 
    \hline
    Algorithm   & Pr$\rightarrow$Ar & Ar$\rightarrow$Pr & Cl$\rightarrow$Ar & Ar$\rightarrow$Cl & Rw$\rightarrow$Ar & Ar$\rightarrow$Rw & Pr$\rightarrow$Cl & Cl$\rightarrow$Pr& Rw $\rightarrow$ Pr & Pr$\rightarrow$Rw & Rw$\rightarrow$Cl & Cl$\rightarrow$Rw  & Avg    \\
    \hline
    ResNet-50 \cite{he2016deep} & 38.5\% & 50\% & 37.4\% &  34.9\% &  53.9\% &  58\% &  31.2\% &  41.9\% & 59.9\% &  60.4\% &  41.2\% &  46.2\% & 46.1\% \\
    
    DANN \cite{ganin2015unsupervised}        &  41.6\% &  59.3\% &  47.0\% &  45.6\% &  63.2\% &  70.1\% &  43.7\% &  58.5\% &  76.8\% &  68.5\% &  51.8\% &  60.9\% & 57.6\%\\
    CDAN \cite{long2018conditional}       &  55.6\% &  69.3\% &  54.4\% &  49.0\% &  68.4\% &  74.5\% &  48.3\% &  66.0\% &  80.5\% &  75.9\% &  55.4\% &  68.4\% & 63.8\%\\
    MSTN \cite{xie2018learning}       &  61.4\% &  70.3\% &  60.4\% &  49.8\% &  70.9\% &  76.3\% &  48.9\% &  68.5\% &  81.1\% &  75.7\% &  55.0\% &  69.6\% & 65.7\%\\
    SymNets \cite{zhang2019domain}     &  63.6\% &  72.9\% &  64.2\% &  47.7\% &  73.8\% &  78.5\% &  47.6\% &  71.3\% &  82.6\% &  79.4\% &  50.8\% &  74.2\% & 67.2\% \\
    GSDA \cite{hu2020unsupervised}         &  65.0\% &  76.1\% &  65.4\% &  61.3\% &  72.2\% &  79.4\% &  53.2\% &  73.3\% &  83.1\% &  80.0\% &  60.6\% &  74.3\% & 70.3\%\\
    GVB-GD \cite{cui2020gradually}      &  65.2\% &  74.7\% &  64.6\% &  57.0\% &  74.6\% &  79.8\% &  55.1\% &  74.1\% &  84.3\% &  81.0\% &  59.7\% &  74.6\% & 70.4\%\\
    RSDA-MSTN \cite{gu2020spherical}   &  67.9\% &  77.7\% &  66.4\% &  53.2\% &  75.8\% &  \textbf{81.3\%} &  53.0\% &  74.0\% &  85.4\% &  \textbf{82.0\%} &  57.8\% &  76.5\% & 70.9\%\\
    SRDC \cite{tang2020unsupervised}        &  \textbf{68.7\%} &  76.3\% &  \textbf{69.5\%} &  52.3\% &  76.3\% &  81.0\% &  53.8\% &  76.2\% &  85.0\% &  81.7\% &  57.1\% &  78.0\% & 71.3\%\\
    Fixbi \cite{na2021fixbi}      &  65.8\% &  77.3\% &  67.7\% &  58.1\% &  \textbf{76.4\%} &  80.4\% &  57.9\% &  79.5\% &  \textbf{86.7\%} &  81.7\% &  62.9\% &  \textbf{78.1\%} & 72.7\%\\
    


    \hline
    MiddleGAN   & 65.0\% & \textbf{86.9\%} & 63.3\% & \textbf{78.2\%} & 66.2\% & 76.8\% & \textbf{73.8\%} & \textbf{86.4\%} & 86.1\% & 71.5\% & \textbf{75.2\%} & 73.7\% & \textbf{75.3\%} \\
    \hline
    \\
    \end{tabular}
    }
    \caption{The results on the domain adaptation tasks among the four domains in the dataset Office-Home. The metric is accuracy.}

    \label{table:eval_office_home}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{ ccc } 
    \hline
    Algorithm   & SVHN $\rightarrow$ MNIST &  MNIST $\rightarrow$ SVHN     \\
    \hline

    Source Only \cite{french2017self}       &  66.5\%    & 25.4\%  \\ 
    Reverse Grad \cite{bousmalis2017unsupervised}         & 73.9\%     &35.6\% \\
    DCRN \cite{bousmalis2016domain}                 & 81.9\%     & 40.0\% \\
    ADDA \cite{tzeng2017adversarial}                &76.0\%     & - \\
    ATT \cite{ganin2015unsupervised}       & 86.2\%           & 52.8\% \\
    SBADA-GAN \cite{ghifary2016deep}           &76.1\%     & 61.0\% \\
    Mean Teacher \cite{french2017self}        &99.2\%     & 97.0\% \\
    \hline
    MiddleGAN           &\textbf{99.5\%}     & \textbf{99.9\%} \\

    \hline
    \\
    \end{tabular}
    \caption{The results on the domain adaptation task of SVHN $\rightarrow$ MNIST and  MNIST $\rightarrow$ SVHN.}

    \label{table:eval_mnist_svhn}
\end{table*}


\label{sec:evaluation}
In this section, we evaluate the MiddleGAN on the following tasks: CIFAR-10 $\leftrightarrow$ STL-10 (two tasks), MNIST $\leftrightarrow$ USPS (two tasks), MNIST $\leftrightarrow$ SVHH (two tasks), and on two domain adaptation benchmarks Office-31 and Office-Home which contain three domains and four domains, respectively. Therefore, there are 6 domain adaptation tasks derived from Office-31 and 12 domain adaptation tasks derived from Office-Home. On all 24 tasks that we evaluate, MiddleGAN outperforms the state-of-the-art by up to 20.1\% on certain benchmarks.

\subsection{Setups}

\subsubsection{Datasets}
The following datasets are used to evaluate the MiddleGAN.

\textbf{CIFAR-10} \cite{cifar10} contains 10 classes of images that are 32 $\times$ 32 pixels in size. It is a fairly large dataset; each of its classes has 6000 images. Its training set contains 50,000 images and its testing set contains 10,000 images. Accuracy on its testing set is calculated in terms of whether the classifier correctly classifies a sample that is of one of the ten classes of images.

\textbf{STL-10} \cite{coates2011analysis} contains 10 classes of images that are 96 $\times$ 96 pixels in size. It is different from CIFAR-10 by one class. For each of its classes, there are 500 training samples, and 800 testing samples. Accuracy on its testing set is calculated in terms of if the classifier correctly classifies a sample that is one of the ten classes of images. 

\textbf{MNIST} \cite{lecun1998gradient} contains 10 classes of handwritten digits. They are 28 $\times$ 28 pixels in size. There are 60,000 training samples and 10,000 testing samples. Accuracy on its testing set is calculated in terms of if the classifier correctly classifies a sample that is of one of the ten handwritten digits.

\textbf{USPS} \cite{uspsdataset} contains 10 classes of handwritten digits obtained via scanning the envelopes from the USPS. There are 9298 images in total of the 10 classes, and each of them is of size 16 $\times$ 16 pixels. The samples are in grayscale. We have converted it to RGB. Accuracy on its testing set is calculated in terms of if the classifier correctly classifies a sample that is one of the ten handwritten digits.

\textbf{SVHN} \cite{netzer2011reading} stands for Street View House Numbers. It contains 10 classes of digits obtained by street view cameras. There are 600,000 samples of printed images of size 32 $\times$ 32 pixels. Accuracy on its testing set is calculated in terms of whether the classifier correctly classifies a sample that is of one of the ten street view digits.

\textbf{Office-31} \cite{saenko2010adapting} has three domains: Amazon (A), Dslr (D), and Webcam (W). Each domain contains 31 classes of office objects such as  projectors and rulers. In total, there are 4,110 images. Six domain adaptation tasks can be formed from the Office-31 dataset and we evaluate the MiddleGAN against state-of-the-art solutions on all of the domain adaptation tasks. Accuracy on its testing set is calculated in terms of if the classifier correctly classifies a sample that is one of the 31 object classes.

\textbf{Office-Home} \cite{venkateswara2017deep} has four domains: Art (Ar), Clipart (Cl), Real World (Rw), and Product (Pr). Each domain contains 65 classes of objects that can be found in an office or a home, such as flowers and bikes. In total, there are  15,500  images. Twelve domain adaptation tasks can be formed from the Office-Home dataset and we evaluate the MiddleGAN against state-of-the-art solutions on all of the domain adaptation tasks. Accuracy on its testing set is calculated in terms of if the classifier correctly classifies a sample that is one of the 65 object classes.

\subsubsection{Implementation details}
On all 12 domain adaptation tasks, our source discriminator $D_s$, target discriminator $D_t$, and generator $G$ share the same structures. For $D_s$ and $D_t$, we have 5 2D convolutional layers followed by Leaky ReLu layers with a negative slope of 0.2. After each of the 2nd, 3rd, and 4th 2D convolutional layers, a 2D batch norm layer is added. The activation function is Sigmoid. The learning rate of the Adam optimizers for both $D_s$ and $D_t$ are 0.0002. For the generator $G$, its structure contains 5 transposed 2D convolutional layers. After each of the 1st, 2nd, 3rd, and 4th layers, a 2D batch norm layer is added. The activation function is tanh. The learning rate of the Adam optimizer for $G$ is 0.0002. The structures of the discriminators and generator are based on the DCGAN \cite{radford2015unsupervised} (Note that there is only one discriminator in DCGAN and we use the DCGAN's discriminator's structure for both our discriminators). There is no weight decay for any of the three neural nets. Regarding the final classifier trained on the combination of the source training set, the target training set, and the fake images, its architecture is Inception v3 \cite{szegedy2016rethinking, szegedy2017inception}. We train on a NVIDIA A100 GPU. For each domain adaptation task, the number of generated images is empirically determined that result in the final classifier to give the best performance measured in accuracy scores. %For more details, please see our implementation at \href{https://github.com/lindagaw/liara}{https://github.com/lindagaw/liara}.



\subsection{CIFAR-10 $\leftrightarrow$ STL-10}
Table \ref{table:eval_cifar_stl} demonstrates the comparison of the MiddleGAN against 5 state-of-the-art baselines in terms of accuracy: VADA \cite{shu2018dirt}, IIMT \cite{yan2020improve}, Enforced Transfer \cite{gao2022enforced}, SE \cite{french2017self} and SEMA \cite{zuo2021margin}. The Source Only algorithm indicates the performance of training a classifier on the source domain and directly applies it to the target domain without mitigating the domain shift. On the task of CIFAR-10 $\rightarrow$ STL-10, it outperforms the second best-performing algorithm, the Enforced Transfer, by 3.4\%; on the task of STL-10 $\rightarrow$ CIFAR-10, it outperforms the second best-performing algorithm, SEMA, by 12.1\%. The superiority of the MiddleGAN suggests that the fake samples are invariant to domain shift.

\begin{table*}[h]
    \centering
    \begin{tabular}{ ccc } 
    \hline
    Algorithm   & CIFAR-10 $\rightarrow$ STL-10     & STL-10 $\rightarrow$ CIFAR-10\\
    \hline

    Source Only \cite{yan2020improve}       & 75.9\%    & 61.8\%  \\ 
    VADA \cite{shu2018dirt}                 & 80.0\%    & 73.5\% \\
    IIMT \cite{yan2020improve}              & 83.1\%    & 81.6\% \\
    Enforced Transfer \cite{gao2022enforced}     & 86.1\%    & - \\
    SE \cite{french2017self}                & 76.3\%    &83.9\% \\
    SEMA \cite{zuo2021margin}               & 78.7\%    & 86.6\% \\
    \hline
    MiddleGAN                             & \textbf{89.5\%} & \textbf{98.7\%}   \\

    \hline
    \\
    \end{tabular}
    \caption{The results on the domain adaptation task of CIFAR-10 $\rightarrow$ STL-10 and  STL-10 $\rightarrow$ CIFAR-10 of 5 state-of-the-art domain adaptation algorithms and the MiddleGAN. On both tasks, we outperform the second best-performing algorithms by a large margin (3.4\% on CIFAR-10 $\rightarrow$ STL-10 and 12.1\% on STL-10 $\rightarrow$ CIFAR-10), which demonstrates the superiority of the MiddleGAN. The metric is accuracy.}

    \label{table:eval_cifar_stl}
\end{table*}

\subsection{Office-31}
In Table \ref{table:eval_office_31}, we compare the MiddleGAN against ResNet-50 and 10 other state-of-the-art domain adaptation algorithms. Again, the metric is accuracy. Out of the six domain adaptation tasks, we achieve the state-of-the-art performance on three of them. However, note that our improvement over the state-of-the-art algorithms is very significant: On the task D $\rightarrow$ A, we improve over the second best-performing algorithm Fixbi by 6.2\%. On tasks that we do not outperform the state-of-the-art, the difference between the MiddleGAN's performance and the state-of-the-art's performance is usually small. For example, on the task of A $\rightarrow$ D, the state-of-the-art performance is 95.8\% and we are only 1.7\% off. Overall, the MiddleGAN achieves state-of-the-art performance on average, outperforming all the other baselines in comparison.

\subsection{Office-Home}
In Table \ref{table:eval_office_home}, we compare the MiddleGAN against Resnet-50 and 9 state-of-the-art domain adaptation algorithms on the Office-Home dataset. Since there are four sub-domains in the Office-Home dataset, there are in total 12 domain adaptation tasks to be done. Out of the 12 domain adaptation algorithms, the MiddleGAN achieves state-of-the-art performance on 5 of them. When the MiddleGAN achieves state-of-the-art performance on a domain adaptation task, it usually outperforms the second best-performing algorithm by a large margin. For example, on the task of Pr $\rightarrow$ Cl, we outperform the second best-performing algorithm Fixbi by 20.1\%. Overall, the MiddleGAN achieves state-of-the-art performance on average, which is an accuracy score of 75.3\%, 2.6\% higher than the second best-performing algorithm, Fixbi.

%\subsection{MNIST $\leftrightarrow$ USPS}
%For complementary purposes, in this subsection and the next, we show our MiddleGAN's performance on two simpler (simpler than Office-31, Office-Home and CIFAR-10 $\leftrightarrow$ STL-10) domain adaptation tasks. In Table \ref{table:eval_mnist_usps}, we compare the MiddleGAN against five state-of-the-art domain adaptation algorithms. Again, the MiddleGAN achieves the state-of-the-art performance: It achieves 99.5\% on the task of USPS $\rightarrow$ MNIST and 98.3\% on the task of MNIST $\rightarrow$ USPS. On the former task, the MiddleGAN's performance result ties with the performance result of the Mean Teacher. On the latter task, the MiddleGAN's performance is only marginally better than the Mean Teacher, outperforming the Mean Teacher by only 0.1\%. However, it is worth noting that the Mean Teacher as a state-of-the-art algorithm already achieves nearly perfect (nearly 100\%) results in terms of accuracy, and there is not enough room for improvement.

%\begin{table}[h]
%    \centering
%    \begin{tabular}{ ccc } 
%    \hline
%    Algorithm   & USPS $\rightarrow$ MNIST &  MNIST $\rightarrow$ USPS     \\
%    \hline

%    Source Only \cite{french2017self}       & 77.5\%    & 82.0\%  \\ 
%    Reverse Grad \cite{bousmalis2017unsupervised}         &74.0\%     &91.1\% \\
%    DCRN \cite{bousmalis2016domain}                 &73.6\%     & 91.8\% \\
%    ADDA \cite{tzeng2017adversarial}                &90.1\%     & 89.4\% \\
%    SBADA-GAN \cite{ghifary2016deep}           &97.6\%     & 95.0\% \\
%    Mean Teacher \cite{french2017self}        &\textbf{99.5\%}     & 98.2\% \\
%    \hline
%    MiddleGAN           &\textbf{99.5\%}     & %\textbf{98.3\%} \\

%    \hline
%    \\
%    \end{tabular}
%    \caption{The results on the domain adaptation task of USPS $\rightarrow$ MNIST and  MNIST $\rightarrow$ USPS.}

%    \label{table:eval_mnist_usps}
%\end{table}

\subsection{MNIST $\leftrightarrow$ SVHN}
In Table \ref{table:eval_mnist_svhn} we compare the MiddleGAN against six state-of-the-art algorithms and we observe that the MiddleGAN achieves the new state-of-the-art performance on both SVHN $\rightarrow$ MNIST and MNIST $\rightarrow$ SVHN: on the first task, it achieves an accuracy score of 99.5\% and on the second task an accuracy score of 99.9\%. Both scores are nearly 100\%. Compared to the second best-performing algorithm, the Mean Teacher, the MiddleGAN only achieves an improvement of 0.3\% on the first task. This is because there is not enough room for improvement, considering that the Mean Teacher already achieves an accuracy score of 99.2\% on the first task. On the second task, the second best-performing algorithm the Mean Teacher achieves an accuracy score of 97.0\%, and there is more room for improvement since it is not nearly 100\%. As a result, on the second task, we outperform the Mean Teacher by 2.9\%, a more significant improvement compared to our improvement on the first task.


%\subsection{The Distribution of Generated Images}
%In this Section, we display the t-SNE of the distribution of the generated images when compared with the distributions of the source and target domains - see Figure \ref{fig:t_sne}. A  key observation from \ref{fig:t_sne} is that the euclidean distance between the centroid of the generated samples to the centroid of the source samples is roughly equal to the euclidean distance between the centroid of the generated samples to the centroid of the target samples.

%\begin{figure}
%\centering
%\includegraphics[width=0.5\textwidth]{figures/tsne_middle.png}
%\caption{ The t-SNE plot for generated samples based on the class "airplane" in Amazon in Office-31 and Webcam in Office-31. The red dots are samples from the source domain. The green dots are samples from the target domain.}
%\label{fig:t_sne}
%\end{figure}

