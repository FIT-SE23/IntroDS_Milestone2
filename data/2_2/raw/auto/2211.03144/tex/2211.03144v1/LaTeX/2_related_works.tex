\section{Related Works}
In this Section we present two important areas of research related to our work: the Generative Adversarial Nets (GANs), and recent advances in domain adaptation.

\subsection{Generative Adversarial Nets}
Goodfellow et al. \cite{goodfellow2014generative} propose the original GAN which consists of two neural nets: the discriminator and the generator. The two nets engage in a minimax game where the generator attempts to generate images from noise to fool the discriminator, while the discriminator attempts to distinguish generated images from real ones. %\hnnote{It is necessary to explain why/how the following works are related to us.} 
Inspired by this work, works such as the CGAN \cite{mirza2014conditional} and ACGAN \cite{odena2017conditional} attempt to regulate the classes of the generated images. The CGAN \cite{mirza2014conditional} is constructed in the way that, during training, the label information is fed to both the generator and the discriminator. The discriminator of the ACGAN \cite{odena2017conditional} has two objective functions: to maximize the log likelihood that a given sample is of the correct source (generated or real), and to maximize the log likelihood that the label (which class is this sample from) of the sample is correct. Moving past the GANs that leverage label information, another set of GANs focus on cycle consistency of the generated images. For example, the CycleGAN \cite{zhu2017unpaired} employs two generators, one to translate an image from the source to the target and the other to translate back a translated image by the first generator. A cycle consistency loss is added to minimize the discrepancy between an original, unaltered image, and the image translated by the first generator and then translated back by the second generator. StarGAN \cite{choi2018stargan} addresses the scalability issue that different GAN models need to be created for all pairs of domains. Unlike the previous works \cite{mirza2014conditional, odena2017conditional} which use only one generator, the MiddleGAN employs two discriminators and one generator and aims to generate samples that are similar to both the source domain samples and the target domain samples. MiddleGAN is a GAN designed specifically for Domain Adaptation while the other previous works on GAN mentioned in this section seek to generate realistic samples or achieve style transfer.

\subsection{Domain Adaptation}
One trend in the area of domain adaptation is to find domain-invariant features. DANN \cite{ganin2016domain} proposes a new neural network architecture based on the traditional feed-forward architecture by adding extra layers and a gradient reversal layer. Through adversarial training, it is able to find domain-invariant features and correctly classify source and target samples based on these features. Another way to find domain invariant features is to reduce the Maximum Mean Discrepancy (MMD) \cite{gretton2012kernel} between the feature representations of the source samples and the feature representations of the target samples. Tzeng et al. \cite{tzeng2014deep} and Long et al. \cite{long2015learning} both reduce the MMD while training a model to perform well on the source domain. A variation of this trend is to find the association of the source features and target features so that the domain discrepancy on the learned features is reduced, such as Haeusser et al \cite{haeusser2017associative}. Another variation of this trend is to use a generator/encoder to encode target samples so they are projected to the source feature space and, therefore, can be classified by the source model \cite{tzeng2017adversarial}. Other works that focus on finding domain-invariant features that are discriminative to the classification task are: \cite{gopalan2011domain, sun2016deep, long2017deep, ganin2015unsupervised}. However, the aforementioned approaches fail to address the scenario in which the discrepancy of the source and target is very large because domain-invariant features will be harder to find. As a result, another new trend is to find the middle domains of the source and target domains. Fixbi \cite{na2021fixbi} proposes to establish one source-dominant domain and one-target dominant domain as two intermediate domains to help bridge the gap between the source and target domain. Unlike the MiddleGAN, Fixbi is not a generative model so it does not generate domain invariant samples using the two intermediate domains. Instead, it uses the two intermediate domains to train a source-dominant model and a target-dominant model via bi-directional feature matching. The bi-directional feature matching guarantees that the parameters of source-dominant model and target-dominant model converge. Other (unsupervised) domain adaptation algorithm include RSDA \cite{gu2020spherical}, which proposes an adversarial domain adaptation scheme in the spherical feature space, SRDC \cite{tang2020unsupervised}, which proposes to achieve unsupervised domain adaptation via structurally regularized deep clustering, CAN \cite{kang2019contrastive} proposes to explicitly model intra-class and inter-class domain discrepancy. SEMA \cite{zuo2021margin} attempts to address the issue that most domain adaptation algorithms ignore the discriminative features among classes. The Enforced Transfer \cite{gao2022enforced} is based on the idea that some target samples closer to the distribution of the source domain should be directly processed by the source classifier, instead of training a target classifier to process them.

%Compared to the state-of-the-art domain adaptation algorithms, our MiddleGAN is one of a kind by not relying on explicitly specified domain-invariant samples. Instead, it focus 

