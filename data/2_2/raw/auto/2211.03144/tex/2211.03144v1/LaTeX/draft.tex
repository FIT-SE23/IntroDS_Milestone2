\subsection{Compared with State-of-the-Art Baselines on Two Popular domain adaptation Benchmarks (Handwritten and Street View Digits)}

%\begin{figure}
%    \centering
%  \begin{subfigure}[b]{0.2\textwidth}
%    \includegraphics[width=\textwidth]{graphs/mnist example.PNG}
%    \caption{MNIST examples}
%    \label{fig:1}
%  \end{subfigure}

%  \begin{subfigure}[b]{0.2\textwidth}
%    \includegraphics[width=\textwidth]{graphs/svhn example.PNG}
%    \caption{SVHN examples}
%    \label{fig:2}
%  \end{subfigure}

%  \begin{subfigure}[b]{0.2\textwidth}
%    \includegraphics[width=\textwidth]{graphs/usps example.PNG}
%    \caption{USPS examples}
%    \label{fig:2}
%  \end{subfigure}
%  \caption{six examples from the MNIST dataset \cite{lecun1998gradient}, six examples from the SVHN dataset \cite{netzer2011reading}, and six examples from the USPS dataset \cite{uspsdataset}.}
%\end{figure}


\begin{table*}[h!]
\begin{center}
    \begin{tabular}{ ccc } 
    \hline
    Algorithm & MNIST $\rightarrow$ USPS & SVHN $\rightarrow$ MNIST \\
    \hline
    Source only         & 75.2\%        & 60.1\% \\
    Gradient Reversal \cite{ganin2015unsupervised}   & 77.1\%        & 73.9\% \\
    Domain Confusion \cite{tzeng2015simultaneous}   & 79.1\%        & 68.1\% \\
    CoDAN \cite{liu2016coupled}               & 91.2\%        & did not converge \\
    ADDA \cite{tzeng2017adversarial}                & 89.4\%        & 76.0\% \\
    Associative \cite{haeusser2017associative}        & 94.1\%        & 93.6\% \\
    DANN \cite{ganin2016domain}                & 60.8\%        & 76.3\% \\
    Deep Coral \cite{sun2016deep}          & 69.5\%        & 76.3\% \\
    VADA \cite{shu2018dirt}                & 90.6\%        & 92.6\% \\
    ET         & \textbf{95.4\%}        & \textbf{95.4\%} \\
    \hline
    \\
    \\
    \end{tabular}
    
    \caption{We compare our technique, Enforced Transfer, with nine other state-of-the-art deep domain adaptation techniques on two tasks (the performance is measured in accuracy, per the evaluation standard of the computer vision community).}
    
    \label{tab:eval_compare_against_baselines}
\end{center}
\end{table*}

\begin{table}[h]
    \centering
    \begin{tabular}{ ccc } 
    \hline
                 &  MNIST $\rightarrow$ USPS    & SVHN $\rightarrow$ MNIST\\
    \hline
    after 1st conv       & 95.41\%          & 80.30\%   \\ 
    after 2nd conv       & 80.88\%          & 95.43\%   \\ 
    \hline
    \\
    \end{tabular}
    \caption{The two potential Enforced Transfer architectures' performance on the two domain adaptation tasks. We observe that the optimal convolutional layer after which we should construct the Enforced Transfer for both tasks is after the first convolutional layer. The metric is accuracy.}
    \label{table:eval_individual_digits}
\end{table}


In Table \ref{tab:eval_compare_against_baselines}, we compare our technique, the Enforced Transfer, with eight other state-of-the-art deep domain adaptation techniques on two pupular domain adaptation benchmarks: the first one is to domain-adapt the MNIST dataset to the USPS dataset, while the other is to domain-adapt the SVHN dataset to MNIST dataset. 

MNIST, USPS, and SVHN datasets all have ten classes of hand-written digits. The source classifier for the two domain adaptation tasks share the same architecture - a modified LeNet architecture. The discriminator we use has the same architecture as Tzeng et al., whereas there are 3 fully connected layers and each of the two hidden layers have 500 neurons with ReLU as the activation function and the output layer has one that produces the discriminator's final decision on if the sample that resulted in its given input is more likely to be in the source or target domains.

The first task, which is to domain-adapt from MNIST to USPS, is considered easier while the second task, which is to domain-adapt from SVHN to MNIST, is considered more challenging. This claim is supported by the observation that, in Table \ref{tab:eval_compare_against_baselines}, five out of the state-of-the-art domain adaptation algorithms and the baseline of directly using source classifier on the target dataset results in lower performance of the second task compared to the first task. The Enforced Transfer achieve an accuracy of 95.41\% on the first task and 95.43\% on the second. On the first task, it outperforms the best-performing state-of-the-art baseline, Associative, by 1.31\%. On the second task, it outpeforms the best-performing state-of-the-art baseline by 2.83\%. %This indicates that our approach is generic enough to be applied on general domain adaptation tasks as it works in both the acoustic domain and the computer vision domain. We hypothesize that the small improvement occurs because this task already has very high performance and it is a relatively simple vision task. 

The source classifier has two convolutional layers and the Table  \ref{table:eval_individual_digits} shows the two potential Enforced Transfer architectures constructed after the two convolutional layers and their performances on the two tasks. We observe that for the first task, after the first convolutional layer, the Enforced Transfer results in a Enforced Transfer architecture that achieves the maximal performance (an accuracy score of 95.41\%). For the second task, after the second convolutional layer, the Enforced Transfer result in a Enforced Transfer architecture that achieves the maximal performance (an accuracy score of 95.43\%).

\subsection{Compared with State-of-the-Art Baselines on Another Popular domain adaptation Benchmark (Non-digits)}

%\begin{figure}
%    \centering
%  \begin{subfigure}[b]{0.2\textwidth}
%    \includegraphics[width=\textwidth]{graphs/CIFAR-10.png}
%    \caption{CIFAR-10 examples}
%    \label{fig:1}
%  \end{subfigure}
%  \quad
%  \begin{subfigure}[b]{0.2\textwidth}
%    \includegraphics[width=\textwidth]{graphs/STL-10.png}
%    \caption{STL-10 examples}
%    \label{fig:2}
%  \end{subfigure}
%  \caption{Nine examples from the CIFAR-10 dataset \cite{krizhevsky2009learning} and nine examples from the STL-10 dataset \cite{coates2011analysis}. The classes are: "cat" (the first row), "deer" (the second row), and "dog" (the third row).}
%\end{figure}

\subsubsection{STL $\rightarrow$ CIFAR-10}
%In the previous Section, we showed that the Enforced Transfer outperforms 8 state-of-the-art baselines on domain adaptation on digits. 
In this section, we further investigate if the Enforced Transfer can outperform state-of-the-art baselines on a more complicated vision task that is not digits. Therefore, we transfer learn from STL-10 to CIFAR-10. Both CIFAR-10 and STL-10 are image datasets that contain 10 classes. %The two datasets differ by one class so we remove that class from our training and testing purpose. As a result, we are dealing with domain adapting from a dataset with 9 classes to a dataset with 9 classes, as did the five state-of-the-art baselines that we compare against. 
We outperform all the other five state-of-the-art deep domain adaptation baselines and outperform the second best-performing algorithm, Co-DA, by 9.7\%. The source classifier that we use to train is ResNet-50 \cite{he2016deep}. We yield the highest performance of an accuracy score of 86.1\% after we inject the Enforced Transfer Cell after the fifth layer.

Note that, compared to the previous section, we choose a different set of baselines to fully evaluate our solution, the Enforced Transfer, against as many baselines as possible. The task to domain-adapt from STL-10 to CIFAR-10, which is more complex than domain-adapt among MNIST, SVHN, and USPS, as these three datasets contain only digits. On the contrary, CIFAR-10 and STL-10 contains images such as the automobile and dog classes.

\begin{table}[h!]
\begin{center}
    \begin{tabular}{ cc } 
    \hline
    Algorithm & STL-10 $\rightarrow$ CIFAR-10 \\
    \hline
    DRCN \cite{ghifary2016deep}                 & 58.6\%        \\
    SE \cite{french2017self}       & 64.2\%        \\
    Source only                                 & 63.6\%        \\
    VADA \cite{shu2018dirt}                     & 75.3\%        \\
    Co-DA \cite{kumar2018co}                    & 76.4\%        \\
    DTA \cite{kumar2018co}                      & 72.8\%        \\
    ET                                          & \textbf{86.1\%}        \\
    \hline
    \\
    \\
    \end{tabular}
    
    \caption{We compare our technique, Enforced Transfer, with five other state-of-the-art deep domain adaptation techniques on the domain adaptation task to domain-adapt from STL-10 to CIFAR-10 (the performance is measured in accuracy, per the evaluation standard of the computer vision community).}
    
    \label{tab:eval_compare_against_baselines_cifar_stl}
\end{center}
\end{table}

%\subsection{What about Even More Complex domain adaptation Tasks?}

%We have considered further testing of the Enforced Transfer on a new dataset, the Office-Home dataset, which is much more complex than digit recognition such as the datasets CIFAR-10 and STL-10. The Office-Home dataset \cite{venkateswara2017deep} has four subsets: Art, Clipart, Product, and Real World (Objects' Pictures), each of which has 65 classes of objects that can be found in homes and offices such as a spoon, sink, etc. We have considered using each of the subsets as the source dataset and the others as target datasets (for example, using the Art subset as the source dataset and use the Enforced Transfer three times - each time with one of the other three subsets as the target dataset). However, the Enforced Transfer's assumption is that we have a good source model, and one of the state-of-the-art models for image recognition, VGG-13 \cite{simonyan2014very}, only achieves an accuracy of less than 20\% on the Product subset. Since training a new classifier that achieves high performance on the Office-Home dataset is out of the scope of this paper, we have not tested the Enforced Transfer on this dataset.

\subsubsection{Office-31}

\subsubsection{Office-Home}