\section{Discussion and Future Work}

\subsection{Threats to Validity}

As in any empirical study, the greatest threats to our claims lie in the correctness and generality of our experimental results, which affect the internal validity and external validity, respectively.

\noindent\textbf{Internal Validity}\ \ The peculiar difficulty in our experiments is that the compared fuzzers share the same understructure.
In Section~\ref{sec:banditcomparison}, the fuzzers are all variations of \OurMethodName-AFL++, with the only difference between them being the bandit algorithm employed.
Similarly, in Section~\ref{sec:evaluation}, the fuzzers are implemented upon AFL++; hence, the seed scheduling or mutation operators themselves are identical.
As we showed in these sections, these fuzzers wholly exhibited performance improvement to some extent, and consequently, it was sometimes difficult to observe evident differences in the quality of their improvement.
To avoid missing these small differences to the extent feasible, we tried to analyze the results of the experiments comprehensively or statistically (as recommended in \cite{FuzzReliability}) while increasing the number of instances of fuzzers and PUTs, within time and resource constraints.
At the same time, we were aware that excessive use of statistical tests can cause multiple testing problems \cite{MultipleComparison, MultipleComparisonVis}, which may lead to spurious conclusions. To avoid these, while finding significant differences as much as possible, we cautiously selected the differences to apply statistical tests to and corrected the resultant p-values using the Holm-Bonferroni method \cite{Holm}.
Another threat to internal validity is the wrong implementation of fuzzers. While implementing them, we referred to their original source code if it was available.
Moreover, for each algorithm, we set up an implementer and a reviewer to ensure that the original algorithm was reproduced. 

\noindent\textbf{External Validity}\ \ To mitigate the threat of the tested PUTs being biased, we ensured the use of various types of real-world PUTs in the evaluations in Section~\ref{sec:evaluation}. For this purpose, we selected PUTs from well-known benchmarks FuzzBench and MAGMA. While we did not adopt all the PUTs in FuzzBench to keep the experimental cost reasonable, we randomly selected 10 PUTs, which accept a wide variety of input file formats as a result.
Another concern regarding external validity is the baseline of the fuzzers. Through all the experiments, we consistently set AFL++ as the suitable baseline because the mutation of AFL++ now consists solely of \textit{havoc} by default \cite{AFLppChangelog}, and AFL++ has high performance among such fuzzers on average according to past benchmarks \cite{FuzzbenchReport}, which is very important for obtaining more rewards and distinguishing optimization methods.
It is possible that the selection of different baselines leads to different conclusions. However, we believe similar results will appear as long as the mutation scheme of a fuzzer is the same because the random mutation \textit{havoc} dominates the performance of the fuzzer, as studied by Wu et al. \cite{HavocMAB}.

\subsection{Correlation of Arms}

One factor that we have not considered when viewing fuzzing as a bandit problem is the correlation between the arms.
For example, in a PUT that accepts only an input consisting of ASCII printable characters, mutation operators that tend to produce non-ASCII printable characters are not considered good choices.
Alternatively, in a PUT that has a limit on the length of inputs to be accepted, mutations such as inserting a constant string or copying a partial byte sequence from another seed are not promising arms. 

While the distributions of rewards are assumed to be independent in standard stochastic bandit problems, there are studies on such problem settings where the arms are correlated, aimed at reducing regret further compared to the standard settings \cite{BanditCorrArm, BanditDepArm, NewAppCorrMAB}.
Even assuming independence, bandit algorithms can greatly improve the efficiency of the fuzzer.
Nonetheless, they suggest the possibility of further improvement by considering this correlation.
