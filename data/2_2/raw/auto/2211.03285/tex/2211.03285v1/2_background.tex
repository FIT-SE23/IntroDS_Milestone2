\section{Background}

\subsection{Mutation-Based Fuzzing}

Given a target program (program under test; PUT) and a set of initial test cases for the program, mutation-based fuzzers run in a semi-permanent loop, generating inputs and feeding them to the program.
In particular, in each iteration of the loop, the fuzzers perform the following steps.

\noindent\textbf{Seed scheduling}\ \ First, the fuzzers choose a source test case to generate a new input and the number of inputs to be generated.
In fuzzing, the test cases are called seeds because the chosen test case works as a sample of the new inputs, and the policy of selecting seeds is called \textit{seed scheduling}.
Common seed-scheduling methods include selecting seeds from a seed queue in order or at random.

\noindent\textbf{Mutation}\ \ The fuzzers then modify the chosen seed the determined number of times to create a new input.
In many cases, modifications made to the seed are selected from a predetermined set of operations, such as bit flipping or insertion of a constant byte sequence.
These operations are known as \textit{mutation operators}. 
Usually, fuzzers also decide which position in the input to apply the mutation operator and the other arguments required by the mutation operator.

\noindent\textbf{Execution and Update}\ \ After generating an input, the fuzzers execute the PUT and obtain feedback as a result.
In greybox fuzzing, for example, they can acquire code coverage, by performing lightweight instrumentation on the PUT.
If they find the new input valuable through feedback, they save it as a seed.
When the fuzzers employ dynamic optimizations in their scheduling or mutation, they then update the components based on feedback.

Thus, mutation-based fuzzers have many choices to make in their fuzzing process, which affect the efficiency of the fuzzers.

\subsection{Multi-Armed Bandit Problem}

The multi-armed bandit problem is analogous to playing slot machines, whose goal is to find the most profitable arm (choice) from a fixed set of multiple arms and keep pulling (selecting) it as many times as possible.
Formally, a decision maker is given $K$ probability distributions over real numbers, and in each round, the decision maker chooses one of the distributions and is rewarded with a real number sampled from that distribution.
The goal is to maximize the sum of rewards obtained as much as possible over $T$ rounds, without knowing the parameters of the $K$ probability distributions.

Despite the simplicity of its problem formulation, there are many variations; in the most basic variation (i.e., a regular stochastic bandit problem), the $K$ probability distributions are defined as independent Bernoulli distributions.
However, they can be substituted by other distributions such as normal distributions in other settings.
The non-stationary stochastic bandit problem assumes the probability distributions of the rewards to change with time.
The adversarial bandit problem assumes the most extreme situation, where the rewards given by the arms in $T$ rounds are arbitrary and not assumed to follow any probability distribution.

In the study of the application of bandit algorithms to fuzzing, rewards are often assumed to take only the discrete value of $0$ or $1$.
This is because the plausible rewards settable in fuzzing are whether a new crash, code block, or execution path is found by executing a PUT.
Our study is no exception: we use the discovery of execution paths as a reward.
Note that our proposed method is valid even if the reward is replaced by the discovery of crashes or code blocks.
