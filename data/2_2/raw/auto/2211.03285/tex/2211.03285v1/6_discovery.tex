\section{Vulnerability Discovery}

In addition to the evaluations in Section~\ref{sec:evaluation}, we ran \OurMethodName-AFL++ on large real-world programs for a longer period of time.
The purpose of this trial is to see if employing \OurMethodName{} is valuable in practice for identifying bugs, because we found in Section~\ref{subsec:magma} that improving code coverage does not immediately lead to a drastic increase in bugs detected.
Therefore, we ran 10 instances of \OurMethodName-AFL++ for 7 days on 12 PUTs (listed in Table~\ref{tab:commit-ids}) that were manually selected from OSS-Fuzz.
The selection criteria for PUTs were that the PUT must be large in code size so that fuzzers are likely to continuously find new code blocks (thus preventing saturation) and that the project of the PUT must be active and contactable in cases where vulnerabilities are found.
The configuration of each PUT, including initial seeds and dictionaries, was unchanged from those used in the infrastructure of OSS-Fuzz, and the execution environment was the same as Section~\ref{subsec:eval-setup}.

Through manual triage, we found that \OurMethodName-AFL++ discovered 17 bugs, as listed in Table~\ref{tab:7d-bug}. Although we do not deny the possibility of AFL++ being able to find these bugs, we must note that AFL++ has already been fuzzing the PUTs for an enormous number of CPU days on the infrastructure of OSS-Fuzz (estimated to be more than hundreds of CPU years) \cite{OSSFuzzPage, OSSFuzzInFuzzCon}, and thus the possibility is extremely low. In fact, three of these bugs were confirmed as unseen vulnerabilities and were assigned CVE IDs. We believe this result supports the practicability of online optimization methods for code coverage in real-world software testing.

