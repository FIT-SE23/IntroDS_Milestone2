\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out. 
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{url}
\usepackage{bbm}

\usepackage{fancyhdr}

% My code
\usepackage[hidelinks]{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  %linkcolor    = blue, %Colour of internal links
  citecolor   = green %Colour of citations
}
% End of my code

\begin{document}
% \title{Improving hand pose estimation with hand kinematic model based new objective functions*\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% \thanks{This material is based upon work supported by the Air Force Office of Scientific Research under award number FA2386-20-1-4053}
% }
\title{LG-Hand: Advancing 3D Hand Pose Estimation with Locally and Globally Kinematic Knowledge*\\
\thanks{This research is funded by Hanoi University of Science and Technology (HUST) under project number T2021-SAHEP-003.}}
% LE Xuân Tú, Trân Quang Trung, Đoan Thị Ngọc Hiền, Thanh-Hai Tran
% \author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% }

\author{\IEEEauthorblockN{Tu Le-Xuan\IEEEauthorrefmark{1}\textsuperscript{\textsection},
Trung Tran-Quang\IEEEauthorrefmark{3}\textsuperscript{\textsection}, Thi Ngoc Hien Doan\IEEEauthorrefmark{1}\IEEEauthorrefmark{2} and
Thanh-Hai Tran\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}}
\\
\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Electronic Engineering (SEEE), Hanoi University of Science and Technology, Hanoi, Vietnam \\
\IEEEauthorrefmark{2}MICA International Research Institute, Hanoi University of Science and Technology, Hanoi, Vietnam \\
\IEEEauthorrefmark{3}Research and Development Team, Asilla Inc., Tokyo, Japan \\
% \IEEEauthorrefmark{4}information 4 \\
Email: \textit{letu071299@gmail.com}, \textit{tranquangtrunghnvn@gmail.com}, \textit{\{hien.doanthingoc,hai.tranthithanh1\}@hust.edu.vn}
}
}
\maketitle
\begingroup\renewcommand\thefootnote{\textsection}
\footnotetext{Equal contribution. This research was done when Tu was an intern at Research and Development Team, Asilla Inc.}
\endgroup

% My code
%\thispagestyle{plain}
%\pagestyle{plain}
\thispagestyle{fancy}
\pagestyle{fancy}
\fancyhf{}
%\chead{2020 25th International Conference on Pattern Recognition (ICPR)}
\lhead{The 14th IEEE International Conference on Knowledge and Systems Engineering (KSE 2022)}
\rhead{October 19-21, 2022}
%\rhead{Overleaf}
%\lhead{Guides and tutorials}
%\cfoot{Page \thepage}
\cfoot{\thepage}
% End of my code

\begin{abstract}
3D hand pose estimation from RGB images suffers from the difficulty of obtaining the depth information. Therefore, a great deal of attention has been spent on estimating 3D hand pose from 2D hand joints. In this paper, we leverage the advantage of spatial-temporal Graph Convolutional Neural Networks and propose LG-Hand, a powerful method for 3D hand pose estimation. Our method incorporates both spatial and temporal dependencies into a single process. We argue that kinematic information plays an important role, contributing to the performance of 3D hand pose estimation. We thereby introduce two new objective functions, Angle and Direction loss, to take the hand structure into account. While Angle loss covers locally kinematic information, Direction loss handles globally kinematic one. Our LG-Hand achieves promising results on the First-Person Hand Action Benchmark (FPHAB) dataset. We also perform an ablation study to show the efficacy of the two proposed objective functions.
\end{abstract}

\begin{IEEEkeywords}
hand pose estimation, kinematic knowledge, spatial-temporal graph, local and global constraints
\end{IEEEkeywords}

\section{Introduction}

Hand is one of vital parts of human, serving a wide range of actions such as holding, writing, or handshaking. Hand pose estimation, an interesting task of computer vision, thereby plays a critical role in real-world applications. These applications include robotics, human-machine interaction, and entertainment. When it comes from 2D to 3D prediction, both hand and human pose estimation suffer from depth ambiguity \cite{li2019generating} and multi-view variation \cite{dong2019fast}. The hand, a deformable object with a high degree of freedom, is also prune to be occluded as it interacts with the world, so 3D hand pose estimation still remains challenging.

Most of existing methods try to solve 3D hand pose estimation by firstly localizing joints in a 2D space, then lifting 3D joints from estimated 2D joints \cite{garcia2018first,ge2016robust,panteleris2018using,doosti2020hope,cai2019exploiting}. The depth information can be used for 3D hand joints prediction, as shown in \cite{garcia2018first,ge2016robust}. Especially, Liuhao et al. \cite{ge2016robust} utilize the depth image to project 3D points onto three orthogonal planes and then regresses multi-view 2D heatmaps. These heatmaps are finally fused together to estimate 3D hand joint coordinates. However, the depth information is limited in outdoor scenarios and even hard to obtain. Therefore, many efforts have been paid to estimate 3D hand pose from a single RGB image, as proposed in \cite{panteleris2018using,doosti2020hope,cai2019exploiting}. These three methods use off-the-shelf techniques for 2D hand pose estimation and then build 3D estimators to predict 3D hand joints without using the depth information. While Paschalis et al. \cite{panteleris2018using} use non-linear least-squares minimization to fit a 3D hand model to the estimated 2D joints, HOPE-Net \cite{doosti2020hope} and Yujun et al. \cite{cai2019exploiting} take the advantage of Graph Convolutional Neural Networks to reconstruct 3D hand joints from 2D coordinates. Instead of using a spatial graph as in \cite{doosti2020hope}, Yujun et al. \cite{cai2019exploiting} propose exploiting spatial-temporal Graph Convolutional Neural Networks to alleviate the issue of depth ambiguity. However, geometric constraints of hand structure (bone length for example) are not taken into account. Therefore, SST-GCN \cite{le2021sst} introduces a new partial loss function, the consistency of the finger bone length, to better estimate 3D hand joints.

\begin{figure}[h!]
\centerline{\includegraphics[width=0.8\linewidth]{figs/Fig5.png}}
\caption{Example of weird predictions even when the finger bone length is the same (the yellow finger) or the angles of the knuckles are matched (the red finger).}
\label{fig_weird_hand}
\end{figure}

\begin{figure*}[h!]
\centerline{\includegraphics[width=1.0\linewidth]{figs/Fig1.png}}
\caption{Overall framework of LG-Hand. Our method takes as input a sequence of 2D hand skeletons and outputs 3D hand joint coordinates.}
\label{fig_overall_framework}
\end{figure*}

We argue that the consistency of the finger bone length is not enough for supporting 3D hand pose estimation. For example, when the ground-truth and predicted finger have the same bone length, we are still not sure that the predicted finger matches the ground-truth one. The predicted finger might also look weird, as illustrated by the yellow finger in Fig. \ref{fig_weird_hand}. Motivated by this observation, we propose LG-Hand, a method based on spatial-temporal Graph Convolutional Neural Networks, for 3D hand pose estimation. We introduce locally and globally kinematic constraints to make the predicted 3D hand joints more accurate. Concretely, we encourage the model to learn the angle of the knuckles within a finger, which could be referred to as local information. However, two fingers can still be different even if they have the same angles of the knuckles, as depicted by the red finger in Fig. \ref{fig_weird_hand}. To solve this issue, we thereby add the constraints of the direction of the knuckles, which could be regarded as global information. Our key contributions are summarized as follows:
\begin{itemize}
    \item We propose LG-Hand, an end-to-end framework to estimate 3D hand joints from 2D hand joints.
    \item We introduce new objective functions to model locally and globally kinematic constraints.
    \item We obtain promising results on FPHAB dataset, including the average MPJPE of 17.25 (mm).
\end{itemize}

\section{Related Work}

3D hand joints prediction could be obtained by using magnetic sensors or single RGB images. This section presents the methods for 3D hand pose estimation using RGB images. Especially, Graph Convolutional Neural (GCN) Networks are also reviewed because they directly relate to our method.

% \noindent
\textit{\textbf{3D hand pose estimation from 2D images.}} HOPE-Net \cite{doosti2020hope} utilized off-the-shell 2D pose estimators to generate 2D joints and then introduced Adaptive Graph U-Net architecture to match the hand from 2D to 3D space. Liuhao et al. developed a GCN-based method to generate 3D hand mesh from a single RGB image \cite{ge20193d}. A dataset with 3D annotations was created to train the model in a suppervised manner. The model was then fine-tuned on real-world datasets without 3D annotations. Especially, 3D depth maps were generated from rendered 3D meshes to monitor the training process.

% \noindent
\textit{\textbf{3D hand pose estimation from 2D joints.}} Julieta et al. \cite{martinez2017simple} directly obtained 3D hand joints from 2D coordinates by using conventional components such as linear layers, batch normalization, dropout, and Relu activations. On the other hand, Christian and Thomas \cite{zimmermann2017learning} first generated score maps with respect to hand joint locations. They then used a PosePrior network to predict 3D hand joint coordinates.

% \noindent
\textit{\textbf{Graph Convolutional Neural Networks.}} While conventional convolutions only handle the grid-based data such as images, there are various types of data in reality. These types of data include social data or user interaction. Conventional convolutions showed the limitation in addressing non-grid data. Therefore, GCN Networks were proposed to deal with this problem. We also utilize GCN Networks in our method, which will be presented in Section \ref{sec_method}.

\section{Methodology}
\label{sec_method}

This section first presents the overall framework of LG-Hand especially the procedue of constructing an input hand sequence. Next, we present Graph Convolutional Neural (GCN) Networks to process input hand sequences. We then describe the local-to-global network, one of important parts of our method for multi-scale training. Finally, the objective function, including our locally and globally kinematic constraints, will be shown in detail. Note that we call the method, proposed by Yujun et al. \cite{cai2019exploiting}, ST-GCN for a convenient explanation.  

\subsection{Overall Framework}

\begin{figure*}[h!]
\centerline{\includegraphics[width=1.0\linewidth]{figs/Fig9.png}}
\caption{The architecture of GCN-based local-to-global network.}
\label{fig_local_to_global_network}
\end{figure*}

The overall framework of LG-Hand is illustrated in Fig. \ref{fig_overall_framework}. LG-Hand takes as input a sequence of 2D hand skeletons of consecutive frames and outputs the 3D hand joints of the target frame. Concretely, the predicted 2D hand joints of consecutive frames (..., $\Gamma_{t-2}$, $\Gamma_{t-1}$, $\Gamma_t$, $\Gamma_{t+1}$, $\Gamma_{t+2}$, ...) are combined into a spatial-temporal graph and then fed into a GCN-based local-to-global network, which is described in Fig. \ref{fig_local_to_global_network}, to produce 3D hand joints of the frame $\Gamma_t$. At the end of the pipeline, we propose two new objective functions, called Angle and Direction loss, to leverage kinematic characteristics. The GCN-based local-to-global network and the objective function will be elaborated in Section \ref{sec_local_to_global_net} and \ref{sec_objective_function}, respectively. The remainder of this section presents the way to construct the spatial-temporal graph from the sequence of 2D hand skeletons.

\begin{figure}[h!]
\centerline{\includegraphics[width=0.65\linewidth]{figs/STGraph.png}}
\caption{Illustration of spatial-temporal graph.}
\label{fig_st_graph}
\end{figure}

From a sequence of skeletons with $N$ joints and $T$ frames, we construct an undirected spatial-temporal graph $G=(V,E,A)$ as in Fig. \ref{fig_st_graph}, where $V$ and $E$ are the set of nodes and edges of the graph respectively, and $A$ denotes the adjacency matrix. To build the graph $G$, we first connect the joints of one frame following the natural connections of the human hand. Next, the joints of the same type are connected between the consecutive frames. By doing so, our method can work with an arbitraty amount of joints. The set of nodes contains all joints of all input frames: $V=\{v_{ti}: t\in (1,..,T), i\in (1,...,N)\}$. The set of edges is divided into two subsets, $E_p=\{v_{ti}v_{tj}\}$ and $E_s=\{v_{ti}v_{(t+1)i}\}$, for the natural connections within one frame and the connections of the same joints between consecutive frames, respectively. Note that $t$ depicts the frame index, $i$ and $j$ are for the joint index. Finally, the adjacency matrix $A$ is defined as: $A=(a_{ij})_{M\times M}$ with $M=N\times T$. $a_{ij}$ is equal to $0$ if the connection $(i,j)$ is not in $E$. Otherwise, $a_{ij}$ is equal to $1$. 

\subsection{Graph Convolutional Neural Networks for 3D hand pose estimation}

For Graph Convolutional Neural (GCN) Networks, Kipf and Welling \cite{kipf2016semi} proposed:
\begin{equation}
    Z=\widetilde{D}^{-\frac{1}{2}}\widetilde{A}\widetilde{D}^{-\frac{1}{2}}X\Theta\label{equation_graph}
\end{equation}
where $X\in\mathbb{R}^{N\times C}$ is the input signal representing $C$-dimensional features of $N$ nodes on the graph, $\Theta\in\mathbb{R}^{C\times F}$ is the matrix of filter parameters with $F$ is the number of filters, $\widetilde{A}$ and $\widetilde{D}$ are the normalized versions of the adjacency matrix and degree matrix respectively ($\widetilde{A}=A+I_N$, $\widetilde{D}^{ii}=\sum_{j}\widetilde{A}^{ij}$, and $I_N$ is the identity matrix), and $Z\in\mathbb{R}^{N\times F}$ is the convolved signal matrix.

\begin{figure}[h!]
\centerline{\includegraphics[width=0.75\linewidth]{figs/Fig3.jpg}}
\caption{Illustration of neighboring nodes.}
\label{fig_node_classification}
\end{figure}

Equation (\ref{equation_graph}) can be extended for hand pose estimation. First, the neighboring nodes are divided into five groups according to their position relative to the central node, as illustrated in Fig. \ref{fig_node_classification}. These five groups include a central node (green), a time-forward node (orange), a time-backward node (purple), physically-connected nodes containing the one closer (yellow) to and the one further (blue) from the skeleton root (red). GCN Networks are thereby extended to:
\begin{equation}
    Z=\sum_k D_k^{-\frac{1}{2}}A_k D_k^{-\frac{1}{2}}X\Theta_k
\end{equation}
where $k$ is the index of the neighbor types, and $\Theta_k$ is the filter matrix for the $k$-th type with 1-hop neighboring nodes. Notably, the normalized adjacency matrix $\widetilde{A}$ is dismantled into $k$ sub-matrices with $\widetilde{A}=\sum_k A_k$ and $D_k^{ii}=\sum_j A_k^{ij}$.

\subsection{GCN-Based Local-to-Global Network}
\label{sec_local_to_global_net}

GCN-based local-to-global network plays an important role in our method, being responsible for processing and combining the features at different scales, as described in Fig. \ref{fig_local_to_global_network}. The pipeline can be split into two stages: bottom-up and top-down. At the bottom-up stage, the network takes as input the spatial-temporal graph and uses graph convolutional and graph pooling layers to extract the features. The later top-down stage conducts upsampling and combining the upsampled features with the high-resolution ones from the bottom layers. To effectively remain the information learned in the bottom-up stage, an element-wise concatenation is applied for the same-scale features of the bottom-up and top-down stage. Finally, a non-local block is used before generating 3D hand pose sequences to support the full hand reconstruction.

\begin{figure}[h!]
\centerline{\includegraphics[width=0.9\linewidth]{figs/Fig2.png}}
\caption{Illustration of graph pooling and upsampling for hand pose estimation. The same color depicts the nodes from the same group.}
\label{fig_graph_pooling}
\end{figure}

One of the important parts of GCN-based local-to-global network is graph pooling and upsampling, as illustrated in Fig. \ref{fig_graph_pooling}. The 21 hand joints are first split into groups according to the palm and the fingers. A max pooling operation is then applied to each group, resulting in a smaller graph with 6 nodes, where each node represents the local information of a region of the hand. An extra max pooling operation is applied to the 6-node graph, leading to a 1-node graph. This 1-node graph contains the global information of the hand. In contrast, upsampling operation is responsible for converting a sparse graph to a full graph, as described in Fig. \ref{fig_graph_pooling}.

\subsection{Objective Function}
\label{sec_objective_function}

ST-GCN \cite{cai2019exploiting} and SST-GCN \cite{le2021sst} both utilize spatial-temporal Graph Convolutional Neural Networks for 3D hand pose estimation, but their predictions might produce weird fingers, as illustrated in Fig. \ref{fig_weird_hand}. The fingers might violate the naturally kinematic characteristics of the hand. Inspired by this issue, we propose two new objective functions, being Angle and Direction loss, to remain the kinematic characteristics of the predicted hand. Let $U_t=\{u_{ti}:i\in (1,...,P)\}$ denote the set of $P$ knuckles of the ground-truth hand of the $t$-th frame. Let $U'_t=\{u'_{ti}:i\in (1,...,P)\}$ denote the set of $P$ knuckles of the predicted hand of the $t$-th frame. When computing the angle between two knuckles, we build the knuckle vectors $\overrightarrow{u}_{ti}$ and $\overrightarrow{u'}_{ti}$ by assigning the direction to the knuckles, as illustrated in Fig. \ref{fig_angle_direction_loss}. Angle and Direction loss are formulated as follows.

\begin{figure}[h!]
\centerline{\includegraphics[width=1.0\linewidth]{figs/Fig4.png}}
\caption{Illustration of vectors for computing Angle and Direction loss.}
\label{fig_angle_direction_loss}
\end{figure}

\textbf{Angle loss:}
\begin{equation}
    \mathcal{L}_a = \sum_{t=1}^{T}\sum_{i=1}^{P}\sum_{j=1}^{P}f(i,j)\|\widehat{(\overrightarrow{u}_{ti},\overrightarrow{u}_{tj})}-\widehat{(\overrightarrow{u'}_{ti},\overrightarrow{u'}_{tj})}\|_2
\end{equation}
where $f(i,j)$ is equal to 1 if two knuckles are consecutive. Otherwise, $f(i,j)$ is equal to 0.

\textbf{Direction loss:}
\begin{equation}
    \mathcal{L}_d = \sum_{t=1}^{T}\sum_{i=1}^{P}\widehat{(\overrightarrow{u}_{ti},\overrightarrow{u'}_{ti})}
\end{equation}

In addition to the proposed loss functions, we also use the 3D pose loss and finger length loss as follows.

\textbf{3D pose loss:}
\begin{equation}
    \mathcal{L}_p = \sum_{t=1}^{T}\sum_{i=1}^{N}\|{x}_{ti}-\widehat{x}_{ti}\|_2
\end{equation}
where $x_{ti}$ and $\widehat{x}_{ti}$ are the ground-truth and predicted 3D coordinates of the $i$-th hand joint of the $t$-th frame, respectively.

\textbf{Finger length loss:}
\begin{equation}
    \mathcal{L}_f = \sum_{t=1}^{T}\sum_{i=1}^{L}\|{p}_{ti}-\widehat{p}_{ti}\|_2
\end{equation}
where $p_{ti}$ and $\widehat{p}_{ti}$ are the ground-truth and predicted finger length of the $i$-th finger of the $t$-th frame, respectively.

\textbf{Overall objective function} is defined as:
\begin{equation}
    \mathcal{L} = \lambda_p \mathcal{L}_p + \lambda_f \mathcal{L}_f + \lambda_a \mathcal{L}_a + \lambda_d \mathcal{L}_d
\end{equation}
where $\lambda_p$, $\lambda_f$, $\lambda_a$, and $\lambda_d$ are the loss weights.

\section{Experiments}

We evaluate LG-Hand on First-Person Hand Action Benchmark (FPHAB) dataset \cite{garcia2018first}. Our method is compared with ST-GCN \cite{cai2019exploiting} and SST-GCN \cite{le2021sst}. Notably, we train and evaluate the methods using a same codebase for fair comparisons.

\subsection{Training Details}

FPHAB dataset is a large-scale dataset, consisting more than $100K$ frames and $1175$ action sequences . These $1175$ sequences are divided into $45$ action classes, including interactions with $26$ objects at various places such as office or kitchen. 3D hand joint coordinates are created by using 6 magnetic sensors attached to the hand joints. Each hand has $21$ joints. %, including $1$ wrist and $20$ joints from the fingers. 
In FPHAB dataset, each action consists of multiple sequences, and each sequence has a different length. We use the $3$-rd sequence for evaluation and the rest for training.

\begin{table*}[h!]
\caption{Comparison of the methods on FPHAB dataset. All results are MPJPE (mm). All methods are implemented using the same codebase.}
\begin{center}
\begin{tabular}{|l||c|c|c|c|c|c|c|c|c|c|c|c||c|}
\hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
 & \textbf{was.} & \textbf{unfo.} & \textbf{ope.} & \textbf{rea.} & \textbf{tea.} & \textbf{put.} & \textbf{lig.} & \textbf{toa.} & \textbf{fli.} & \textbf{clo.} & \textbf{use.} & \textbf{squ.} & \textbf{Avg.} \\
 \hline
 ST-GCN \cite{cai2019exploiting} & 18.21 & 16.44 & 20.09 & 22.88 & 20.22 & 22.60 & 18.39 & 21.61 & 18.60 & 17.05 & 34.63 & 21.96 & 20.25 \\
\hline
SST-GCN \cite{le2021sst} & 17.77 & 15.60 & 17.18 & 20.24 & 19.74 & 22.19 & 17.37 & 21.11 & 18.28 & 16.59 & 34.10 & 20.50 & 19.97 \\
\hline
LG-Hand (\textbf{Ours}) & \textbf{13.51} & \textbf{13.39} & \textbf{16.18} & \textbf{16.41} & \textbf{14.84} & \textbf{20.02} & \textbf{15.06} & \textbf{17.81} & \textbf{15.86} & \textbf{14.96} & \textbf{30.02} & \textbf{15.54} & \textbf{17.25} \\
\hline
\end{tabular}
\label{table_main_result}
\end{center}
\end{table*}

The models are trained for $30$ epochs with a batch size of $256$. We use Adam optimizer. The initial learning rate is set to $1e-3$. The learning rate is decreased by a factor of $0.95$ per epoch. Notably, a learning rate decay of $0.5$ is used after each 10 epochs. We use $3$ frames ($T=3$) in our experiments. For the loss weights, we set: $\lambda_p=1$, $\lambda_f=0.1$, $\lambda_a=0.1$, and $\lambda_d=0.01$. All experiments have been done on the hardware using GeForce GTX 1080 GPU and CUDA 11.0. The evaluation metric is Mean Per Joint Position Error (MPJPE), measuring the average Euclidean distance from prediction to ground-truth joint positions.

\subsection{Main Results}

MPJPE measures the average error between the predicted 3D hand joints and the ground-truth 3D hand joints. The results are shown in Table \ref{table_main_result}. Overall, LG-Hand obtains a best result compared to ST-GCN and SST-GCN. Concretely, LG-Hand reduces the overall MPJPE, which is MPJPE over all actions, by 3.00 and 2.72 mm compared to ST-GCN and SST-GCN, as presented in the last column of Table \ref{table_main_result}. Due to the limited space, we only presents the results of 12 actions as in Table \ref{table_main_result}. The result of each action also indicates that LG-Hand outperforms both ST-GCN and SST-GCN. For example with the ``squeeze paper" (``squ.") action, LG-Hand strongly reduces MPJPE by 6.42 and 4.96 mm compared to ST-GCN and SST-GCN, respectively.

\begin{figure}[h!]
\centerline{\includegraphics[width=1.0\linewidth]{figs/Fig11.png}}
\caption{MPJPE (mm) across different parts of the hand.}
\label{fig_joint_finger_error}
\end{figure}

We elaborate the effect of our Angle and Direction loss by measuring MPJPE across different parts of the hand, as illustrated in Fig. \ref{fig_joint_finger_error}. For each method, MPJPEs across the fingers are similar while MPJPEs across the types of joints have a larger gap. This observation is understandable because the fingers play the similar roles while the different types of joints have different connections. For example, wrist connects to 5 MCP-joints of 5 fingers while PIP-joint connects to MCP-joint and DIP-joint on each finger. Especially, the results show that LG-Hand significantly reduces MPJPE of all parts of the hand compared to ST-GCN and SST-GCN.

\subsection{Qualitative Results}

\begin{figure}[h!]
\centerline{\includegraphics[width=1.0\linewidth]{figs/Fig14_1.png}}
\caption{Visualization of the predicted 3D hand joints of the methods for the ``pour wine" action.}
\label{fig_visualization}
\end{figure}

In the sequence of predicted 3D hand joints for the ``pour wine" action, we take the $60$-th frame for visualization in 3D space, as described in Fig. \ref{fig_visualization}.
\begin{itemize}
    \item The results of ST-GCN and SST-GCN show weird predictions such as the ring finger and the pinky finger. These fingers violate the kinematic characteristics of the hand.
    \item By using Angle and Direction loss, LG-Hand is able to produce the better results, where no weird prediction appears.
    \item We also display MPJPE, Angle loss, and Direction loss of the methods as in Fig. \ref{fig_visualization}. LG-Hand obtains the smallest error compared to both ST-GCN and SST-GCN.
\end{itemize}

\begin{figure}[h!]
\centerline{\includegraphics[width=1.0\linewidth]{figs/Fig12.png}}
\caption{MPJPE, Angle loss, and Direction loss of the methods along a sequence of the ``squeeze paper" action.}
\label{fig_visualization_loss}
\end{figure}

We also examine the tendency of MPJPE, Angle loss, and Direction loss along the sequence of the ``squeeze paper" action, as illustrated in Fig. \ref{fig_visualization_loss}. Compared to ST-GCN and SST-GCN, our LG-Hand has the smaller values of the losses, and our losses tend to strongly decrease along the sequence.

\subsection{Ablation Study}

\subsubsection{Loss weights}
 
We examine the effect of each loss element by varying the loss weights, as shown in Table \ref{table_ablation_loss_weight}. The results are reported on FPHAB dataset with 45 actions. We first train LG-Hand with using only 3D pose loss, and the result is presented in the first row of Table \ref{table_ablation_loss_weight}. We then gradually add Finger length loss, Angle loss, and Direction loss to see how the model behaves. The results show that the model performance is significantly improved by using all four loss functions. For instance, the best setting, the last row of Table \ref{table_ablation_loss_weight}, exhibits a MPJPE improvement of 3.00 mm compared to only using 3D pose loss.

\begin{table}[h!]
\caption{Ablation study with the loss weights.}
\begin{center}
\begin{tabular}{|c|c|c|c||c|}
\hline
\textbf{$\lambda_p$} & \textbf{$\lambda_f$} & \textbf{$\lambda_a$} & \textbf{$\lambda_d$} & \textbf{MPJPE (mm)} \\ 
 \hline
 1 & 0 & 0 & 0 & 20.25 \\
\hline
1 & 0.1 & 0 & 0 & 19.97 \\
\hline
1 & 0.1 & 0.1 & 0 & 18.57 \\
\hline
1 & 0.1 & 0.1 & 0.1 & 18.77 \\
\hline
1 & 0.1 & 0.1 & 0.01 & \textbf{17.25} \\
\hline
\end{tabular}
\label{table_ablation_loss_weight}
\end{center}
\end{table}

\begin{figure}[h!]
\centerline{\includegraphics[width=0.8\linewidth]{figs/threshold.png}}
\caption{The percentage of correct 3D hand pose on FPHAB dataset.}
\label{fig_pcp}
\end{figure}

\subsubsection{Percentage of correct 3D hand pose}

To elaborate the performance of LG-Hand, we compute the percentage of correct 3D hand poses for various thresholds (measured in millimeters) on FPHAB dataset. A prediction is correct if MPJPE between the ground-truth and predicted 3D hand pose is less than the predefined threshold. As shown in Fig. \ref{fig_pcp}, LG-Hand outperforms SST-GCN and ST-GCN with all thresholds.

\subsubsection{Number of skeleton frames}

\begin{table}[h!]
\caption{Ablation study with the number of skeleton frames.}
\begin{center}
\begin{tabular}{|l||c|c|c|c|c|c|}
\hline
\textbf{\# of frames} & 3 & 5 & 7 & 9 & 11 & 13 \\ 
\hline
\textbf{MPJPE (mm)} & 17.25 & 16.83 & 17.83 & 16.69 & 16.62 & 17.42 \\
\hline
\end{tabular}
\label{table_ablation_n_skeleton_frames}
\end{center}
\end{table}

We examine the behavior of LG-Hand with various amounts of skeleton frames, as shown in Table \ref{table_ablation_n_skeleton_frames}. There is no benefit when increasing the number of skeleton frames. Therefore, LG-Hand uses 3 skeleton frames for a computational efficiency.

% \subsubsection{Another data setting}

% To elaborate the performance of LG-Hand, we conduct the comparisons with another data setting. Instead of using the ground-truth 2D hand joints as input, we train ST-GCN, SST-GCN, and LG-Hand with using the prediction of HOPE-Net as input. The results are shown in Talbe \ref{table_hopenet_output}. Note that the result of HOPE-Net is computed by using the ground-truth 3D hand joints from FPHAB dataset. Table \ref{table_hopenet_output} shows that LG-Hand obtains the best result which is 51.25 MPJPE, improving 8.12, 0.58, and 0.25 points compared to HOPE-Net, ST-GCN, and SST-GCN, respectively.

% \begin{table}[h!]
% \caption{Comparisons of the methods with the training data coming from the outputs of HOPE-Net.}
% \begin{center}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
%  & \textbf{HOPE-Net} & \textbf{ST-GCN} & \textbf{SST-GCN} & \textbf{LG-Hand} \\ 
%  \hline
% \textbf{MPJPE (mm)} & 59.37 & 51.83 & 51.50 & \textbf{51.25} \\
% \hline
% \end{tabular}
% \label{table_hopenet_output}
% \end{center}
% \end{table}

\section{Conclusion}

In this paper, we propose LG-Hand, a spatial-temporal GCN-based method for 3D hand pose estimation. We take as input a sequence of consecutive frames of 2D hand joints and output 3D hand joint coordinates. We especially introduce Angle and Direction loss, which can be known as local and global constraint respectively, to incorporate the kinematic information of the hand into the overall objective function. The experimental results show that LG-Hand surpasses the previous methods such as ST-GCN and SST-GCN. For future work, we are interested in researching more objective functions related to the kinematic constraints of the hand to further improve the performance of 3D hand pose estimation.

% \section{FORMULA}
% Proposed methodology
% % \item G = (V, E, A)
% % \item V = ${V_t_i$}

% \section{Ease of Use}

% \subsection{Maintaining the Integrity of the Specifications}

% The IEEEtran class file is used to format your paper and style the text. All margins, 
% column widths, line spaces, and text fonts are prescribed; please do not 
% alter them. You may note peculiarities. For example, the head margin
% measures proportionately more than is customary. This measurement 
% and others are deliberate, using specifications that anticipate your paper 
% as one part of the entire proceedings, and not as an independent document. 
% Please do not revise any of the current designations.

% \section{Prepare Your Paper Before Styling}
% Before you begin to format your paper, first write and save the content as a 
% separate text file. Complete all content and organizational editing before 
% formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
% proofreading, spelling and grammar.

% Keep your text and graphic files separate until after the text has been 
% formatted and styled. Do not number text heads---{\LaTeX} will do that 
% for you.

% \subsection{Abbreviations and Acronyms}\label{AA}
% Define abbreviations and acronyms the first time they are used in the text, 
% even after they have been defined in the abstract. Abbreviations such as 
% IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
% abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}
% \begin{itemize}
% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
% \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
% \end{itemize}

% \subsection{Equations}
% Number equations consecutively. To make your 
% equations more compact, you may use the solidus (~/~), the exp function, or 
% appropriate exponents. Italicize Roman symbols for quantities and variables, 
% but not Greek symbols. Use a long dash rather than a hyphen for a minus 
% sign. Punctuate equations with commas or periods when they are part of a 
% sentence, as in:
% \begin{equation}
% a+b=\alpha\label{eq}
% \end{equation}

% Be sure that the 
% symbols in your equation have been defined before or immediately following 
% the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
% the beginning of a sentence: ``Equation \eqref{eq} is . . .''

% \subsection{\LaTeX-Specific Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is \cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{The class file is designed for, but not limited to, six authors.} A 
% minimum of one author is required for all conference articles. Author names 
% should be listed starting from left to right and then moving down to the 
% next line. This is the author sequence that will be used in future citations 
% and by indexing services. Names should not be listed in columns nor group by 
% affiliation. Please keep your affiliations as succinct as possible (for 
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Figure captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% % \begin{figure}[htbp]
% % \centerline{\includegraphics{fig1.png}}
% % \caption{Example of a figure caption.}
% % \label{fig}
% % \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.

\bibliographystyle{unsrt}
\bibliography{bibliography}

% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \end{thebibliography}
% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.


\end{document}
