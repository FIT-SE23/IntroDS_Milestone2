\section{Bicriteria Approximation: Inflating the Budget}
\label{sec:budget inflation}

% Consider the case that the state is perfectly revealed if the agent gets an informative signal.

% Let $Z\subseteq [n]$ be the subset of tasks that can possibly be incentivized given bounded scoring rules, 
% i.e., $\cost_i \leq \frac{p}{2}$. 
% This is directly implied by \cref{lem:single opt} and \cref{lem:monotone task}.
In this section, we show that there exists a proper 
truncated separate scoring rule with a constant budget 
that achieves higher value for the principal
than the optimal mechanism with budget $1$. 
Specifically, we show that by inflating the budget by a constant factor, 
the principal is able to attain at least the optimal objective value given budget~$1$
with relaxed incentive constraints. 

The approximation mechanism we design for the knapsack scoring problem uses the (approximately) optimal solution for the knapsack problem as a blackbox. 
Note that for general submodular valuations, computing the optimal solution for $\vopt$ is NP-hard. 
The following lemma shows that there exists a polynomial time algorithm to get an $\sfrac{e}{(e-1)}$-approximation.
\begin{lemma}[\citealp*{sviridenko2004note}]
\label{lem:submodular greedy}
For submodular valuation $\val$, there exists a polynomial time algorithm that computes a feasible solution $\effortset$
such that $\val(\effortset)\geq (1-\sfrac{1}{e})\vopt$.
\end{lemma}
% \begin{lemma}[\citealp*{sviridenko2004note}]
% \label{lem:submodular greedy}
% For submodular valuation $\val$, 
% the greedy algorithm, which runs in polynomial time, 
% can compute a solution $\effortset$
% such that $\sum_{i\in \effortset} \cost_i \leq 2$
% and $\val(\effortset)\geq (1-\sfrac{1}{e})\vopt$.
% \end{lemma}
% \begin{align*}

% \fknapsack = \max_{x\in[0,1]^n} \quad& \sum_{i\in [n]} \val_i x_i\\
% \text{s.t.} \quad& \sum_{i\in [n]} x_i \cost_i \leq 1.
% \end{align*}
% It is easy to see that $\fknapsack\geq \opt$.


% \begin{figure}[htbp]
%     \centering
%     \fbox{
%     \parbox{\textwidth}{

%     \textbf{Truncated Scoring Mechanism} for additive values with budget $11$
%     \begin{itemize}
%         \item Recommendation set $\effortset$
        
%         Greedily include tasks by value-cost ratio with a budget $\frac{3}{2}$ on the total cost. 
        
%         \item Scoring rule $\score$: truncated scoring rule
%         \begin{itemize}
%             \item For each assignment $i\in \effortset$, let the budget-minimal scoring rule be $\hat{\score}_i$.
            
%             Posting single dimensional scoring rules:
%                 \begin{align*}
% \score_i(\signal_i,\outcome_i)=\frac{9}{8}\hat{\score}_i(\signal_i,\outcome_i) = 
% \begin{cases}
% \frac{9\cost_i}{8p_i} & \signal = \bot\\
% \frac{9\cost_i}{4p_i}\cdot \indicate{\signal_i=\outcome_i}  & \text{otherwise}
% \end{cases}
% \end{align*}
%             \item Sum over the single dimensional scores, and truncate back to $[0, 11]$:
%             \begin{align*}
%                 \score=\max\lbr{0, \min\lbr{11, \sum_i\score_i-d}},
%             \end{align*}
%             where $d=-\frac{11}{2}+\frac{9}{8}\sum_{i\in\effortset} \frac{\cost_i}{p_i}$ is the shift on the sum. 
%         \end{itemize}
%     \end{itemize}
%     }
%     }
%     \caption{Truncated Scoring Mechanism.}
%     \label{fig:truncated scoring mechanism}
% \end{figure}


\begin{figure}[t]
    \centering
    \fbox{
    \parbox{0.96\textwidth}{

    \textbf{Truncated Scoring Mechanism} for additive values with budget $11$
    
    Post the truncated scoring rule on a recommendation set $\effortset$
    
        \begin{itemize}
            \item For each assignment $i\in \effortset$, let the budget-minimal scoring rule be $\hat{\score}_i$.
            
            Posting single dimensional scoring rules:
                \begin{align*}
\score_i(\signal_i,\outcome_i)=\frac{9}{8}\hat{\score}_i(\signal_i,\outcome_i) = 
\begin{cases}
\frac{9\cost_i}{8p_i} & \signal = \bot\\
\frac{9\cost_i}{4p_i}\cdot \indicate{\signal_i=\outcome_i}  & \text{otherwise}
\end{cases}
\end{align*}
            \item Sum over the single dimensional scores, and truncate back to $[0, 11]$:
            \begin{align*}
                \score=\max\lbr{0, \min\lbr{11, \sum_i\score_i-d}},
            \end{align*}
            where $d=-\frac{11}{2}+\frac{9}{8}\sum_{i\in\effortset} \frac{\cost_i}{p_i}$ is the shift on the sum. 
            \end{itemize}

    }
    }
    \caption{Truncated Scoring Mechanism.}
    \label{fig:truncated scoring mechanism}
\end{figure}

\begin{figure}[t]
     \centering
    \fbox{
    \parbox{0.96\textwidth}{

\textbf{Recommendation set $\effortset$} for truncated scoring mechanism
        
        Input: ground set $\groundset$
        
        Output: set $\effortset$
        
        Greedily include tasks from $\groundset$ to $\effortset$, by value-cost ratio with a budget $\frac{3}{2}$ on the total cost. 
    }
    }
    \caption{Procedure for identifying optimal recommendation set for truncated scoring mechanism.}
    \label{fig:truncated scoring set}
\end{figure}
\begin{theorem}\label{thm:budget inflation}
% For any profile of values $\{\val_i\}_{i=1}^n$ and costs $\{\cost_i\}_{i=1}^n$, 
The truncated scoring mechanism in \Cref{fig:truncated scoring mechanism} with a budget $B=11$
 guarantees value at least 
the optimal knapsack value (\vopt). 
% such that the value of the principal is at least the optimal objective value given budget~$1$. 
Moreover,  for submodular values, 
there is a polynomial time algorithm for computing the recommendation set $\effortset$, which attains an $\sfrac{e}{(e-1)}$-approximation.
\end{theorem}

The main idea is that with multiple tasks, the sum of the scores on different tasks concentrates
around its expectation. 
Therefore, we can take the sum of the scores and shift it such that the expected score of not exerting any effort is only one half of the budget $11$. 
Moreover, with an inflated budget,
we can ensure that the ex post shifted sum remains in the range of $[0,11]$ with high probability,
and hence the agent's incentive is almost aligned with his incentive in separate scoring rules without the truncation. 
This allows us to show that the designed truncated separate scoring rule is proper, 
and the agent has the incentive to follow the recommendation. 
The detailed proof of the theorem is provided in \cref{apx:proof budget inflation}.



%\input{Notes/opt_cover}