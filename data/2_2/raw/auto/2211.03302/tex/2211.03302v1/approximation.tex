\section{Value Approximation}
\label{sec:approximation}

In this section, we show that the better of 
a truncated separate scoring rule
and a threshold scoring rule 
is a constant approximation to the optimal value of the knapsack scoring problem (\opt). 
The idea is to divide the set of tasks into two subsets based on whether the sum of
optimal individual single-dimensional scoring rule concentrates, 
and then design approximately optimal scoring rule for each subset separately. 
This concept is analogous to the core-tail decomposition adopted for multi-item auctions \citep*{BILW-20}, 
while the details for proving the results are quite different.

The first case is to consider tasks such that their costs are small 
compared to their probabilities of revealing the state when the agent exerts effort. 
In this case, the budget required for incentivizing each single task is small.
Thus, analogous to \cref{thm:budget inflation},
the variance of the score for incentivizing each task separately is small
and the sum of the scores concentrates well given the total budget 1. 
This implies that the ex post sum is close to its expectation with high probability.
By truncating the sum of optimal single-dimensional scoring rules to comply with the ex post budget constraint, 
the incentives of the agent for exerting effort is barely affected, 
and we obtain a constant approximation to the knapsack solution in this case. 

The second case is to consider tasks such that their costs are large 
compared to their probabilities of revealing the states when the agent exerts effort.
Unlike the traditional knapsack problem where large costs on the tasks indicate the existence of a single task with valuation close to the optimal, 
in the effort incentivization problem, 
there still exists the hard case where in the optimal mechanism, the agent need to be incentivized to exert effort on a large number of tasks and each task only contributes to a small fraction of the optimal objective value. 
Moreover, since the probabilities of revealing the states are small, 
the expected number of tasks on which the agent receives informative signals is small 
and hence the sum of scores may not concentrate. 
Alternatively, we show that in this case, 
the score of the agent has to be close to the budget if 
he receives an informative signal on any task. 
Therefore, to incentivize the agent to exert effort on any task $i$, 
the total probability that the agent gets an informative signal on any task $i'\neq i$ cannot be too large
because otherwise the principal will not have enough budget to incentivize task $i$ after rewarding the agent for acquiring an informative signal on task $i'$. 
Thus, an upper bound is imposed on the sum of probabilities
for the set of incentivizable tasks. 
We find a set of tasks that can be incentivized by a threshold scoring rule through a greedy algorithm on the ratio of the value to the probability,
and show that the value of this set is a constant approximation to the value given by the optimal scoring rule. 
% By adopting greedy solution on the ratio of the value and the probability to find a set of incentivizable tasks 
% and use the threshold scoring rule, 
% we are able to attain a constant approximation to the optimal. 

% Moreover, the extra budget is not sufficient to incentivize additional tasks beyond a constant factor. 
% Thus we show that the threshold scoring rule is a constant approximation to the optimal objective value in this case. 

% \begin{figure}
%     \centering
%     \fbox{
%     \parbox{\textwidth}{

%     \textbf{Threshold Scoring Mechanism} for additive values with budget $1$
%     \begin{itemize}
%         \item Recommendation set $\effortset$
        
%         For each task $j$ in the ground set $Y_2$:
%         \begin{itemize}
%             \item  initialize by adding $j$ into the recommendation $\effortset^j=\{j\}$;
%             \item update the ground set $Y_2$:
%             $Y^j_2=\{i\in Y_2\mid 1-\frac{2\cost_j}{p_j}+p_j\leq 1-\frac{2\cost_j}{p_i}+p_j\}$;
            
%             \item greedily include tasks from $Y_2^j$ by the value-probability ratio $\frac{\val_i}{p_i}$ with a  budget\\
%             $\sum_{j\in \effortset^j}p_i\leq 1-\frac{2\cost_j}{p_j}+p_j$;
%             \item Consider set $\effortset'^j=\{j, j^*\}$, where $j^*=\argmax_{i\in Y_2^j}\val(i)$ is the most valuable task.
            
%             Take the better of the knapsack solution and the set $\effortset'^j$
            
%         \end{itemize}
        
%         Output the set with maximum value: $\effortset=\argmax_{\effortset^j}\val(\effortset^j)$.
        
%         \item Scoring rule $\score$
%         \begin{itemize}
%             \item Score $1$ if both (1) at least $1$ reported signal is informative; and (2) any task reported signal that is informative is correct.
%             \item Score $0$ otherwise.
%         \end{itemize}
%     \end{itemize}
%     }
%     }
%     \caption{Threshold Scoring Mechanism.}
%     \label{fig:threshold scoring mechanism}
% \end{figure}
\begin{figure}[t]
    \centering
    \fbox{
    \parbox{0.96\textwidth}{

    \textbf{Threshold Scoring Mechanism} for additive values with budget $1$ 
    
    
    
    Post the threshold scoring rule on a recommendation set $\effortset$
        \begin{itemize}
            \item Score $1$ if both (1) at least $1$ reported signal in $\effortset$ is informative; and (2) any task reported signal that is informative is correct.
            \item Score $0$ otherwise.
        \end{itemize}
    }
    }
    \caption{Threshold Scoring Mechanism.}
    \label{fig:threshold scoring mechanism}
\end{figure}
\begin{figure}[t]
    \centering
    \fbox{
    \parbox{0.96\textwidth}{

    \textbf{Recommendation set $\effortset$} for threshold scoring mechanism
    
    Input: ground set $\groundset$.
    
        For each task $j$ in the ground set $\groundset$:
        \begin{itemize}
            \item  initialize by adding $j$ into the recommendation $\effortset^j=\{j\}$;
            \item update the ground set $\groundset$:
            $\groundset^j=\{i\in \groundset\mid 1-\frac{2\cost_j}{p_j}+p_j\leq 1-\frac{2\cost_j}{p_i}+p_j\}$;
            
            \item greedily include tasks from $\groundset^j$ by the value-probability ratio $\frac{\val_i}{p_i}$ with a  budget\\
            $\sum_{j\in \effortset^j}p_i\leq 1-\frac{2\cost_j}{p_j}+p_j$;
            \item Consider set $\effortset'^j=\{j, j^*\}$, where $j^*=\argmax_{i\in \groundset^j}\val(i)$ is the most valuable task.
            
            Take the better of the knapsack solution and the set $\effortset'^j$
            
        \end{itemize}
        
        Output the set with maximum value: $\effortset=\argmax_{\effortset^j}\val(\effortset^j)$.
    }
    }
    \caption{Procedure for identifying approximately optimal recommendation set.}
    \label{fig:recommendation set static}
\end{figure}


\begin{theorem}\label{thm:approximation}
The better of 
a truncated separate scoring rule
and a threshold scoring rule 
%such that the value of the principal 
is a $1091$-approximation to the optimal value of the knapsack scoring problem (\opt).
Moreover, for additive values, the parameters of such mechanism can be computed in polynomial time, 
and for submodular values, 
there is a polynomial time algorithm for computing the parameters that loses an additional multiplicative factor of $\sfrac{e}{(e-1)}$ in approximation ratio.
\end{theorem}

We first show an upper bound on the sum of state revelation probabilities for each set of incentivizable tasks 
when the ratio of the cost to the probability for any task in this set is large. 


\begin{lemma}\label{lem:asym-upper-bound}
For any set $\effortset\subseteq[n]$ such that
$p_i\leq \frac{1}{4}$ and
$\frac{2\cost_i}{p_i}\geq \frac{15}{16}$ for all tasks $i\in\effortset$, 
if the set $\effortset$ can be incentivized by a proper scoring rule with budget $1$, there exists a budget-pivotal task $i^*=\argmin_{i\in\effortset}\frac{16}{3}\rbr{1-\frac{2\cost_{i^*}}{p_{i^*}}}+p_{i^*}$, such that the budget over total revealing probabilities is determined by $i^*$:
$$\sum_{i\in \effortset} p_i \leq \frac{16}{3}\rbr{1-\frac{2\cost_{i^*}}{p_{i^*}}}+p_{i^*}.$$
%for any $i^*\in\effortset$,
%we have that 
%$\sum_{i\in \effortset\backslash\lbr{i^*}} p_i \leq \frac{16}{3}\rbr{1-\frac{2\cost_{i^*}}{p_{i^*}}}$.
\end{lemma}
\begin{proof}
We first define several useful notations. We define $\event$ to be the event that the agent receives no informative signal on all tasks in $\effortset$. 
Let $q_0= \prob{\event}=\Pi_{j\in \effortset} (1-p_j)$ be the probability that event~$\event$ happens. 
Let $s_0 = \expect[\outcome\sim\signal]{\score(\signal, \outcome)\given \event}$
be the expected score of the agent when he receives no informative signal. We also define $\event_i$ to be the event that the agent receives no informative signal on all tasks in $\effortset \backslash \{i\}$. let $q_i=\prob{\event_i} = \Pi_{i\in \effortset\setminus\{i\}}(1-p_j)$ be the probability that the event $\event_i$ happens.
Let $s_i=\expect[\outcome\sim\signal]{\score(\signal, \outcome)\given \event_i, \signal_i \neq \bot}$ be the expected score of the agent when he only receives an informative signal on task~$i$. 
% Let $\hat{i} = \argmin_{i\neq i^*} s_i$.
% Note that $s_0\geq s_{\hat{i}}$ since otherwise the agent will randomly guess the state on task $\hat{i}$
% when he has no informative signal. 
% The properness of the scoring rule also implies 
% that the expected score of the agent is at least $s_0+s_{\hat{i}}$
% when he receives at least one informative signal from the set $[n]\backslash\{\hat{i}\}$. 

% Let $q_0=\Pi_{j\in \effortset} (1-p_j)$ be the probability that the agent does not receive any informative signal on all tasks in $\effortset$, 
% and let $q_i=\Pi_{i\in \effortset\setminus\{i\}}(1-p_j)$ be the probability that the agent does not receive any informative signal for all tasks in $\effortset\backslash\lbr{i}$.
Next we divide the analysis into two cases: (1) $q_0 \geq \sfrac{1}{2}$; and (2) $q_0 < \sfrac{1}{2}$. 
\begin{enumerate}[{Case} 1:]
\item $q_0 \geq \sfrac{1}{2}$. 
In this case, we first show that the expected score for no informative signal $s_0$ can not be less than $\sfrac{1}{4}$. Suppose $s_0 < \sfrac{1}{4}$, then we show that the incentive constraint for exerting effort on any task $i$ is violated.
%the incentive constraint on any task $i$ implies that 
The utility increase of the agent for exerting effort on task $i$ is 
\begin{align*}
&\expect[\signal\sim\effortset]{\expect[\outcome\sim \signal]{\score(\signal,\outcome)}} - \expect[\signal\sim\effortset\setminus\{i\}]{\expect[\outcome\sim\signal]{\score(\signal,\outcome)}} \\
&= p_{i} \rbr{\expect[\signal\sim\effortset]{\expect[\outcome\sim\signal]{\score(\signal,\outcome)\given \signal_{i}\neq\bot}} 
- \expect[\signal\sim\effortset]{\expect[\outcome\sim\signal]{S(\signal,\outcome) \given \signal_{i} = \bot}}}
%&\le p_i q_i(\expect[\signal\sim\effortset]{\expect[\outcome\sim\signal]{\score(\signal, \outcome)\given \signal_i\neq \bot, \signal_j=\bot, \forall j\neq i}}-s_0)\\
%&\qquad +p_i(1-q_i)(\expect[\signal\sim\effortset]{\expect[\outcome\sim\signal]{\score(\signal, \outcome)\given \signal_i\neq \bot, \signal_j=\bot, \forall j\neq i}}-s_0)\\
% &\leq p_{i} \rbr{q_{i} (s_i-s_0) + \frac{1}{2}(1-q_{i})} 
% < \frac{3p_{i}}{8} < \cost_{i},
\end{align*}
Then, we bound the expected score increase for receiving an informative signal on task~$i$. 
Conditioned on event $\event_i$, the expected score difference is $s_i - s_0$. Since the scoring rule is proper, 
we have $s_0 \geq s_i/2$, which implies $s_i - s_0 \leq s_0 \leq \sfrac{1}{4}$.
Conditioned on the complement event $\bar{\event}_i$, by the properness of scoring rule, the expected score difference is at most $\sfrac{1}{2}$.
Thus, the utility increase for exerting effort on task $i$ is at most
$$
\expect[\signal\sim\effortset]{\expect[\outcome\sim \signal]{\score(\signal,\outcome)}} - \expect[\signal\sim\effortset\setminus\{i\}]{\expect[\outcome\sim\signal]{\score(\signal,\outcome)}} \leq p_{i} \rbr{q_{i} (s_i-s_0) + \frac{1}{2}(1-q_{i})} 
< \frac{3p_{i}}{8} < \cost_{i},
$$
which violates the incentive constraint for exerting effort on task $i$.

Therefore, we have $s_0\geq \sfrac{1}{4}$. We now lower bound the expected score $s_i$ for receiving only one informative signal on task $i$.
For any task $i$, the incentive constraint implies that 
\begin{align*}
\cost_{i}&\leq 
\expect[\signal\sim\effortset]{\expect[\outcome\sim\signal]{S(\signal,\outcome)}} - \expect[\effortset\backslash\{i\}]{\expect[\signal]{S(\signal,\outcome)}} \\
&= p_{i} \rbr{\expect[\signal\sim\effortset]{\expect[\outcome\sim\signal]{S(\signal,\outcome)\given \signal_{i}\neq\bot}} 
- \expect[\signal\sim\effortset]{\expect[\outcome\sim\signal]{S(\signal,\outcome) \given \signal_{i} = \bot}}} \\
&\leq p_{i} \rbr{q_{i} (s_{i}-s_0) + \frac{1}{2} (1-q_{i})}.
\end{align*}
Since $q_i \geq q_0 \geq \sfrac{1}{2}$ and $\sfrac{c_i}{p_i}\geq \sfrac{15}{32}$, this further implies that 
\begin{align*}
s_{i} \geq
s_0 + \frac{\frac{\cost_{i}}{p_{i}}-\frac{1}{2}(1-q_{i})}{q_{i}} \geq \frac{11}{16}.
\end{align*}

Consider any fixed task $i^*\in \effortset$. Let $\hat{s} = \expect[\signal\sim\effortset]{\expect[\omega\sim\signal]{S(\signal,\omega)\given \signal_{i^*} = \bot, \bar{\event}_{i^*}}}$ be the expected score of the agent when he has no signal on task $i^*$, 
and at least one informative signal on tasks in $\effortset\backslash\lbr{i^*}$.
Since the scoring rule is proper, $\hat{s} \geq \min_{i}s_{i} \geq \sfrac{11}{16}$. 
The incentive constraint on task $i^*$ implies that 
\begin{align*}
\cost_{i^*}&
\leq p_{i^*} \rbr{\expect[\signal\sim\effortset]{\expect[\outcome\sim\signal]{S(\signal,\outcome)\given \signal_{i^*}\neq\bot}} 
- \expect[\signal\sim\effortset]{\expect[\outcome\sim\signal]{S(\signal,\outcome) \given \signal_{i^*} = \bot}}} \\
&\leq p_{i^*} \rbr{\frac{q_{i^*}}{2} + (1-q_{i^*})(1-\hat{s})},
\end{align*}
where the last inequality is due to the expected score difference conditioned on $\bar{\event}_i$ is at most $1-\hat{s}$.
Hence, we have 
\begin{align*}
q_{i^*} \geq 1-\frac{8}{3}\rbr{1-\frac{2\cost_{i^*}}{p_{i^*}}}.
\end{align*}
Note that the probability that the agent receives at least one informative signal in $\effortset\backslash\lbr{i^*}$
is at least the sum of probability that 
the agent receives an informative signal on task $i$
and zero informative signal on tasks in $\effortset\backslash\lbr{i^*,i}$.
Note that the probability of the latter event is at least $q_0\geq \sfrac{1}{2}$.
Thus, it holds that
\begin{align*}
1-q_{i^*} \geq \frac{1}{2}\sum_{i\in\effortset\backslash\{i^*\}} p_i.
\end{align*}
By combining the two inequalities above, we have 
\begin{align*}
\sum_{i\in \effortset\backslash\lbr{i^*}} p_i \leq 2(1-q_{i^*}) 
\leq \frac{16}{3}\rbr{1-\frac{2\cost_{i^*}}{p_{i^*}}}.
\end{align*}

\item Suppose $q_0 < \sfrac{1}{2}$. Consider any fixed task $i^*\in \effortset$. In this case, we first show that there exists a subset $\bar{\effortset}\subseteq\effortset$
which satisfies the following three properties: (1) $i^*\in\bar{\effortset}$;
(2) $\bar{\effortset}$ can be incentivized by a proper scoring rule;
and (3) the probability of no informative signal in $\bar{\effortset}\backslash \{i^*\}$ is between $[\sfrac{1}{2},\sfrac{2}{3})$. By case $1$, this subset $\bar{\effortset}$ cannot be incentivized, which is a contradiction. 

To find such a subset, we remove tasks in $\effortset \backslash \{i^*\}$ from $\effortset$ one by one randomly. Since $p_{i^*} \leq \sfrac{1}{4}$ and $q_0 < \sfrac{1}{2}$, we have $q_{i^*} = q_0/(1-p_{i^*}) < \sfrac{2}{3}$. If $q_{i^*} \in [\sfrac{1}{2},\sfrac{2}{3})$, then $\effortset$ satisfies three properties. We use $\effortset'$ to denote the subset in this deletion process. Let $q_{i^*}'$ be the probability of no informative signal in $\effortset' \backslash\{i^*\}$. If $q_{i^*} < \sfrac{1}{2}$, then we have $q_{i^*}'$ increases from $q_{i^*}$ to $1$ during this process. If there is no $q_{i^*}' \in [\sfrac{1}{2},\sfrac{2}{3})$ in this process, then there exists a task $i \in \effortset$ with $p_i > \sfrac{1}{4}$, which contradicts the assumption. Let $\bar{\effortset}$ be the subset with probability $\bar{q}_{i^*} \in [\sfrac{1}{2},\sfrac{2}{3})$ during this process. It is easy to see that $\bar{\effortset}$ satisfies other two properties. 


However, by union bound, 
\begin{align*}
\sum_{i\in\bar{\effortset}\backslash\{i^*\}} p_i \geq 1-\bar{q}_{i^*} > \frac{1}{3} 
\geq \frac{16}{3}\rbr{1-\frac{2\cost_{i^*}}{p_{i^*}}},
\end{align*}
which contradicts the assumption that $\bar{\effortset}$ can be incentivized according to the case 1. \qedhere
\end{enumerate}
\end{proof}

\begin{proof}[Proof of \cref{thm:approximation}]
We first prove the theorem for additive valuations, 
and then at the end we introduce the details for generalizing our techniques to submodular valuations. 
Recall that for any task $i$, we have $p_i\geq 2\cost_i$ since otherwise that task cannot be incentivized by the principal. 
Thus, we divide the tasks into two sets $X, Y$ based on the ratio $\sfrac{p}{2\cost_i}$ as follows
$$
X = \lbr{i: \frac{p_i}{2\cost_i} > 11}; 
\qquad Y =\lbr{i: 1 \leq \frac{p_i}{2\cost_i} \leq 11}.
$$
By \Cref{thm:budget inflation}, there is a truncated separate scoring rule with budget $1$ that is an $11$-approximation on the set~$X$ 
since this case can be viewed the same as the one in \cref{thm:budget inflation}
by scaling the score and the costs by the same constant factor~$11$. 

We divide the set $Y$ into three subsets. 
\begin{align*}
Y_1 = \lbr{i: p_i\geq \frac{1}{4},
1 \leq \frac{p_i}{2\cost_i} \leq \frac{16}{15}}; \quad
Y_2 =\lbr{i: p_i< \frac{1}{4}, 
1 \leq \frac{p_i}{2\cost_i} \leq \frac{16}{15}};\quad
Y_3 =\lbr{i: \frac{16}{15} < \frac{p_i}{2\cost_i} \leq 11}.
\end{align*}
Intuitively, set $Y_1$ corresponds to the case that the costs of effort are large, and it is sufficient to only incentivize one task with highest value in this set. 
Both set $Y_2$ and $Y_3$ corresponds to the situation where 
the probabilities of revealing the states are small compared to the costs, and hence the concentration technique cannot be applied.
In both cases, we utilize \cref{lem:asym-upper-bound} to bound the sum of probabilities for any set of incentivizable tasks, 
and hence showing that the set of tasks we identified by our polynomial time algorithm is approximately optimal.


\begin{enumerate}[{Case} 1:]
\item $Y_1 = \lbr{i: p_i\geq \frac{1}{4},
1 \leq \frac{p_i}{2\cost_i} \leq \frac{16}{15}}$. 
In this case, 
$\cost_i\geq \frac{15p_i}{32} \geq \frac{15}{128}$.
Therefore, at most $8$ tasks in $Y_1$ can be incentivized simultaneously in the optimal mechanism. 
By choosing the task in $Y_1$ with highest value, 
the principal attains an $8$-approximation by only incentivizing that task.

\item $Y_2 =\lbr{i: p_i< \frac{1}{4}, 
1 \leq \frac{p_i}{2\cost_i} \leq \frac{16}{15}}$. We use the threshold mechanism in \Cref{fig:threshold scoring mechanism}, with a recommendation set generated by running \Cref{fig:recommendation set static} on set $Y_2$.

We prove it is a $\frac{32}{3}$-approximation by showing: (1) the threshold scoring mechanism is incentive compatible (i.e.\ the agent's best response is to exert effort on all tasks in the recommendation set); and (2) the total value in the recommendation set $\effortset$ is a $16$-approximation of the optimal solution. 

\begin{enumerate}
    \item[(1)] The threshold scoring mechanism is incentive compatible. Specifically, we show that the set $\effortset_j$ can be incentivized for any task $j \in Y_2$.
For any task $j \in Y_2$, and any $i'\neq j, i'\in \effortset_{j}$, according to two constraints used in the construction of $\effortset_{j}$,
we have 
\begin{align*}
\sum_{i\in \effortset_{j}\backslash\{i'\}} p_i
= \sum_{i\in \effortset_{j}\backslash\{j\}} p_i 
- p_{i'} + p_{j}
\leq \rbr{1-\frac{2\cost_{i'}}{p_{i'}}}.
\end{align*}
Given the threshold scoring rule with threshold $\eta=1$ on effort set $\effortset_{j}$,
the expected score increase of exerting effort on task $i'$ is at least the probability of receiving no informative signal on tasks in $\effortset_{j}\backslash\{i'\}$
times the conditional score increase for exerting effort. By the union bound, we have the probability of receiving no informative signal on tasks in $\effortset_{j}\backslash\{i'\}$ is at least $\Pi_{i\in \effortset_{j}\backslash\{i'\}} (1-p_i) \geq 1-\sum_{i\in \effortset_{j}\backslash\{i'\}} p_i$. Conditioned on this event, the expected score increase for exerting effort on $i'$ is $p_{i'} + p_{i'}/2 - 1/2 = p_{i'}/2$. Thus, we have the expected score increase of exerting effort on task $i'$ is at least
\begin{align*}
\rbr{1-\sum_{i\in \effortset_{j}\backslash\{i'\}} p_i}\cdot \frac{p_{i'}}{2}
\geq \cost_{i'}.
\end{align*}
Therefore, for all searches $j \in Y_2$, a threshold scoring rule with threshold $1$ and recommendation set $\effortset_{j}$ is incentive compatible.

\item[(2)] The total value in the recommendation set $\effortset$ is a $16$-approximation of the optimal solution. 
By \cref{lem:asym-upper-bound}, 
for any set $\effortset'\subseteq Y_2$ that can be incentivized, and any $i^*\in\effortset'$, 
we have
\begin{align*}
\sum_{i\in \effortset'\backslash\lbr{i^*}} p_i \leq 
\frac{16}{3} \rbr{1-\frac{2\cost_{i^*}}{p_{i^*}}}.
\end{align*}

Let $\optset$ be the optimal effort set in the knapsack scoring problem when the set of available tasks is $Y_2$. 
Let $\hat{i}=\argmin_{i\in \optset}\rbr{ 1-\frac{2\cost_{i}}{p_{i}}+p_i}$ be the budget-pivotal task. This can be interpreted as a budget over the total probabilities in the optimal set $\optset$:
\begin{align*}
    \sum_{i\in \optset} p_i \leq
\frac{16}{3} \rbr{ 1-\frac{2\cost_{\hat{i}}}{p_{\hat{i}}}}+p_{\hat{i}}\leq \frac{16}{3} \rbr{ 1-\frac{2\cost_{\hat{i}}}{p_{\hat{i}}}+p_{\hat{i}}}.
\end{align*}

%We will show that, by reducing this budget by a factor of $\sfrac{16}{3}$, to $\rbr{ 1-\frac{2\cost_{\hat{i}}}{p_{\hat{i}}}+p_{\hat{i}}}$, 
% We then show that we can find a subset $\effortset'$ of $Y_2$ that is a $16$-approximation to the total value in $\optset$. For any task $j \in Y_2$, we initialize $\effortset_{j}=\{j\}$. Then we greedily add tasks in $Y_2$ into $\effortset_{j}$ according to the ratio $\frac{\val_i}{p_i}$ under the following two constraints:
% $$
% (1) 1-\frac{2\cost_{i}}{p_{i}} + p_i \geq 1-\frac{2\cost_{j}}{p_{j}} + p_{j}; \qquad (2) \sum_{i\in \effortset_{j}\backslash\{j\}} p_i
% \leq \rbr{1-\frac{2\cost_{j}}{p_{j}}}.
% $$
% We set $\effortset'$ to be the set with the maximum value among sets $\effortset_{j}$ for all $j \in Y_2$. 
%For this set to be incentive compatible with a threshold scoring rule, we include $\hat{i}$ in $\effortset'$, so the remaining budget is $1-\frac{2\cost_{\hat{i}}}{p_{\hat{i}}}$.  

Suppose we are given an optimal set $\optset$. Divide it into two sets based on the probability.%, by if or not the probability $p_i$ exceeds the remaining budget:
\begin{align*}
    \optset_1=\lbr{i\in\optset\setminus\{\hat{i}\}: p_i>\rbr{ 1-\frac{2\cost_{\hat{i}}}{p_{\hat{i}}}}};\qquad \optset_2=\lbr{i\in\optset\setminus\{\hat{i}\}: p_i\leq\rbr{ 1-\frac{2\cost_{\hat{i}}}{p_{\hat{i}}}}}.
\end{align*}

For the set $\optset_1$, by Lemma \ref{lem:asym-upper-bound}, there are at most $\sfrac{16}{3}$ tasks in $\optset_1$. By picking the most valuable task among $\optset$, the set $\effortset'^j$ achieve a $\sfrac{16}{3}$-approximation to the value of $\optset_1$.

% For the set $\optset_2$, we initialize $\effortset_{\hat{i}}=\{\hat{i}\}$. Then we greedily add tasks in $Y_2$ into $\effortset_{\hat{i}}$ according to the marginal value for the principal under the following two constraints:
% $$
% (1) 1-\frac{2\cost_{i}}{p_{i}} + p_i \geq 1-\frac{2\cost_{\hat{i}}}{p_{\hat{i}}} + p_{\hat{i}}; \qquad (2) \sum_{i\in \effortset_{\hat{i}}\backslash\{\hat{i}\}} p_i
% \leq \rbr{1-\frac{2\cost_{\hat{i}}}{p_{\hat{i}}}}.
% $$

% Given the budget constraint  $\sum_{i\in \effortset'} p_i \leq
% \frac{16}{3} \rbr{ 1-\frac{2\cost_{\hat{i}}}{p_{\hat{i}}}+p_{\hat{i}}}$, greedily add tasks in $\optset_2$ to $\effortset'$, according to the marginal value for the principal.  The subset $\effortset'$ is a $\frac{32}{3}$-approximation to set $\optset_2\cup\{\hat{i}\}$. By taking the better of $\effortset'$ and the most valuable task in $\optset_1$, we get a set that is a $16$-approximation to the optimal set $\optset$.



% Our algorithm is the following:\\
% for any $\hat{i}\in Y_2$,
% % Let $\hat{i} = \argmin_{i\in\effortset} \rbr{1-\frac{2\cost_{i}}{p_{i}} + p_i}$,
% % and 
% let $\effortset_{\hat{i}} \subseteq Y_2$ 
% be the set that greedily includes tasks $i$ according to its marginal value for the principal
% given the constraint that 
% $1-\frac{2\cost_{i}}{p_{i}} + p_i \geq 1-\frac{2\cost_{\hat{i}}}{p_{\hat{i}}} + p_{\hat{i}}$
% and 
% \begin{align*}
% \sum_{i\in \effortset_{\hat{i}}\backslash\{\hat{i}\}} p_i
% \leq \rbr{1-\frac{2\cost_{\hat{i}}}{p_{\hat{i}}}}.
% \end{align*}
%It outputs $\effortset=\max_{\hat{i}}\effortset_{\hat{i}}\cup\{\hat{i}\}$.
For the set $\optset_2$, we take the knapsack solution with a budget reduced by $\frac{16}{3}$ factor. %Similar to the analysis of $2$-approximation for knapsack, b
By enumerating over the budget-pivotal task $\hat{i}$, the recommendation set in \Cref{fig:threshold scoring mechanism} provides a $\sfrac{32}{3}$-approximation to the value of $\optset_2$. 

\end{enumerate}

%In this case, 

%Since it searches over all possible $\hat{i}$'s,  it is at least the $16$-approximation subset of the optimal set $\optset$:
Combining the above two cases, we have
\begin{align*}
\rbr{\frac{16}{3}+\frac{32}{3}}\val(\effortset) \geq \val(\optset_1)+\val(\optset_2) = v(\optset),
\end{align*}
which implies the recommendation set $\effortset$ is a $16$-approximation to the value of $\optset$.

% We then show the algorithm outputs a  set $\effortset'$ that can be incentivized by a threshold scoring rule with threshold $1$. Specifically, we show that the set $\effortset_j$ can be incentivized for any task $j \in Y_2$.
% For any task $j \in Y_2$, and any $i'\neq j, i'\in \effortset_{j}$, according to two constraints used in the construction of $\effortset_{j}$,
% we have 
% \begin{align*}
% \sum_{i\in \effortset_{j}\backslash\{i'\}} p_i
% = \sum_{i\in \effortset_{j}\backslash\{j\}} p_i 
% - p_{i'} + p_{j}
% \leq \rbr{1-\frac{2\cost_{i'}}{p_{i'}}}.
% \end{align*}
% Given the threshold scoring rule with threshold $\eta=1$ on effort set $\effortset_{j}$,
% the expected score increase of exerting effort on task $i'$ is at least the probability of receiving no informative signal on tasks in $\effortset_{j}\backslash\{i'\}$
% times the conditional score increase for exerting effort. By the union bound, we have the probability of receiving no informative signal on tasks in $\effortset_{j}\backslash\{i'\}$ is at least $\Pi_{i\in \effortset_{j}\backslash\{i'\}} (1-p_i) \geq 1-\sum_{i\in \effortset_{j}\backslash\{i'\}} p_i$. Conditioned on this event, the expected score increase for exerting effort on $i'$ is $p_{i'} + p_{i'}/2 - 1/2 = p_{i'}/2$. Thus, we have the expected score increase of exerting effort on task $i'$ is at least
% \begin{align*}
% \rbr{1-\sum_{i\in \effortset_{j}\backslash\{i'\}} p_i}\cdot \frac{p_{i'}}{2}
% \geq \cost_{i'}.
% \end{align*}
% Therefore, for all searches $j \in Y_2$, a threshold scoring rule with threshold $1$ and recommendation set $\effortset_{j}$ is incentive compatible. 
% The algorithm outputs the best of all possible $j$'s, which  is a $16$-approximation to the optimal in the knapsack scoring problem when the set of available tasks is $Y_2$. 

% Note that to implement the mechanism efficiently, 
% instead of computing the optimal $\effortset$ in $Y_2$, 
% it is sufficient to enumerate all $\hat{i}$
% and add tasks into $\effortset_i$ greedily, 
% and choose the $\hat{i}$ that generate highest expected value for the principal.




\item $Y_3 =\lbr{i: \frac{16}{15} < \frac{p_i}{2\cost_i} \leq 11}$.
In this case, for any set $\effortset\subseteq Y_3$ that can be incentivized, and any $i^*\in\effortset$, 
we have
\begin{align*}
\sum_{i\in \effortset\backslash\lbr{i^*}} p_i \leq 
\sum_{i\in \effortset\backslash\lbr{i^*}} 22 \cost_i
\leq 22 \leq 352 \rbr{1-\frac{2\cost_{i^*}}{p_{i^*}}}
\end{align*}
where the last inequality holds since 
$\frac{2\cost_{i^*}}{p_{i^*}}\leq\frac{15}{16}$.
By the same argument as case 2, the threshold mechanism is a $1056$-approximation to the optimal in the knapsack scoring problem when the set of available tasks is $Y_3$.
\end{enumerate}

Combining all cases, for additive valuations, the maximum between truncated separate scoring rule 
and threshold scoring rule is a
$1091$-approximation to the optimal value \opt,
and the parameters can be computed in polynomial time.
Finally, for submodular valuation, 
the only difference is that the greedy solution we adopted for finding the set of incentivizable tasks 
loses an additional approximation factor of $\sfrac{e}{(e-1)}$ in valuations \citep*{sviridenko2004note}. 
Note that this additional factor can be save if we don't require computational efficiency 
and brute force search for the optimal set that can be incentivized given our proposed scoring rule. 
\end{proof}



