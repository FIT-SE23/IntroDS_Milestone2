\section{General Information Structure}
\label{sec:general info}

In this section, we consider the problem of incentivizing effort with general information structures
and illustrate the intrinsic challenges for generalizing our results to general information structures.
Here, when the agent exerts effort, 
instead of assuming that the he observes the true state $\outcome_i$ with probability $p_i$ as in previous sections,
the agent receives a signal $\sigma_i \in \signals$ given by a signal structure that 
induces a distribution 
$\distoverposterior_i$ over posterior $\posterior_i\in \Delta(\outcomes)$. We show that the optimal value of the knapsack scoring problem can differ a lot under two different information structures
even if the optimal scoring rules for the single task problems are the same given those two information structures. 
Therefore, new ideas for designing approximately optimal scoring rules are required for general information structures. 
% When the agent exerts effort on task $i$, the agent receives a signal $\sigma_i \in \signals$ given by a signal structure instead of receiving the true state $\outcome_i$ with probability $p_i$ as in previous sections. 


First, the following lemma characterizes whether a single task can be incentivized by an incentive compatible mechanism
under general information structure environments. 
\begin{lemma}[\citealp{HLSW-20}]
\label{lem:single opt-general}
For the knapsack scoring problem with general information structures, the agent can be incentivized to exert effort on a single task $\effortset = \{i\}$ with budget $1$ 
if and only if 
$$
\expect[\posterior_i\sim\distoverposterior_i]{\abs{\posterior_i-\prior}}\geq c_i,
$$ 
where $\abs{\posterior_i-\prior}$ is the difference of the mean between the posterior and the prior.
\end{lemma}


When there are multiple tasks, 
a crucial statistic that affects the set of the incentivizable tasks 
is the expected \KL-divergence between the prior and the posterior.
Specifically, let 
\begin{align*}
\divergence_i \triangleq \expect[\posterior_i\sim\distoverposterior_i]{\KL(\prior\| \posterior_i)}
\end{align*}
where $\KL(\prior \| \posterior_i) = \sum_{\outcome\in\outcomes} \prior(\outcome)\cdot 
\ln \frac{\prior(\outcome)}{\posterior_i(\outcome)}$
is the \KL-divergence between the prior $\prior$ and the posterior~$\posterior_i$.
This distance measures how easy for the agent to mimic the signal distributions without exerting effort. 
The following lemma provides an upper bound on the set of incentivizable tasks given asymmetric and general information structures.

\begin{lemma}\label{lem:general info upper}
For the knapsack scoring problem with general information structures, for any set $\effortset^*$ such that there exists an incentive compatible mechanism 
where the agent's optimal effort choice is~$\effortset^*$, 
we have 
\begin{align*}
\sum_{i\in\optset}\cost_i \leq 
\sqrt{\frac{1}{2} \sum_{i\in\optset} \divergence_i}.
\end{align*}
\end{lemma}
\begin{proof}
Note that given any proper scoring rule $\score$, 
one feasible choice of the agent is to exert no effort, 
simulate the posterior distribution on set $\optset$, 
and report the simulated posterior to the principal. 
Let $P$ be the distribution over the profile of reports, and states for all tasks in $\optset$
given the simulations on $\effortset^*$. Let $Q$ be such distribution
when the agent exerts effort on all tasks in~$\optset$
and get the true informative signals. 
It is easy to verify that the \KL-divergence between
$P$ and $Q$ is $\sum_{i\in\optset} \divergence_i$. 
Let $\event$ be the event such that the profile of reports and states does not coincide 
given the true posterior generating process and the simulated reports.
Then we have 
\begin{align*}
&\expect[\sigma\sim\effortset]{\expect[\omega\sim\signal]{\score(\signal,\outcome)}} 
- \expect[\sigma\sim\emptyset]{\expect[\omega\sim\signal]{\score(\signal,\outcome)}} 
\leq \expect[Q]{\score(\signal,\outcome)} 
- \expect[P]{\score(\signal,\outcome)} \\
& \leq \abs{\prob[P]{\event} - \prob[Q]{\event}} 
\leq \sqrt{\frac{1}{2}\KL(P \| Q)}
= \sqrt{\frac{1}{2} \sum_{i\in\optset} \divergence_i}
\end{align*}
where the second inequality holds since the payment of the principal is at most 1, 
and the third inequality holds by Pinsker's inequality (\cref{lem:pinsker}).
\end{proof}

Next we show that given two different information structures such that 
the design of the optimal scoring rule for both cases are the same 
in the single task problem, 
the set of incentivizable tasks may differ a lot when there are multiple tasks. 
% given the same cost of effort in both cases,


Specifically, consider the symmetric environment with identical information structures and costs~$\cost$ for all tasks, 
\cref{lem:general info upper} implies that 
$\abs{\optset} \leq \frac{\divergence}{2\cost^2}$. 
Fixing $p>0$, consider the following two information structures when the agent exerts effort on any single task:
\begin{itemize}
    \item the agent receives an informative signal $\signal = \outcome$ with probability $p$,
    and receives an uninformative signal $\signal=\bot$ regardless of the realized state with probability $1-p$; 
    \item the agent receives an informative signal that induces posterior $\posterior = \frac{1+p}{2}$ and $\frac{1-p}{2}$ with probability $\frac{1}{2}$ each.
\end{itemize}

Given both information structures above, in the single task problem, by \cref{lem:single opt-general}, we know that the agent can be incentivized to exert effort on the single task
if and only if the cost of effort is at most $\sfrac{p}{2}$.

For the multi-task problem, suppose that the cost of effort on a single task is $\cost = \frac{p}{4}$. 
Given the first information structure, it is easy to show that the optimal scoring rule can incentivize the agent to exert effort on $O(\frac{1}{c})$ tasks.
By \cref{thm:approximation},
the agent can be incentivized to exert effort on $O(\frac{1}{c})$ tasks by the threshold scoring rule. 
In contrast, given the second information structure, 
we have that $\divergence = O(p^2)$
and hence by \cref{lem:general info upper}, 
the size of the incentivizable tasks is at most 
$\frac{\divergence}{2\cost^2} = O(1)$. 
The gap on the size of the incentivizable tasks between two different information structures are unbounded when $p$ and $c$ are sufficiently small. 


The above observation indicates that the design of the (approximately) optimal scoring rules depends on the fine details of different information structures
even if they have the same performance on the single task problem. 
Thus it is unlikely to directly generalize our results for the special case to general information structures,
or derive a unified approach for reducing the multi-task knapsack scoring problems to single-task ones.
It is an interesting open question to identify tight upper bounds of the optimal solution for the knapsack scoring problem with general information structures, 
and design approximately optimal scoring rules to approximate the upper bound. 
