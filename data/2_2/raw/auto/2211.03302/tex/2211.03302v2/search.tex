\section{Sequential Effort}
\label{sec:search}
In this section, we show that the value approximation results for the static effort model can be generalized 
when the effort choice is made sequentially by applying the same family of scoring rules. 





% \subsection{Sequential Search Model}
In the sequential search model, the agent makes the effort choice on tasks sequentially with the order of his choice.
Our designed scoring rule is robust against the strategy the agent adopts for exerting efforts on the recommendation set
as long as the strategy is not obviously dominated.
% For any set $\effortset$, let $\signal_{\effortset}$ be the history of received signals for exerting effort on all tasks in $\effortset$. 
% The agent's strategy $\strategy$ is to determined whether to stop or which task to exert effort on next
% based on the set of tasks he has exerted effort on
% and the corresponding received signals,
% i.e., $\strategy(\effortset,\history_{\effortset}) \in [n]\cup\{\stopping\}\backslash \effortset$
% where $\stopping$ represents stopping. 
% The timeline of our search model is shown as follows.
% \begin{enumerate}
% \item The principal commits to a proper scoring rule
% $\score: \signals\times\outcomes\to [0,1]$.
% \item The agent adopts a dynamic strategy $\strategy$ for exerting effort sequentially
% and receives a profile of signals $\signal$. 
% \item The agent reports the signals $\signal$ truthfully to the principal and receives score according to $\score$.
% \end{enumerate}




\begin{theorem}\label{thm:sequential-approx}
The better of 
a truncated separate scoring rule
and a threshold scoring rule 
%such that the value of the principal 
is a $561$-approximation to the optimal value of the knapsack scoring problem (\opt)
when the agent does not adopt obliviously dominated strategies.
Moreover, for additive values, the parameters of such mechanism can be computed in polynomial time, 
and for submodular values, 
there is a polynomial time algorithm for computing the parameters that loses an additional multiplicative factor of $\sfrac{e}{(e-1)}$ in approximation ratio. 
\end{theorem}

 
% We prove this by showing that the agent has positive marginal gain by doing any one more task, with constant probability. 

\begin{proof}
Again, we first prove the theorem for additive valuations.
Similarly as \Cref{thm:approximation}, we divide the tasks into two sets $X, Y$ based on the ratio $\sfrac{p_i}{2\cost_i}$ as follows
$$
X = \lbr{i: \frac{p_i}{2\cost_i} > 11}; 
\qquad Y =\lbr{i: 1 \leq \frac{p_i}{2\cost_i} \leq 11}.
$$

On set $X$, the mechanism in \Cref{fig:truncated scoring mechanism} with budget $1$ achieves a $\frac{99}{8}$-approximation. Let the last assignment completed be $i$. 
By the same proof of \Cref{thm:budget inflation},  
for any task $i\in \effortset$, the probability that the scoring rule runs out of budget before the agent exerting effort on task $i$ can be bounded by $\frac{8}{9}$. 
Hence, when adopting strategies that are not obviously dominated, 
with ex ante probability at least $\frac{8}{9}$, 
the agent will stop after finishing all the tasks in the recommendation set. 
The same mechanism looses another $\frac{8}{9}$ factor in the approximation ratio.


On set $Y$, we divide the tasks into two sets by the probability $p_i$ of knowing the truth.

\begin{align*}
Y_1 = \lbr{i: p_i\geq 0.1}; \quad
Y_2 =\lbr{i: p_i< 0.1}.
\end{align*}


On set $Y_1$, it is sufficient to pick the highest-value task and post the threshold scoring rule. By the probability-cost ratio $\frac{p_i}{\cost_i}\leq 22$, each task has $\cost_i\geq \frac{1}{220}$. At most $440$ tasks can be incentivized in $Y_1$. Hence a $440$-approximation on $Y_1$.



\begin{figure}[t]
    \centering
    \fbox{
    \parbox{0.96\textwidth}{

    %\textbf{Threshold Scoring Mechanism} for additive values with budget $1$
        \textbf{Recommendation set $\effortset$} for threshold mechanism, with sequentially responding agent
        
        Input: ground set $\groundset$
        
        Output: set $\effortset$
        
        Greedily add tasks from $\groundset$ to $\effortset$, by value-probability ratio $\frac{\val_i}{p_i}$ with a budget $0.55$ on the total probabilities $\sum_{i\in \effortset}p_i$ of knowing the truth.
    }
    }
    \caption{Procedure for identifying approximately optimal recommendation set with sequentially responding agent.}
    \label{fig:threshold scoring mechanism sequential}
\end{figure}

% \begin{figure}
%     \centering
%     \fbox{
%     \parbox{\textwidth}{

%     \textbf{Threshold Scoring Mechanism} for additive values with budget $1$
%     \begin{itemize}
%         \item Recommendation set $\effortset$: 
        
%         Greedily include tasks by value-probability ratio $\frac{\val_i}{p_i}$ with a budget $0.55$ on the total probabilities $\sum_{i\in \effortset}p_i$ of knowing the truth.
    
        
%         \item Scoring rule $\score$: threshold scoring rule with threshold $1$.
%         \begin{itemize}
%             \item Score $1$ if both (1) at least $1$ reported signal is informative; and (2) any task reported signal that is informative is correct.
%             \item Score $0$ otherwise.
%         \end{itemize}
%     \end{itemize}
%     }
%     }
%     \caption{Threshold Scoring Mechanism.}
%     \label{fig:threshold scoring mechanism sequential}
% \end{figure}

On set $Y_2$, we use the scoring mechanism in \Cref{fig:threshold scoring mechanism sequential}.
We show this mechanism achieves a $109$-approximation, by showing when the adopted strategy is not obviously dominated: 
(1) with probability at least $0.45$, the agent completes all the tasks in the recommendation set; 
and 
(2) the total value in the set  is a $109$-approximation.

\begin{itemize}
    \item The agent completes tasks in recommendation set with probability at least $0.45$. 
    %All tasks in $Y_2$ has $p_i<0.1$, so the total probabilities $\sum_{i\in \effortset}p_i\geq 0.45$. 
    By union bound, the probability that the agent gets any informative signal is $1-\prod_i(1-p_i)\leq \sum_i p_i\leq 0.55$. For any order of completing the task, the agent gets no informative signal with probability at least $0.45$. The marginal gain of doing one more task is always positive, so the agent will finish the recommendation set with probability at least $0.45$. 
    \item The total value in the set is a $49$-approximation to the optimal.  All tasks in $Y_2$ has $p_i<0.1$, so by setting the budget at $0.55$, the total probabilities in $\effortset$ is at least the optimal knapsack value with budget $0.45$ on total probabilities. Since the probability-cost ratio $\frac{p_i}{c_i}\leq 22$, there is a budget on the total probabilities in any set that can be incentivized: $\sum_{i}p_i\leq 22$. Hence a $49$-approximation.
\end{itemize}
Combining the claims above, the better of the truncated scoring mechanism and the threshold scoring mechanism achieves a $561$-approximation when the agent is responding sequentially. 
\end{proof}

