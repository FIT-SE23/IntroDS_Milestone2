\section{Missing Proofs and Constructions}
\subsection{Alternative Formulation of Threshold Scoring Rules}
\label{apx:exp proper for belief}
Here we present an alternative formulation of the threshold scoring rule in the special case of threshold $1$
given outcome space $\outcomes=\{0, 1\}^n$.
% coincides with the canonical \mos scoring rule proposed in \citet*{HLSW-20}, 
% which is proper for belief elicitation. 


\begin{definition}[\citealp*{HLSW-20}]\label{def:optimal mos}
Consider the $n$-dimensional outcome space $\outcomes=\{0, 1\}^n$.
Given single-dimensional scoring rules
\begin{align*}
\score_i(\mu_i, \outcome_i)=\begin{cases}
1& \mu_i\leq \sfrac{1}{2} \text{ and } \outcome_i=0, \text{ or } \mu_i> \sfrac{1}{2} \text{ and } \outcome_i=1,\\
0& \text{otherwise}.
\end{cases}
\end{align*}
The canonical \mos scoring rule $\score$ 
is defined as 
\begin{align*}
    \score(\mu, \outcome)=\score_i(\mu_i, \outcome_i), \text{ where }i=\argmax\expect[\outcome_i\sim\mu_i]{\score_i(\mu_i, \outcome_i)},
\end{align*}
\end{definition}


By \citet*{HLSW-20}, the canonical \mos scoring rule is proper for belief elicitation. 
Moreover, it is easy to verify that it coincides with the threshold scoring rule with threshold $1$ given any belief of the agent.




\subsection{Proof of \cref{thm:budget inflation}}
\label{apx:proof budget inflation}

We show that the mechanism in \Cref{fig:truncated scoring mechanism}  is incentive compatible,
by first showing that scoring rule $S$ is proper, 
and then showing that $\effortset$ is the agent's best effort choice. 

\paragraph{Proper.} 
For each task $i$, conditional on receiving signal $\signal_i\neq\bot$,
the score $\score_i(\signal_i,\outcome_i)$ first order stochastically dominates $\score_i(\signal'_i,\outcome_i)$ for any $\signal'_i$.
Thus, the agent has incentives to truthfully report the signal $\signal_i$ if $\signal_i\neq\bot$.

We then show that the agent has no incentives to misreport on tasks with uninformative signal $\signal_i = \bot$ by contradiction.
Suppose that the agent has incentives to misreport given signal $\bot$ on some tasks. 
We partition the tasks into three sets. 
Let $Z_0$ be the set of tasks $i$ such that $\signal_i\neq \bot$, 
$Z_1$ be the set of tasks $i$ such that $\signal_i= \bot$ and where the agent truthfully reports the signal,
and $Z_2$ be the set of tasks $i$ such that $\signal_i= \bot$ and where the agent misreports the signal. 
First note that if 
$\sum_{i\in Z_0} 2\cdot\sfrac{9c_i}{8p_i} \geq 11$,
then by truthful reporting the signals the agent can secure a deterministic score $11$, 
which is the maximum possible score. 
Hence the agent has no incentive to misreport in this case.

Next we focus on the case when $\sum_{i\in Z_0} \sfrac{9c_i}{8p_i} < \sfrac{11}{2}$.
Let $\eta_i$ be a Bernoulli random variable with probability $\sfrac{1}{2}$ drawn independently for each task $i\in Z_2$. We use $\eta_i$ to indicate whether the agent guesses correctly on the task $i \in Z_2$. Let 
\begin{align*}
s = \sum_{i\in Z_0} \frac{9c_i}{8p_i} + \sum_{i\in Z_2} \frac{9c_i}{4p_i}\left(\eta_i - \frac{1}{2}\right) + \frac{11}{2}.
\end{align*}
Note that $s$ is the random variable corresponding to the score without truncation by the interval $[0,11]$.
Consider an alternative setting where the score is truncated by the interval $[\sum_{i\in Z_0} \sfrac{9c_i}{4p_i}, 11]$. 
Since the distribution of $s$ is symmetric with respect to the mean $\rbr{\sum_{i\in Z_0} \sfrac{9c_i}{4p_i} + 11}/2$, the score distribution under the truncation by $[\sum_{i\in Z_0} \sfrac{9c_i}{4p_i}, 11]$ is also symmetric with respect to the mean.
Thus, the utility of the agent for misreporting in this alternative setting is exactly the same as the utility for truthful reporting, $\rbr{\sum_{i\in Z_0} \sfrac{9c_i}{4p_i} + 11}/2$. 
Since $\sum_{i\in Z_0} \sfrac{9c_i}{4p_i} > 0$, the utility of the agent for misreporting with truncation by $[0,11]$ is strictly less than the utility for misreporting with truncation by $[\sum_{i\in Z_0} \sfrac{9c_i}{4p_i}, 11]$.  
Therefore, the agent will not have an incentive to misreport in the original setting when the lower truncation is $0$. 


\paragraph{Effort Set.}
We prove that the agent's optimal choice is to exert effort in tasks $\effortset$.
First note that we set the score to be zero for $i\not\in\effortset$. 
This immediately implies that the agent will not exert effort on task $i\not\in\effortset$. 
Fix the agent's effort choice in $\effortset$.
Suppose there exists a task $\hat{i}\in \effortset$ such that the agent's effort on task $\hat{i}$ is $0$.
Let~$\hat{\event}_{\hat{i}}$ be the event that $-d+\sum_{i\in\effortset \backslash \{\hat{i}\}} \score_{i}(\signal_{i},\outcome_{i}) \in [0,11-\sfrac{9c_{\hat{i}}}{8p_{\hat{i}}}]$. 
Let $\hat{Z}\subseteq\effortset$ be the set on which the agent exerts effort.
%By taking parameters $B \geq k$, it is easy to verify that $-d+\sum_{i\in\effortset \backslash \{\hat{i}\}} \score_{i}(\signal_{i},\outcome_{i}) \geq 0$ given our scoring rule when the agent is reporting the signals truthfully. 
%We can verify that $-d+\sum_{i\in\effortset \backslash \{\hat{i}\}} \score_{i}(\signal_{i},\outcome_{i}) \geq 0$ given our scoring rule when the agent is reporting the signals truthfully. 
Therefore, 
\begin{align*}
\prob{\hat{\event}_{\hat{i}}} &= 1-
\prob{-d+\sum_{i\in\effortset \backslash \{\hat{i}\}} \score_{i}(\signal_{i},\outcome_{i}) > 11-\frac{9c_{\hat{i}}}{8p_{\hat{i}}}}\\
&= 1-
\prob{\sum_{i\in\hat{Z}} \indicate{\signal_i \neq \bot} \cdot \frac{9c_i}{8p_i} > \frac{11}{2}-\frac{9c_{\hat{i}}}{4p_{\hat{i}}}}\\
&\geq 1-\exp\rbr{-\frac{\frac{1}{2}\rbr{\frac{11}{2}-\frac{9c_{\hat{i}}}{4p_{\hat{i}}} 
- \sum_{i\in\hat{Z}}\frac{9 c_i}{8}}^2}
{\frac{1}{4}\sum_{i\in\hat{Z}} \frac{1}{p_i}\cdot \frac{9c_i}{4}^2
+ \frac{1}{6}\max_{i\in\hat{Z}} \frac{9c_i}{4p_i}}} \\
&\geq 1-\exp\rbr{-\frac{\rbr{11 - \frac{45}{8}}^2}{6\cdot \frac{9}{8}^2+\frac{3}{2}}} \geq \frac{8}{9},
\end{align*}
where the first inequality holds by applying Bernstein's inequality (\cref{lem:bernstein}).
The second inequality holds since (1) $\sum_{i\in\hat{Z}} \frac{9c_i}{4} = \frac{9}{4} \sum_{i\in\hat{Z}} c_i \leq \frac{27}{8}$; 
(2) $\sum_{i\in\hat{Z}} \sum_{i\in\hat{Z}} \frac{1}{p_i}\cdot \frac{9c_i}{4}^2 \leq \sum_{i\in\hat{Z}} 2(9/8)^2 c_i \leq 3(9/8)^2$; 
and (3) $\max_{i\in\hat{Z}} \sfrac{9c_i}{4p_i} \leq 9/8$.
%Setting $k=\frac{9}{8}$ and $B = 11$, we have 
%$\prob{\hat{\event}_{\hat{i}}} \geq \sfrac{8}{9}$. 
Hence, by exerting effort on task $\hat{i}$, 
the score of the agent increases by at least $\prob{\hat{\event}_{\hat{i}}} \cdot \sfrac{9 c_{\hat{i}}}{8}  \geq c_{\hat{i}}$, which provides
a contradiction. 
% Finally, it is easy to verify that mechanism with score $S$ and recommendation $\effortset$ is computed in polynomial time 
% and thus, \cref{thm:budget inflation} holds.

% Fix $\beta\geq 1$, and let $\effortset\subseteq [n]$ be the set of tasks such that 
% the value from set $\effortset$ is at least $1/\beta$ times the optimal objective value in the relaxed program and the costs from set $\effortset$ does not exceed $\frac{3}{2}$,
% i.e., $\beta\cdot \val(\effortset) \geq \vopt$
% and $\sum_{i\in \effortset} \cost_i \leq \frac{3}{2}$.
% For additive values, we have $\beta = 1$
% by greedily adding tasks into $\effortset$ 
% according to the ratio $\frac{\val_i}{\cost_i}$
% until exceeding the budget 1.
% For submodular values, we have $\beta = \sfrac{e}{(e-1)}$ by adopting \cref{lem:submodular greedy} if we want a polynomial time algorithm, 
% and we have $\beta = 1$ for the theoretical bound without computation constraints.
% Next we will design a mechanism using truncated separate scoring rules
% such that the agent's effort choice is $\effortset$. 
% Thus, the value of the principal is $\val(\effortset) \geq \frac{1}{\beta}\vopt$
% as desired. 

For submodular values, we lose a $\sfrac{e}{e-1}$ factor in the value approximation ratio by computing the recommendation set $\effortset$ in polynomial time (\cref{lem:submodular greedy}). Without computation constraints, we have a scoring rule that achieves the theoretical bound. 
