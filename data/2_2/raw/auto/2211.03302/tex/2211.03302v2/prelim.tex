\section{Preliminaries}
\label{sec:prelim}

This paper considers the problem of incentivizing effort from an agent to learn about an unknown state. 
There are $n$ tasks with state space $\outcomes = \times_{i=1}^n \outcomes_i$ where $\outcomes_i = \{0,1\}$.
For each task $i\in [n]$, 
state $\outcome_i\in\outcomes_i=\{0, 1\}$ is realized independently according to prior distribution $\prior$ which is the uniform distribution on $\outcomes_i$.
Exerting effort on task $i$ induces cost $\cost_i$ to the agent.
% We first focus the static effort model,
% and in \cref{sec:dynamic prelim}, 
% we will introduce the generalization when the agent can exert effort dynamically.
The agent can choose to exert effort on a set $\effortset\subseteq [n]$ of tasks at a cost $\sum_{i\in\effortset}\cost_i$.
%The agent chooses a binary effort $\effort_i \in \{0,1\}$ on task $i$ with cost $\effort_i \cost_i$.
Let $\signals$ be the signal space where $\bot\in\signals$ is an uninformative signal.
If the agent does not exert effort on task $i$, i.e.\ $i\notin \effortset$, 
with probability~$1$,
the agent receives an uninformative signal $\signal_i=\bot$ regardless of the realized state. 
If the agent exerts effort on task $i$, i.e.\ $i\in \effortset$, the agent receives a signal $\signal_i\in\signals$ according to a signal structure, % $\signaldist_i$, 
 which is a random mapping from the states to the signal space.
Note that the signal structure on task $i$ induces a distribution 
$\distoverposterior_i$ over posterior $\posterior_i\in \Delta(\outcomes)$.

A special case that is of particular interest for our paper is when 
$\signals=\{0,1,\bot\}^n$
and the posterior belief is supported on $\{0,1,\sfrac{1}{2}\}^n$. 
In this case, if the agent exerts effort on task $i$, i.e.\ $i\in \effortset$,
with probability $p_i$,
the agent receives an informative signal $\signal_i = \outcome_i$,
% with probability $q\geq \frac{1}{2}$ 
% and $\signal_i = 1-\outcome_i$ otherwise,
and with probability $1-p_i$, the agent receives an uninformative signal $\signal_i=\bot$ regardless of the realized state. We call $p_i$ the state revelation probability of each task $i$. 
In the main body of the paper, we will focus on this special model, and discuss the extensions to general information structures in \cref{sec:general info}. 

Given the set of tasks $\effortset$ that the agent exerts effort on, 
the value of the principal is $\val(\effortset)$.
We assume that the valuation function $\val$ is \emph{submodular}: for every $\effortset'\subseteq \effortset\subseteq [n]$ of assignments, the principal's marginal value decreases, i.e.\ 
\begin{equation*}
    \forall i\in [n]\setminus \effortset,\, \val(\effortset'\cup \{i\})-\val(\effortset')\geq \val(\effortset\cup\{i\})-\val(\effortset).
\end{equation*} 
A special case of the submodular valuation is additive valuation, 
where $\val(\effortset) = \sum_{i\in\effortset} \val_i$
for given profile of $\{\val_i\}_{i\in[n]}$. 
The goal of the principal is to design a mechanism that maximizes her value subject to the budget constraint,
i.e., the payment to the agent is bounded between $0$ and $1$.
% Let $\optset$ be the set of tasks on which the agent chooses to exert effort in the optimal mechanism, 
% and denote the optimal value of the principal by 
% $\opt = \val(\optset)$.
Note that if the effort choice of the agent can be observed by the principal, this problem reduces to the classical knapsack problem. 
The novel feature in our model is that effort is unobservable, and the principal can only score the agent according to the reported signals and realized states. 

\subsection{Static Effort Model}
In the static effort model, we assume that the agent makes the effort choice on all tasks simultaneously, 
and after the effort choice, 
the agent receives the signals on all tasks simultaneously.
By the revelation principle, 
it is without loss to restrict attention to mechanisms that 
recommend a set of tasks $\effortset$ for the agent to exert effort,
and after exerting effort,
incentivize the agent to truthfully report the received signal to the principal. 
Let $\prob[\sigma\sim\effortset]{\cdot}$ and $\expect[\sigma\sim\effortset]{\cdot}$ 
be the probability and expectation with respect to the distribution over signals conditional on exerting effort on set $\effortset$,
and let $\prob[\omega\sim\signal]{\cdot}$ and $\expect[\omega\sim\signal]{\cdot}$ 
be the probability and expectation with respect to the posterior belief of the agent conditional on receiving signal $\signal\in\signals$.
\begin{definition}\label{def:proper}
A scoring rule
$\score: \signals\times\outcomes\to [0,1]$ is \emph{proper} if 
for any $\signal,\signal'\in\signals$, 
\begin{align*}
    \expect[\omega\sim\signal]{\score(\signal,\outcome)} \geq \expect[\omega\sim\signal]{\score(\signal',\outcome)}.
\end{align*}
\end{definition}

Note that our definition of properness relies on the information structure and the set of signal realizations $\signals$. 
In principle, a scoring rule that satisfies our definition of properness may incentive the agent to misreport his belief that cannot be induced by those signal realizations. 
This may raise a concern for the robustness of the implemented scoring rule.
In \cref{apx:proper belief}, we show that it is without loss of generality to focus on scoring rules that are only proper for signal realizations in the support, 
by converting any such scoring rule to one that is 
proper for all possible beliefs without performance loss. 

% However, the following claim show that this restriction is without loss of generality, with proof given in \Cref{apx:proper belief}.



\begin{definition}\label{def:ic}
A mechanism composed by a scoring rule
$\score: \signals\times\outcomes\to [0,1]$
and a corresponding recommendation set $\effortset$ is \emph{incentive compatible} 
if $\score$ is proper and 
for any $\effortset'\subseteq [n]$,\footnote{An alternative formulation of the mechanism is to only specify the scoring rule and delegate the computation of the optimal effort choice to the agent. 
However, the computation of the optimal effort choice may be NP-hard. 
The main advantage of our formulation is that we can ensure that the computation of the agent is simple. }
\begin{align*}
    \expect[\sigma\sim\effortset]{\expect[\omega\sim\signal]{\score(\signal,\outcome)}} - \sum_{i\in\effortset} \cost_i
    \geq \expect[\sigma\sim\effortset']{\expect[\omega\sim\signal]{\score(\signal,\outcome)}} - \sum_{i\in\effortset'} \cost_i.
\end{align*}
\end{definition}
\noindent The reward of the agent should be non-negative and the principal has a budget of $1$ for rewarding the agent. Thus, the score is ex-post bounded in $[0,1]$. 
Given the incentive constraints and reward constraints, the timeline of our model is as follows:
\begin{enumerate}
\item The principal commits to an incentive compatible mechanism with scoring rule
$\score: \signals\times\outcomes\to [0,1]$
and recommendation set $\effortset$.
\item The agent chooses a set $\bar{\effortset}$
of tasks on which to exert effort
and pays cost $\sum_{i\in \bar{\effortset}} \cost_i$.
\item States $\outcome=\{\outcome_i\}_{i=1}^n$ are realized, 
and the agent receives the signals $\signal\in\signals$.
\item The agent reports $\signal'$
and receives score $\score(\signal', \outcome)$.
\item The principal receives utility $\val(\bar{\effortset})$.
\end{enumerate}
Note that the agent is incentivized to choose $\bar{\effortset} = \effortset$ and truthfully reveal the signals in an incentive compatible mechanism. 
The \emph{knapsack scoring problem} for value function $\val$, costs $\{\cost_i\}_{i=1}^n$ and state revelation probabilities $\{p_i\}_{i=1}^n$ is formally defined as the following optimization program:
\begin{align*}
\opt(\val,\{\cost_i\}_{i=1}^n,\{p_i\}_{i=1}^n) =& \max_{\score,\effortset} \quad \val (\effortset)\\ %\sum_{i\in \effortset} \val_i (\effortset)\\
&\quad\text{s.t.} \quad
(\score,\effortset) \text{ is incentive compatible for $\{\cost_i\}_{i=1}^n$ and $\{p_i\}_{i=1}^n$,} \\
&\qquad \quad\;\score(\signal,\outcome)\in[0,1], \quad \forall \signal,\outcome.\\
\intertext{We use the \emph{knapsack problem} for value function $\val$ and costs $\cost_i$ without incentive constraints as an upper bound on the knapsack scoring problem:}
\vopt(\val,\{\cost_i\}_{i=1}^n) =& \max_{\effortset\subseteq[n]} \quad \val (\effortset)\\ %\sum_{i\in \effortset} \val_i (\effortset)\\
&\quad\text{s.t.} \quad \sum_{i\in \effortset} \cost_i \leq 1.
\end{align*}
It is easy to see that $\vopt(\val,\{\cost_i\}_{i=1}^n)\geq \opt(\val,\{\cost_i\}_{i=1}^n,\{p_i\}_{i=1}^n)$ for any $\val,\{\cost_i\}_{i=1}^n$ and $\{p_i\}_{i=1}^n$. 
%From now on, we will omit the input in notations if there is no ambiguity.

The following characterization shows the budget-minimal scoring rule for incentivizing a single task. To minimize the budget, the agent is indifferent between: 1) reporting truthfully and non-truthfully; 2) exerting effort and not exerting the effort on the task. 
\begin{lemma}[\citealp*{HLSW-20}]
\label{lem:single opt}
With minimal budget $\frac{2\cost_i}{p_i}$, the agent can be incentivized to exert effort on a single task $\effortset = \{i\}$ with cost $\cost_i$ and probability $p_i$ of revealing. %budget $1$ 
%if and only if $p_i\geq 2c_i$. 
Moreover, the budget-minimal scoring rule for incentivizing effort is\footnote{By \cref{claim:proper belief}, $\frac{2\cost_i}{p_i}$ is also the minimum budget required for any scoring rule proper for belief elicitation
in order to incentivize the agent to exert effort on single task $\{i\}$.} 
\begin{align*}
\score_i(\signal_i,\outcome_i) = 
\begin{cases}
\frac{\cost_i}{p_i} & \signal = \bot\\
\frac{2\cost_i}{p_i}\indicate{\signal_i=\outcome_i} & \text{otherwise}.
\end{cases}
\end{align*}
\end{lemma}

 

% \yw{%This budget-minimal scoring rule can be implemented by the following optimal scoring rule  proper for belief elicitation in \citet*{HLSW-20}.


% \begin{definition}[\citealp*{HLSW-20}]\label{def:single optimal}
% When the prior belief distribution is $\Pr[\outcome=1]=\sfrac{1}{2}$, the single dimensional optimal scoring rule that properly elicits belief $\mu$ is 
% \begin{align*}
%     \score(\mu, \outcome)=\begin{cases}
%      (1-\mu)\cdot\frac{2\cost_i}{p_i}    & \mu\leq \sfrac{1}{2} \\
%      \mu\cdot\frac{2\cost_i}{p_i}     & otherwise.
%     \end{cases}
% \end{align*}
% \end{definition}

% \begin{remark}
% The budget-minimal scoring rule can be implemented by the scoring rule proper for belief elicitation in \Cref{def:single optimal}.
% \end{remark}
% }
By \Cref{lem:single opt}, with budget $1$, the agent can be incentivized to exert effort on a single task if and only if $\frac{2\cost_i}{p_i}\leq 1$.
\cref{lem:monotone task} shows that for multiple tasks there is a monotonicity property for the set of incentivizable tasks.
\begin{lemma}[Monotonicity in tasks]~\label{lem:monotone task}
For any set of assignments $\effortset\subseteq [n]$, 
if there exists a proper scoring rule~$\score$ 
such that the agent exerts effort on tasks $\effortset$, 
for any subset $\effortset'\subseteq\effortset$, 
there exists a proper scoring rule $\score'$ 
such that the agent exerts effort on tasks $i\in\effortset'$.
\end{lemma}
\begin{proof}
To incentivize effort on $\effortset'$, we construct $\score'$ by simulating the agent's effort on the set $\effortset\setminus\effortset'$. 
For any reported signal profile $\signal'$, 
let $\score'(\signal', \outcome)=\expect[\sigma\sim\effortset]{\expect[\omega\sim\signal]{\score(\signal,\outcome) \given \signal_i = \signal'_i, \forall i\in\effortset'}}$ be the score that ignores the report in set $\effortset\setminus\effortset'$, and takes expected score over this set by simulating the signals assuming effort.

The proof follows by showing that exerting effort on set $\effortset'$ is the optimal strategy for the agent with scoring rule $\score'$. 
Since the new scoring rule $\score'$ only depends on reported signals in set $\effortset'$, 
the agent has no incentive to exert effort on any task outside $\effortset'$. 
For any subset $\hat{\effortset}\subseteq\effortset'$, 
the expected utility difference between exerting effort in sets $\hat{\effortset}$ and $\effortset'$ given scoring rule $\score'$
is identical to 
the expected utility difference between exerting effort in sets $\hat{\effortset}\cup(\effortset\setminus\effortset')$ and $\effortset$ given scoring rule $\score$. 
Since exerting effort on all tasks in set $\effortset$ is optimal for scoring rule $\score$, 
exerting effort on all tasks in set $\effortset'$ is optimal for scoring rule $\score'$.
\end{proof}
By \cref{lem:single opt,lem:monotone task}, 
it is without loss to assume that $p_i\geq 2\cost_i$
for all tasks $i\in[n]$, 
and we will maintain this assumption throughout the paper. 

There are two families of scoring rules that will arise in our analysis,
\emph{truncated separate scoring rules}
and \emph{threshold scoring rules}.
Intuitively, 
the truncated separate scoring rules specify a scoring rule for each task,
and the total score is the sum of scores on each task, 
truncated by $0$ and the budget.
% Let $\project_{[0,B]}[\cdot] \triangleq \min\{B, \max\{0, \cdot\}\}$ be the truncation according to the payment constraint $0$ and $B$.


\begin{definition}\label{def:separate cap}
A scoring rule $\score$ is a \emph{truncated separate scoring rule} with budget $B > 0$
if there exists single-dimensional scoring rules $\score_1,\dots,\score_n$ and shifting parameter $d\geq 0$
such that 
$\score(\signal,\outcome) = \min\lbr{B, \max\lbr{0, -d+\sum_{i\in[n]} \score_i(\signal_i,\outcome_i)}}$.
% \project_{[0,B]}\cbr{-d+\sum_{i\in[n]} \score_i(\signal_i,\outcome_i)}$.
% $r_1,\dots,r_n\geq 0$ and $d,\alpha\geq 0$ such that
% for any $i\in[n]$, 
% \begin{align*}
% \score_i(\signal_i,\outcome_i) = 
% \begin{cases}
% \frac{r_i}{2}(1+\alpha) & \signal = \bot\\
% r_i\cdot\indicate{\signal_i=\outcome_i} & \text{otherwise}
% \end{cases}
% \end{align*}
% and 
% \begin{align*}
% \score(\signal,\outcome) = \project_{[0,B]}\cbr{-d+\sum_{i\in[n]} \score_i(\signal_i,\outcome_i)}.
% \end{align*}
\end{definition}

Note that due to the truncation to $[0,B]$, scoring rule $S$ may not be proper in general
even if the individual single-dimensional scoring rules are proper.
In later sections, we will properly design the parameter $d$ and single-dimensional scoring rules such that the aggregated scoring rule will remain proper. 
% \begin{lemma}\label{lem:separate cap ic}
% The separate scoring rules with score cap $B$ is proper if .  
% \end{lemma}
% \begin{proof}
% For each task $i$, 
% for any $\signal_i\neq\bot$ and any $\signal'_i$, 
% conditional on receiving signal $\signal_i$,
% the score $\score_i(\signal_i,\outcome_i)$ first order stochastically dominates $\score_i(\signal'_i,\outcome_i)$. 
% Thus, the agent has incentives to truthfully report the signal $\signal_i$ if $\signal_i\neq\bot$.

% If $\signal_i=\bot$, by deviating the report, the expected score decreases on task $i$ decreases by $d_0$. 
% However, 


% $\sum_{i\in[n]} \score_i(\signal_i,\outcome_i)$
% first order stochastically dominates 
% $\sum_{i\in[n]} \score_i(\signal'_i,\outcome_i)$
% conditional on receiving signals $\signal$, 
% and the stochastic order preserves by shifting the reward and projecting it to the interval of $[0,B]$. 
% This directly implies that the agent's utility is maximized when reporting the signals truthfully to the principal. 
% \end{proof}





\begin{definition}\label{def:threshold}
A scoring rule $\score$ is a \emph{threshold scoring rule}
if there exist a recommendation set 
$\effortset\subseteq [n]$ and a threshold $ \threshold \geq 0$ on the number of tasks for the agent to predict correctly,
such that:
\begin{itemize}
    \item the score is $0$ if there exists task $i\in\effortset$ such that the reported signal is informative but wrong, i.e.\  $\signal_i\neq\bot$ and $\signal_i\neq\outcome_i$;
    \item let $k\triangleq\#\{i\in\effortset: \signal_i=\outcome_i\}$ be the number of tasks that the agent predicts correctly. 
    The score is $1$ if the agent's correct prediction exceeds the threshold, i.e.\ $k\geq \eta$; and $\frac{1}{2^{\threshold-k}}$ otherwise.
    %is $\min\{1, 2^{k-\threshold}\}$ if for all task $i\in\effortset$ with reported signal such that $\signal_i\neq\bot$, 
    %we have $\signal_i=\outcome_i$, 
    %where $k\triangleq\#\{i\in\effortset: \signal_i=\outcome_i\}$ is the number of tasks such that the agent correctly predicts the state.
\end{itemize}
\end{definition}

The threshold scoring rule in \Cref{def:threshold} is proper. 
In \cref{apx:exp proper for belief}, to help with the understanding, we provide an equivalent formulation of threshold scoring rules in the special case of threshold $1$ such that it is also proper for eliciting the belief.
Here we show that it is also equivalent to the following non-proper scoring rule with the same recommendation set $\effortset$ and threshold $\threshold$:
\begin{itemize}
    \item the score is $1$ if both (1) the number of reported informative signal exceeds the threshold, i.e., $\#\{i\in\effortset: \signal_i\neq \bot\}\geq \threshold$;
    and (2) any task $i\in\effortset$ such that the reported signal is informative is correct, i.e., $\signal_i\neq\outcome_i$ if $\signal_i\neq\bot$;
    \item the score is $0$ otherwise.
\end{itemize}

Conditioning on the agent receiving $k\leq \threshold$ informative signals, his best response is to guess the rest $\threshold -k$ signals, with  a probability $\frac{1}{2^{\threshold -k}}$ that he can guess all correctly and receive score $1$. His expected utility is thus $\frac{1}{2^{\threshold -k}}$, which implies this non-proper scoring rule is equivalent to the proper scoring rule in \Cref{def:threshold}.


%Intuitively, the threshold scoring rule provides a score of $1$ if the number of tasks such that the agent correctly predicts the state 
%exceeds the threshold. 
%If the agent makes any wrong prediction, the score is $0$. 
%Moreover, the score of the agent when he has fewer informative signals than the threshold is chosen such that the agent will not prefer random guessing than truthful reporting. 


\subsection{Sequential Effort Model}

In the sequential effort model, we assume that the agent can sequentially exert effort on different tasks 
before the interaction with the seller,
and the agent can make effort decisions based on the signals he has received on previous tasks. 
Formally, at any moment, let $\hat{\effortset}$ be the set of tasks that the agent has exerted effort, 
and let $\signal_{\hat{\effortset}}$
be the set of signals on those tasks. 
The agent's strategy $\strategy(\hat{\effortset},\signal_{\hat{\effortset}})\in[n]\cup\{\stopping\}\backslash\hat{\effortset}$
specifies a new task to exert effort on or stop exerting more efforts (represented by $\stopping$)
based on history observations.
The timeline of our model is as follows:
\begin{enumerate}
\item The principal commits to a proper scoring rule
$\score: \signals\times\outcomes\to [0,1]$.
\item The agent adopts a sequential strategy $\strategy$ for exerting effort on tasks.
\item States $\outcome=\{\outcome_i\}_{i=1}^n$ are realized. 
The agent receives signals $\signal$ 
and pays cost $\sum_{i\in \bar{\effortset}} \cost_i$
where $\bar{\effortset}$ is the set of tasks that the agent has exerted effort on before stopping.
\item The agent reports $\signal$
and receives score $\score(\signal, \outcome)$.
\end{enumerate}
Let $\expecto[\strategy]{\cdot}$ be the expectation when the agent follows strategy $\strategy$ for exerting efforts. 

Note that for the sequential effort setting, we do not require the principal to make strategy recommendations to the agent. 
The main reason is because the agent's optimal search problem might be computationally hard
given the designed scoring rules. 
In this case, it would be unreasonable to prove the performance guarantee of our proposed scoring rules assuming that the agent can best responding to the mechanism. 
Instead, we make a weak assumption on agent's behavior, and show that for any reasonable response of the agent, 
the expected value of the set of tasks that the agent has exerted effort on is large enough. 

\begin{definition}\label{def:obvious dominated}
A strategy $\strategy$ is \emph{obviously dominated} if there exists $\hat{\effortset}$, signal $\signal_{\hat{\effortset}}$
and task $i\not\in\hat{\effortset}$ such that $\strategy(\hat{\effortset},\signal_{\effortset})=\stopping$ 
and the agent increases his expected utility by exerting effort on task $i$ compared to stopping, i.e., 
\begin{align*}
\expect[\sigma_i\sim\{i\}]{\expect[\omega\sim\signal_i\cup\signal_{\hat{\effortset}}]{\score(\signal,\outcome)}}
- \cost_i
\geq \expect[\omega\sim\signal_{\hat{\effortset}}]{\score(\signal,\outcome)}.
\end{align*}
% A strategy $\strategy$ is \emph{obviously undominated} if it is not obviously dominated.
\end{definition}

Requiring the agent's strategy to be not obviously dominated is in the same spirit of undominated strategies in \citet*{babaioff2009single} and advised strategies in \citet*{cai2020implementation},
with the adaption to sequential environments \citep*[c.f.,][]{li2017obviously}.
In \cref{sec:search}, we will show that the principal's payoff is approximately optimal given our proposed scoring rules
if the agent's strategy is not obviously dominated.
