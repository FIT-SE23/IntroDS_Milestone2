\subsection{Symmetric Environments}
First, we consider the symmetric case where $n$ assignments have the same revealing probability $p$, cost $c$, and value $v$. 
\begin{theorem}\label{thm:symmetric-approx}
For $n$ i.i.d.\ assignments with the same revealing probability $p$, cost $c$, and value $v$, 
the maximum of the threshold scoring rule with threshold $1$ and a separate scoring rule with cap $1$ achieves an $\apxiid$-approximation to the optimal. The parameters of this separate scoring rule with score cap $1$ can be computed in polynomial time. 
% there exists a threshold scoring rule with parameter $k$,
% which requires the agent to choose a set $\effortset \subseteq [n]$ with $|\effortset|=k$
% and provides a reward of $1$
% if and only if the agent predicts all states correctly in $\effortset$,
% that is a constant approximation to the optimal scoring rule.
\end{theorem}


\begin{lemma}\label{lem:small c}
For any profile of values $\{\val_i\}_{i=1}^n$ and costs $\{\cost_i\}_{i=1}^n$, 
if $\cost_i\leq\frac{p}{96}$ for all $i\in[n]$,
there exists a separate scoring rule with score cap $1$ that is a $48$-approximation to the optimal. The parameters of this separate scoring rule with score cap $1$ can be computed in polynomial time.  
\end{lemma}
\begin{proof}
Let $\effortset$ be the set of tasks that are added greedily according to the ratio $\frac{\val_i}{\cost_i}$ until exceeding $\frac{1}{48}$.
By \cref{thm:budget inflation}, 
there exists a polynomial time algorithm for computing an incentive compatible mechanism 
using a separate scoring rule with score cap $1$ 
that incentives the agent to exert effort for all tasks in $\effortset$.
Moreover, $\sum_{i\in\effortset} \val_i$ is at least the optimal fractional knapsack value given budget $\frac{1}{48}$, 
where the latter is a $48$-approximation to the optimal utility of the principal given budget $1$.
\end{proof}






\begin{lemma}\label{lem:symmetric opt}
For $n$ i.i.d.\ assignments with the same revealing probability $p$, cost $c$, and value $v$, 
there exists an optimal proper scoring rule which assigns reward $\score(\signal,\outcome) = s_k$ if and only if the agent reports exactly $k$ informative signals, i.e. $|\{i:\sigma_i \neq \bot\}| = k$. 
This optimal scoring rule satisfies $s_{k+1}\geq s_k \geq \frac{s_{k+1}}{2}$
for all $1 \leq k \leq n-1$.
% $n$ rewards $s_{k}$ for all $k \in [n]$
% such that the agent chooses a subset $\effortset \subseteq [n]$
% and receives reward $s_k \in [0,1]$ where $k = |\effortset|$ be the size of $\effortset$
% if and only if the agent predicts all outcomes correctly in $\effortset$.
\end{lemma}

\begin{proof}
We show that any effort level incentivized by a proper scoring rule $\score$ can be incentivized by a scoring rule $\score'$ that assigns reward $s_k$ for $k$ informative signals. 

Since the outcomes of assignments are i.i.d.\ , without loss of generality, suppose the agent only exerts effort on the first $\neffort$ assignments $\effortset=\{1,\dots, \neffort\}$ for scoring rule $\score$. The expected score by exerting effort on $\effortset$ is $\expect[\effortset]{\expect[\signal]{\score(\signal,\outcome)}}$. For the assignments on which the agent exerts no effort, the posterior beliefs on these assignments do not change. Thus, $\expect[\effortset]{\expect[\signal]{\score(\signal,\outcome)}}$ does not depend on the posterior beliefs of the assignments $i>\neffort$. 

For each $k \in [l]$, let $s_k$ be the expected score of $\score$ over all posteriors with exactly $k$ informative signals. Then, the scoring rule $\score'$ with score $s_k$ for $k$ informative signals achieves the same expected score given by $\score$ if the agent exerts effort on any set $\effortset' \subseteq \effortset$. Since $\effortset$ is incentive compatible for $\score$, this set $\effortset$ is also incentive compatible for $\score'$.
%This expected score function $\hat{\score}$ can be implemented by such a scoring rule with rewards $r_i=\hat{\score}(i)$ for all $i \in [l]$.
%Let the number of assignments that the agent receives the informative signals be $\knowthetruth$. Then, the score with $k$ informative signals is $S(\knowthetruth)$. This expected score function $\hat{\score}$ can be implemented by such a scoring rule with rewards $r_i=\hat{\score}(i)$ for all $i \in [l]$.
%Therefore, the expected score $\tilde{S}$ can be written as a function of the number of assignments that the agent knows the truth. Let the number of assignments that the agent receives the informative signals be $\knowthetruth$. Then, the score with $k$ informative signals is $S(\knowthetruth)$. This expected utility function $u$ can be implemented such a scoring rule with rewards $r_i=S(i)$ for all $i \in [N]$.

By incentive compatibility, the rewards $s_k$ is weakly increasing in $\knowthetruth$, for $0 \leq \knowthetruth \leq \neffort$. Since the agent can guess the outcome of each assignment with probability $\sfrac{1}{2}$, we have that $s_k \geq \sfrac{s_{k+1}}{2}$, which completes the proof. 
% By the symmetry of assignments, $\hat{\util}$ does not depend on the no-effort assignments $\posterior_i, i>\neffort$, and can be written as a function of the number of assignments that the agent knows the truth. Let the number of assignments that the agent has belief $0$ or $1$ be $\knowthetruth$, the utility be $\util(\knowthetruth)$. By incentive compatibility, $\util$ is weakly increasing in $\knowthetruth$, with $\knowthetruth$ taking value in $[\neffort]$. We also know that $\util(\knowthetruth)\geq \sfrac{1}{2}\cdot\util(\knowthetruth+1)$, since the agent can guess the correct answer with probability $\sfrac{1}{2}$.
\end{proof}



\begin{lemma}\label{lem:symmetric-eps-small}
For $n$ i.i.d.\ assignments with $p \leq 1/2$ and $\epsilon = \frac{p}{2c} -1 \in (0,1/8)$, the maximum effort level that can be incentivized by a scoring rule is at most $-4 \frac{\log(\epsilon+1)}{\log(1-p)}$.
\end{lemma}

\begin{proof}
We show that the incentive compatibility constraint is not feasible for the effort level $-4\frac{\log(\epsilon+1)}{\log(1-p)}+1$ in the optimal scoring rule $\score$. By Lemma~\ref{lem:monotone task}, all effort levels greater than $-4 \frac{\log(\epsilon+1)}{\log(1-p)}$ can not be incentivized. 

The agent chooses to exert effort on $l+1$ assignments if and only if the expected utility of effort level $l+1$ is at least the expected utility of effort level $l$. By Lemma~\ref{lem:symmetric opt}, the optimal scoring rule assigns reward $s_k$ for $k$ informative signals. Since $n$ i.i.d. assignments have the same revealing probability $p$, the number of informative signals $k$ is from a binomial distribution $\binomial(\neffort,p)$ if the agent exerts effort on $\neffort$ assignments. The optimal scoring rule must satisfy the following incentive compatibility constraint
$$
\expect[\knowthetruth\sim\binomial(\neffort+1, p)]{\reward_\knowthetruth}-(\neffort+1)\cost \geq
\expect[\knowthetruth\sim\binomial(\neffort, p)]{\reward_\knowthetruth}-\neffort\cost.
$$
Then, we show that this incentive compatibility constraint is not satisfied when $l = -4 \frac{\log(\epsilon+1)}{\log(1-p)}$. Let $X_{\neffort,p}$ be a random variable from the binomial distribution $\binomial(\neffort, p)$. Let $f(k, \neffort, p) = \prob{X_{\neffort,p} = k}$ be the probability that $X_{\neffort,p}$ takes value $k$.  The difference between the expected utilities of effort levels $l+1$ and $l$ is 
\begin{align}\label{eqn:ic}
&\expect[\knowthetruth\sim\binomial(\neffort+1, p)]{\reward_\knowthetruth} - \expect[\knowthetruth\sim\binomial(\neffort, p)]{\reward_\knowthetruth} - \cost \nonumber\\
&= p \sum_{k=0}^{l} f(k,l,p) \reward_{\knowthetruth+1} + (1-p)\sum_{k=0}^{l} f(k,l,p) \reward_{\knowthetruth} - \sum_{k=0}^{l} f(k,l,p) \reward_{\knowthetruth} - \cost \nonumber\\
&=p \sum_{k=0}^{l} f(k,l,p) (\reward_{\knowthetruth+1} - \reward_{\knowthetruth}) - \cost.
\end{align}

We now show the right hand side in equation~(\ref{eqn:ic}) is strictly negative when the effort level $l = -4 \frac{\log(\epsilon+1)}{\log(1-p)}$. Recall $\epsilon = \sfrac{p}{2c}-1$. Since $-\log(1-p) \geq p$ and $\log(1+\epsilon) \leq \epsilon$, we have $\neffort \leq \sfrac{4\epsilon}{p}$. The mean of the Binomial distribution $\binomial(\neffort, p)$ is 
$$
lp \leq \frac{4\epsilon}{p} \cdot p =  4\epsilon \leq 1/2.
$$ 
The probability mass function $f(k,l,p)$ of the Binomial distribution $\binomial(\neffort, p)$ is monotone decreasing for $k > \neffort p$. Sicne $p\leq 1/2$ and $\neffort p \leq 1/2$, we have 
$$
f(0,\neffort,p) = (1-p)^\neffort = \frac{1-p}{\neffort p} \cdot f(1,\neffort,p) \geq f(1,\neffort,p).  
$$
Therefore, the probabilities $f(k,l,p)$ are non-increasing with respect to $k$. By Lemma~\ref{lem:symmetric opt}, the rewards of the optimal scoring rule satisfies $\reward_{k+1} \geq \reward_{k} \geq \reward_{k+1}/2$ for all $1\leq k \leq n$. For any $k \leq n$,  we have $\reward_0 \geq 2^{-k}\reward_k$, which implies $s_k-s_0 \leq (1-2^{-k})s_k \leq (1-2^{-k})$. If we consider $s_{k+1}-s_k$ in the right side of (\ref{eqn:ic}) as the weight on probability $f(k,\neffort,p)$, then these weights should satisfy the constraints $\sum_{i=0}^k (s_{i+1}-s_i) \leq (1-2^{-(k+1)})$ for $0 \leq k \leq \neffort$. Since the probabilities $f(k,l,p)$ are non-increasing, the first term in the right side of (\ref{eqn:ic}) is maximized by moving the weights to small $k$ terms under the constraints. By taking $\reward_{k+1}-\reward_k = 2^{-(k+1)}$ for all $k$, the first term in the utility difference is upper bounded by
\begin{align*}
p\sum_{k=0}^{l} f(k,l,p)2^{-(k+1)} = \frac{p}{2} \Big(1-\frac{p}{2}\Big)^l\sum_{k=0}^{l} f(k,l,\sfrac{p}{2-p}) = \frac{p}{2} \Big(1-\frac{p}{2}\Big)^l.
\end{align*}
Since $l = -4 \frac{\log(\epsilon+1)}{\log(1-p)} \geq \frac{2}{p}\log(1+\epsilon)$ for $p\leq \sfrac{1}{2}$ and $\epsilon = \frac{p}{2c}-1$, we have
\begin{align*}
\frac{p}{2} \Big(1-\frac{p}{2}\Big)^l - \cost \leq p\Big[\frac{1}{2} \Big(1-\frac{p}{2}\Big)^{\frac{2}{p}\cdot \log(1+\epsilon)}-\frac{c}{p}\Big] < p\Big(\frac{1}{2} \frac{1}{1+\epsilon} - \frac{c}{p}\Big) = 0.
\end{align*}
Hence, the incentive compatibility constraint is not satisfied for the effort level $l = -4 \frac{\log(\epsilon+1)}{\log(1-p)}$.

\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:symmetric-approx}]
We consider the following two cases based on the ratio of $c$ and $p$: (1) $\sfrac{p}{c} \geq 96$; (2) $\sfrac{p}{c} < 96$. For the first case $\sfrac{p}{c} \geq 96$, by Lemma~\ref{lem:small c}, we find a separate scoring rule with cap $1$ to achieve a $48$-approximation. We then focus on the second case where $\sfrac{p}{c} < 96$. Let $\epsilon = \sfrac{p}{2c}-1$. If $\epsilon\leq 0$, by \Cref{lem:single opt} and  \Cref{lem:monotone task}, the optimal level of effort is $0$.

For the case where $\epsilon \in (0,47)$, we show that the threshold scoring rule with threshold $k = 1$ achieves a $480$-approximation.

When the agent exerts effort on $\neffort$ assignments, the agent will get at least one informative signal with the probability $1-(1-p)^{\neffort}$. In this case, the threshold scoring rule assigns score $1$ to the agent. Otherwise, the agent gets no informative signal and guesses the outcome for one assignment with expected score $1/2$. Thus, the total expected utility for exerting effort on $\neffort$ assignments is 
$$
1-(1-p)^{\neffort}+\sfrac{1}{2}(1-p)^{\neffort} - \neffort\cost.
$$
% will be the expected score $[1-(1-p)^{\neffort}+\sfrac{1}{2}(1-p)^{\neffort}]$ minus the total cost $\neffort\cdot\cost$.

The marginal gain in the expected score for exerting effort on $\neffort+1$ assignments is
\begin{align}
    [1-(1-p)^{\neffort+1}+\sfrac{1}{2}(1-p)^{\neffort+1}]-[1-(1-p)^{\neffort}+\sfrac{1}{2}(1-p)^{\neffort}]=\sfrac{1}{2}\cdot p(1-p)^\neffort
\end{align}

The marginal gain is monotone decreasing in the amount of effort $\neffort$. The agent will exert effort until $\sfrac{1}{2}\cdot p(1-p)^\neffort>\cost$, which implies $l\geq \min\{\lceil -\frac{\log(\epsilon+1)}{\log(1-p)}\rceil, n\}$, where $\epsilon+1=\frac{p}{2\cost}$. The optimal amount of effort will be at least $1$. 
When $n < \lceil -\frac{\log(\epsilon+1)}{\log(1-p)}\rceil$, the threshold scoring rule is optimal since all $n$ assignments are incentivized by the threshold scoring rule. 
When $n \geq \lceil -\frac{\log(\epsilon+1)}{\log(1-p)}\rceil$, we analyze the threshold scoring rule in the following four cases. 

For $\epsilon\in(0, 1/8), p\leq 1/2$, by \Cref{lem:symmetric-eps-small}, the optimal level of effort is at most $4\neffort$, which implies this threshold scoring rule is a $4$-approximation.

For $\epsilon\in (0, 1/8), p> 1/2$, the optimal level of effort is $\opteffort\leq\frac{1}{c}\leq \frac{3}{p}\leq 6$. Since this threshold scoring rule incentivizes at least one assignment, this threshold scoring rule achieves a $6$-approximation.
    
For $\epsilon\in[1/8, 47), p\leq 1/2$, the level of effort incentivized by the threshold scoring rule is 
$$
\neffort \geq \lceil -\frac{\log(\epsilon+1)}{\log(1-p)}\rceil\geq \frac{\log(9/8)}{2p}\geq\frac{\log(9/8)}{2\cdot 96c}\geq \frac{1}{1700c}\geq \frac{\opteffort}{1700}.
$$ 
The approximation ratio is $1700$.
    
For $\epsilon\in [1/8, 47), p\geq 1/2$, the optimal level of effort is $\opteffort\leq\frac{1}{c}\leq \frac{96}{p}\leq 192$. The approximation ratio is at most $192$.
\end{proof}


% Since $\sum_{k=0}^l \reward_{\knowthetruth+1} - \reward_{\knowthetruth} = \reward_{l+1} - \reward_0\leq 1$, the first term in the utility difference is upper bounded by $p\prob{}{0;l,p} = p(1-p)^l$. For $l =  - \frac{\log(\epsilon+1)}{\log(1-p)}$, we have
% $$
% p(1-p)^l - \cost = p\Big(\frac{1}{1+\epsilon} - \frac{\cost}{p}\Big) = p\Big(\frac{2\cost}{p} - \frac{\cost}{p}\Big)
% $$



\begin{theorem}\label{thm:approx nonsymmetric cost}
For $n$ assignments with the same revealing probability $p$ and value $\val$, but different costs $\{\cost_i\}_{i\in [N]}$, there exists a polynomial time algorithm for computing an incentive compatible mechanism that is an $\apxratioc$-approximation to the optimal.
\end{theorem}

\begin{proof}
We divide the $n$ assignments into two sets $A_1,A_2$ based on the ratio $\sfrac{p}{\cost_i}$ as follows
$$
A_1 = \{i: \sfrac{p}{\cost_i} \geq 96\}; \qquad A_2 = \{i: \sfrac{p}{\cost_i} < 96\}.
$$
We show that, on each of the two sets $A_1, A_2$, we find a scoring rule achieves an $\apxc$-approximation to the optimal scoring rule on that set. By using the scoring rule on the set with a higher total value incentivized by the optimal scoring rule, the mechanism achieves a $\apxratioc$ approximation.



The following benchmark environment will be useful in the analysis:   the $n$ assignments, the same revealing probability $p$, and cost $c$ for any $c > \sfrac{p}{96}$. Let $\opt(\cost)$ be the number of assignments incentivized by the optimal scoring rule.  
Let $\obj(\cost)$ be the number of assignments that is incentivized by the threshold scoring rule with threshold $k=1$. 

By \Cref{lem:small c}, we compute a separate scoring rule with cap $1$ that achieves a $48$-approximation on the set $A_1$. It only remains to show there is a $\apxc$-approximation scoring rule on the set $A_2$. We use the threshold scoring rule with threshold $k=1$ on the set $A_2$ as in \Cref{thm:symmetric-approx}. 

We first lower bound the level of effort for the threshold scoring rule with threshold $k=1$. For the threshold scoring rule, the agent will start exerting effort on the set $A_2$ in increasing order of cost $\cost_i$. Therefore, we sort the assignments in the set $A_2$ in increasing order of cost: $\cost_1\leq \cost_2\leq \cdots\leq \cost_{n_2}$, where $n_2 = |A_2|$. Let $\neffort$ be the number of assignments incentivized by a threshold scoring rule with threshold $k=1$. Similar as Proof of \Cref{thm:symmetric-approx}, agent stops when cost of doing assignment $\neffort+1$ is less than the marginal gain on the expected score for doing one more assignment. Thus, we have
\begin{align*}
    \frac{p}{2}(1-p)^{\neffort}<\cost_{\neffort+1};
    \qquad
     \frac{p}{2}(1-p)^{\neffort-1}\geq \cost_{\neffort}.
\end{align*}
Since costs are sorted in increasing order, the threshold scoring rule with threshold $k=1$ can incentivize more assignments in non-symmetric case than that in the symmetric case with cost $\cost_{\neffort+1}$, i.e. $\neffort \geq \obj(\cost_{\neffort+1})$. 

We now upper bound the level of effort for the optimal scoring rule. Let $\effortset$ be the set of assignments incentivized by the optimal scoring rule on the set $A_2$. By the monotonicity in assignments in \Cref{lem:monotone task}, we have $\effortset' = \effortset \cap \{i: i\in A_2, \cost_i \geq \cost_{\neffort+1}\}$ can also be incentivized. By the monotonicity in cost in \Cref{lem:monotone-cost}, the set $\effortset'$ can still be incentivized if we change all costs in $\effortset'$ to $\cost_{\neffort+1}$, which implies that $\opt(\cost_{\neffort+1})\geq |\effortset'| \geq |\effortset|-\neffort$. 

By \Cref{thm:symmetric-approx}, we have $\apxiid\,\obj(\cost_{\neffort+1})\geq \opt(\cost_{\neffort+1})$. Thus, we have 
$$
\apxiid\, \neffort \geq \apxiid\, \obj(\cost_{\neffort+1}) \geq \opt(\cost_{\neffort+1}) \geq |\effortset| - \neffort, 
$$
which implies
$\apxc \neffort\geq |\effortset|$, i.e.\ the threshold scoring rule with threshold $k =  1$ is a $\apxc$-approximation on the set $A_2$.
\end{proof}


\begin{lemma}[Monotonicity in cost]~\label{lem:monotone-cost}
Fix the probability $p$ of receiving an informative signal. For any two set of costs $\{\cost_i\}_{i\in[N]}$
and $\{\cost'_i\}_{i\in[N]}$ such that  $\cost'_i\leq\cost_i$
for all $i\in [N]$. 
For any subset $\effortset\subseteq [N]$ of assignments, 
if there exists an incentive compatible mechanism with scoring rule $\score$  and recommendation set $\effortset\subseteq [N]$
 given $\{\cost_i\}_{i\in[N]}$, 
there also exists an incentive compatible mechanism with  scoring rule $\score'$ and recommendation set $\effortset$
given $\{\cost'_i\}_{i\in[N]}$.
\end{lemma}

\begin{proof}[Proof of \cref{lem:monotone-cost}]
Let the signal space be the product of signal space in the recommendation set and in the non-recommendation set, i.e.\  $\signal=\signal_{\effortset}\times\signal_{[N]\setminus \effortset}$. Truncate scoring rule $\score$ to the recommendation set $\effortset$ by setting score to a constant with regard to the signals in the non-recommendation set: $\score'(\signal, \outcome)=\score(\signal_{\effortset}\times \{\bot\}^{N-|\effortset|}, \outcome)$. Thus, the agent has no incentive to exert effort on the non-recommendation set $[N]\setminus\effortset$. For any set of effort $\effortset'$, consider the expected utility difference of exerting effort on $\effortset$ and $\effortset'$, which equals $\expect[\effortset]{\score}-\expect[\effortset']{\score}$. Decreasing the costs only makes this utility difference larger, which means $\effortset$ is incentive compatible with scoring rule $\score'$.
\end{proof}

\begin{lemma}\label{lem:asym-upper-bound}
For any set $\effortset$ such that $p\leq 1/2$ and $\cost_i \geq \sfrac{p}{2.1}$ for all task $i\in\effortset$, if the set $\effortset$ can be incentivized by a proper scoring rule, 
then $\abs{\effortset} \leq \frac{-3\log(p/2c^*)}{\log(1-p)} +1$, where $c^* = \max_{i \in \effortset} \cost_i$.
\end{lemma}
\begin{proof}
We first consider the  incentive compatibility constraint of the assignment in $\effortset$ with the highest cost. Let $i^* = \argmax_{i\in\effortset}\cost_i$. 
By the incentive compatibility constraint on assignment $i^*$, we have
$$
\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)}} - \expect[\effortset\backslash\{i^*\}]{\expect[\signal]{S(\signal,\outcome)}} - \cost_{i^*} \geq 0.
$$
Let $\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_{i^*} = \bot}$ be the expected score conditioned on signal $\signal_i = \bot$ when the agent exerts effort on $\effortset$. We have $\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_{i^*} = \bot} = \expect[\effortset\backslash\{i^*\}]{\expect[\signal]{S(\signal,\outcome)}}$. Similarly, let $\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_{i^*} \neq \bot}$ be the expected score conditioned on $\signal_{i^*} \neq \bot$ with effort set $\effortset$. Then, the incentive compatibility constraint on $i^*$ is
\begin{align}\label{eqn:asym_ic}
&p\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_{i^*} \neq \bot} + (1-p)\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_{i^*} = \bot} - \expect[\effortset\backslash\{i^*\}]{\expect[\signal]{S(\signal,\outcome)}} = \nonumber\\
&= p\bigg(\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_{i^*} \neq \bot} - \expect[\effortset\backslash\{i^*\}]{\expect[\signal]{S(\signal,\outcome)}}\bigg) \geq \cost_{i^*}.
\end{align}

We now show the upper bound on the difference between the expected score with signal $\signal_{i^*}\neq \bot$ and the expected score with signal $\signal_{i^*} = \bot$. For any $i\in\effortset$, let $\event_i$ be the event that at least one signal that is not $\bot$ from tasks $\effortset\backslash\{i\}$.  
Let $q$ be the probability that $\event_i$ happens
\begin{align*}
q=\prob[\effortset]{\event_i} = (1-(1-p)^{\abs{\effortset}-1}). 
\end{align*} 

We bound the expected score difference under the following two cases: (1) event $\event_{i^*}$ does not happen; (2) event $\event_{i^*}$ happens.
% Then, we have
% $$
% \expect[\effortset\backslash\{i^*\}]{\expect[\signal]{S(\signal,\outcome)}} = q\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \event} + (1-q)\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_i = \bot, \forall i \in \effortset}.
% $$

We first upper bound the expected score difference conditioned on event $\bar{\event}_{i^*}$. Since the agent can guess the outcome of assignment $i$ with probability $1/2$, we have
$$
\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_{i^*}= \bot, \bar{\event}_{i^*} } \geq \expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_{i^*}\neq \bot, \bar{\event}_{i^*} }/2,
$$
which implies the expected score difference conditioned on $\bar{\event}_{i^*}$ is at most $1/2$. 

We now upper bound the expected score difference conditioned on event $\event_{i^*}$. We show that $\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_{i^*} = \bot, \event_{i^*}} \geq 3/4$, which implies the expected score difference conditioned on $\event_{i^*}$ is at most $1/4$. For any assignment $i\neq i^*, i \in \effortset$, by the incentive compatibility constraint on assignment $i$, we have
$$
p\bigg(\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_i \neq \bot} - \expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_i = \bot}\bigg) \geq \cost_i.
$$
Since the scoring rule is proper, we have $\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_i \neq \bot} \geq \expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_i = \bot}/2$. Thus, the expected score conditioned on signal $\signal_i \neq \bot$ is at least
$$
\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_i \neq \bot} \geq \frac{2\cost_i}{p} \geq \frac{1}{1+\epsilon}.
$$
Let $\event_i$ be the event that there is at least one signal is not $\bot$ from $\effortset\backslash \{i\}$. We upper bound the expected score conditioned on signal $\signal_i \neq \bot$ by 
\begin{align*}
\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_i \neq \bot} =& (1-p)^{|\effortset|-1}\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_i \neq \bot, \bar{\event_i}} \\
&+ (1- (1-p)^{|\effortset|-1})\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_i \neq \bot, \event_i} \\
\leq & \expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_i \neq \bot, \bar{\event_i}} + (1- (1-p)^{|\effortset|-1}).
\end{align*}
By the monotonicity in cost in Lemma~\ref{lem:monotone-cost} and Lemma~\ref{lem:symmetric-eps-small}, we have $|\effortset| \leq -4\frac{\log(1+\epsilon)}{\log(1-p)}$. Thus, we have
\begin{align*}
\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_i \neq \bot, \bar{\event_i}} &\geq \expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_i \neq \bot} - (1- (1-p)^{|\effortset|-1}) \\
&\geq \frac{1}{1+\epsilon} - \bigg(1-\frac{1}{(1+\epsilon)^4}\bigg)\geq \frac{3}{4}.
\end{align*}
Since the scoring rule is proper, we have  $\expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_{i^*} = \bot, \event_{i^*}} \geq 3/4$, which implies the expected score difference conditioned on $\event_{i^*}$ is at most $1/4$. 

Combining the above two cases, the expected score difference in the equality~(\ref{eqn:asym_ic}) is
\begin{align*}
    \expect[\effortset]{\expect[\signal]{S(\signal,\outcome)} \mid \signal_{i^*} \neq \bot} - \expect[\effortset\backslash\{i^*\}]{\expect[\signal]{S(\signal,\outcome)}} 
    &\leq \prob{\event_{i^*}}\frac{1}{4} + \prob{\bar{\event}_{i^*}}\frac{1}{2} \\
    &\leq q/4 + (1-q)/2 = 1/2 -q/4.
\end{align*}
By the inequality (\ref{eqn:asym_ic}), we have 
$$
p\bigg(\frac{1}{2}-\frac{q}{4}\bigg) \geq \cost_{i^*}.
$$
Let $\epsilon^* = \sfrac{p}{2\cost_{i^*}}-1$. Then, the maximum cost $\cost_{i^*} = \frac{p}{2(\epsilon^*+1)}$, which implies 
$$
q \leq \frac{2\epsilon^*}{1+\epsilon^*} \leq 2\epsilon^*.
$$
Since $q = 1-(1-p)^{|\effortset|-1}$ and $-\log(1-2\epsilon^*) \leq 3\log(1+\epsilon^*)$ for $\epsilon^* \in (0,0.05)$, we obtain 
$$
|\effortset| \leq \frac{\log(1-2\epsilon^*)}{\log(1-p)} +1 \leq \frac{-3\log(1+\epsilon^*)}{\log(1-p)} +1,
$$
where $\epsilon^* = \sfrac{p}{2\cost_{i^*}}-1$.
\end{proof}
