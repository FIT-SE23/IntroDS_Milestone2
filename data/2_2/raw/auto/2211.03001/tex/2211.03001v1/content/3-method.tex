
\section{Reading in VR}
Reading documents in a virtual reality typically involves reading a 2D document placed in a window in a 3D virtual world. This is different from users’ experience of reading in 2D displays, as document windows can be positioned at various depths and orientations. This gap might cause discomfort and inconvenience in terms of VR reading, resulting in users being reluctant to attempt this activity on a VR platform. Since prior work has found that users did not prefer head-fixed presentation when reading a paragraph of texts in VR~\cite{rzayev2021reading} our goal was to study and develop tools that allow the reader to move freely in VR, select and attach/detach documents to a reading frame easily and enhance readability. We envisioned a scenario where readers might encounter multiple documents, both long and short, in VR and that users would need to select and deselect these documents to read them.  Whereas previous works have mostly focused on short paragraphs (approximately 100 words) with a uniform font size~\cite{rzayev2018reading} we instead chose to observe users' reading behavior with longer documents with different font sizes for structure.  Based on observations from our formative study, we identified user pain points for this task using current interaction techniques.



\subsection{Formative Study}
The goal of our formative study was to identify user pain points for reading in VR from selection to reading completion with currently available tools. We observed eight users interacting with six documents of varying lengths using available VR interaction techniques. 

\subsubsection{Selected Manipulation Method}
Users wore a state-of-the-art VR Headset and were provided with a set of interaction tools that are commonly used across VR platforms for object manipulation and 2D canvas interaction (as surveyed in Section 2). Manipulation was done using the HTC Vive controller. We first asked users to select document windows and make translation movements at distance using a "laser-pointer" raycasting method~\cite{steamvr2019, vrtoolkit2019}. When a VR controller button is pressed by the user, a laser, or a ray cast, is projected in the direction to which the user is pointing, and the first colliding object in the virtual world is selected. Users can also make translation movements by moving the controller while pressing the dedicated button. For 6DOF manipulation, we use a method where users can ``grab’' and move or rotate 2D windows, which is also a common object manipulation method~\cite{wang20116d}.  Here, when a direct collision is detected between the VR controller and a 2D document window while the assigned button is pressed, the object follows the motion of the controller. An example is depicted in Figure.~\ref{fig:formative}.
Based on these interactions, users can manipulate the 2D document windows that are at various orientations.



%\begin{figure}[t]


\begin{figure*}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{pictures/vrdoc3.png}
	\caption{Interactions provided with \textit{VRDoc}. With \textit{Gaze Select-and-Snap}, (a) the document window is highlighted in green when focused and snaps in front of the user's head position when selected. \textit{Gaze MagGlass} creates a floating canvas that creates a magnifying effect shown in (b). \textit{Gaze Scroll} provides (d) gaze-activated buttons that scroll up or down by a sentence when gazed at.}
	
	\label{fig:vrdoc}
	\vspace{-1em}
\end{figure*}


\subsubsection{Participants and Procedure}
We recruited eight participants (three females, five males). Three had previous experience in VR but not in reading texts in VR. Participants wore an HTC Vive Pro Eye~\cite{vive} for the VR HMD, which has  $1440  \times 1600$ pixels per eye ($2880 \times 1600$ pixels combined), a $90$ Hz refresh rate, and a $110^{\circ}$ for the field of view. It also provides eye tracking capabilities.

Participants start from a fixed position with six documents placed in front of them as seen in Fig.~\ref{fig:formative}. The documents were placed such that they formed a semicircle around the user’s starting position. Four of the documents were short passages with about $100$ words, while the other two were long passages with about $500$ words in length. Following Dingler et al ~\cite{dingler2018vr}, the VR text presentation guideline by having a black background color and white text color for all documents. All six documents had the same canvas size, requiring the long documents to have scroll-able windows. The participants were able to scroll by selecting a document and moving their fingers vertically on the VR controller trackpad.

Each participant was given $20$ minutes to freely read all six documents without any specific order.  We followed the experience with a semi-constructed interview. The questions focused on the general experience of reading in VR, satisfaction with current interaction techniques, and desired user features.


\subsubsection{Observations and Feedback}
Through observing users’ behavior and collecting feedback through an interview, we were able to identify the following user pain points.

\paragraph{1. Positioning:} When selecting a document, it took participants multiple attempts  to re-orient the document window into the desired position. Unlike general object manipulation, participants tended to have a preferred distance and orientation (upright) for reading. Six participants noted that positioning the document to this preferred location and orientation took more time than expected. P1 commented, ``{\em It took me a while to figure out what position worked for me the best to fully view the document.}’’ All eight participants mentioned that switching their attention between multiple documents made them more aware of this inconvenience. P3 commented, ``{\em During the trial there was a moment when the document windows start to overlap as I select and position them. The pile definitely made it difficult to identify and select the documents. I wish there could be an easy way where I can quickly pick up a document that I want to read.}’’. Five participants brought up the need to automate the positioning procedure as they already knew how they wanted the document of interest to be oriented: up right in front of their head position. 

\paragraph{2. Readability:} All eight participants reported that document readability was poor due to its resolution and distortion. P2 and P5 commonly mentioned that when reading on a 2D display, they were even able to read texts sideways, but with the VR headset the text appeared blurry and this was not possible. Five participants noted that unless the document was perfectly upright, the slant created a distortion that decreased readability. We observed that participants positioned the documents with smaller font sizes closer, effectively magnifying the font, to enhance readability.   

\paragraph{3. Arm Fatigue:} During the $20$ minute trial, participants tended to hold the VR controller up constantly to interact with the documents. This behavior was consistently observed even when users were using the controller's trackpad for scrolling since the rest of the interactions, such as selecting and moving the document, required the user to hold up the physical device. Four of the participants reported arm fatigue which is a known issue ("Gorilla Arm") for gesture-based interactions~\cite{jang2017modeling} which are prevalent in VR. This is exacerbated by the additional weight of the controller when compared to watch-based gesture interactions. Such issues of fatigue would likely increase in longer VR experiences. 
\vspace{0.2em}
Based on this formative study, we developed tools specifically to address positioning, readability, and arm fatigue.


\subsection{Our Approach: VRDoc}
To address user pain points we developed three new tools: Gaze Select-and-Snap, Gaze MagGlass, and Gaze Scroll, which we collectively refer to as VRDoc, tools for better document reading in VR. This section describes our design process from user needs to potential solutions. 

Focused on the pain points of positioning, readability, and arm fatigue, our design thinking progressed as follows: 
\begin{enumerate} 
\itemsep-0.1em
\item Users do not seem to require or desire as much object manipulation freedom when reading documents.  Given the user tendency for a specific positioning with respect to documents, we should automate and simplify moving the document to a near-optimal position once selected.  Automatically positioning the document in an upright non-skewed position will enhance readability.  
\item Users manipulated documents to effectively magnify text, but this often led to documents being positioned too close for the reader to easily contextualize their place in the document.  A better solution would be to selectively magnify the current text the user is reading.
\item VR controller use and arm gestures should be minimized. While some VR controller use may still be required, in a virtual office, for longer reading tasks the user should not need to use their arms at all, enabling them to set the controller down.  We believe this will reduce fatigue and improve the overall experience.
\end{enumerate}
We believed that the novel eye tracking capability of the Vive headset could be leveraged to develop solutions for some of these issues.  Eye-tracking is becoming increasingly available in commodity HMDs~\cite{vive} and since reading naturally evokes specific eye movements, document interactions with gaze are natural and intuitive. 
%Therefore, we designed \textit{\textbf{VRDoc}}, a set of gaze-based interaction methods that would enhance users' reading experience. 
In our approach,  we use the SDK provided by HTC Vive\footnote{https://developer.vive.com/documents/718/VIVE\_Pro\_Eye\_user\_guide.pdf} for eye tracking calibration and data with the Unity game engine. The headset provides eye tracking with an accuracy of  $0.5^{\circ}$ to $1.1^{\circ}$ at 120 Hz.


%\begin{figure*}[h]
%	\centering
%	\includegraphics[width=1\linewidth]{pictures/VRDoc.png}
%	\caption{Interactions provided with \textit{VRDoc}. With \textit{Gaze Select-and-Snap}, (a) the document window is highlighted in green when selected and (b) the document snaps in front of the user's head position. (c) \textit{Gaze MagGlass} creates a floating canvas that creates a magnifying effect. \textit{d} provides gaze-activated buttons that scrolls up or down by a sentence when gazed at.}
%	\label{fig:VRDoc}
%\end{figure*}

\subsubsection{Positioning: Gaze Select-and-Snap}
The infinite freedom of object manipulation in 3D was actually a negative factor in document positioning.  Documents were only readable in the upright position near the users' direct line of sight.  We developed \textit{Gaze Select-and-Snap} to automate the action of selecting and positioning through the user's gaze. Prior work has established that users value the ability to select 3D objects with gaze and bring these closer to the user's hand~\cite{kennedy1993simulator}, but this is the first method designed for document objects (a virtual object that is tagged as ``document'') that both rotates the 3D object into a specific position and snaps it into a fixed effective 2D perspective specifically for reading. 

To engage Gaze Select-and-Snap, the user first directs their gaze toward the 3D document object, the gaze focus is detected and the document object is highlighted with a green stroke to visualize its selection for the user.  With a single click of the trigger button, the 3D document object is brought forward towards the head position and snapped into an effective 2D position in front of the user. The window is snapped parallel to the user's head position, ensuring the window stays upright. An example of this interaction is shown in Figure.~\ref{fig:vrdoc} (a).

When multiple documents overlap, Gaze Select-and-Snap first highlights the top document, then if the top document is not selected, Gaze Select-and-Snap sequentially brings hidden documents to the forefront until the desired document is identified and selected. 


\subsubsection{Readability: Gaze MagGlass}
To improve text readability once the document was in position, we incorporated a magnifying glass effect that is activated by users' eye movements: ~\textit{Gaze MagGlass}.  For low-vision computer users, video-based eye trackers have been used effectively to increase the on-screen magnification in traditional computing settings~\cite{wittich2018effectiveness, maus2020gaze}, however, to the best of our knowledge, this is the first implementation of interactive selective text magnification in VR. To enhance usability, \textit{Gaze MagGlass} is only activated when (1) the user gazes at a document, (2) the document is within a certain distance ($< 0.5 m$), and (3) when the user gazes at the document window for more than $1.5$ seconds. These robust heuristics were designed to ensure that the document in view is the specific document that the user wants to read. 

When the activation conditions are met, a second virtual camera is created on the collision point of the user’s gaze and the document object. The virtual camera is perpendicular to the document while following the user’s gaze.  The captured scene is rendered at a texture of a 2d plane that is rendered in front of the main camera. The field of view of the virtual camera and the distance from the document object are heuristically determined so that it magnifies the document by 150\% with a size that covers approximately $4$ to $5$ words of three consecutive sentences.

Directly applying raw gaze position data to the virtual camera causes great jittering as eye tracker data are inherently noisy and include tracking errors. This worsens the user experience as the jittering is visualized in a magnified way. To address this issue, we generally follow the saccade detection and smoothing algorithm from~\cite{kumar2007guide} so that the position of \textit{Gaze MagGlass} is calculated as a weighted mean of the set of points within a fixation window.

\textit{Gaze MagGlass} is automatically initiated when the activation conditions are met but can be manually turned on or off by the user if necessary. Note that activation conditions and the degree of magnification were heuristically determined for the study. An example of the activation is depicted in Figure.~\ref{fig:vrdoc} (b).

%To improve readability in an interaction perspective, we incorporated a magnifying glass effect that is activated by users' eye movements: ~\textit{Gaze MagGlass}. The idea of a gaze-guided magnifying lens has been actively researched for computer users~\cite{maus2020gaze}. \textit{Gaze MagGlass} is only activated when (1) the user gazes at a document, (2) the document is within a certain distance ($< 0.5 m$), and (3) when the user gazes at the document window for more than $1.5$ seconds. This is used to ensure that the document in view is the specific document that the user wants to read. 

%When the activation condition is met, a rectangular canvas that magnifies the document by 150\% with a size that covers approximately $5$ to $6$ words of three consecutive sentences appears on the document window. The floating canvas follows the user's eye movement. \textit{Gaze MagGlass} is automatically initiated when the activation conditions are met but can be manually turned on or off by the user if necessary. Note that activation conditions and the degree of magnification were heuristically determined for the study. An example of the activation is depicted in Figure.~\ref{fig:VRDoc} (c).

%With \textit{Gaze MagGlass}, users are relieved of the burden of continuously adjusting the document window when reading texts with different font sizes. The readability of the text is ensured because it enhances areas on which the eyes focus. Also, there is an advantage of being able to keep track of your position within the document. We evaluate it using studies and describe more details in Section 4.


\subsubsection{Gaze Scroll}
Since longer documents are rarely considered in VR reading studies, Gaze Scroll is the first tool designed specifically to help alleviate fatigue when reading longer documents.  The objective of gaze scroll is to avoid the necessity of users having to re-engage with the controller after they have begun reading.  At this point, the document should be snapped into the 2D reading position and the user should be able to put the controller down.  To facilitate document navigation without a controller, two buttons were placed within a document each on the top and the bottom of the window, as seen in Figure~\ref{fig:vrdoc} (c). When the user's gaze reaches the button for $0.5$ seconds, the document scrolls up or down by a full sentence.  Fixating the gaze on the button can increase the number of sentences. For example, if a user stares at the lower button for 2 seconds, the document scrolls down by four sentences.  This activation condition creates a controlled advancement and minimizes focal changes, which can be frequent when scrolling is rapid or uncontrolled. 

\vspace{0.5em}
\textit{VRDoc} tools are designed to facilitate a better reading experience in VR, from automating the selection and positioning of document windows to magnifying text for readability to allowing gaze-based navigation of longer documents.

