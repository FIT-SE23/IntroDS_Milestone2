\section{Evaluation}
After implementing our tools we conducted a series of user studies to investigate how users’ reading experiences changed with VRDoc tools. We aimed to answer the following research questions through our evaluation: 
\vspace{-0.3em}
\begin{itemize}
\itemsep -0.3em
    \item Does Gaze Select-and-Snap improve document handling (positioning)?
    \item Does Gaze MagGlass improve readability?
    \item Do VRDoc tools including Gaze Scroll for navigation lessen feelings of fatigue? 
    \item Overall do VRDoc tools work together to improve efficiency and usability? 
    \item Do readers prefer having access to VRDoc tools when reading in VR?
\end{itemize}
\vspace{-0.3em}
We first evaluated each tool of VRDoc by comparing it to the basic object manipulation defined in Section 3.1 (henceforth, \textit{baseline}). Then, we conducted a study where users were given access to all VRDoc tools versus a baseline.  
% After implementing our tools we conducted a user study to investigate how users' reading experiences are affected by the addition of VRDoc tools. We compared compared VRDoc reading to reading with only the basic manipulation tools provided in the pilot study (described in Section 3.1). 
% With \textit{VRDoc} added to the baseline, users can fully utilize the automated actions for document reading while also maintaining the freedom to manipulate document windows if needed.

\begin{figure*}[h]
	\centering
	%\includegraphics[width=1.05\linewidth]{pictures/tasks.png}
	\makebox[\textwidth][c]{\includegraphics[width=1.05\textwidth]{pictures/tasks.png}}
	\vspace{-2.0em}
	\caption{Our experiment setup for evaluation. For individual tool evaluation; (a) Task 1: Five 100-word passages, (b) Task 2: A 100-word passage, and (c) Task 3: A 500-word passage with a scroll bar. For the combined evaluation; (d) Two 100-word passages and one 500-word passage with a scroll bar.}
	\label{fig:expsetting}
	\vspace{-1.5em}
\end{figure*}

\subsection{Participants}
Thirteen participants (eight male, five female) were recruited from a convenience sample of university students for the evaluation experiment (age range $23$-$32$, $\mu=27.08$, $\sigma=2.63$). Eight participants wore glasses, three wore contact lenses, and the rest did not require vision correction. All of our participants were proficient in English. Eight of the participants had previously experienced VR systems. Participants were compensated $10$ USD after the experiment.
%Seventeen participants (ten females, seven males) were recruited from a convenience sample of university students for the evaluation experiment (age range $20$-$31$, $\mu=24.59$, $\sigma=2.67$). Participants were compensated  $10$ USD. All subjects had normal or corrected-to-normal vision. Five of the subjects had experienced VR systems. 

\subsection{Settings}
The experiments were set in a virtual office environment for immersion. The tracking area was set to $1.5m \times 1.5m$ as our task did not require much movement from the participants. We followed the guidelines suggested from previous work ~\cite{dingler2018vr} for our experiment setup. The document window contained a view box that displays a total of $9$ lines with each line comprising around $65$ characters. A white sans-serif Arial font (size $12$) was used for text and the background was set to black. The texts were left-aligned and the line spacing was set to $1.2$. The text materials were selected from the ``Asian and Pacific Speed Reading for ESL Learners’’~\cite{quinn2007asian} to guarantee a similar difficulty level. Note that the length of the text materials was slightly edited for the experiments: around 100 words for \textit{Short passages} so that they do not require scrolling and around 500 words for \textit{Long passages}.



\subsection{Experiment Procedure}
We employed within subject experiments in the order of (1) individual tool evaluation and (2) combined evaluation.
Each of the tools of VRDoc (Gaze Select-and-Snap, Gaze MagGlass, and Gaze Scroll) was compared to baseline with separate tasks. The order of tasks for individual tool evaluation and tool presentation (VRDoc, baseline) for all experiments was counterbalanced using Latin-square.



\paragraph{Individual Evaluation}
\vspace{-0.1em}
\begin{itemize}
\itemsep-0.1em
    \item \textbf{Task 1 (Gaze Select-and-Snap):} Five \textit{Short passages} are placed in front of the starting position in a semicircle, as seen in Fig.~\ref{fig:expsetting} (a). The order of the \textit{Short passages} is random. Readers are required to select each of the five documents to read them.  When the reader finishes the fifth document, a five-question reading comprehension test appears. While taking the test, readers can choose to review any of the documents by re-selecting them to help answer the questions. 
    
     \item \textbf{Task 2 (Gaze MagGlass):} One \textit{Short passage} is placed in front of the reader in the starting position as seen in Fig.~\ref{fig:expsetting}.  Gaze MagGlass tracks the reader's gaze and magnifies the font ~150\%.  When the reader reaches the end of the text, a two-question reading comprehension test appears.
     
     \item \textbf{Task 3 (Gaze Scroll):} One \textit{Long passage} is placed in front of the starting position as seen in Fig.~\ref{fig:expsetting} with no magnification. When the reader reaches the end of the text, a three-question reading comprehension test appears.
\end{itemize}



\paragraph{Combined Evaluation}
\begin{itemize}
    \item \textbf{Task 4 (VRDoc):} Two \textit{Short passages} and one \textit{Long passage} are placed in front of the starting position in a semicircle, as seen in Fig.~\ref{fig:expsetting} (a). The order of the passages is random. The reading comprehension test consists of five questions. Readers are required to select each of the three documents to read them. During the test, readers can choose to review any of the documents (by reselecting) them to help them answer the questions.
\end{itemize}

\vspace{0.2em}

Users are asked to solve a reading comprehension test after reading the given documents. 
The reading comprehension test consists of fact-check, multiple-choice questions based on the given passage. Once the participant verbally determined that they were ready to take the quiz, a 2D plane containing the questions is displayed in front of the user in the VR environment. The task completion time (TCT) was measured separately by \textit{reading time} and \textit{solving time}. The \textit{reading time} is defined as the time from the start of the trial until before reading the questions, and the \textit{solving time} is the time from after reading the questions until the participant verbally determines the final answer.

The user study was conducted in the following order: A participant was first introduced to the overview and goal of the experiment and filled out a demographic survey. Then, the participant performs each of the individual evaluation tasks in a given order, and finally the combined evaluation task. Each task consists of two trials, one performed using our proposed method and the other using baseline. The order of individual evaluation tasks and the order trials for all tasks were counterbalanced among the participants using Latin-square. Prior to each trial, participants filled out a pre-SSQ~\cite{kennedy1993simulator} survey and were given a tutorial including a five-minute training session. After each trial, participants filled out a SUS questionnaire~\cite{brooke1996sus} for system usability, Raw-TLX~\cite{hart2006nasa} for workload, post-SSQ for VR sickness, and a subjective evaluation questionnaire on a 5-point Likert scale for readability, effectiveness, and preference (See appendix \ref{custom_questions}). The experimenter reminded participants to consider the interaction aspect when answering the questions. They were given an additional 5-minute break in between trials. Note that, before each trial, we ran HTC Vive Pro Eye's eye calibration software to ensure proper gaze tracking.

After each task was performed, we conducted a semi-structured interview to collect participants’ comments and feedback. The entire experiment lasted around 90 minutes per participant. All trials performed by the participants were recorded in video files for accurate observation and evaluation. In addition to TCT for all tasks, we measured the time Gaze MagGlass was activated for Task 2 and Task 4.

\subsection{Results}
In this section, we divide the evaluation items collected during Task 1 through Task 4 into objective measures and subjective measures.
Note that the results are not directly comparable between tasks as the reading conditions vary. The visualized results can be found in appendix \ref{stat_charts}.

\subsubsection{Objective Measures} 

\paragraph{\textbf{TCT}}
%Recall that TCT is the sum of reading time and solving time.
%For Task 1, participants took longer to finish the task with baseline ($\mu=27.08$, $\sigma=2.63$) than with Gaze Select-and-Snap ($\mu=27.08$, $\sigma=2.63$). The Wilcoxon signed-rank test revealed a significant difference between the two methods in reading time ($Z = 34.111, p < 0.001$), solving time ($Z = 34.111, p < 0.001$), and TCT ($Z = 25.000, p < 0.001$). The results of Task 2 indicated that participants tend to read faster with Gaze MagGlass ($\mu=27.08$, $\sigma=2.63$) than baseline ($\mu=27.08$, $\sigma=2.63$). However, no significant difference between the two methods ($p > 0.05$). The same goes for solving time and TCT ($p > 0.05$). 
%For Task 3, participants read slightly faster with Gaze Scroll ($Z = 25.000, p < 0.001$) than baseline ($Z = 25.000, p < 0.001$), while the solving time of baseline ($Z = 25.000, p < 0.001$) was slightly faster than Gaze Scroll ($Z = 25.000, p < 0.001$). The Wilcoxon signed-rank test revealed  no significant difference between the two methods in reading time, solving time, and TCT ($p > 0.05$). 
%Finally, with Task 4, participants read faster with VRDoc ($\mu=27.08$, $\sigma=2.63$) than with baseline ($\mu=27.08$, $\sigma=2.63$), solved faster with VRDoc ($\mu=27.08$, $\sigma=2.63$) than with baseline ($\mu=27.08$, $\sigma=2.63$), which resulted in a shorter TCT with VRDoc ($\mu=27.08$, $\sigma=2.63$) than with baseline ($\mu=27.08$, $\sigma=2.63$). The Wilcoxon signed-rank test revealed a significant difference between the two methods in reading time ($Z = 34.111, p < 0.001$), solving time ($Z = 34.111, p < 0.001$), and TCT ($Z = 34.111, p < 0.001$).


We measured Task Completion Time (TCT), the sum of reading time and solving time for all four tasks:
\begin{itemize}
\itemsep-0.1em
    \item \textbf{Task 1}: Gaze Select-and-Snap allowed readers to complete both the reading task and the comprehension quiz faster than baseline.  Reading Time: Gaze Select-and-Snap ($\mu=237.73 sec$, $\sigma=21.20$), baseline ($\mu=266.62 sec$, $\sigma=34.01$); Solving Time: Gaze Select-and-Snap ($\mu=83.824 sec$, $\sigma=39.65$) baseline($\mu=107.810 sec$, $\sigma=50.91$). Both reading time and solving time revealed a significant difference between the two methods according the the Wilcoxon signed-rank test: reading time ($Z = -2.121, p = 0.034$),  solving time ($Z = -2.366, p = 0.018$), and TCT ($Z = -2.904, p = 0.004$). 
    
    \item \textbf{Task 2}: Readers read slightly faster with Gaze MagGlass ($\mu=38.11 sec$, $\sigma=13.40$) than the baseline ($\mu=41.31 sec$, $\sigma=10.73$) but the difference was not significant ($p = 0.388$). The solving time and TCT differences were also not significant (solving time: $p = 0.814$, solving TCT: $p = 0.530$). Solving Time: Gaze MagGlass ($\mu=29.41 sec$, $\sigma=10.39$), baseline ($\mu=28.91 sec$, $\sigma=6.99$); TCT: Gaze MagGlass  ($\mu=67.52 sec$, $\sigma=18.79$) baseline($\mu=70.21 sec$, $\sigma=13.39$).
    
    \item \textbf{Task 3}: Gaze Scroll allowed readers to read the longer passages slightly faster than baseline, however, Gaze Scroll had a negative impact on test solving time. Reading Time: Gaze Scroll ($\mu=157.17 sec$, $\sigma=20.41$), baseline ($\mu=164.48 sec$, $\sigma=19.67$); Solving Time: Gaze Scroll  ($\mu=92.85 sec$, $\sigma=36.64$) baseline($\mu=85.74 sec$, $\sigma=31.80$). Nevertheless, the differences were not significant which is also reflected in TCT: Gaze Scroll ($\mu=250.01 sec$, $\sigma=46.02$) baseline($\mu=250.21$, $\sigma=40.30$) ($p = $0.235).
    
     \item \textbf{Task 4}: With VRDoc, readers have significantly faster reading time and solving time than with baseline. Reading Time: VRDoc ($\mu=284.25 sec$, $\sigma=26.02$), baseline ($\mu=304.86 sec$, $\sigma=41.30$); Solving Time: VRDoc ($\mu=109.55 sec$, $\sigma=37.85$) baseline($\mu=135.78 sec$, $\sigma=40.76$); TCT:  VRDoc ($\mu=393.80$, $\sigma=50.48$) baseline($\mu=440.64 sec$, $\sigma=52.90$). Both reading time and solving time revealed a significant difference between the two methods according the the Wilcoxon signed-rank test: reading time ($Z = -2.432, p = 0.015$),  solving time ($Z = -3.101, p = 0.002$), and TCT ($Z = -2.746, p = 0.006$). 
\end{itemize}


% The results of Task 2 indicated that participants tend to read faster with Gaze MagGlass ($\mu=27.08$, $\sigma=2.63$) than baseline ($\mu=27.08$, $\sigma=2.63$). However, no significant difference between the two methods ($p > 0.05$). The same goes for solving time and TCT ($p > 0.05$). For Task 3, participants read slightly faster with Gaze Scroll ($Z = 25.000, p < 0.001$) than baseline ($Z = 25.000, p < 0.001$), while the solving time of baseline ($Z = 25.000, p < 0.001$) was slightly faster than Gaze Scroll ($Z = 25.000, p < 0.001$). The Wilcoxon signed-rank test revealed  no significant difference between the two methods in reading time, solving time, and TCT ($p > 0.05$). Finally, with Task 4, participants read faster with VRDoc ($\mu=27.08$, $\sigma=2.63$) than with baseline ($\mu=27.08$, $\sigma=2.63$), solved faster with VRDoc ($\mu=27.08$, $\sigma=2.63$) than with baseline ($\mu=27.08$, $\sigma=2.63$), which resulted in a shorter TCT with VRDoc ($\mu=27.08$, $\sigma=2.63$) than with baseline ($\mu=27.08$, $\sigma=2.63$). The Wilcoxon signed-rank test revealed a significant difference between the two methods in reading time ($Z = 34.111, p < 0.001$), solving time ($Z = 34.111, p < 0.001$), and TCT ($Z = 34.111, p < 0.001$).


\paragraph{\textbf{Reading Comprehension Test}} 
There were no significant differences ($p = 0.382$) in comprehension results in any of the four tasks between VRDoc and baseline according to the Wilcoxon signed-rank test.  This was expected as (1) VRDoc tools are designed to make the reading task easier and more efficient but do not explicitly aid comprehension, and (2) readers were allowed to review the passages after reading the questions. The results ensure that the participants put the same amount of effort into all tasks.

\paragraph{\textbf{Gaze MagGlass Activation Time}} 
For Task 2, Gaze MagGlass was activated in an average of 51.72\% of the reading time. With Task 4, where all of the tools are integrated, Gaze MagGlass was activated in an average of 67.18\% of the reading time.



\subsubsection{Subjective Measures} 

\paragraph{\textbf{Usability}} 
We measured the usability of the methods by calculating the SUS scores.
\vspace{-0.5em}
\begin{itemize}
\itemsep-0.1em
    \item \textbf{Task 1}: Readers found Gaze Select-and-Snap ($\mu=73.05$, $\sigma=13.05$) more usable than baseline ($\mu=51.67$, $\sigma=9.20$) with a significant difference according the the Wilcoxon signed-rank test ($Z = -2.905, p = 0.004$).
    \item \textbf{Task 2}: Readers found Gaze MagGlass ($\mu=58.11 $, $\sigma=11.49$) more usable  than baseline ($\mu=50.69$, $\sigma=12.25$) with a significant difference ($Z = -2.045, p = 0.041$).
    \item \textbf{Task 3}: The usability of Gaze Scroll ($\mu=60.89$, $\sigma=11.89$) scored and the baseline ($\mu=60.02$, $\sigma=8.48$), trackpad scrolling, were similar in terms of usability, showing no significant differences despite Gaze Scroll having a slightly higher score($p = 0.875$).
    \item \textbf{Task 4}: Readers found VRDoc ($\mu=73.20$, $\sigma=13.21$) more usable than baseline ($\mu=45.61$, $\sigma=14.53$) with a significant difference according the the Wilcoxon signed-rank test ($Z = -3.059, p = 0.002$).
\end{itemize}

 
\paragraph{\textbf{Workload}} 
For all four tasks, our proposed method had significantly lower RTLX scores than the baseline according to Wilcoxon signed-rank test.
\vspace{-0.5em}
\begin{itemize}
\itemsep-0.1em
    \item \textbf{Task 1}: Gaze Select-and-Snap ($\mu=31.35$, $\sigma=12.79$) scored lower than baseline ($\mu=51.67$, $\sigma=9.20$) with a significant difference ($Z = -2.591, p = 0.010$).
    \item \textbf{Task 2}: Gaze MagGlass ($\mu=28.89$, $\sigma=15.05$) scored lower than baseline ($\mu=33.98$, $\sigma=16.12$) with a significant difference ($Z = -2.367, p = 0.018$).
    \item \textbf{Task 3}: Gaze Scroll ($\mu=34.84$, $\sigma=13.63$) scored lower than baseline ($\mu=43.81$, $\sigma=16.49$) with a significant difference ($Z = -2.121, p = 0.034$).
    \item \textbf{Task 4}: VRDoc ($\mu=33.61$, $\sigma=12.66$) scored lower than baseline ($\mu=46.61$, $\sigma=17.65$) with a significant difference ($Z = -2.667, p = 0.008$).
\end{itemize}


\paragraph{\textbf{Readability}} 
For all four tasks, participants reported higher readability with our proposed method than with baseline. All differences were revealed to be significant by the Wilcoxon signed-rank test.
\vspace{-0.5em}
\begin{itemize}
\itemsep-0.1em
    \item \textbf{Task 1}: Gaze Select-and-Snap ($\mu=4.25$, $\sigma=0.75$) scored higher than baseline ($\mu=1.5$, $\sigma=0.67$) with a significant difference ($Z = -3.111, p = 0.002$).
    \item \textbf{Task 2}: Gaze MagGlass ($\mu=3.08$, $\sigma=0.79$) scored higher than baseline ($\mu=2.25$, $\sigma=0.62$) with a significant difference ($Z = -2.057, p =0.040$).
    \item \textbf{Task 3}: Gaze Scroll ($\mu=2.83$, $\sigma=0.58$) scored higher than baseline ($\mu=2$, $\sigma=0.60$) with a significant difference ($Z = -2.673, p = 0.008$).
    \item \textbf{Task 4}: VRDoc ($\mu=4.16$, $\sigma=1.02$) scored higher than baseline ($\mu=1.67$, $\sigma=0.78$) with a significant difference ($Z = -2.929, p = 0.003$).
\end{itemize}

\paragraph{\textbf{Effectiveness} }
We measured how `effective' readers found the methods to be on a 5-point Likert scale.
\vspace{-0.5em}
\begin{itemize}
\itemsep-0.1em
    \item \textbf{Task 1}: Readers found Gaze Select-and-Snap ($\mu=4.16$, $\sigma=0.83$) more effective than baseline ($\mu=1.58$, $\sigma=0.67$) with a significant difference ($Z = -3.097, p = 0.002$).
    \item \textbf{Task 2}: Readers found Gaze MagGlass ($\mu=2.67$, $\sigma=0.98$) more effective than baseline ($\mu=1.83$, $\sigma=0.93$) with a significant difference ($Z = -2.066, p = 0.039$).
    \item \textbf{Task 3}: The perceived effectiveness of Gaze Scroll ($\mu=3.58$, $\sigma=0.75$) was similar to that of baseline ($\mu=3.5$, $\sigma=0.67$) showing no significant statistical difference ($p = 0.763$).
    \item \textbf{Task 4}: Readers found VRDoc ($\mu=4.41$, $\sigma=0.79$) more effective than baseline ($\mu=1.83$, $\sigma=0.72$) with a significant difference ($Z = -2.965, p = 0.003$).
\end{itemize}

\paragraph{\textbf{Preference}} 
Subjects' preference for each method was measured through a 5-point Likert scale.
\vspace{-0.5em}
\begin{itemize}
\itemsep-0.1em
    \item \textbf{Task 1}: Readers preferred Gaze Select-and-Snap ($\mu=4.08$, $\sigma=0.67$) more than baseline ($\mu=1.83$, $\sigma=0.83$) with a significant difference ($Z = -2.971, p = 0.003$).
    \item \textbf{Task 2}: Readers preferred Gaze MagGlass ($\mu=2.67$, $\sigma=0.89$) more than baseline ($\mu=1.83$, $\sigma=0.58$) with a significant difference ($Z = -2.157, p = 0.031$).
    \item \textbf{Task 3}: Gaze Scroll ($\mu=3.18$, $\sigma=0.79$) had a similar preference score with baseline ($\mu=3.25$, $\sigma=0.75$) with no significant difference ($p = 0.603$).
    \item \textbf{Task 4}: Readers preferred VRDoc ($\mu=4.06$, $\sigma=0.74$) more than baseline ($\mu=2.29$, $\sigma=0.77$) with a significant difference ($Z = -3.126, p = 0.002$).
\end{itemize}

\paragraph{\textbf{Sickness}} 
We evaluated the sickness between the pre- and post-SSQs for each task. The results are shown in Table~\ref{table:exp_ssq}. The Wilcoxon signed-rank test revealed that there were no significant differences between the pre- and post-SSQ scores for the two methods in any of the tasks ($p > 0.05$).
\vspace{-0.5em}
\begin{table}[h]
	\small
	\centering
	\begin{tabular}{ >{\centering\arraybackslash}m{0.3cm} >{\centering\arraybackslash}m{1.2cm} |>{\centering\arraybackslash}m{0.5cm}>{\centering\arraybackslash}m{0.5cm}|>{\centering\arraybackslash}m{0.5cm}>{\centering\arraybackslash}m{0.5cm}|>{\centering\arraybackslash}m{0.6cm}|>{\centering\arraybackslash}m{1.0cm}}
		\specialrule{1.5pt}{0pt}{0pt}
		\multicolumn{2}{c|}{\multirow{2}{*}{\textbf{\rule{0pt}{4ex} \makecell{methods}}}} & \multicolumn{2}{c|}{pre-SSQ} & \multicolumn{2}{c|}{post-SSQ} &  \multirow{2}{*}{\rule{0pt}{4ex} Z} & \multirow{2}{*}{\rule{0pt}{4ex} \emph{p}}  \\ \hhline{~~----~}
		&& \thead{$ \mu$} &  \thead{$\sigma$} & \thead{$ \mu$} &  \thead{$\sigma$}&  & \\ \hline\hline
		\multirow{2}{*}{\textbf{\makecell{T1}}} & \emph{baseline} & 0.91 & 1.07 & 0.92 & 1.06 & -1.41 & 0.16  \\
		&\emph{Gaze Select-and-Snap} & 0.78 & 1.00 & 0.78 & 1.06 & -1.89 & 0.59 \\
		\hline
		\multirow{2}{*}{\textbf{\makecell{T2}}} &\textit{baseline} & 0.76 & 1.01 & 0.78 & 1.03 & -1.73 & 0.83  \\
		&\textit{Gaze MagGlass} & 0.75 & 1.08 & 0.73 & 1.05 & -1.55 &  0.61 \\
		\hline
		\multirow{2}{*}{\textbf{\makecell{T3}}} &\textit{baseline} & 0.83 & 1.12 & 0.84 & 1.03 & -1.64 & 0.65  \\
		&\textit{Gaze Scroll} & 0.80 & 1.04 & 0.83 & 1.06 & -1.21 &  0.59 \\
		\hline
		\multirow{2}{*}{\textbf{\makecell{T4}}} &\textit{baseline} & 0.86 & 1.06 & 0.97 & 1.05 & -1.50 & 0.42  \\
		&\textit{VRDoc} & 0.88 & 1.00 & 0.93 & 1.08 & -1.43 &  0.51 \\
		\specialrule{1.5pt}{0pt}{1pt}
	\end{tabular}
	\caption{SSQ analysis for the evaluation tasks, showing no significant differences between the pre- and post-SSQ results.}~\label{table:exp_ssq}
	\vspace*{-0.1in}
\end{table}

\vspace{-0.5em}

% We tested two reading comprehension tasks using non-fiction on documents.  Participants were initially asked a few questions to determine if they were familiar with the material in the documents.  All participants were unfamiliar with the material.  The evaluation comprised two parts:  (1) reading multiple documents (henceforth, R1) and (2) reading a long document (henceforth, R2). The two tasks represents common reading scenarios where users must switch between reading multiple documents and a long document that has a structure with different font sizes. The task was to read passages about history and take reading comprehension tests afterwards. For text presentation, we used a sans-serif font in white on a black background for the document windows to align with guidelines suggested from previous work~\cite{dingler2018vr}. The virtual environment had a solid sky-blue background to minimize the distractions.

% For R1, five 100-word documents were placed in front of the user's starting position in a semicircle with a radius of $0.8m$, as seen in Fig.~\ref{fig:expsetting} (a). Our goal was to represent a realistic scenario; when users work in VR, they will often move to different positions with documents placed in different 3D orientations. All five documents had the same canvas size and did not require any scrolling.

% For R2, a 500-word document was placed $0.8$m away in front of the user's starting position, as shown in Fig.~\ref{fig:expsetting} (b). The content of the document was presented through main paragraphs and sub paragraphs with 30\% smaller font size.

% Before starting the experiment, each subject filled out a demographic survey, and the experiment procedure was explained in detail. The order of the reading tasks was counter-balanced between subjects. Prior to each tasks, a tutorial including a ten-minute training was given. There was a five-minute break between the tasks.


% \subsection{Evaluation Items}
% All trials performed by the participants were recorded in video files for accurate observation and evaluation. Task completion time was measured alongside feature-specific evaluation items.

% Users were asked to solve a reading comprehension test after reading the given documents. 
% The reading comprehension test consisted of fact-check questions based on the given passage. Each document in R1 had one associated question while in R2, the long document had five questions to solve. Once the participant verbally determined that they were ready to take the quiz, a 2D window containing five questions was displayed in front of the user in the VR environment. The task completion time was measured separately by \textit{reading time} and \textit{solving time}. The \textit{reading time} is defined as the time consumed for selecting and reading the document, the \textit{solving time} is the time needed to answer the questions in the reading comprehension tests.

% The subjects were asked to fill out surveys after performing each reading task: SUS questionnaire~\cite{brooke1996sus}  for system usability, Raw TLX~\cite{hart2006nasa} for work load, and SSQ~\cite{kennedy1993simulator} for VR sickness, and a subjective evaluation questionnaire in a 5-point Likert scale for readability, effectiveness, and preference. The experimenter reminded participants to consider the interaction aspect when answering the questions. 

% We also counted and measured a few features that are specific to \textit{GazeDoc} during the reading tasks:
% \begin{itemize}
% 	\item The number of times \textit{Gaze Select-and-Snap} was activated.
% 	\item The length of time \textit{Gaze MagGlass} was activated when one's gaze within a document projector was active
% 	\item The number of times \textit{Gaze Scroll} was activated.
% \end{itemize}


%\subsection{Results}



% \subsubsection{R1: Reading Multiple Documents}
% This section analyzes the results of the experiment with the task of reading multiple 100-word documents. Note that for R1, scrolling was not required as the participants were reading short documents.

% \vspace{0.5em}

% % \begin{figure}[h]
% % 	\centering
% % 	\includegraphics[width=1.2\linewidth]{pictures/R1_TCT.png}
% % 	\caption{Task Completion Time in R1: reading of multiple documents. With \textit{GazeDoc}, participants' reading times were significantly shorter than that with the baseline. The average time taken to answer questions was slightly shorter but was not statistically significant. Overall, \textit{GazeDoc} is time efficient in selecting and reading documents.}
% % 	\label{fig:r1_tct}
% % \end{figure}


% \textbf{Task Completion Time}
% The results for the reading and solving times are illustrated in Figure~\ref{fig:r1_tct}, where the \emph{task completion time} is defined as the sum of the reading and solving times. The Wilcoxon signed-rank test revealed a significant difference between the two methods in reading time ($Z = 34.111, p < 0.001$) and task completion time ($Z = 25.000, p < 0.001$). The results are presented in Table~\ref{table:exp1_time}.

% \begin{table}[h]
% 	\small
% 	\centering	
% 	\begin{tabular}{
% 			>{\centering\arraybackslash}m{1.5cm} |
% 			>{\centering\arraybackslash}m{0.05cm}|
% 			>{\centering\arraybackslash}m{1.5cm} |
% 			>{\centering\arraybackslash}m{1.5cm} 
% 		}
% 		\specialrule{1.5pt}{0pt}{0pt}    % Thick top border
% 		\multicolumn{2}{c|}{method} & \textit{baseline} & \textit{GazeDoc} \\
% 		\hline \hline
% 		\multirow{2}{*}{reading time}&$Z$ &-3.574&-3.621\\
% 		\hhline{~---}
% 		&$p$ &\cellcolor{lightgray}$<$ .001&\cellcolor{lightgray}$<$ .001\\
% 		\hline
% 		\multirow{2}{*}{solving time}&$Z$ &-2.627&-3.621\\
% 		\hhline{~---}
% 		&$p$ &.069&1.12\\
% 		\hline
% 		\multirow{2}{*}{\makecell{task completion\\time}}&$Z$ &-2.959&-3.621\\
% 		\hhline{~---}
% 		&$p$ &\cellcolor{lightgray}.003&\cellcolor{lightgray}$<$ .001\\
		
% 		\specialrule{1.5pt}{0pt}{0pt}    % Thick bottom border
				
% 	\end{tabular}
% 	\caption{R1 analysis results for task completion time, which corresponds to sum of the reading and solving time. We highlight the benefits of GazeDoc for R1.}~\label{table:exp1_time}
% \end{table}

% \vspace{0.5em}

% \textbf{Reading Comprehension Test}
% The average number of wrong answers were compared with each method. Participants made an average of 1. 37 number of questions wrong with \textit{GazeDoc} and 1.51 with \textit{baseline}. However, the Wilcoxon signed-rank test revealed no significant difference between the two methods ($p > 0.05$).

% \vspace{0.5em}

% % \begin{figure}[h]
% % 	\centering
% % 	\includegraphics[width=1.0\linewidth]{pictures/R1_UW.png}
% % 	\caption{Usability and Workload in R1 of reading multiple documents. We observe significant difference by using GazeDoc over the baseline method described in Section 3.1.}
% % 	\label{fig:r1_uw}
% % \end{figure}

% \textbf{Usability}
% The SUS questionnaire was answered on a 5-point Likert scale, and the statistical results are shown on the left in Figure~\ref{fig:r1_uw}.
% The Wilcoxon signed-rank test revealed a significant difference between \textit{baseline} and \textit{GazeDoc} ($Z = -2.771, p = 0.006$).


% \vspace{0.5em}

% \textbf{Work load}
% The statistical results for work load are shown on the right in Figure~\ref{fig:r1_uw}.
% The Wilcoxon signed-rank test revealed a significant difference between \textit{baseline} and \textit{GazeDoc} ($Z = -3.054, p = 0.002$).

% \vspace{0.5em}

% \textbf{\textit{GazeDoc} Activation}
% Listed below are the averages of the counts and ratio of the measurements:
% \begin{itemize}
% 	\item \textit{Gaze Select-and-Snap}: Activated in an average of 9.39 times.
% 	\item \textit{Gaze MagGlass}: Activated in an average of 43.3\% of the reading time.
% \end{itemize}

% \vspace{0.5em}


% % \begin{figure}[h]
% % 	\centering
% % 	\includegraphics[width=1.0\linewidth]{pictures/R1_SEQ.png}
% % 	\caption{Statistical results for readability, effectiveness, preference in R1. We observe significant difference by using GazeDoc.}
% % 	\label{fig:r1_seq}
% % \end{figure}

% \textbf{Readability}
% The statistical results for participant's perceived readability are shown in Figure~\ref{fig:r1_seq}.
% The Wilcoxon signed-rank test revealed a significant difference between \textit{baseline} and \textit{GazeDoc} ($Z = -3.096, p = 0.002$).

% \vspace{0.5em}

% \textbf{Effectiveness}
% The statistical results for effectiveness are shown in Figure~\ref{fig:r1_seq}.
% The Wilcoxon signed-rank test revealed a significant difference between \textit{baseline} and \textit{GazeDoc} ($Z = -2.768, p = 0.002$).

% \vspace{0.5em}

% \textbf{Preference}
% The statistical results for participants' preference toward each interaction method are shown in Figure~\ref{fig:r1_seq}.
% The Wilcoxon signed-rank test revealed a significant difference between \textit{baseline} and \textit{GazeDoc} ($Z = -3.323, p = 0.002$).

% \vspace{0.5em}

% \textbf{Sickness}
% We evaluated the sickness between the pre- and post-SSQs for each interaction method. The statistical results are shown in Table~\ref{table:exp1_ssq}.
% The Wilcoxon signed-rank test revealed that there were no significant differences between the pre- and post-SSQ scores for the two methods ($p > 0.05$).



% \begin{table}[h]
% 	\small
% 	\centering
% 	\begin{tabular}{ >{\centering\arraybackslash}m{1.5cm} |>{\centering\arraybackslash}m{0.7cm}>{\centering\arraybackslash}m{0.7cm}|>{\centering\arraybackslash}m{0.7cm}>{\centering\arraybackslash}m{0.7cm}|>{\centering\arraybackslash}m{0.6cm}|>{\centering\arraybackslash}m{1.1cm}}
% 		\specialrule{1.5pt}{0pt}{0pt}
% 		\multirow{2}{*}{\textbf{\rule{0pt}{4ex} \makecell{methods}}} & \multicolumn{2}{c|}{pre-SSQ} & \multicolumn{2}{c|}{post-SSQ} &  \multirow{2}{*}{\rule{0pt}{4ex} Z} & \multirow{2}{*}{\rule{0pt}{4ex} \emph{p}}  \\ \hhline{~----~}
% 		& \thead{$ \mu$} &  \thead{$\sigma$} & \thead{$ \mu$} &  \thead{$\sigma$}&  & \\ \hline\hline
% 		\emph{baseline} & 0.91 & 1.07 & 0.92 & 1.06 & -1.41 & 0.16  \\
% 		\emph{GazeDoc} & 0.78 & 1.00 & 0.78 & 1.06 & -1.89 & 0.59 \\
% 		\specialrule{1.5pt}{0pt}{1pt}
% 	\end{tabular}
% 	\caption{R1 analysis results for SSQ. There were no significant differences between the pre- and post-SSQ results, indicating that both interaction methods did not cause VR sickness when reading multiple short documents.}~\label{table:exp1_ssq}
	
% \end{table}

% \subsubsection{R2: Reading a Long Structured Document}
% This section analyzes the results of the experiment made with the task of reading a 500-word document with different font sizes.

% % \vspace{0.5em}

% % \begin{figure}[h]
% % 	\centering
% % 	\includegraphics[width=1.2\linewidth]{pictures/R2_TCT.png}
% % 	\caption{Task Completion Time in R2.  We observe the improvement using GazeDoc.}
% % 	\label{fig:r2_tct}
% % \end{figure}


% \textbf{Task Completion Time}
% The statistical results for the reading and solving times are illustrated in Figure~\ref{fig:r2_tct}.
% The Wilcoxon signed-rank test revealed a significant difference between the two methods in both reading time ($Z = 28.778, p < 0.001$) and task completion time ($Z = 28.778, p < 0.001$) . The results are presented in Table~\ref{table:exp2_time}.

% %	\multicolumn{2}{c|}{method} & \makecell{\textit{baseline}} & \makecell{\textit{with}\\
% \begin{table}[h]
% 	\small
% 	\centering	
% 	\begin{tabular}{
% 			>{\centering\arraybackslash}m{1.5cm} |
% 			>{\centering\arraybackslash}m{0.05cm}|
% 			>{\centering\arraybackslash}m{1.5cm} |
% 			>{\centering\arraybackslash}m{1.5cm} 
% 		}
% 		\specialrule{1.5pt}{0pt}{0pt}    % Thick top border
% 	    \multicolumn{2}{c|}{method} & \textit{baseline} & \textit{GazeDoc} \\
% 		\hline \hline
% 		\multirow{2}{*}{reading time}&$Z$ &-3.574&-3.621\\
% 		\hhline{~---}
% 		&$p$ &\cellcolor{lightgray}$<$ .001&\cellcolor{lightgray}$<$ .001\\
% 		\hline
% 		\multirow{2}{*}{solving time}&$Z$ &-2.627&-3.621\\
% 		\hhline{~---}
% 		&$p$ &1.02&.59\\
% 		\hline
% 		\multirow{2}{*}{\makecell{task completion\\time}}&$Z$ &-2.959&-3.621\\
% 		\hhline{~---}
% 		&$p$ &\cellcolor{lightgray}.003&\cellcolor{lightgray}$<$ .001\\
		
% 		\specialrule{1.5pt}{0pt}{0pt}    % Thick bottom border
				
% 	\end{tabular}
% 	\caption{R2 analysis results for task completion time, which highlights the benefits of GazeDoc.}~\label{table:exp2_time}
% \end{table}

% \vspace{0.5em}

% \textbf{Reading Comprehension Test}
% The average number of wrong answers were compared with each method. Participants made an average of 0. 98 number of questions wrong with \textit{GazeDoc} and 1.22 with \textit{baseline}. However, the Wilcoxon signed-rank test revealed no significant difference between the two methods ($p > 0.05$).

% % \vspace{0.5em}

% % \begin{figure}[h]
% % 	\centering
% % 	\includegraphics[width=1.0\linewidth]{pictures/R2_UW.png}
% % 	\caption{Usability and Workload in R2. We observe significant improvement using GazeDoc.}
% % 	\label{fig:r2_uw}
% % \end{figure}


% \textbf{Usability}
% The statistical results for usability are shown on the left of Figure~\ref{fig:r2_uw}.
% The Wilcoxon signed-rank test revealed a significant difference between \textit{baseline} and \textit{GazeDoc} ($Z = -3.153, p = 0.002$).

% \vspace{0.5em}

% \textbf{Work load}
% The statistical results for work load are shown on the right of Figure~\ref{fig:r2_uw}.
% The Wilcoxon signed-rank test revealed a significant difference between \textit{baseline} and \textit{GazeDoc} ($Z = -3.145, p = 0.002$).

% \vspace{0.5em}

% \textbf{\textit{GazeDoc} Activation}
% Listed below are the averages of the counts and ratio of the measurements:
% \begin{itemize}
% 	\item \textit{Gaze Select-and-Snap}: Activated in an average of 3.48 times.
% 	\item \textit{Gaze MagGlass}: Activated in an average of 51.72\% of the reading time.
% 	\item \textit{Gaze Scroll}: Activated in an average of 5.35 times.
% \end{itemize}

% \vspace{0.5em}

% % \begin{figure}[h]
% % 	\centering
% % 	\includegraphics[width=1.0\linewidth]{pictures/R2_SEQ.png}
% % 	\caption{Statistical results for readability, effectiveness, preference in R2. We observe significant differences using GazeDoc.}
% % 	\label{fig:r2_seq}
% % \end{figure}

% \textbf{Readability}
% The statistical results for participant's perceived readability are shown in Figure~\ref{fig:r2_seq}.
% The Wilcoxon signed-rank test revealed a significant difference between \textit{baseline} and \textit{GazeDoc} ($Z = -4.145, p = 0.002$).

% \vspace{0.5em}

% \textbf{Effectiveness}
% The statistical results for effectiveness are shown in Figure~\ref{fig:r2_seq}.
% The Wilcoxon signed-rank test revealed a significant difference between \textit{baseline} and \textit{GazeDoc} ($Z = -2.243, p = 0.002$).

% \vspace{0.5em}

% \textbf{Preference}
% The statistical results for participants' preference toward each interaction method are shown in Figure~\ref{fig:r2_seq}.
% The Wilcoxon signed-rank test revealed a significant difference between \textit{baseline} and \textit{GazeDoc} ($Z = -3.568, p = 0.002$).

% \vspace{0.5em}

% \textbf{Sickness}
% We evaluated the sickness between the pre- and post-SSQs for each interaction method. The results are shown in Table~\ref{table:exp2_ssq}. The Wilcoxon signed-rank test revealed that there were no significant differences between the pre- and post-SSQ scores for the two methods ($p > 0.05$).

% \vspace{0.5em}


% \begin{table}[h]
% 	\small
% 	\centering
% 	\begin{tabular}{ >{\centering\arraybackslash}m{1.5cm} |>{\centering\arraybackslash}m{0.7cm}>{\centering\arraybackslash}m{0.7cm}|>{\centering\arraybackslash}m{0.7cm}>{\centering\arraybackslash}m{0.7cm}|>{\centering\arraybackslash}m{0.6cm}|>{\centering\arraybackslash}m{1.1cm}}
% 		\specialrule{1.5pt}{0pt}{0pt}
% 		\multirow{2}{*}{\textbf{\rule{0pt}{4ex} \makecell{methods}}} & \multicolumn{2}{c|}{pre-SSQ} & \multicolumn{2}{c|}{post-SSQ} &  \multirow{2}{*}{\rule{0pt}{4ex} Z} & \multirow{2}{*}{\rule{0pt}{4ex} \emph{p}}  \\ \hhline{~----~}
% 		& \thead{$ \mu$} &  \thead{$\sigma$} & \thead{$ \mu$} &  \thead{$\sigma$} &  \\ \hline\hline
% 		\textit{baseline} & 0.76 & 1.01 & 0.78 & 1.03 & -1.73 & 0.83  \\
% 		\textit{GazeDoc} & 0.80 & 1.04 & 0.83 & 1.06 & -1.21 &  0.59 \\
% 		\specialrule{1.5pt}{0pt}{1pt}
% 	\end{tabular}
% 	\caption{R2 analysis results for SSQ. There were no significant differences between the pre- and post-SSQ results, indicating that both interaction methods did not cause VR sickness when reading a long document.}~\label{table:exp2_ssq}
	
% \end{table}
