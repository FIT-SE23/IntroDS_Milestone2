
\section{Financial Reinforcement Learning and FinRL-Meta Framework}

\definecolor{Gray}{RGB}{217,234,211}
\begin{table}
\caption{List of state space, action space, and reward function.}
\small
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{|l|l|l|}
    \hline   
    \textbf{Key components} & \textbf{Attributes} 
    \\ \hline
    State & Balance ${b}_{t}\in \mathbb{R}_+$;~Shares $\bm{h}_{t}\in \mathbb{Z}_+^{n}$ \\
    & Opening/high/low/close price $\bm{o}_{t}, \bm{h}_{t}, \bm{l}_{t},\bm{p}_{t} \in \mathbb{R}_+^{n}$ \\ 
    & Trading volume $\bm{v}_{t}\in \mathbb{R}_+^{n}$ \\
    & Fundamental indicators; Technical indicators \\
    & Social data; Sentiment data \\
    &  Smart beta indexes, etc. \\ 
    \hline
     Action & Buy/Sell/Hold   \\
    & Short/Long \\
    & Portfolio weights 
    \\ \hline
     Reward &  Change of portfolio value \\  
    & Portfolio log-return \\
    & Sharpe ratio 
    \\ \hline
     Environments &  Dow-$30$, S\&P-$500$, NASDAQ-$100$ \\  
    &  Cryptocurrencies \\
    & Foreign currency and exchange \\ 
    &  Futures;~Options;~ETFs;~Forex \\ 
    & CN securities;~US securities;~NMS US securities\\
    & Paper trading; Living Trading 
    \\ \hline
\end{tabular}\vspace{0.050in}
\label{event:eventTypes}
\end{table}

We describe financial reinforcement learning and its challenges, then provide an overview of our FinRL-Meta framework.

\subsection{Financial Reinforcement Learning and Challenges}

Assuming full observability, we model a trading task as a Markov Decision Process (MDP) with five tuples \cite{sutton2018reinforcement} $(\mathcal{S}, \mathcal{A}, \mathbb{P}, r, \gamma)$, where $\mathcal{S}$ and $\mathcal{A}$ denote the state space and action space, respectively, $\mathbb{P}(s'|s,a)$ is the transition probability of an unknown environment, $r(s,a, s')$ is a reward function, and $\gamma \in (0, 1] $ is a discount factor. A trading agent learns a policy $\pi(s_t|a_t)$ that maximizes the discounted cumulative return $R = \sum^{T}_{t=0} \gamma ^t r(s_t, a_t, s_{t+1})$ over a trading period $t=0,1,...,T$.

The historical dataset before time $0$ is used to train the trading agent. Note that we process the dataset into a market environment, following the \textit{de facto} standard of OpenAI gym \cite{brockman2016openai}. In Table \ref{event:eventTypes}, we list the state space, action space, and reward function.
\begin{itemize} [leftmargin=*]
    \item \textbf{State} $s \in \mathcal{S}$: A state represents an agent's perception of a market environment, which may include balance, shares, OHLCV values, technical indicators, social data, sentiment data, etc. 
    \item \textbf{Action} $a \in \mathcal{A}$: An action is taken from the allowed action set at a state. Actions may vary for different trading tasks, e.g., for stock trading, the actions are the number of shares to buy/sell for each stock, while for portfolio allocation, the actions are the allocation weights of the capital. 
    \item \textbf{Reward} $r(s,a,s')$: Reward is an incentive mechanism for an agent to learn a better policy. Several common reward functions are provided: 1). Change of portfolio value $r(s,a,s') = v' - v$, where $v'$ and $v$ are portfolio values at state $s'$ and $s$, respectively; 2). Portfolio log return $r(s,a,s') = \log(v'/v)$; and 3). Sharpe ratio \cite{Sharpe} defined in Section \ref{sec:performance_metrics}.
    %$S_T = \frac{\text{mean}(R_t) - r_f}{\text{std}(R_t)}$, where $R_t = \frac{v_t - v_{t-1}}{v_{t-1}}$, and $r_f$ is the risk-free rate, and $t=1,...,T$.
\end{itemize}

The above full observability assumption can be extended to partial observation (the underlying states cannot be directly observed), i.e., partially observable Markov Decision Process (POMDP). A POMDP model utilizes a Hidden Markov Model (HMM) \cite{mamon2007hidden} to model a time series that is caused by a sequence of unobservable states. 
Considering the noisy financial data, it is natural to assume that a trading agent cannot directly observe market states. Studies suggested that the POMDP model can be solved by using recurrent neural networks, e.g., an off-policy Recurrent Deterministic Policy Gradient (RDPG) algorithm \cite{liu2020adaptive}, and a  long short-term memory (LSTM) network that encodes partial observations into a state of a reinforcement learning algorithm \cite{rundo2019deep}.

Training and testing environments based on historical data may not simulate real markets accurately due to the \textit{simulation-to-reality gap} \cite{DulacArnold2020AnEI,dulac2019challenges}, and thus a trained agent cannot be directly deployed in real-world markets. We summarize three major factors for the \textit{simulation-to-reality gap} in financial reinforcement learning as follows:
\begin{itemize} [leftmargin=*]
\item \textbf{Low signal-to-noise ratio (SNR) of financial data}: Data from different sources may contain large noise \cite{wilkman2020feasibility} such as random noise, outliers, etc. It is challenging to identify alpha signals or build smart beta indexes using noisy datasets.
\item \textbf{Survivorship bias of historical market data}: Survivorship bias is caused by a tendency to focusing on existing stocks and funds without consideration of those that are delisted \cite{brown1992survivorship}. It could lead to an overestimation of stocks and funds, which will mislead the agent.
\item \textbf{Model overfitting in backtesting stage}: Existing research mainly report backtesting results. It is possible to tune hyper-parameters and retrain the agent multiple times \footnote{There is information leakage.} to obtain better backtesting results, causing model overfitting \cite{gort2022deep,de2018advances}. 
\end{itemize}

\subsection{Overview of FinRL-Meta}

\begin{figure*}
\centering
\includegraphics[scale =0.15]{figs/finrl_meta_fig1.PNG}\vspace{-0.1in}
\caption{Overview of FinRL-Meta framework.}
\vspace{-0.1in}
\label{fig:finrl-meta overview}
\vspace{-2mm}
\end{figure*}

FinRL-Meta builds a universe of market environments for data-driven financial reinforcement learning. FinRL-Meta follows the \textit{de facto} standard of OpenAI Gym \cite{brockman2016openai} and the \textit{lean principle} of software development. It has the following unique features.

\textbf{Layer structure and extensibility}:
As shown in Fig. \ref{fig:finrl-meta overview}, we adopt a layered structure that consists of three layers, data layer, environment layer, and agent layer. Layers interact through end-to-end interfaces, achieving high extensibility. For updates and substitutes inside a layer, this structure minimizes the impact on the whole system.  Moreover, the layer structure allows easy extension of user-defined functions and fast updating of algorithms with high performance.

\textbf{Training-testing-trading pipeline}:
We employ a training-testing-trading pipeline that the DRL approach follows a standard end-to-end pipeline. The DRL agent is first trained in a training environment and then fined-tuned (adjusting hyperparameters) in a validation environment. Then the validated agent is tested on historical datasets (backtesting). Finally, the tested agent will be deployed in paper trading or live trading markets.

\textbf{Plug-and-play mode}: In the above training-testing-trading pipeline, a DRL agent can be directly plugged in, then trained and tested. The following DRL libraries are supported:
\begin{itemize} [leftmargin=*]
    %\item \textbf{OpenAI Baselines \cite{openai_baselines}:} A set of high-quality implementations of RL algorithms. 
    \item \textbf{ElegantRL \cite{elegantrl}}: Lightweight, efficient and stable algorithms using PyTorch.
    \item \textbf{Stable-Baselines3 \cite{stable-baselines}}: Improved DRL algorithms based on OpenAI Baselines.
    \item \textbf{RLlib \cite{liang2018rllib}:} An open-source DRL library that offers high scalability and unified APIs. 
\end{itemize}

