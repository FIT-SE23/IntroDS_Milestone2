
\section{Tutorials and Benchmarks of Financial Reinforcement Learning} \label{benchmarks}

\begin{figure}
\centering
\includegraphics[scale=0.2]{figs/FinRL-Meta_Tutorials.pdf}
\caption{Demos of FinRL-Meta, organized in a curriculum structure.}
\label{fig:tutorials}
\vspace{-2mm}
\end{figure}

We provide tens of tutorial notebooks to serve as stepping stones for newcomers and reproduce popular papers as benchmarks for follow-up research.

\subsection{Metrics and Baselines for Evaluating Performance}
\label{sec:performance_metrics}

We provide the following metrics to measure the trading performance: 
\begin{itemize}[leftmargin=*]
  \item \textbf{Cumulative return} $R = \frac{v - v_0}{v_0}$, where $v$ is the final portfolio value, and $v_0$ is the original capital.
  \item \textbf{Annualized return} $r = (1+R)^\frac{365}{t}-1$, where $t$ is the number of trading days.
  \item \textbf{Annualized volatility} ${\sigma}_a = \sqrt{\frac{\sum_{i=1}^{n}{(r_i-\bar{r})^2}}{n-1}}$, where $r_i$ is the annualized return in year $i$, $\bar{r}$ is the average annualized return, and $n$ is the number of years.
  \item \textbf{Sharpe ratio} \cite{Sharpe} 
$S_T = \frac{\text{mean}(R_t) - r_f}{\text{std}(R_t)}$, where $R_t = \frac{v_t - v_{t-1}}{v_{t-1}}$, $r_f$ is the risk-free rate, and $t=1,...,T$.
  \item \textbf{Max. drawdown}: The maximal percentage loss in portfolio value.
\end{itemize}

The following baseline trading strategies are provided for comparisons:
\begin{itemize}[leftmargin=*]
    \item \textbf{Passive trading strategy} \cite{malkiel2003passive} is a well-known long-term strategy. The investors just buy and hold selected stocks or indexes without further activities.
    \item \textbf{Mean-variance and min-variance strategy} \cite{ang2012mean} are two widely used strategies that look for a balance between risks and profits. They select a diversified portfolio in order to achieve higher profits at a lower risk.
    \item \textbf{Equally weighted strategy} is a portfolio allocation strategy that gives equal weights to different assets, avoiding allocating overly high weights on particular stocks. 
\end{itemize}


\subsection{Tutorials and Demos in Jupyter Notebooks}

For educational purposes, we provide Jupyter notebooks as tutorials\footnote{https://github.com/AI4Finance-Foundation/FinRL-Tutorials} to help newcomers get familiar with the whole pipeline.
\begin{itemize}[leftmargin=*]
    \item \textbf{Stock trading} \cite{xiong2018practical}: We apply popular DRL algorithms to trade multiple stocks.
    \item \textbf{Portfolio allocation} \cite{liu2020finrl}: We use DRL agents to optimize asset allocation in a set of stocks.
    \item \textbf{Cryptocurrency trading} \cite{liu2020finrl}: We reproduce the experiment \cite{liu2020finrl} on $10$ popular cryptocurrencies.
    \item \textbf{Multi-agent RL for liquidation strategy analysis} \cite{bao2019multiagent}: We reproduce the experiment in \cite{bao2019multiagent}. The multi-agent optimizes the shortfalls in the liquidation task, which is to sell given shares of one stock sequentially within a given period, considering the costs arising from the market impact and the risk aversion.
    \item \textbf{Ensemble strategy for stock trading} \cite{yang2020deep}: We reproduce the experiment in \cite{yang2020deep} that employed an ensemble strategy of several DRL algorithms on the stock trading task.
    \item \textbf{Paper trading demo}: We provide a demo for paper trading. Users could combine their own strategies or trained agents in paper trading.
    \item \textbf{China A-share demo}: We provide a demo based on the China A-share market data.
    \item \textbf{Hyperparameter tuning}: We provide several demos for hyperparameter tuning using Optuna \cite{akiba2019optuna} or Ray Tune \cite{liaw2018tune}, since hyperparameter tuning is critical for better performance.
\end{itemize}


\begin{figure}
\centering
\includegraphics[scale=0.51]{figs/stock_trading_fig.png}
\caption{Reproducing stock trading (left) of \cite{xiong2018practical}.}
\label{fig:stock_trading_performance}
\vspace{-1mm}
\end{figure}

\subsection{Reproducing Prior Papers as Benchmarks}

We have reproduced experiments in several papers as benchmarks. Users can study our codes for research purpose or use them as stepping stones for deploying trading strategies in live markets. In this subsection, we introduce three home-grown examples specifically. For more benchmarks, please refer to Appendix.  \ref{sec:appendixB}.

\textbf{Stock trading task} \cite{xiong2018practical}: We access Yahoo! Finance database and select the 30 constituent stocks (accessed at 07/01/2020) in Dow Jones Industrial Average (DJIA). We use data from 01/01/2009 to 06/30/2020 for training and data from 07/01/2020 to 05/31/2022 for testing. We use technical indicators in our state space, e.g., Moving Average Convergence Divergence (MACD),  Relative Strength Index (RSI), Commodity Channel Index (CCI), Average Directional Index (ADX), etc.

As shown in Fig.~\ref{fig:stock_trading_performance} (left), we train five popular DRL algorithms to trade and compare their results with the DJIA index. We show a detailed walkthrough of how DRL works in the stock trading task, on which many subsequent works are based \cite{xiong2018practical}. This benchmark is beneficial for getting into the field of RL in finance. 

\textbf{Podracer on the cloud \cite{finrl_podracer_2021,liu2021podracer}}: We reproduce cloud solutions of population-based training, e.g., generational evolution \citep{finrl_podracer_2021} and tournament-based evolution \citep{liu2021podracer}. If GPUs are abundant, users can take advantage of this benchmark to meet the real-time requirement of high-frequency trading tasks. Detailed instructions are provided on our website.

\textbf{Ensemble strategy} \cite{yang2020deep}: The ensemble method combines different agents to obtain an adaptive one, which inherits the best features of the agents and performs remarkably well in practice. We consider three component algorithms, Proximal Policy Optimization (PPO), Advantage Actor-Critic (A2C), and Deep Deterministic Policy Gradient (DDPG), which have different strengths and weaknesses. For instance, A2C is good at dealing with a bearish trend market. PPO is good at following trends and acts well in generating more returns in a bullish market. DDPG can be used as a complementary strategy to PPO in a bullish trend. Using a rolling window, an ensemble agent automatically selects the best model for each test period. Again on the 30 constituent stocks of the DJIA index, we use data from 04/01/2009 to 06/30/2019 for training, and data from 07/01/2020 to 03/31/2022 for validation and testing through a quarterly rolling window.

From Fig.~\ref{fig:compare_returns_ensemble} and Table~\ref{tab:ensemble_performance}, we observe that the ensemble agent outperforms other agents. In the experiment, the ensemble agent has the highest Sharpe ratio of $1.53$, which means it performs the best in balancing risks and profits. This benchmark demonstrates that the ensemble strategy is effective in constructing a more reliable agent based on several component DRL agents.\looseness=-1


\begin{figure}
\centering
\includegraphics[scale=0.51]{figs/compare_returns_ensemble.png}
\caption{Reproducing the ensemble strategy of \cite{yang2020deep}: cumulative return.}
\label{fig:compare_returns_ensemble}
\vspace{-1mm}
\end{figure}

\begin{table*}[t]
 \centering
 \renewcommand{\arraystretch}{1.3}
\resizebox{1\textwidth}{!}{
\begin{tabular}{|c| c |c |c |c |c |}
\hline
(2020/07/01-2022/03/31) & Ensemble \cite{yang2020deep} & A2C & PPO & DDPG & DJIA index \\
\hline
Annual Return & 25.9\% & 23.3\% & 13.1\% & 12.7\% & 19.7\% \\
\hline
Annual Volatility & 15.9\% & 16.2\% & 13.4\% & 15.0\% & 14.4\% \\
\hline
Sharpe Ratio & 1.53 & 1.37 & 0.99 & 0.88 & 1.32 \\
\hline
% Sortino Ratio & 0 & 0 & 0 & 0 & 0 \\
% \hline
Calmar Ratio & 2.27 & 1.97 & 0.88 & 0.85 & 1.74 \\
\hline
Max Drawdown & -11.4\% & -11.8\% & -14.9\% & -14.9\% & -11.3\% \\
\hline
\end{tabular}}
\caption{Reproducing the ensemble strategy of \cite{yang2020deep}.}
\label{tab:ensemble_performance}
\vspace{-2mm}
\end{table*}