\section{Related Works}

We review DataOps practices and existing works on data-driven reinforcement learning.

\textbf{DataOps practices}: DataOps \cite{ereth2018dataops, atwal2019practical, DataOps} applies the ideas of lean development and DevOps to the data analytics field. DataOps practices have been developed in companies and organizations to improve the quality and efficiency of data analytics \cite{atwal2019practical}. These implementations consolidate various data sources, unify and automate the pipeline of data analytics, including data accessing, cleaning, analysis, and visualization.

However, the DataOps methodology has not been applied to financial reinforcement learning researches. Most researchers access data, clean data, and extract technical indicators (features) in a case-by-case manner, which involves heavy manual work and may not guarantee the data quality.

\textbf{Data-driven reinforcement learning}: Environments are crucial for training DRL agents \cite{sutton2018reinforcement}. 
\begin{itemize}[leftmargin=*]
    \item \textbf{OpenAI gym} \cite{brockman2016openai} provides standardized environments for a collection of benchmark problems that expose a common interface, which is widely supported by many libraries \cite{stable-baselines, liang2018rllib, elegantrl}. Three trading environments, \textsf{TradingEnv, ForexEnv, and StocksEnv}, are included to support Stock and FOREX markets. However, it has not been updated for years.
    \item \textbf{D4RL} \cite{fu2020d4rl} introduces the idea of \textit{Datasets for deep data-driven reinforcement learning} (D4RL). It provides benchmarks in offline RL. However, D4RL does not provide financial environments.
    \item \textbf{FinRL} \cite{liu2020finrl,liu2021finrl} is an open-source library that builds a full pipeline for financial reinforcement learning. It contains three market environments, i.e., stock trading, portfolio allocation, crypto trading, and two data sources, i.e., Yahoo Finance and WRDS. However, those market environments of FinRL cannot meet the community's growing demands.
    \item \textbf{NeoRL} \cite{qin2021neorl} collected offline RL environments for four areas, CityLearn \cite{vazquez2019cityLearn}, FinRL \cite{liu2020finrl,liu2021finrl}, Industrial Benchmark \cite{hein2017benchmark}, and MuJoCo \cite{todorov2012mujoco}, where each area contains several gym-style environments. Regarding financial aspects, it directly borrows market environments from FinRL.
\end{itemize}

\textbf{Benchmarks of financial reinforcement learning}:
Many researches applied DRL algorithms in quantitative finance \cite{xiong2018practical, yang2020deep, zhang2020deep, ardon2021towards, amrouni2021abides, coletta2021towards} by building their own market environments. Despite the above-mentioned open-source libraries that provide some useful environments, there are no established benchmarks yet. On the other hand, the data accessing, cleaning and factor extraction processes are usually limited to data sources like Yahoo Finance and Wharton Research Data Services (WRDS).

