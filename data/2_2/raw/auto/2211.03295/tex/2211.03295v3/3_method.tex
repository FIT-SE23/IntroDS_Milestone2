\section{Related Work}
\label{sec:preliminaries}
\subsection{Vision Transformers}

Since the success of Transformer~\citep{vaswani2017attention} in natural language processing~\citep{devlin2018bert},
ViT has been proposed~\citep{iclr2021vit} and attained impressive results on ImageNet~\citep{cvpr2009imagenet}. %It splits raw images into non-overlapping fixed-size patches as visual tokens to capture long-range interactions among these tokens by self-attention. 
% Hybird ViTs: Swin, Uniformer, Next-ViT
Yet, compared to ConvNets, ViTs are over-parameterized and rely on large-scale pre-training \citep{ bao2021beit, cvpr2022mae, li2022A2MIM}. Targeting this problem, one branch of researchers presents lightweight ViTs~\citep{nips2021vitc, iclr2022mobilevit, nips2022EfficientFormer, chen2022CFViT} with efficient attentions~\citep{nips2020linformer}.
Meanwhile, the incorporation of self-attention and convolution as a hybrid backbone has been studied~\citep{guo2021cmt, wu2021cvt, nips2021coatnet, d2021convit, iclr2022uniformer, aaai2022LIT, nips2022iformer} for imparting locality priors to ViTs.
%
By introducing local inductive bias~\citep{zhu2020deformable, chen2021pre, jiang2021transgan, arnab2021vivit}, advanced training strategies~\citep{icml2021deit, yuan2021tokens, eccv2022deit3} or extra knowledge~\citep{nips2021TL, Lin2022SuperViT, eccv2022tinyvit}, ViTs can achieve superior performance and have been extended to various vision areas.
%
MetaFormer~\citep{yu2022metaformer} %as shown in Fig. \ref{fig:framework} 
considerably influenced the roadmap of deep architecture design, where all ViTs~\citep{2022convmixer, aaai2022shiftvit} can be classified by the token-mixing strategy, such as relative position encoding~\citep{wu2021rethinking}, local window shifting~\citep{liu2021swin} and MLP layer~\citep{nips2021MLPMixer}, \textit{etc.}

% fig: framework
\begin{figure*}[t]
    \vspace{-2.25em}
    \centering
    \includegraphics[width=0.85\textwidth]{Figs/fig_moga_framework.pdf}
    \vspace{-0.5em}
    \caption{\textbf{MogaNet architecture with four stages.} Similar to~\citep{liu2021swin, cvpr2022convnext}, MogaNet uses hierarchical architecture of 4 stages. Each stage $i$ consists of an embedding stem and $N_{i}$ Moga Blocks, which contain spatial aggregation blocks and channel aggregation blocks.
    }
    \label{fig:moga_framework}
    \vspace{-1.0em}
\end{figure*}


\subsection{Post-ViT Modern ConvNets}
%Classical ConvNets~\citep{simonyan2014very, xie2017aggregated} fails to capture long-range interactions constrained by their receptive fields.
Taking the merits of ViT-style framework design~\citep{yu2022metaformer}, modern ConvNets~\citep{cvpr2022convnext, Liu2022SLak, nips2022hornet, nips2022focalnet} show superior performance with large kernel depth-wise convolutions~\citep{han2021demystifying} for global perception (\pl{view Appendix~\ref{sec:related_work} for detail backgrounds}).
%Images require operations with local and geometrical inductive biases such as convolutions~\citep{cvpr2016inceptionv3, iclr2016dilated, chollet2017xception}, spatial MLP~\citep{nips2021MLPMixer}, or even non-parametric operations like pooling~\citep{yu2022metaformer} and spatial shifting~\citep{aaai2022shiftvit}.  
%
It primarily comprises three components: (\romannumeral1) embedding stem, (\romannumeral2) spatial mixing block, and (\romannumeral3) channel mixing block.
Embedding stem downsamples the input to reduce redundancies and computational overload. We assume the input feature $X$ %and output $Z$ are in the same shape $\mathbb{R}^{C\times H\times W}$, we have:
\pl{is in the shape} $\mathbb{R}^{C\times H\times W}$, we have:
% \vspace{-0.30em}
\begin{equation}
    Z = \mathrm{Stem}(X),
    \vspace{-0.40em}
\end{equation}
where $Z$ is downsampled features, \textit{e.g.,}. 
%
Then, the feature flows to a stack of residual blocks. In each stage, the network modules can be decoupled into two separate functional components, $\mathrm{SMixer}(\cdot)$ and $\mathrm{CMixer}(\cdot)$ for spatial-wise and channel-wise information propagation,
\vspace{-0.50em}
\begin{equation}
    \label{eq:smixer}
    Y = X + \mathrm{SMixer}\big(\mathrm{Norm}(X)\big),
    \vspace{-0.50em}
\end{equation}
\begin{equation}
    \label{eq:cmixer}
    Z = Y + \mathrm{CMixer}\big(\mathrm{Norm}(Y)\big),
    \vspace{-0.20em}
\end{equation}
where $\mathrm{Norm}(\cdot)$ denotes a normalization layer, \textit{e.g.,} BatchNorm~\citep{ioffe2015batch} (BN). $\mathrm{SMixer}(\cdot)$ can be various spatial operations (\textit{e.g.,} self-attention, convolution), while $\mathrm{CMixer}(\cdot)$ is usually achieved by channel MLP with inverted bottleneck~\citep{cvpr2018mobilenetv2} and expand ratio $r$.
%
Notably, we abstract \textit{context aggregation} in modern ConvNets as a series of operations that can \textit{adaptively} aggregate contextual information while suppressing trivial redundancies in spatial mixing block $\mathrm{SMixer}(\cdot)$ between two embedded features:
\vspace{-0.40em}
\begin{equation}\label{eq:aggregate}
    O = \mathcal{S}\big(\mathcal{F}_{\phi}(X), \mathcal{G}_{\psi}(X)\big),
    \vspace{-0.40em}
\end{equation}
where $\mathcal{F}_{\phi}(\cdot)$ and $\mathcal{G}_{\psi}(\cdot)$ are the aggregation and context branches with parameters $\phi$ and $\psi$. Context aggregation models the importance of each position on $X$ by the aggregation branch $\mathcal{F}_{\phi}(X)$ and reweights the embedded feature from the context branch $\mathcal{G}_{\psi}(X)$ by operation $\mathcal{S}(\cdot,\cdot)$.

%\input{Tabs/tab_attention.tex}

%As shown in Table~\ref{tab:attention}, there are mainly two types of context aggregations for modern ConvNets: self-attention mechanism~\citep{vaswani2017attention, wang2018non, iclr2021vit} and gating attention~\citep{dauphin2017language, hu2018squeeze}.
%The importance of each position on $X$ is calculated by global interactions of all other positions in $\mathcal{F}_{\phi}(\cdot)$ with a dot-product, which results in quadratic computational complexity. To overcome this limitation, attention variants in linear complexity~\citep{nips2021SOFT, iclr2022cosFormer} were proposed to substitute vanilla self-attention, \textit{e.g.,} linear attention~\citep{nips2020linformer, nips2022hilo} in the second line of Table~\ref{tab:attention}, but they might degenerate to trivial attentions~\citep{icml2022Flowformer}. Unlike self-attention, gating unit employs an element-wise product $\odot$ as $\mathcal{S}(\cdot,\cdot)$ in linear complexity, \textit{e.g.,} gated linear unit (GLU) variants~\citep{Shazeer2020GLU} and squeeze-and-excitation (SE) modules~\citep{hu2018squeeze} in the last two lines of Table~\ref{tab:attention}. % However, they only aggregate the information of each position or the overall context with global average pooling (GAP), which lacks spatial interactions.


%\section{Representation Bottleneck of DNNs from the View of Multi-order Game-theoretic Interaction}
\section{Multi-order Game-theoretic Interaction for Deep Architecture Design}
% \vspace{-0.5em}
\label{sec:rep_bottleneck}
\paragraph{Representation Bottleneck of DNNs}
Recent studies toward the generalizability~\citep{iclr2019shapebias, ancona2019explaining, 2021shapebias, nips2021partial} and robustness~\citep{naseer2021intriguing, icml2022FAN, iclr2022how} of DNNs delivers a new perspective to improve deep architectures.
Apart from them, the investigation of multi-order game-theoretic interaction unveils the representation bottleneck of DNNs.
% As shown in Fig.~\ref{fig:mask}, 
Methodologically, multi-order interactions between two input variables represent marginal contribution brought by collaborations among these two and other involved contextual variables, where the order indicates the number of contextual variables within the collaboration.
%DNNs can still recognize the target object under extreme occlusion ratios (\textit{e.g.,} only 10$\sim$20\% visible patches) but produce less information gain with intermediate occlusions~\citep{deng2021discovering, naseer2021intriguing}.
%Interestingly, our human brains attain the sharpest knowledge upsurge from images with around 50\% patches, which indicates an intriguing cognition gap between human vision and deep models.
%
Formally, it can be explained by $m$-th order game-theoretic interaction $I^{(m)}(i,j)$ and $m$-order interaction strength $J^{(m)}$, as defined in \citep{zhang2020interpreting, deng2021discovering}.
Considering the image with $n$ patches in total, $I^{(m)}(i,j)$ measures the average interaction complexity between the patch pair $i,j$ over all contexts consisting of $m$ patches, where $0\le m \le n-2$ and the order $m$ reflects the scale of the context involved in the game-theoretic interactions between pixels $i$ and $j$. Normalized by the average of interaction strength, the relative interaction strength $J^{(m)}$ with $m\in (0,1)$ measures the complexity of interactions encoded in DNNs. 
%
Notably, low-order interactions tend to encode \textbf{common} or \textbf{widely-shared local texture}, and the high-order ones are inclined to forcibly memorize the pattern of \textbf{rare outliers}~\citep{deng2021discovering, cheng2021game}. 
%
As shown in Fig.~\ref{fig:spatial_interaction}, existing DNNs are implicitly prone to excessively low- or high-order interactions while suppressing the most expressive and versatile middle-order ones~\citep{deng2021discovering, cheng2021game}.
%
Refer to Appendix~\ref{app:interaction} for definitions and more details.

\begin{wrapfigure}{r}{0.425\linewidth}
    \vspace{-2.0em}
    \begin{center}
    \includegraphics[width=1.0\linewidth]{Figs/fig_analysis_interaction.pdf}
    \end{center}
    \vspace{-1.5em}
    \caption{
    \textbf{Distributions of the interaction strength $J^{(m)}$} for Transformers and ConvNets on ImageNet-1K with $224^2$ resolutions and $n$ = $14\times 14$. %Middle-order strengths mean the middle-complex interaction, where a medium number of patches (\textit{e.g.,} 0.2$\sim$0.8n) participate.
    }
    \label{fig:spatial_interaction}
    \vspace{-1.5em}
\end{wrapfigure}

\vspace{-1.0em}
\paragraph{Multi-order Interaction for Architecture Design}
Existing deep architecture design is usually derived from intuitive insights, lacking hierarchical theoretic guidance. Multi-order interaction can serve as a reference that fits well with the already gained insights on computer vision and further guides the ongoing quest.
% For instance, the extremely high-order interactions encoded in ViTs (\textit{e.g.}, DeiT in Fig.~\ref{fig:spatial_interaction}) indicate its global-range feature interactions from the self-attention mechanism.
For instance, the extremely high-order interactions encoded in ViTs (\textit{e.g.}, DeiT in Fig.~\ref{fig:spatial_interaction}) \pl{may stem from its adaptive global-range self-attention mechanism.}
Its superior robustness can be attributed to its excessive low-order interactions, representing common and widely shared local patterns.
%
However, \pl{the absence of locality priors still leaves ViTs lacking middle-order interactions, which cannot be replaced by the low-order ones.}
%
As for modern ConvNets (\textit{e.g.}, SLaK in Fig.~\ref{fig:spatial_interaction}), despite the $51\times51$ kernel size, it still fails to encode enough expressive interactions (\pl{view more results in Appendix~\ref{app:interaction}}).
%
Likewise, we argue that such dilemma may be attributed to the inappropriate composition of convolutional locality priors and global context injections~\citep{treisman1980feature, 2021shapebias, li2022A2MIM}. 
A naive combination of self-attention or convolutions can be intrinsically prone to the strong bias of global shape~\citep{nips2021partial, cvpr2022replknet} or local texture~\citep{hermann2020origins}, infusing extreme-order interaction preference to models.
%
\pl{In MogaNet, we aim to provide an architecture that can \textit{adaptively force the network to encode expressive interactions that would have otherwise been ignored inherently}}.

\begin{figure*}[b!]
    \vspace{-1.25em}
    \centering
    \subfloat[$\mathrm{Moga}(\cdot)$ Block]{\label{fig:moga_multiorder}
    \hspace{-0.5em}
    \includegraphics[width=0.375\linewidth,trim= 0 0 0 0,clip]{Figs/fig_moga_spatial.pdf}
    }
    \subfloat[$\mathrm{CA}(\cdot)$ Block]{\label{fig:channal_moga}
    \includegraphics[width=0.315\linewidth,trim= 0 0 0 0,clip]{Figs/fig_moga_channel.pdf}
    }
    \subfloat[]{\label{fig:channal_analysis}
    \hspace{-0.5em}
    \includegraphics[width=0.310\linewidth,trim= 1 8 0 1,clip]{Figs/fig_analysis_decomp.pdf}
    }
    \vspace{-0.75em}
    \caption{
    \textbf{(a) Structure of spatial aggregation block $\mathrm{Moga}(\cdot)$.}
    \textbf{(b) Structure of channel aggregation block.}
    \textbf{(c) Analysis of channel MLP and the channel aggregation module.} Based on MogaNet-S, performances and model sizes of the raw channel MLP, MLP with SE block, and the channel aggregation is compared with the MLP ratio of $\{2,4,6,8\}$ on ImageNet-1K.
    }
    \vspace{-0.75em}
\end{figure*}

\section{Methodology}
\label{sec:method}
% In this section, we instantiate the overall framework as MogaNet equipped with spatial and channel aggregation blocks for multi-order context aggregation and channel-wise multi-order features reallocation.
% In this section, we instantiate the overall framework as MogaNet with convolution-based modules that efficiently combine regionality perception and context aggregation.

\vspace{-0.5em}
\subsection{Overview of MogaNet}
\label{sec:overview}
% Fig.~\ref{fig:app_moga_framework} comprehensively illustrates the four-stage MogaNet architecture.
Built upon modern ConvNets, we design a four-stage MogaNet architecture as illustrated in 
Fig.~\ref{fig:moga_framework}.
For stage $i$, the input image or feature is first fed into an embedding stem to regulate the resolutions and embed into $C_{i}$ dimensions. Assuming the input image in $H\times W$ resolutions, features of the four stages are in $\frac{H}{4}\times\frac{W}{4}$, $\frac{H}{8}\times\frac{W}{8}$, $\frac{H}{16}\times\frac{W}{16}$, and $\frac{H}{32}\times\frac{W}{32}$ resolutions respectively.
Then, the embedded feature flows into $N_{i}$ Moga Blocks, consisting of spatial and channel aggregation blocks (in Sec.~\ref{sec:moga} and \ref{sec:channel}), for further context aggregation. 
After the final output, GAP and a linear layer are added for classification tasks. As for dense prediction tasks~\citep{2017iccvmaskrcnn, eccv2018upernet}, the output from four stages can be used through neck modules~\citep{cvpr2017fpn, cvpr2019semanticFPN}.


%\subsection{Multi-order Gated Aggregation}
\subsection{Multi-order Spatial Gated Aggregation}
\label{sec:moga}
%However, as discussed in Sec.~\ref{sec:rep_bottleneck}, the sole presence of regionality perception or context aggregation exhibits the inability to learn comprehensive contextual features with multi-order interactions~\citep{iclr2022uniformer, pinto2022impartial, deng2021discovering}.
As discussed in Sec.~\ref{sec:rep_bottleneck}, DNNs with the incompatible composition of locality perception and context aggregation can be implicitly prone to extreme-order game-theoretic interaction strengths while suppressing the more robust and expressive middle-order ones~\citep{iclr2022uniformer, pinto2022impartial, deng2021discovering}.
As shown in Fig.~\ref{fig:ablation_cam}, the primary obstacle pertains to \pl{how to \textbf{force} the network to encode the originally ignored expressive interactions and informative features.}
%Fig.~\ref{fig:interaction} shows conventional DNNs tend to focus on extremely low or high-order interactions. They are missing the most informative middle-order interactions. Thus, the primary challenge is how to capture contextual multi-order interactions effectively and efficiently.
%
We first suppose that the essential \textit{adaptive} nature of attention in ViTs has not been well leveraged and grafted into ConvNets. 
Thus, we propose spatial aggregation (SA) block as an instantiation of $\mathrm{SMixer}(\cdot)$ to learn representations of multi-order interactions in a unified design, as shown in Fig.~\ref{fig:moga_multiorder}, consisting of two cascaded components. We instantiate Eq.~(\ref{eq:smixer}) as:
\vspace{-0.40em}
\begin{equation}
    \label{eq:moga_block}
    Z = X + \mathrm{Moga}\Big( \mathrm{FD}\big(\mathrm{Norm}(X)\big) \Big),
    \vspace{-0.40em}
\end{equation}
where $\mathrm{FD}(\cdot)$ indicates a feature decomposition module (FD) and $\mathrm{Moga}(\cdot)$ denotes a multi-order gated aggregation module comprising the gating $\mathcal{F}_{\phi}(\cdot)$ and context branch $\mathcal{G}_{\psi}(\cdot)$.

\begin{figure*}[t]
\vspace{-2.5em}
\centering
\begin{minipage}{0.61\linewidth}
    % \vspace{-0.5em}
    \includegraphics[width=1.0\linewidth]{Figs/fig_ablation_gradcam.pdf}
    \vspace{-1.75em}
    \caption{\textbf{Grad-CAM visualization of ablations}. 1:\ 0:\ 0 and 0:\ 0:\ 1 denote only using $C_l$ or $C_h$ for Multi-order DWConv Layers in SA block. The models encoded extremely low- ($C_l$) or high- ($C_h$) order interactions are sensitive to similar regional textures (1:\ 0:\ 0) or excessive discriminative parts (0:\ 0:\ 1), not localizing precise semantic parts. Gating effectively eliminates the disturbing contextual noise (\textit{$\backslash$wo} Gating).}
    \label{fig:ablation_cam}
\end{minipage}
\begin{minipage}{0.38\linewidth}
    \vspace{-0.5em}
    \input{Tabs/tab_ablation.tex}
    \vspace{-1.0em}
\end{minipage}
\vspace{-1.5em}
\end{figure*}

\paragraph{Context Extraction.}
\vspace{-1.0em}
As a pure ConvNet structure, we extract multi-order features with both \textit{static} and \textit{adaptive} locality perceptions. 
% Since convolutions are inherently high-pass filters~\citep{iclr2022how, wang2022anti}, we design $\mathrm{FD}(\cdot)$ to adaptively decompose and enhance the high-frequency details to learn more informative features of full-frequency bands, which is formulated as: 
There are two complementary counterparts, fine-grained local texture (low-order) and complex global shape (middle-order), which are instantiated by $\mathrm{Conv}_{1\times 1}(\cdot)$ and $\mathrm{GAP}(\cdot)$ respectively. To \textbf{force} the network against its \pl{implicitly inclined interaction strengths}, we design $\mathrm{FD}(\cdot)$ to adaptively exclude the trivial (overlooked) interactions, defined as:
\vspace{-0.25em}
\begin{align}
    \label{eq:FD_proj}
    Y &= \mathrm{Conv}_{1\times 1}(X),\\
    \label{eq:FD}
    Z &= \mathrm{GELU}\Big(Y + \gamma_{s}\odot\big( Y-\mathrm{GAP}(Y)\big) \Big),
    \vspace{-1.5em}
\end{align}
where ${\gamma}_{s} \in \mathbb{R}^{C\times 1}$ denotes a scaling factor initialized as zeros. By re-weighting the complementary interaction component $Y - \mathrm{GAP}(Y)$, $\mathrm{FD}(\cdot)$ also increases spatial feature diversities~\citep{iclr2022how, wang2022anti}.
Then, we ensemble depth-wise convolutions (DWConv) to encode multi-order features in the context branch of $\mathrm{Moga}(\cdot)$. Unlike previous works that simply combine DWConv with self-attentions to model local and global interactions~\citep{eccv2022edgeformer, nips2022hilo, nips2022iformer, nips2022hornet} , we employ three different DWConv layers with dilation ratios $d\in \{1,2,3\}$ in parallel to capture low, middle, and high-order interactions: given the input feature $X\in \mathbb{R}^{C\times HW}$, $\mathrm{DW}_{5\times 5, d=1}$ is first applied for low-order features; then, the output is factorized into ${X}_l \in \mathbb{R}^{C_l \times HW}$, ${X}_m \in \mathbb{R}^{C_m \times HW}$, and ${X}_h \in \mathbb{R}^{C_h \times HW}$ along the channel dimension, where $C_l + C_m + C_h =C$; afterward, ${X}_m$ and ${X}_h$ are assigned to $\mathrm{DW}_{5\times 5, d=2}$ and $\mathrm{DW}_{7\times 7, d=3}$, respectively, while ${X}_l$ serves as identical mapping; finally, the output of ${X}_l$, ${X}_m$, and ${X}_h$ are concatenated to form multi-order contexts, $Y_{C} = \mathrm{Concat}(Y_{l, 1:C_{l}}, Y_{m}, Y_{h})$. 
Notice that the proposed $\mathrm{FD}(\cdot)$ and multi-order DWConv layers only require a little extra computational overhead and parameters in comparison to $\mathrm{DW}_{7\times 7}$ used in ConvNeXt~\citep{cvpr2022convnext}, \textit{e.g.,} +multi-order and +$\mathrm{FD}(\cdot)$ increase 0.04M parameters and 0.01G FLOPS over $\mathrm{DW}_{7\times 7}$ as shown in Table~\ref{tab:ablation}.

\vspace{-1.0em}
\paragraph{Gated Aggregation.}
To \textit{adaptively} aggregate the extracted feature from the context branch, we employ SiLU~\citep{elfwing2018sigmoid} activation in the gating branch, \textit{i.e.,} $x\cdot \mathrm{Sigmoid}(x)$, which has been well-acknowledged as an advanced version of Sigmoid activation. 
As illustrated in Appendix~\ref{app:ablation_gating}, we empirically show that SiLU in MogaNet exhibits both the gating effects as Sigmoid and the stable training property.
Taking the output from $\mathrm{FD}(\cdot)$ as the input, we instantiate Eq.~(\ref{eq:aggregate}):
\vspace{-0.25em}
\begin{align}
    \label{eq:moga}
    Z &= \underbrace{\mathrm{SiLU}\big( \mathrm{Conv}_{1\times 1}(X) \big)}_{\mathcal{F}_{\phi}} \odot \underbrace{\mathrm{SiLU}\big( \mathrm{Conv}_{1\times 1}(Y_{C}) \big)}_{\mathcal{G}_{\psi}},
    \vspace{-1.25em}
\end{align}
% where $\mathcal{G}_{\psi}(\cdot)$ and $\mathcal{F}_{\phi}(\cdot)$ are defined as $\mathrm{SiLU}(\mathrm{Conv}_{1\times 1}(\cdot))$.
With the proposed SA blocks, MogaNet captures more middle-order interactions, as validated in Fig.~\ref{fig:spatial_interaction}. 
The SA block produces discriminative multi-order representations with similar parameters and FLOPs as $\mathrm{DW}_{7\times 7}$ in ConvNeXt, which is well beyond the reach of existing methods without the cost-consuming self-attentions.

\begin{wrapfigure}{r}{0.49\linewidth}
    \vspace{-1.5em}
    \begin{center}
    \includegraphics[width=1.0\linewidth]{Figs/channel_en_saliency.pdf}
    \end{center}
    \vspace{-1.5em}
    \caption{
    \textbf{Channel energy ranks and channel saliency maps (CSM)}~\citep{cvpr2022ReflashDIS} with or without our CA block based on MogaNet-S. The energy reflects the importance of the channel, while the highlighted regions of CSMs are the activated spatial features of each channel.
    }
    \label{fig:channel_en_saliency}
    \vspace{-1.5em}
\end{wrapfigure}

%\subsection{Multi-order Feature Reallocation by Channel Aggregation}
\subsection{Multi-order Channel Reallocation}
\vspace{-0.25em}
\label{sec:channel}
Prevalent architectures, as illustrated in Sec.~\ref{sec:preliminaries}, perform channel-mixing $\mathrm{CMixer}(\cdot)$ mainly by two linear projections, \textit{e.g.,} 2-layer channel-wise MLP~\citep{iclr2021vit, liu2021swin, nips2021MLPMixer} with a expand ratio $r$ or the MLP with a $3\times 3$ DWConv in between~\citep{cvmj2022PVTv2, aaai2022LIT, nips2022hilo}.
Due to the information redundancy cross channels~\citep{eccv2018CBAM, iccv2019GCNet, icml2019efficientnet, cvpr2020Orthogonal}, vanilla MLP requires a number of parameters ($r$ default to 4 or 8) to achieve expected performance, showing low computational efficiency as plotted in Fig.~\ref{fig:channal_analysis}. 
To address this issue, most current methods directly insert a channel enhancement module, \textit{e.g.,} SE module~\citep{hu2018squeeze}, into MLP.
%This problem might arise from the inherent redundancy cross channels~\citep{eccv2018CBAM, iccv2019GCNet, icml2019efficientnet, cvpr2020Orthogonal}, and most current methods try to address this issue by starchedly inserting a channel enhancement module, %\textit{e.g.,} SE module~\citep{hu2018squeeze}, into MLP.
Unlike these designs requiring additional MLP bottleneck, \pl{motivated by $\mathrm{FD}(\cdot)$,} we introduce a lightweight channel aggregation module $\mathrm{CA}(\cdot)$ to adaptive reallocate channel-wise features in high-dimensional hidden spaces and further extend it to a channel aggregation (CA) block. 
% Unlike these attempts, we introduce a lightweight channel aggregation module $\mathrm{CA}(\cdot)$ to conduct adaptive channel-wise reallocation in high-dimensional hidden spaces and further extend it to a channel aggregation (CA) block. 
As shown in Fig.~\ref{fig:channal_moga}, we rewrite Eq.~(\ref{eq:cmixer}) for our CA block as:
\vspace{-0.5em}
\begin{equation}
\begin{aligned}
    Y &= \mathrm{GELU}\Big(\mathrm{DW_{3\times 3}}\big(\mathrm{Conv_{1\times 1}}(\mathrm{Norm}(X))\big)\Big),\\
    Z &= \mathrm{Conv_{1 \times 1}}\big(\mathrm{CA}(Y)\big) + X.
\end{aligned}
\vspace{-0.4em}
\end{equation}
Concretely, $\mathrm{CA}(\cdot)$ is implemented by a channel-reducing projection $W_{r}: \mathbb{R}^{C\times HW}\rightarrow \mathbb{R}^{1\times HW}$ and GELU to gather and reallocate channel-wise information:
\vspace{-0.5em}
\begin{equation}
    \mathrm{CA}(X) = X + \gamma_{c}\odot\big(X - \mathrm{GELU}(XW_{r})\big),
\vspace{-0.4em}
\end{equation}
where $\gamma_{c}$ is the channel-wise scaling factor initialized as zeros. It reallocates the channel-wise feature \pl{with} the complementary interactions $(X - \mathrm{GELU}(XW_{r}))$. As shown in Fig.~\ref{fig:ablation_interaction}, $\mathrm{CA}(\cdot)$ enhances \pl{originally overlooked} game-theoretic interactions.
Fig.~\ref{fig:channal_analysis} and Fig.~\ref{fig:channel_en_saliency} verify the effectiveness of $\mathrm{CA}(\cdot)$ compared with vanilla MLP and MLP with SE module in channel-wise effiency and representation ability. Despite some improvements to the baseline, the MLP $w/$ SE module still requires large MLP ratios (\textit{e.g.,} $r$ = 6) to achieve expected performance while bringing extra parameters and overhead. 
Yet, our $\mathrm{CA}(\cdot)$ with $r$ = 4 brings 0.6\% gain over the baseline at a small extra cost (0.04M extra parameters \& 0.01G FLOPs) while achieving the same performance as the baseline with $r$ = 8.
% It is implemented by various attention mechanisms in recent vision Transformer models or spatial Multi-layer Perceptron (MLP) in MLP-like models. Note that the main function of the token mixer is to propagate token information, although some token mixers can also mix channels, like attention. At Stage II, the second sub-block primarily consists of a two-layered MLP with non-linear activation, looking at cross-channel correlations via a set of $1\times 1$ convolutions. 


\subsection{Implementation Details}
\label{sec:details}
Following the network design style of ConvNets~\citep{cvpr2022convnext}, we scale up MogaNet for six model sizes (X-Tiny, Tiny, Small, Base, Large, and X-Large) via stacking the different number of spatial and channel aggregation blocks at each stage, which has similar numbers of parameters as RegNet~\citep{cvpr2020regnet} variants. Network configurations and hyper-parameters are detailed in Table~\ref{tab:app_architecture}. FLOPs and throughputs are analyzed in Appendix \ref{app:flops_throughput}.
We set the channels of the multi-order DWConv layers to $C_l:$ $C_m:$ $C_h$ = 1:3:4 (see 
 Appendix~\ref{app:ablation_multiorder}).
% The embedding stem in canonical ResNet contains a 7$\times$7 convolution layer with stride 2, followed by a max-pooling layer, which results in a 4$\times$ downsampling of the input images.
Similar to \citep{2021patchconvnet, iclr2022uniformer, nips2022EfficientFormer}, the first embedding stem in MogaNet is designed as two stacked 3$\times$3 convolution layers with the stride of 2 while adopting the single-layer version for embedding stems in other three stages.
We select GELU~\citep{hendrycks2016bridging} as the common activation function and only use SiLU in the Moga module as Eq.~(\ref{eq:moga}).
