\begin{abstract}

By contextualizing the kernel as global as possible, Modern ConvNets have shown great potential in computer vision tasks.
%However, recent progress of \textit{multi-order game-theoretic interaction} in deep neural networks (DNNs) reveals that the representation capacity of modern ConvNets has not been well unleashed, where the most expressive interactions have not been effectively encoded with the increased kernel size.
However, recent progress on \textit{multi-order game-theoretic interaction} within deep neural networks (DNNs) \pl{reveals the representation bottleneck of modern ConvNets}, where the expressive interactions have not been effectively encoded with the increased kernel size. 
%\pl{Attempts have been made to address this challenge through loss function design and data distribution alteration.}
To tackle this challenge, we propose a new family of modern ConvNets, dubbed MogaNet, for discriminative visual representation learning in pure ConvNet-based models with favorable complexity-performance trade-offs. 
%In this work, we propose a new family of modern ConvNets, dubbed MogaNet, for discriminative visual representation learning in pure ConvNet-based models, with preferable complexity-performance trade-offs. 
%In this paper, we explore the representation ability of ConvNet through the prism of \textit{multi-order game-theoretic interaction}, which portrays inter-variable interaction effects w.r.t.~varying scales of context via game theory.
%
MogaNet encapsulates conceptually simple yet effective convolutions and gated aggregation into a compact module, where discriminative features are efficiently gathered and contextualized adaptively.
%Extensive experiments show that MogaNet exhibits great scalability, impressive efficiency of model parameters, and competitive performance compared to state-of-the-art ViTs and ConvNets on ImageNet and various downstream vision benchmarks, including COCO object detection, ADE20K semantic segmentation, 2D\&3D human pose estimation, and video prediction.
MogaNet exhibits great scalability, impressive efficiency of parameters, and competitive performance compared to state-of-the-art ViTs and ConvNets on ImageNet and various downstream vision benchmarks, including COCO object detection, ADE20K semantic segmentation, 2D\&3D human pose estimation, and video prediction.
Notably, MogaNet hits 80.0\% and 87.8\% accuracy with 5.2M and 181M parameters on ImageNet-1K, outperforming ParC-Net and ConvNeXt-L, while saving 59\% FLOPs and 17M parameters, respectively.
% 
% code (arxiv & final version)
The source code is available at \url{https://github.com/Westlake-AI/MogaNet}.
% \vspace{-0.5em}


\end{abstract}
