% \section{Related Work}
% \label{sec:related_work}
% \paragraph{Post-ViT Modern ConvNets}

\section{Related Work}
\label{sec:preliminaries}
\vspace{-0.25em}
\subsection{Vision Transformers}
Since the success of Transformer~\citep{vaswani2017attention} in natural language processing~\citep{devlin2018bert},
ViT has been proposed~\citep{iclr2021vit} and attained impressive results on ImageNet~\citep{cvpr2009imagenet}. %It splits raw images into non-overlapping fixed-size patches as visual tokens to capture long-range interactions among these tokens by self-attention. 
% Hybrid ViTs: Swin, Uniformer, Next-ViT
Yet, compared to ConvNets, ViTs are over-parameterized and rely on large-scale pre-training \citep{ bao2021beit, cvpr2022mae, li2022A2MIM}. Targeting this problem, one branch of researchers presents lightweight ViTs~\citep{nips2021vitc, iclr2022mobilevit, nips2022EfficientFormer, chen2022CFViT} with efficient attentions~\citep{nips2020linformer}.
Meanwhile, the incorporation of self-attention and convolution as a hybrid backbone has been studied~\citep{guo2021cmt, wu2021cvt, nips2021coatnet, d2021convit, iclr2022uniformer, aaai2022LIT, nips2022iformer} for imparting locality priors to ViTs.
%
By introducing local inductive bias~\citep{zhu2020deformable, chen2021pre, jiang2021transgan, arnab2021vivit}, advanced training strategies~\citep{icml2021deit, yuan2021tokens, eccv2022deit3} or extra knowledge~\citep{nips2021TL, Lin2022SuperViT, eccv2022tinyvit}, ViTs can achieve superior performance and have been extended to various vision areas.
%
MetaFormer~\citep{yu2022metaformer} %as shown in Fig. \ref{fig:framework} 
considerably influenced the roadmap of deep architecture design, where all ViTs~\citep{2022convmixer, aaai2022shiftvit} can be classified by the token-mixing strategy, such as relative position encoding~\citep{wu2021rethinking}, local window shifting~\citep{liu2021swin} and MLP layer~\citep{nips2021MLPMixer}, \textit{etc.}

% fig: framework
\begin{figure*}[t]
    \vspace{-2.5em}
    \centering
    \includegraphics[width=0.85\textwidth]{Figs/fig_moga_framework.pdf}
    \vspace{-0.5em}
    \caption{\textbf{MogaNet architecture with four stages.} Similar to~\citep{liu2021swin, cvpr2022convnext}, MogaNet uses hierarchical architecture of 4 stages. Each stage $i$ consists of an embedding stem and $N_{i}$ Moga Blocks, which contain spatial aggregation blocks and channel aggregation blocks.
    }
    \label{fig:moga_framework}
    \vspace{-1.0em}
\end{figure*}


\vspace{-0.25em}
\subsection{Post-ViT Modern ConvNets}
%Classical ConvNets~\citep{simonyan2014very, xie2017aggregated} fails to capture long-range interactions constrained by their receptive fields.
Taking the merits of ViT-style framework design~\citep{yu2022metaformer}, modern ConvNets~\citep{cvpr2022convnext, Liu2022SLak, nips2022hornet, nips2022focalnet} show superior performance with large kernel depth-wise convolutions~\citep{han2021demystifying} for global perception (\pl{view Appendix~\ref{sec:related_work} for detail backgrounds}).
%Images require operations with local and geometrical inductive biases such as convolutions~\citep{cvpr2016inceptionv3, iclr2016dilated, chollet2017xception}, spatial MLP~\citep{nips2021MLPMixer}, or even non-parametric operations like pooling~\citep{yu2022metaformer} and spatial shifting~\citep{aaai2022shiftvit}.  
%
It primarily comprises three components: (\romannumeral1) embedding stem, (\romannumeral2) spatial mixing block, and (\romannumeral3) channel mixing block.
Embedding stem downsamples the input to reduce redundancies and computational overload. We assume the input feature $X$ %and output $Z$ are in the same shape $\mathbb{R}^{C\times H\times W}$, we have:
\pl{is in the shape} $\mathbb{R}^{C\times H\times W}$, we have:
% \vspace{-0.30em}
\begin{equation}
    Z = \mathrm{Stem}(X),
    \vspace{-0.40em}
\end{equation}
where $Z$ is downsampled features, \textit{e.g.,}. 
%
Then, the feature flows to a stack of residual blocks. In each stage, the network modules can be decoupled into two separate functional components, $\mathrm{SMixer}(\cdot)$ and $\mathrm{CMixer}(\cdot)$ for spatial-wise and channel-wise information propagation,
\vspace{-0.50em}
\begin{equation}
    \label{eq:smixer}
    Y = X + \mathrm{SMixer}\big(\mathrm{Norm}(X)\big),
    \vspace{-0.50em}
\end{equation}
\begin{equation}
    \label{eq:cmixer}
    Z = Y + \mathrm{CMixer}\big(\mathrm{Norm}(Y)\big),
    \vspace{-0.20em}
\end{equation}
where $\mathrm{Norm}(\cdot)$ denotes a normalization layer, \textit{e.g.,} BatchNorm~\citep{ioffe2015batch} (BN). $\mathrm{SMixer}(\cdot)$ can be various spatial operations (\textit{e.g.,} self-attention, convolution), while $\mathrm{CMixer}(\cdot)$ is usually achieved by channel MLP with inverted bottleneck~\citep{cvpr2018mobilenetv2} and expand ratio $r$.
%
Notably, we abstract \textit{context aggregation} in modern ConvNets as a series of operations that can \textit{adaptively} aggregate contextual information while suppressing trivial redundancies in spatial mixing block $\mathrm{SMixer}(\cdot)$ between two embedded features:
\vspace{-0.40em}
\begin{equation}\label{eq:aggregate}
    O = \mathcal{S}\big(\mathcal{F}_{\phi}(X), \mathcal{G}_{\psi}(X)\big),
    \vspace{-0.40em}
\end{equation}
where $\mathcal{F}_{\phi}(\cdot)$ and $\mathcal{G}_{\psi}(\cdot)$ are the aggregation and context branches with parameters $\phi$ and $\psi$. Context aggregation models the importance of each position on $X$ by the aggregation branch $\mathcal{F}_{\phi}(X)$ and reweights the embedded feature from the context branch $\mathcal{G}_{\psi}(X)$ by operation $\mathcal{S}(\cdot,\cdot)$.

%\input{Tabs/tab_attention.tex}

%As shown in Table~\ref{tab:attention}, there are mainly two types of context aggregations for modern ConvNets: self-attention mechanism~\citep{vaswani2017attention, wang2018non, iclr2021vit} and gating attention~\citep{dauphin2017language, hu2018squeeze}.
%The importance of each position on $X$ is calculated by global interactions of all other positions in $\mathcal{F}_{\phi}(\cdot)$ with a dot-product, which results in quadratic computational complexity. To overcome this limitation, attention variants in linear complexity~\citep{nips2021SOFT, iclr2022cosFormer} were proposed to substitute vanilla self-attention, \textit{e.g.,} linear attention~\citep{nips2020linformer, nips2022hilo} in the second line of Table~\ref{tab:attention}, but they might degenerate to trivial attentions~\citep{icml2022Flowformer}. Unlike self-attention, gating unit employs an element-wise product $\odot$ as $\mathcal{S}(\cdot,\cdot)$ in linear complexity, \textit{e.g.,} gated linear unit (GLU) variants~\citep{Shazeer2020GLU} and squeeze-and-excitation (SE) modules~\citep{hu2018squeeze} in the last two lines of Table~\ref{tab:attention}. % However, they only aggregate the information of each position or the overall context with global average pooling (GAP), which lacks spatial interactions.
