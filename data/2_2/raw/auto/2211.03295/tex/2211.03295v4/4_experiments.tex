% \begin{figure*}[t!]
% \vspace{-3.0em}
% \centering
% \begin{minipage}{0.49\linewidth}
%     \centering
%     \input{Tabs/tab_in1k_tiny.tex}
%     \vspace{-2.25em}
%     \input{Tabs/tab_in1k}
% \end{minipage}
% ~\begin{minipage}{0.49\linewidth}
%     \centering
%     \input{Tabs/tab_coco.tex}
%     \vspace{-2.25em}
%     \input{Tabs/tab_ade20k.tex}
%     \vspace{-2.25em}
%     \input{Tabs/tab_coco_pose}
%     \vspace{-1.25em}
%     % \input{Tabs/tab_3d_vp}
% \end{minipage}
% \vspace{-3.0em}
% \end{figure*}


\section{Experiments}
\label{sec:expriments}
To impartially evaluate and compare MogaNet with the leading network architectures, we conduct extensive experiments across various popular vision tasks, including image classification, object detection, instance and semantic segmentation, 2D and 3D pose estimation, and video prediction. 
The experiments are implemented with PyTorch and run on NVIDIA A100 GPUs.
% To verify the effectiveness of our method, we conduct extensive experiments on ImageNet-1K \citep{cvpr2009imagenet} for image classification, COCO~\citep{2014MicrosoftCOCO} for object detection, instance segmentation, and pose estimation, and ADE20K~\citep{Zhou2018ADE20k} for semantic segmentation. All experiments are implemented with PyTorch on Ubuntu workstations with NVIDIA A100 GPUs. \textbf{Bold} and \hl{gray} indicate the best performance and our models.


\subsection{ImageNet Classification}
\label{sec:exp_in1k}
\vspace{-0.25em}
\paragraph{Settings.} 
For classification experiments on ImageNet \citep{cvpr2009imagenet}, we train our MogaNet following the standard procedure \citep{icml2021deit, liu2021swin} on ImageNet-1K (IN-1K) for a fair comparison, training 300 epochs with AdamW~\citep{iclr2019AdamW} optimizer, a basic learning rate of $1\times 10^{-3}$, and a cosine scheduler~\citep{loshchilov2016sgdr}. To explore the large model capacities, we pre-trained MogaNet-XL on ImageNet-21K (IN-21K) for 90 epochs and then fine-tuned 30 epochs on IN-1K following \citep{cvpr2022convnext}. Appendix~\ref{app:in1k_settings} and \ref{app:exp_in1k} provide implementation details and more results.
We compare three classical architectures: \textbf{Pure ConvNets} (C), \textbf{Transformers} (T), and \textbf{Hybrid model} (H) with both self-attention and convolution operations.
% We compare four typical architectures: (\romannumeral1) \textbf{Pure ConvNets} (C) include ResNet, ShuffleNetV2, EfficientNet, MobileNetV3, RegNet, ConvNeXt, RepLKNet, FocalNet, SLak, and HorNet. (\romannumeral2) \textbf{Transformers} (T) include DeiT, Swin, T2T-ViT, PVT, PVTV2 Focal, ViT-C, CSWin, SReT, and LiTV2. (\romannumeral3) \textbf{Hybrid architectures} (H) of attention and convolution include PiT, LeViT, CoaT, BoTNet, ViTAE, Twins, CoAtNet, MobileViT, Uniformer, Mobile-Former, ParC-Net, EfficientFormer, and MaxViT.

\vspace{-0.5em}
\paragraph{Results.}
With regard to the lightweight models, Table~\ref{tab:in1k_cls_tiny} shows that MogaNet-XT/T significantly outperforms existing lightweight architectures with a more efficient usage of parameters and FLOPs. 
%
MogaNet-T achieves 79.0\% top-1 accuracy, which improves models with $\sim$5M parameters by at least 1.1 at $224^2$ resolutions. Using $256^2$ resolutions, MogaNet-T outperforms the current SOTA ParC-Net-S by 1.0 while achieving 80.0\% top-1 accuracy with the refined settings. Even with only 3M parameters, MogaNet-XT still surpasses models with around 4M parameters, \textit{e.g.,} +4.6 over T2T-ViT-7. Particularly, MogaNet-T$^{\S}$ achieves 80.0\% top-1 accuracy using $256^2$ resolutions and the refined training settings (detailed in Appendix~\ref{app:advanced_tiny}).
% which adjusts $lr$ and replaces RandAugment~\citep{cubuk2020randaugment} with 3-Augment~\citep{eccv2022deit3}
As for scaling up models in Table~\ref{tab:in1k_cls_scaling}, MogaNet shows superior or comparable performances to SOTA architectures with similar parameters and computational costs.
For example, MogaNet-S achieves 83.4\% top-1 accuracy, outperforming Swin-T and ConvNeXt-T with a clear margin of 2.1 and 1.2. MogaNet-B/L also improves recently proposed ConvNets with fewer parameters, \textit{e.g.,} +0.3/0.4 and +0.5/0.7 points over HorNet-S/B and SLaK-S/B.
% As for 45M and 80M models, we summarize their performances in Table~\ref{tab:in1k_cls_scaling} and MogaNet-B/L still surpass the current state-of-the-art architectures, especially improving Swin-S/B and ConvNeXt-S/B by 1.2\%/ 1.1\% and 1.1\%/ 0.8\%.
When pre-trained on IN-21K, MogaNet-XL is boosted to 87.8\% top-1 accuracy with 181M parameters, saving 169M compared to ConvNeXt-XL. Noticeably, MogaNet-XL can achieve 85.1\% at $224^2$ resolutions without pre-training and improves ConvNeXt-L by 0.8, indicating MogaNets are easier to converge than existing models (also verified in Appendix~\ref{app:exp_in1k}).
% Additionally, we provide fast training results based on RSB A3 100-epoch scheme~\citep{wightman2021rsb} in Appendix~\ref{app:comparison}.


\subsection{Dense Prediction Tasks}
\label{sec:exp_det_seg}
\vspace{-0.25em}
\paragraph{Object detection and segmentation on COCO.}
We evaluate MogaNet for object detection and instance segmentation tasks on COCO~\citep{2014MicrosoftCOCO} with RetinaNet~\citep{iccv2017retinanet}, Mask-RCNN~\citep{2017iccvmaskrcnn}, and Cascade Mask R-CNN~\citep{tpami2019cascade} as detectors. Following the training and evaluation settings in \citep{liu2021swin, cvpr2022convnext}, we fine-tune the models by the AdamW optimizer for $1\times$ and $3\times$ training schedule on COCO~\textit{train2017} and evaluate on COCO~\textit{val2017}, implemented on 
MMDetection~\citep{mmdetection} codebase. The box mAP (AP$^{b}$) and mask mAP (AP$^{m}$) are adopted as metrics. Refer to Appendix~\ref{app:coco_det_settings} and \ref{app:exp_det_coco} for detailed settings and full results. Table~\ref{tab:coco} shows that detectors with MogaNet variants significantly outperform previous backbones. It is worth noticing that Mask R-CNN with MogaNet-T achieves 42.6 AP$^{b}$, outperforming Swin-T by 0.4 with 48\% and 27\% fewer parameters and FLOPs. Using advanced training settings and IN-21K pre-trained weights, Cascade Mask R-CNN with MogaNet-XL achieves 56.2 AP$^{b}$, +1.4 and +2.3 over ConvNeXt-L and RepLKNet-31L. 
% MogaNet-T gains 3.6\% AP$^{bb}$ and 4.6\% AP$^{mk}$ over ResNet-18; MogaNet-S outperforms Swin-T (Transformers) by 3.9\% AP$^{bb}$ and 2.7\% AP$^{mk}$, and surpasses UniFormer-S (hybrid) by 0.5\% AP$^{bb}$; MogaNet-B outperforms Swin-T and LITV2-M (Transformer) by 2.9\% AP$^{bb}$ and 1.2\% AP$^{mk}$ respectively.

\begin{figure*}[t!]
\vspace{-3.0em}
\centering
\begin{minipage}{0.49\linewidth}
    \centering
    \input{Tabs/tab_in1k_tiny.tex}
    \vspace{-2.25em}
    \input{Tabs/tab_in1k}
\end{minipage}
~\begin{minipage}{0.49\linewidth}
    \centering
    \input{Tabs/tab_coco.tex}
    \vspace{-2.25em}
    \input{Tabs/tab_ade20k.tex}
    \vspace{-2.25em}
    \input{Tabs/tab_coco_pose}
    \vspace{-1.25em}
    % \input{Tabs/tab_3d_vp}
\end{minipage}
\vspace{-3.0em}
\end{figure*}

\vspace{-0.75em}
\paragraph{Semantic segmentation on ADE20K.}
We also evaluate MogaNet for semantic segmentation tasks on ADE20K \citep{Zhou2018ADE20k} with Semantic FPN \citep{cvpr2019semanticFPN} and UperNet \citep{eccv2018upernet} following \citep{liu2021swin, yu2022metaformer}, implemented on MMSegmentation~\citep{mmseg2020} codebase. The performance is measured by single-scale mIoU. Initialized by IN-1K or IN-21K pre-trained weights, Semantic FPN and UperNet are fine-tuned for 80K and 160K iterations by the AdamW optimizer. See Appendix \ref{app:ade20k_seg_settings} and \ref{app:exp_seg_ade20k} for detailed settings and full results.
In Table~\ref{tab:ade20k}, Semantic FPN with MogaNet-S consistently outperforms Swin-T and Uniformer-S by 6.2 and 1.1 points; UperNet with MogaNet-S/B/L improves ConvNeXt-T/S/B by 2.5/1.4/1.8 points. Using higher resolutions and IN-21K pre-training, MogaNet-XL achieves 54.0 SS mIoU, surpassing ConvNeXt-L and RepLKNet-31L by 0.3 and 1.6.
% UperNet with MogaNet-S improves backbones of Transformers (+3.1\% over Swin-T), hybrid architectures (+1.6\% over UniFormer-S), and modern ConvNets (+1.1\% over HorNet-T$_{7\times 7}$. Refer to Appendix~\ref{app:ade20k_seg_settings} for more details.

\vspace{-0.75em}
\paragraph{2D and 3D Human Pose Estimation.}
We evaluate MogaNet on 2D and 3D human pose estimation tasks. As for 2D key points estimation on COCO, we conduct evaluations with SimpleBaseline~\citep{eccv2018simple} following \citep{iccv2021PVT, iclr2022uniformer}, which fine-tunes the model for 210 epochs by Adam optimizer \citep{iclr2014adam}. Table \ref{tab:coco_pose} shows that MogaNet variants yield at least 0.9 AP improvements for $256\times 192$ input, \textit{e.g.,} +2.5 and +1.2 over Swin-T and PVTV2-B2 by MogaNet-S. Using $384\times 288$ input, MogaNet-B outperforms Swin-L and Uniformer-B by 1.0 and 0.6 AP with fewer parameters.
As for 3D face/hand surface reconstruction tasks on Stirling/ESRC 3D \citep{feng2018evaluation} and FreiHAND \citep{iccv2019freihand} datasets, we benchmark backbones with ExPose \citep{eccv2020ExPose}, which fine-tunes the model for 100 epochs by Adam optimizer. 3DRMSE and Mean Per-Joint Position Error (PA-MPJPE) are the metrics. In Table \ref{tab:3d_vp}, MogaNet-S shows the lowest errors compared to Transformers and ConvNets.
We provide detailed implementations and results for 2D and 3D pose estimation tasks in Appendix \ref{app:exp_2d_pose} and \ref{app:exp_3d_pose}.

\input{Tabs/tab_3d_vp}
\vspace{-0.75em}
\paragraph{Video Prediction.}
We further objectively evaluate MogaNet for unsupervised video prediction tasks with SimVP~\citep{cvpr2022simvp} on MMNIST \citep{icml2015mmnist}, where the model predicts the successive 10 frames with the given 10 frames as the input. We train the model for 200 epochs from scratch with the Adam optimizer and evaluate it by MSE and Structural Similarity Index (SSIM). Table \ref{tab:3d_vp} shows that SimVP with MogaNet blocks improves the baseline by 6.58 MSE and outperforms ConvNeXt and HorNet by 1.37 and 4.07 MSE. Appendix \ref{app:mmnist_vp_settings} and \ref{app:exp_vp_mmnist} show more experiment settings and results.

\begin{figure*}[t!]
    \vspace{-3.0em}
\begin{minipage}{0.49\linewidth}
    \vspace{-0.5em}
    \begin{figure}[H]
    \centering
    \begin{minipage}{0.39\linewidth}
        \vspace{-1.5em}
        \centering
        \input{Tabs/tab_ablation_small.tex}
    \end{minipage}
    \hspace{-0.5em}
    \begin{minipage}{0.60\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth,trim= 4 0 0 0,clip]{Figs/fig_ablation_interaction.pdf}
        \vspace{-2.25em}
    \end{minipage}
        \caption{
        \textbf{Ablation of proposed modules} on IN-1K \textbf{Left}: the table ablates MogaNet modules by removing each of them based on the baseline of MogaNet-S. \textbf{Right}: the figure plots distributions of interaction strength $J^{(m)}$, which verifies that $\mathrm{Moga}(\cdot)$ and $\mathrm{CA}(\cdot)$ both contributes to learning multi-order interactions and better performance.
        }
        \label{fig:ablation_interaction}
    \vspace{-0.5em}
    \end{figure}
\end{minipage}
~\begin{minipage}{0.50\linewidth}
    \begin{figure}[H]
        % \vspace{-0.5em}
        \centering
        \includegraphics[width=1.0\linewidth,trim= 4 0 0 0,clip]{Figs/fig_analysis_gradcam.pdf}
        \vspace{-1.5em}
        \caption{
        \textbf{Grad-CAM activation maps} on IN-1K. MogaNet exhibits similar activation maps to attention architectures (Swin), which are located on the semantic targets. Unlike previous ConvNets that might activate some irrelevant regions, activation maps of MogaNet are more semantically gathered. See more results in Appendix~\ref{app:gradcam}.
        }
        \label{fig:analysis_gradcam}
    \end{figure}
\end{minipage}
\vspace{-1.25em}
\end{figure*}


\vspace{-0.25em}
\subsection{Ablation and Analysis}
\label{sec:exp_ablation}
\vspace{-0.25em}
We first ablate the spatial aggregation module and the channel aggregation module \textbf{CA}$(\cdot)$ in Table~\ref{tab:ablation} and Fig.~\ref{fig:ablation_interaction} (left). Spatial modules include \textbf{FD$(\cdot)$} and \textbf{Moga$(\cdot)$}, containing the \textbf{gating branch} and the context branch with multi-order DWConv layers \textbf{Multi-DW$(\cdot)$}. We found that all proposed modules yield improvements with favorable costs. Appendix~\ref{app:ablation} provides more ablation studies.
Furthermore, Fig.~\ref{fig:ablation_interaction} (right) empirically shows design modules can learn more middle-order interactions, and Fig.~\ref{fig:analysis_gradcam} visualizes class activation maps by Grad-CAM~\citep{cvpr2017grad} compared to existing models.
