\section{Introduction}
\label{sec:intro}
% figure: Acc vs Param vs GFLOPs
\begin{wrapfigure}{r}{0.5\linewidth}
    \vspace{-4.5em}
    \begin{center}
    \includegraphics[width=1.0\linewidth]{Figs/fig_acc_param_flops.pdf}
    \end{center}
    \vspace{-1.5em}
    \caption{
    \textbf{Performance on ImageNet-1K validation set at $224^2$ resolutions.} MogaNet %with pure Convolutions 
    outperforms Transformers (DeiT\citep{icml2021deit} and Swin~\citep{liu2021swin}), ConvNets (RegNetY~\citep{cvpr2020regnet} and ConvNeXt~\citep{cvpr2022convnext}), and hybrid models (CoAtNet~\citep{nips2021coatnet}) across all %parameter
    scales.
    }
    \label{fig:in1k_acc_param}
    \vspace{-1.5em}
\end{wrapfigure}

% 1. ConvNets
% Convolutional Neural Networks (ConvNets) have been the method of choice for computer vision~\citep{tpami2015faster, he2016deep, cvpr2019semanticFPN} since the renaissance of deep neural networks (DNNs)~\citep{Krizhevsky2012ImageNetCW}.
%By incorporating hierarchical convolutional layers in-between pooling and non-linear operations~\citep{lecun1998gradient, yamins2014performance, sifre2014rigid, simonyan2014very}, ConvNets can encode underlying patterns of observed images with the built-in translation equivariance constraints~\citep{he2016deep, bmvc2016wrn, Luo2016ERF, xie2017aggregated, hu2018squeeze, cvpr2022resnest} and have further become the fundamental infrastructure in modern computer vision systems.
% With the desired properties, ResNet~\citep{he2016deep} and its variants~\citep{bmvc2016wrn, xie2017aggregated, hu2018squeeze, cvpr2022resnest} have become the most widely-adopted ConvNet architectures in numerous computer vision applications. 
% For practical usage, efficient models~\citep{eccv2018shufflenet, 2017MobileNet, cvpr2018mobilenetv2, iccv2019mobilenetv3, icml2019efficientnet, cvpr2020regnet} are designed for a complexity-accuracy trade-off and hardware devices.
%Nevertheless, representation learned by ConvNets has a strong bias on local texture~\citep{2021shapebias}, resulting in a notable reduction of global information~\citep{Baker2018DeepCN, hermann2020origins, cvpr2022replknet}. Therefore, efforts have been made to upgrade macro-level architecture~\citep{chollet2017xception, xie2017aggregated, cvpr2020regnet, cvpr2022resnest} and context aggregation module~\citep{iccv2017Deformable, wang2018non, hu2018squeeze, eccv2018CBAM, iccv2019GCNet}.
%
% 2. ViTs
By relaxing local inductive bias, Vision Transformers (ViTs)~\citep{iclr2021vit, liu2021swin} have rapidly challenged the long dominance of Convolutional Neural Networks (ConvNets)~\citep{tpami2015faster, he2016deep, cvpr2019semanticFPN} for visual recognition.
It is %a unanimous consensus 
commonly conjectured that such superiority of ViT stems from its self-attention operation \citep{bahdanau2014neural, vaswani2017attention}, which facilitates the global-range feature interaction.
%
From a practical standpoint, however, the quadratic complexity within self-attention prohibitively restricts its computational efficiency~\citep{nips2020linformer, icml2022FLASH} and applications to high-resolution fine-grained scenarios~\citep{zhu2020deformable, jiang2021transgan, cvpr2022VideoSwin}. Additionally, the dearth of local bias induces the detriment of neighborhood correlations~\citep{pinto2022impartial}. 

% Local ViT & Modern ConvNet Architectures
To resolve this problem, endeavors have been made by reintroducing %shift-invariant priors
locality priors~\citep{wu2021cvt, nips2021coatnet, han2021transformer, iclr2022uniformer, cvpr2022MobileFormer} and pyramid-like hierarchical layouts~\citep{liu2021swin, fan2021multiscale, iccv2021PVT} to ViTs, albeit at the expense of model generalizability and expressivity. 
% Recent work has shown that the representation capacity of ViT should mainly be credited to its macro-level framework rather than the commonly-conjectured self-attention mechanism~\citep{nips2021MLPMixer, raghu2021vision, yu2022metaformer}.
Meanwhile, further explorations toward ViTs~\citep{nips2021MLPMixer, raghu2021vision, yu2022metaformer} have triggered the resurgence of modern ConvNets~\citep{cvpr2022convnext, cvpr2022replknet}. 
With advanced training setup and ViT-style framework design, ConvNets can readily deliver competitive performance \emph{w.r.t.}~well-tuned ViTs across a wide range of vision benchmarks~\citep{wightman2021rsb, pinto2022impartial}.
%and further alters the roadmap for deep network architecture design.
% Nevertheless, there remains a representation bottleneck for existing approaches~\citep{hermann2020origins, iclr2022how, deng2021discovering, wu2022bottleneck}: naive implementation of self-attention or large kernels hampers the modeling of discriminative contextual information and global interactions, leading to a cognition gap between DNNs and human visual system.
% As in feature integration theory~\citep{treisman1980feature}, human brains not only extract local features but simultaneously aggregate these features for global perception, which is more compact and efficient than DNNs~\citep{liu2021swin, cvpr2022convnext}.
Essentially, most of the modern ConvNets aim to perform feature extraction in a \textit{local-global blended fashion} by contextualizing the convolutional kernel or the perception module as \textit{global} as possible.
%Despite their superior performance, previous progress of  \textit{local-global blending} operation is usually derived from intuitive insights, lacking a hierarchical theoretic reference and guidance.
%or by injecting the computationally efficient \textit{locality} into ViTs (hierarchical ViTs). 
%Despite their superior performance, existing \textit{local-global blending} operation is usually derived from intuitive insights, lacking a hierarchical theoretic reference and guidance.
%Moreover, the essential \textit{adaptive} nature of attention in ViTs has not been well leveraged and grafted into ConvNets, which unveils the pitfalls but has great potential for modern ConvNet architecture.

Despite their superior performance, recent progress on \textit{multi-order game-theoretic interaction} within DNNs~\citep{icml2019explaining, zhang2020interpreting, cheng2021game} unravels that the representation capacity of modern ConvNets has not been exploited well. 
%
Holistically, low-order interactions tend to model relatively simple and common local visual concepts, which are of poor expressivity and \pl{are incapable of capturing high-level semantic patterns.}
%
In comparison, the high-order ones represent the complex concepts of absolute global scope yet are vulnerable to attacks and with poor generalizability.
%More details about multi-order interaction theory have been discussed in Sec.3.
\cite{deng2021discovering} \pl{first shows that modern networks are implicitly prone to encoding extremely low- or high-order interactions rather than the empirically proved more discriminative middle ones.} 
%As shown in Fig.~\ref{fig:spatial_interaction}, existing modern ConvNets are implicitly prone to encoding extremely low- or high-order interactions with poor expressivity or generalizability rather than the \pl{empirically proved} discriminative middle ones~\citep{deng2021discovering, li2022A2MIM}. 
%
\pl{Attempts have been made to tackle this issue from the perspective of loss function~\citep{deng2021discovering} and modeling contextual relations~\citep{Wu2022DiscoveringTR, li2022A2MIM}.}
%
This unveils the serious challenge but also the great potential for modern ConvNet architecture design.
%

%\textbf{Multi-order interaction in vision.}
%In this work, we cast our sights on \textit{multi-order game-theoretic interaction}~\citep{icml2019explaining, zhang2020interpreting, cheng2021game}, which portrays marginal contribution brought by interactions among two selected pixels and varying scales of involved contexts, where the \textit{order} indicates the scale of involved contextual pixels within such process.
%chould be an overlooked hierarchical analysis tool for DNNs~\citep{deng2021discovering}.
%
%limits their representation abilities and robustness to complex samples.

To this end, we present a new ConvNet architecture named \textbf{{M}}ulti-\textbf{{o}}rder \textbf{{g}}ated \textbf{{a}}ggregation Network (MogaNet) to achieve \textit{adaptive} context extraction and further pursue more discriminative and efficient visual representation learning \pl{first under the guidance of interaction} within modern ConvNets.
%
In MogaNet, we encapsulate both locality perception and gated context aggregation into a compact spatial aggregation block, where features encoded \pl{by the inherent overlooked interactions are forced to }congregated and contextualized efficiently in parallel.
%
From the channel perspective, as existing methods are prone to huge channel-wise information redundancy~\citep{raghu2021vision, icml2022FLASH}, we design a conceptually simple yet
effective channel aggregation block to adaptively \pl{force the network to encode expressive interactions that would have originally been ignored.} Intuitively, it performs channel-wise reallocation to the input, which outperforms prevalent counterparts (\textit{e.g.}, SE~\citep{hu2018squeeze}, \pl{RepMLP~\citep{ding2022repmlpnet}}) with more favorable computational overhead. 

% Experimental results
Extensive experiments demonstrate the consistent efficiency of model parameters and competitive performance of MogaNet at different model scales on various vision tasks, including image classification, object detection, semantic segmentation, instance segmentation, pose estimation, \textit{etc.}
% We empirically show that interaction complexity can serve as an essential indication, like the receptive field, for high-quality visual architecture design.
As shown in Fig.~\ref{fig:in1k_acc_param}, MogaNet achieves 83.4\% and 87.8\% top-1 accuracy with 25M and 181M parameters, which exhibits favorable computational overhead compared with existing lightweight models. 
MogaNet-T attains 80.0\% accuracy on ImageNet-1K, outperforming the state-of-the-art ParC-Net-S~\citep{eccv2022edgeformer} by 1.0\% with 2.04G lower FLOPs. 
MogaNet also shows great performance gain on various downstream tasks, \textit{e.g.,} surpassing Swin-L~\citep{liu2021swin} by 2.3\% AP$^b$ on COCO detection with fewer parameters and computational budget.
%Hence, the observed performance gain cannot just be attributed to the expansion of the model scale but the more efficient use of parameters.
It is surprising that the parameter efficiency of MogaNet exceeds our expectations. This is probably owing to the network encodes more discriminative middle-order interactions, which maximizes the usage of model parameters.
