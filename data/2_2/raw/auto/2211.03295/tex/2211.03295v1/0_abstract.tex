\begin{abstract}

Since the recent success of Vision Transformers (ViTs), explorations toward transformer-style architectures have triggered the resurgence of modern ConvNets. 
% However, plain stacks of self-attentions or convolutions in DNNs hinder feature interactions across space, inducing the representation bottleneck in DNNs.
In this work, we explore the representation ability of DNNs through the lens of interaction complexities.
We empirically show that interaction complexity is an overlooked but essential indicator for visual recognition.
%where current deep models tend to facilitate interactions of extreme complexities rather than the most informative intermediate ones.
% Accordingly, we design a family of novel efficient architectures, namely MogaNet, to pursue informative context mining in pure ConvNet-based models, with preferable complexity-performance trade-offs.
Accordingly, a new family of efficient ConvNets, named MogaNet, is presented to pursue informative context mining in pure ConvNet-based models, with preferable complexity-performance trade-offs.
% To address this challenge, a new family of efficient ConvNets, named MogaNet, is presented to pursue informative context mining in pure convolution-based models, with preferable complexity-performance trade-offs.
% In MogaNet, features with multi-order correlations are enhanced and contextualized concurrently by leveraging two specially designed aggregation blocks in both spatial and channel interaction spaces.
In MogaNet, interactions across multiple complexities are facilitated and contextualized by leveraging two specially designed aggregation blocks in both spatial and channel interaction spaces.
% Experimental highlights
% We conduct extensive studies on downstream tasks to evaluate the performance of our proposed architecture, \textit{i.e.,} ImageNet classification, COCO object detection, and ADE20K semantic segmentation.
Extensive studies are conducted on ImageNet classification, COCO object detection, and ADE20K semantic segmentation tasks.
% The results demonstrate that our MogaNet achieves consistent improvement in mainstream scenarios and all model scales compared to conventional state-of-the-art methods.
The results demonstrate that our MogaNet establishes new state-of-the-art over other popular methods in mainstream scenarios and all model scales.
% at lightweight, medium, and large model scales.
Typically, the lightweight MogaNet-T achieves 80.0\% top-1 accuracy with only 1.44G FLOPs using refined training setup on ImageNet-1K, surpassing ParC-Net-S by 1.4\% accuracy but saving 59\% (2.04G) FLOPs.

\end{abstract}
