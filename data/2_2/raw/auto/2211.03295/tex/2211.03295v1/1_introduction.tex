\section{Introduction}
\label{sec:intro}
% 1. CNNs
% Convolutional Neural Networks (ConvNets) have been the method of choice for various computer vision tasks \cite{he2016deep, tpami2015faster, cvpr2019semanticFPN} over the past decade.
Convolutional Neural Networks (ConvNets) have been the method of choice for computer vision~\cite{he2016deep, tpami2015faster, cvpr2019semanticFPN} since the rise of deep neural networks (DNNs)~\cite{Krizhevsky2012ImageNetCW}.
Inspired by primate visual systems~\cite{lecun1998gradient, yamins2014performance, sifre2014rigid}, a convolutional layer can encode neighborhood correlations of observed images with regional dense connection and translation-equivariance constraints. 
By interleaving hierarchical layers, ConvNets attain passively increasing receptive fields~\cite{Luo2016ERF} and are adept at recognizing underlying semantical patterns.
%
Despite the high performance, representations extracted by ConvNets have been proven to have a strong bias on regional texture~\cite{2021shapebias}, resulting in a significant loss of global context for visual objects \cite{Baker2018DeepCN, hermann2020origins, hermann2020origins, cvpr2022replknet}.
To address this limitation, previous works propose to craft improved macro-architectures~\cite{chollet2017xception, xie2017aggregated, cvpr2020regnet, cvpr2022resnest} and context aggregation modules~\cite{wang2018non, hu2018squeeze, eccv2018CBAM}.

% figure: Acc vs. Param vs. GFLOPs
\begin{figure}[t!]  % Top
    \vspace{-0.25em}
    \centering
    \includegraphics[width=0.925\linewidth]{Figs/fig_acc_param_flops.pdf}
\vspace{-1.50em}
    \caption{\textbf{Performance of models on ImageNet-1K validation set.} Our proposed pure convolution-based MogaNet outperforms previous Transformers (DeiT~\cite{icml2021deit} and Swin~\cite{liu2021swin}), ConvNets (RegNetY~\cite{cvpr2020regnet} and ConvNeXt~\cite{cvpr2022convnext}), and hybrid architectures (CoAtNet~\cite{nips2021coatnet}) at all parameter scales.}
    \label{fig:in1k_acc_param}
    \vspace{-1.25em}
\end{figure}

% 2. ViTs
In contrast, by relaxing local inductive bias, the newly emerged Vision Transformer (ViT)~\cite{iclr2021vit} and its variants~\cite{liu2021swin, iccv2021PVT, nips2021Twins} have rapidly surpassed ConvNets on a wide range of vision benchmarks.
There is an almost unanimous consensus that the competence of ViTs primarily stems from the self-attention mechanism~\cite{bahdanau2014neural, vaswani2017attention}, which facilitates long-range interactions regardless of the topological distance.
%
From a practical standpoint, however, quadratic complexity within pairwise attentions prohibitively restricts the computational efficiency of ViTs~\cite{nips2020linformer, icml2022FLASH, icml2022Flowformer} and its application potential to fine-grained downstream tasks~\cite{cvpr2022VideoSwin, jiang2021transgan, zhu2020deformable}. 
Furthermore, the absence of convolutional inductive bias shatters the inherent 2D structure of images, thereby inevitably inducing the detriment of image-specific neighborhood relationships~\cite{pinto2022impartial}. 
%
As such, several subsequent endeavors have been contributed to reintroduce pyramid-like hierarchical layouts~\cite{liu2021swin, fan2021multiscale, iccv2021PVT} and shift-invariant priors~\cite{wu2021cvt, nips2021coatnet, han2021transformer, iclr2022uniformer, cvpr2022MobileFormer} to ViTs. 

% 3. Architecture analysis
Different from previous research, recent studies have empirically revealed that the expression superiority of ViTs largely hinges on their macro-level architectures rather than the commonly-conjectured token-mixers~\cite{nips2021MLPMixer, raghu2021vision, yu2022metaformer}. 
More importantly, with advanced training setup and structure modernization, ConvNets can readily deliver comparable or even superior performance than well-tuned ViTs without increasing computational budgets \cite{wightman2021rsb, cvpr2022convnext, cvpr2022replknet, pinto2022impartial}.
Nevertheless, there remains a representation bottleneck for existing approaches~\cite{hermann2020origins, iclr2022how, deng2021discovering, wu2022bottleneck}: naive implementation of self-attention or large kernels hampers the modeling of discriminative contextual information and global interactions, leading to the cognition gap between DNNs and human visual system.
%
As in feature integration theory~\cite{treisman1980feature}, human brains not only extract local features but simultaneously aggregate these features for global perception, which is more compact and efficient than DNNs~\cite{liu2021swin, cvpr2022convnext}.

% in Sec.~\ref{sec:rep_bottleneck}
To address this challenge, we investigate the representation capacity of DNNs from the view of feature interaction complexities.
In Figure~\ref{fig:interaction}, most modern DNNs are inclined to encode interactions of extremely low or high complexities rather than the most informative intermediate ones \cite{deng2021discovering}.
%We introduce a generic ConvNet outline through the lens of ViT architecture, comprising stacks of embedding stems and disentangled spatial and channel feature propagation designs. Accordingly, two key operations, \textbf{regionality perception} and \textbf{context aggregation}, that are inextricably bound up with expression capacities of DNNs are illustrated in Sec.~\ref{sec:operations}.
%
To this end, we preview a macro ConvNet framework with corresponding essential operations and further develop a novel family of ConvNets called \textbf{{M}}ulti-\textbf{{o}}rder \textbf{{g}}ated \textbf{{a}}ggregation Network (MogaNet) for expediting contextual information with multiple interaction complexities.
%
In MogaNet, we introduce a multi-order feature aggregation module as per human vision.
% tackling the representation bottleneck.
Our design encapsulates both locality perception and context aggregation into a unified spatial aggregation block, where compound multi-order correlations are efficiently congregated and contextualized with gating mechanism in parallel.
%
From the channel aspect, as existing methods are prone to high channel-wise information redundancy~\cite{raghu2021vision, icml2022FLASH}, we tailor a simple yet effective channel aggregation block, which performs adaptive channel-wise reallocation to the input features and significantly outperforms prevailing counterparts (\textit{e.g.}, SE module~\cite{hu2018squeeze}) with lower computational cost. 

% Experimental results
Extensive experiments demonstrate the impressive performance and efficiency of MogaNet at different model scales on ImageNet-1K and multifarious downstream benchmarks.
% Extensive experiments demonstrate the impressive performance and great efficiency of MogaNet at various computational complexities on ImageNet-1K and multifarious downstream benchmarks.
We empirically prove that interaction complexity can serve as a significant indication, like the receptive field, for high quality visual recognition.
As a result, with 1.44G FLOPs and 5.2M parameters, MogaNet-T achieves top-1 accuracy of 79.6\% and 80.0\% using the default and refined training strategies on ImageNet-1K, surpassing the previous state-of-the-art ParC-Net-S~\cite{eccv2022edgeformer} by 1.0\% with 2.04G fewer FLOPs under the same setup.
Moreover, MogaNet-S attains 83.4\% top-1 accuracy with 4.97G FLOPs and 25.3M parameters, yielding a promising computational overhead compared with popular small-size models, as shown in Figure~\ref{fig:in1k_acc_param}.
% We hope MogaNet can encourage the community to rethink the representation potential of ConvNets and better recognize the underlying principle of deep architecture design.
