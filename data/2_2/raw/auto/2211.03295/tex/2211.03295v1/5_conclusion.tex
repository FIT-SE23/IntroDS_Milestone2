\vspace{-0.5em}
\section{Conclusion}
\label{sec:conclusion}
In this work, we present a novel pure ConvNet for visual representation learning named MogaNet.
% which equips the manner of feature extraction in Transformer architectures under the complexity-performance trade-off paradigm.
Specifically, we design a spatial aggregation block and an adaptive channel aggregation block to capture contextual representations and multi-order interactions under the complexity-performance trade-off paradigm.
% Specifically, we design a spatial aggregation block equipped with a multi-order gated aggregation module and frequency decompose module, and a channel aggregation block as primary components to capture robust and multi-order contextual representations.
% Such a solution is particularly well suited to the implicit model setting because inference in these models requires repeatedly solving for a fixed point of the same nonlinear layer for different inputs.
Extensive experiments verify the substantial superiority of MogaNet, compared to state-of-the-art ConvNets, ViTs, and hybrid architectures across mainstream vision tasks.
%, including image classification, object detection, and semantic segmentation.
Overall, we hope MogaNet can be more than a commodity but also inspire the way toward the further quest of architecture design for computer visual tasks.

% for arXiv
\section*{Acknowledgement}
% This work is supported by the Science and Technology Innovation 2030- Major Project (No. 2021ZD0150100) and the National Natural Science Foundation of China (No. U21A20427). 
This work was done during the internship of Zedong Wang and Zhiyuan Chen at Westlake University. We thank the AI Station of Westlake University for the support of GPUs. We thank Mengzhao Chen for polishing the writing of the manuscript.
