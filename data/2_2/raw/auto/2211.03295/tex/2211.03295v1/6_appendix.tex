\renewcommand\thefigure{A\arabic{figure}}
\renewcommand\thetable{A\arabic{table}}
\setcounter{table}{0}
\setcounter{figure}{0}

\newpage
\appendix

\section{Implementation Details}
\label{app:implement}
\subsection{Architecture Details}
\label{app:architecture}
The detailed architecture specifications of MogaNet are shown in Table~\ref{tab:app_architecture} and Figure~\ref{fig:app_moga_framework}, where an input image size of $224^2$ is assumed for all architectures. We rescale the groups of embedding dimensions the number of Moga Blocks for each stage corresponding to different models of varying magnitudes:
\romannumeral1) MogaNet-X-Tiny and MogaNet-Tiny with embedding dimensions of $\{32, 64, 96, 192\}$ and $\{32, 64, 128, 256\}$ has the competitive computational overload as recently proposed light-weight architectures~\cite{iclr2022mobilevit, cvpr2022MobileFormer, eccv2022edgeformer};
\romannumeral2) MogaNet-Small adopts embedding dimensions of $\{64, 128, 320, 512\}$ in comparison to small-scale architectures~\cite{liu2021swin, cvpr2022convnext};
\romannumeral3) MogaNet-Base with embedding dimensions of $\{64, 160, 320, 512\}$ in comparison to medium size architectures;
\romannumeral4) MogaNet-Large with embedding dimensions of $\{64, 160, 320, 640\}$ is designed for large-scale computer vision tasks.
The FLOPs are measured for image classification on ImageNet~\cite{cvpr2009imagenet} at resolution $224^2$, where a global average pooling (GAP) layer is applied to the output feature map of the last stage, followed by a linear classifier.

\input{Tabs/tab_architecture.tex}

\subsection{Experimental Settings for ImageNet-1K}
\label{app:in1k_settings}
We perform regular ImageNet-1K training mostly following the training settings of DeiT~\cite{icml2021deit} and RSB A2~\cite{wightman2021rsb} in Table~\ref{tab:in1k_config}, which are widely adopted for Transformer and ConvNet models. For all models, the default input image resolution is $224^2$ for training from scratch. We adopt $256^2$ resolutions for lightweight experiments according to MobileViT~\cite{iclr2022mobilevit}. Taking training settings for the model with 25M or more parameters as the default, we train all MogaNet models for 300 epochs by AdamW \cite{iclr2019AdamW} optimizer using a batch size of 1024, a basic learning rate of $1\times 10^{-3}$, a weight decay of 0.05, and a Cosine learning rate scheduler \cite{loshchilov2016sgdr} with 5 epochs of linear warmup~\cite{devlin2018bert}.
As for augmentation and regularization techniques, we adopt most of the data augmentation and regularization strategies applied in DeiT training settings, including RandAugment \cite{cubuk2020randaugment}, Mixup~\cite{zhang2017mixup}, CutMix~\cite{yun2019cutmix}, random erasing~\cite{zhong2020random}, stochastic depth~\cite{eccv2016droppath}, and label smoothing \cite{cvpr2016inceptionv3}. Similar to ConvNeXt~\cite{cvpr2022convnext}, we do not apply Repeated augmentation \cite{cvpr2020repeat} and gradient clipping, which are designed for Transformers but do not enhance the performances of ConvNets, while using Exponential Moving Average (EMA)~\cite{siam1992ema} with the decay rate of 0.9999 by default. We also remove additional augmentation strategies~\cite{cvpr2019AutoAugment, eccv2022AutoMix, Li2021SAMix, 2022decouplemix} for ConvNets, \textit{e.g.,} ColorJitter~\cite{he2016deep} and AutoAugment~\cite{cvpr2019AutoAugment}.
Since lightweight architectures (3$\sim$10M parameters) tend to get under-fitted with strong augmentations and regularization, we adjust the training configurations for MogaNet-XT/T following \cite{iclr2022mobilevit, cvpr2022MobileFormer, eccv2022edgeformer}, including employing the weight decay of 0.03 and 0.04, Mixup with $\alpha$ of 0.1, and RandAugment of $7/0.5$ for MogaNet-XT/T. Since EMA is proposed to stabilize the training process of large models, we also remove it for MogaNet-XT/T as a fair comparison. An increasing degree of stochastic depth path augmentation is employed for larger models, \textit{i.e.}, 0.05, 0.1, 0.1, 0.2, 0.3 for MogaNet-XT, MogaNet-T, MogaNet-S, MogaNet-B, MogaNet-L, respectively. In evaluation, the top-1 accuracy using a single crop with a test crop ratio of 0.9 is reported as \cite{iccv2021t2t, yu2022metaformer, guo2022van}. All experiments are implemented on \texttt{OpenMixup}~\cite{li2022openmixup} codebase.

\input{Tabs/tab_train_conf.tex}

% fig: framework
\begin{figure*}[t]
    \vspace{-0.5em}
    \centering
    \includegraphics[width=0.92\textwidth]{Figs/fig_moga_framework.pdf}
    \vspace{-0.5em}
    \caption{\textbf{The overall framework of MogaNet.} Similar to~\cite{he2016deep, liu2021swin, yu2022metaformer, cvpr2022convnext}, MogaNet uses hierarchical architectures of 4 stages. The stage $i$ consists of an embedding stem and $N_{i}$ Moga Blocks, which contain spatial aggregation blocks and channel aggregation blocks.
    }
    \label{fig:app_moga_framework}
    \vspace{-1.0em}
\end{figure*}

% \section{Experimental Settings for Dense Prediction Tasks}
\subsection{Object Detection and Segmentation on COCO}
\label{app:coco_settings}
Following Swin~\cite{liu2021swin} and PoolFormer~\cite{yu2022metaformer}, we evaluate objection detection and instance segmentation tasks on COCO~\cite{2014MicrosoftCOCO} benchmark, which include 118K training images (\textit{train2017}) and 5K validation images (\textit{val2017}). We adopt Mask-RCNN~\cite{2017iccvmaskrcnn} as the standard detectors and use ImageNet-1K pre-trained weights as the initialization of the backbones. We employ AdamW~\cite{iclr2019AdamW} optimizer for training $1\times$ schedulers (12-epochs) with a basic learning rate of $1\times 10^{-4}$ and a batch size of 16. The shorter side of training images is resized to 800 pixels, and the longer side is resized to not more than 1333 pixels. We calculate the FLOPs of compared models at $800\times 1280$ resolutions. Experiments of COCO are implemented on \texttt{MMDetection}~\cite{mmdetection} codebase and run on 8 NVIDIA A100 GPUs.

\subsection{Semantic Segmentation on ADE20K}
\label{app:ade20k_settings}
We evaluate semantic segmentation on ADE20K~\cite{Zhou2018ADE20k} benchmark, which contains 20K training images and 2K validation images, covering 150 fine-grained semantic categories. 
We first adopt Semantic FPN~\cite{cvpr2019semanticFPN} following PoolFormer~\cite{yu2022metaformer} and Uniformer~\cite{iclr2022uniformer}, which train models for 80K iterations by AdamW~\cite{iclr2019AdamW} optimizer with a basic learning rate of $2\times 10^{-4}$ and a batch size of 16. Then, we utilize UperNet~\cite{eccv2018upernet} following Swin~\cite{liu2021swin}, which employs AdamW optimizer using a basic learning rate of $6\times 10^{-5}$, a weight decay of 0.01, a linear learning rate scheduler with a linear warmup of 1,500 iterations. We use ImageNet-1K pre-trained weights as the initialization of the backbones. The training images are resized to $512^2$ resolutions, and the shorter side of testing images is resized to 512 pixels. We calculate the FLOPs of models at $800\times 2048$ resolutions. The pre-trained weights on ImageNet-1K are used as the initialization of backbones. Experiments of ADE20K are implemented on \texttt{MMSegmentation}~\cite{mmseg2020} codebase and run on 8 NVIDIA A100 GPUs.



\section{Empirical Experiment Results}
\label{app:empirical}
\subsection{Multi-order Interaction}
\label{app:interaction}
In Sec.~\ref{sec:rep_bottleneck}, we interpret the learned representation of backbones by multi-order interaction~\cite{deng2021discovering}. The interaction complexity can be represented by the multi-order interaction $I^{(m)}(i,j)$, which measures the average interaction utility between variables $i,j$ on all contexts consisting of $m$ variables.
Empirically, the $m$-th order interaction $I^{(m)}(i,j)$ is defined to be the average interaction utility between image patches $i$ and $j$ on all contexts consisting of $m$ image patches. Note that $m$ denotes the order of contextual complexity of the interaction. Formally, given an input image $x$ with a set of $n$ patches $N = \{1,\dots,n\}$ (\textit{e.g.}, an image with $n$ pixels in total), the multi-order interaction $I^{(m)}(i,j)$ can be calculated as:
\begin{equation}
\begin{aligned}
    I^{(m)}(i,j) = \mathbb{E}_{S \subseteq N \setminus \{i,j\}, |S|=m}[\Delta f(i,j,S)],
\end{aligned}
    \label{eq:interaction}
\end{equation}
where $\Delta f(i,j,S) = f(S \cup \{i,j\}) - f(S \cup \{i\}) - f(S \cup \{j\}) + f(S)$. $f(S)$ indicates the score of output with patches in $N \setminus S$ kept unchanged but replaced with the baseline value~\cite{ancona2019explaining}, where the context $S\subseteq N$. Notice that the order $m$ reflects the contextual complexity of the interaction $I^{(m)}(i,j)$. For example, a low-order interaction (\textit{e.g.,} $m=0.05n$) means the relatively simple collaboration between variables $i,j$, while a high-order interaction (\textit{e.g.,} $m=0.05n$) corresponds to the complex collaboration. Then, we can measure the overall interaction complexity of deep neural networks (DNNs) by the relative interaction strength $J^{(m)}$ of the encoded $m$-th order interaction:
\begin{equation}
\begin{aligned}
    J^{(m)} = \frac{\mathbb{E}_{x \in \Omega}\mathbb{E}_{i,j}|I^{(m)}(i,j|x)|}{\mathbb{E}_{m^{'}}\mathbb{E}_{x \in \Omega}\mathbb{E}_{i,j}|I^{(m^{'})}(i,j|x)|},
\end{aligned}
    \label{eq:strength}
\end{equation}
where $\Omega$ is the set of all samples and $0\le m \ge n-2$. Note that $J^{(m)}$ is the average interaction strength over all possible patch pairs of the input samples and indicates the distribution (area under curve sums up to one) of the order of interactions of DNNs.
In Figure~\ref{fig:spatial_interaction}, we calculate the interaction strength $J^{(m)}$ with Eq.~\ref{eq:strength} for the models trained on ImageNet-1K using the official implementation{\footnote{\url{https://github.com/Nebularaid2000/bottleneck}}} provided by~\cite{deng2021discovering}. Specially, we use the image of $224\times 224$ resolution as the input and calculate $J^{(m)}$ on $14\times 14$ grids, \textit{i.e.,} $n=14\times 14$. And we set the model output as $f(x_S) = \log \frac{P(\hat y = y|x_S)}{1-P(\hat y = y|x_S)}$ given the masked sample $x_S$, where $y$ denotes the ground-truth label and $P(\hat y = y|x_S)$ denotes the probability of classifying the masked sample $x_S$ to the true category.


\subsection{Visualization of CAM}
\label{app:gradcam}
We further visualize more examples of Grad-CAM~\cite{cvpr2017grad} activation maps of MogaNet-S in comparison to Transformers, including DeiT-S~\cite{icml2021deit}, T2T-ViT-S~\cite{iccv2021t2t}, Twins-S~\cite{nips2021Twins}, and Swin~\cite{liu2021swin}, and ConvNets, including ResNet-50~\cite{he2016deep} and ConvNeXt-T~\cite{cvpr2022convnext}, on ImageNet-1K in Figure~\ref{fig:app_gradcam}. Due to the self-attention mechanism, the pure Transformers architectures (DeiT-S and T2T-ViT-S) show more refined activation maps than ConvNets, but they also activate some irrelevant parts. Combined with the design of local windows, local attention architectures (Twins-S and Swin-T) can locate the full semantic objects. Results of previous ConvNets can roughly localize the semantic target but might contain some background regions.
The activation parts of our proposed MogaNet-S are more similar to local attention architectures than previous ConvNets, which are more gathered on the semantic objects.


\section{More Ablation and Analysis Results}
\label{app:ablation}
In addition to Sec.~\ref{sec:exp_ablation}, we further conduct more ablation and analysis of our proposed MogaNet on ImageNet-1K. We adopt the same experimental settings as Sec.~\ref{tab:ablation}.

\subsection{Ablation of Activation Functions}
\label{app:ablation_gating}
We conduct the ablation of activation functions used in the proposed multi-order gated aggregation module on ImageNet-1K. Table~\ref{tab:ablation_gating} shows that using SiLU~\cite{elfwing2018sigmoid} activation for both branches achieves the best performance. Similar results were also found in Transformers, \textit{e.g.,} GLU variants with SiLU or GELU~\cite{hendrycks2016bridging} yield better performances than using Sigmoid or Tanh activation functions~\cite{Shazeer2020GLU, icml2022FLASH}. We guess that SiLU is the most suitable activation because it has both the property of Sigmoid (gating effects) and GELU (training friendly), which is defined as $x\cdot \mathrm{Sigmoid}(x)$.

\input{Tabs/tab_ablation_gate.tex}


\subsection{Ablation of Multi-order DWConv Layers}
\label{app:ablation_multiorder}
In addition to Sec.~\ref{sec:moga} and Sec.~\ref{sec:exp_ablation}, we also analyze the multi-order depth-wise convolution (DWConv) layers as the static regionality perception in the multi-order aggregation module $\mathrm{Moga}(\cdot)$ on ImageNet-1K. As shown in Table~\ref{tab:ablation_conv}, we analyze the channel configuration of three parallel dilated DWConv layers: $\mathrm{DW}_{5\times 5, d=1}$, $\mathrm{DW}_{5\times 5, d=2}$, and $\mathrm{DW}_{7\times 7, d=3}$ with the channels of $C_l$, $C_m$, $C_h$.
we first compare the performance of serial DWConv layers (\textit{e.g.,} $\mathrm{DW}_{5\times 5, d=1}$+$\mathrm{DW}_{7\times 7, d=3}$) and parallel DWConv layers. We find that the parallel design can achieve the same performance with fewer computational overloads because the DWConv kernel is equally applied to all channels. When we adopt three DWConv layers, the proposed parallel design reduces $C_l+C_h$ and $C_l+C_m$ times computations of $\mathrm{DW}_{5\times 5, d=2}$ and $\mathrm{DW}_{5\times 5, d=2}$ in comparison to the serial stack of these DWConv layers. Then, we empirically explore the optimal configuration of the three channels. We find that $C_l:$ $C_m:$ $C_h$ = 1: 3: 4 yields the best performance, which well balances the small, medium, and large DWConv kernels to learn low, middle, and high-order contextual representations. Similar conclusions are also found in relevant designs~\cite{nips2022hilo, nips2022iformer, nips2022hornet}, where global context aggregations take the majority (\textit{e.g.}, $\frac{1}{2} \sim \frac{3}{4}$ channels or context components). We also verify the parallel design with the optimal configuration based on MogaNet-S/B. Therefore, we can conclude that our proposed multi-order DWConv layers can efficiently learn multi-order contextual information for the context branch of $\mathrm{Moga}(\cdot)$.

\input{Tabs/tab_ablation_conv.tex}


\subsection{Ablation of Normalization Layers}
\label{app:ablation_norm}
For most of ConvNets, BatchNorm~\cite{nips2015batchnorm} (BN) is considered an essential component to improve the convergence speed and prevent overfitting. However, BN might cause some instability~\cite{Wu2021PreciseBN} or harm the final performance of models~\cite{iclr2021characterizing, Brock2021NFNet}. Some recently proposed ConvNets~\cite{cvpr2022convnext, guo2022van} replace BN by LayerNorm~\cite{2016layernorm} (LN), which has been widely used in Transformers~\cite{iclr2021vit} and Metaformer architectures~\cite{yu2022metaformer}, achieving relatively good performances in various scenarios. Here, we conduct an ablation of normalization (Norm) layers in MogaNet on ImageNet-1K, as shown in Table~\ref{tab:ablation_norm}. As discussed in ConvNeXt~\cite{cvpr2022convnext}, the Norm layers used in each block (\textbf{within}) and after each stage (\textbf{after}) have different effects. Thus we study them separately. Table~\ref{tab:ablation_norm} shows that using BN in both places yields better performance than using LN (after) and BN (within), except MogaNet-T with $224^2$ resolutions, while using LN in both places performs the worst.
Consequently, we use BN as the default Norm layers in our proposed MogaNet for two reasons: (\romannumeral1) With pure convolution operators, the rule of combining convolution operations with BN within each stage is still useful for modern ConvNets. (\romannumeral2) Although using LN after each stage might help stabilize the training process of Transformers and hybrid models and might sometimes bring good performance for ConvNets, adopting BN after each stage in pure convolution models still yields better performance.
Moreover, we replace BN with precise BN~\cite{Wu2021PreciseBN} (pBN), which is an optimal alternative normalization strategy to BN. We find slight performance improvements (around 0.1\%), especially when MogaNet-S/B adopts the EMA strategy (by default), indicating that we can further improve MogaNet with advanced BN. As discussed in ConvNeXt, EMA might severely hurt the performances of models with BN. This phenomenon might be caused by the unstable and inaccurate BN statistics estimated by EMA in the vanilla BN with large models, which will deteriorate when using another EMA of model parameters. We solve this dilemma by exponentially increasing the EMA decay from 0.9 to 0.9999 during training as momentum-based contrastive learning methods~\cite{iccv2021dino, bao2021beit}, \textit{e.g.,} BYOL \cite{nips2020byol}. It can also be tackled by advanced BN variants~\cite{NIPS2017GhostBN, Wu2021PreciseBN}.

\input{Tabs/tab_ablation_norm.tex}


\subsection{Refined Training Settings for Lightweight Models}
\label{app:advanced_tiny}
To explore the full power of lightweight models of our MogaNet, we refined the basic training settings for MogaNet-XT/T according to RSB A2~\cite{wightman2021rsb} and DeiT-III~\cite{eccv2022deit3}. Compared to the default setting as provided in Table~\ref{tab:in1k_config}, we only adjust the learning rate and the augmentation strategies for faster convergence while keeping other settings unchanged. As shown in Table~\ref{tab:advanced_tiny}, MogaNet-XT/T gain +0.4$\sim$0.6\% when use the large learning rate of $2\times 10^{-3}$ and 3-Augment~\cite{eccv2022deit3} without complex designs. Based on the advanced setting, MogaNet with $224^2$ input resolutions yields significant performance improvements against previous methods, \textit{e.g.,} MogaNet-T gains +3.5\% over DeiT-T~\cite{icml2021deit} and +1.2\% over Parc-Net-S~\cite{eccv2022edgeformer}.
Especially, MogaNet-T with $256^2$ resolutions achieves top-1 accuracy of 80.0\%, outperforming DeiT-S of 79.8\% reported in the original paper, while MogaNet-XT with $224^2$ resolutions outperforms DeiT-T under the refined training scheme by 1.2\% with only 3M parameters.

\input{Tabs/tab_advanced_tiny.tex}


\section{More Comparison Experiments}
\label{app:comparison}
In addition to Sec.~\ref{sec:exp_in1k}, we further provide comparison results for 100 and 300 epochs training on ImageNet-1K. As for 100-epoch training, we adopt the original RSB A3~\cite{wightman2021rsb} setting for all methods, which adopts LAMB \cite{iclr2020lamb} optimizer and a small training resolution of $160^2$. As for 300-epoch training, we report results of RSB A2 \cite{wightman2021rsb} for classical CNN or the original setting for Transformers or modern ConvNets. In Table~\ref{tab:in1k_app_rsb}, when compared with models of similar parameter size, our proposed MogaNet-XT/T/S/B achieves the best performance in both 100 and 300 epochs training. Results of 100-epoch training show that MogaNet has a faster convergence speed than previous architectures. For example, MogaNet-T outperforms EfficientNet-B0 and DeiT-T by 1.3\% and 7.6\%, MogaNet-S outperforms Swin-T and ConvNeXt-T by 4.1\% and 1.6\%, MogaNet-B outperforms Swin-S and ConvNeXt-S by 2.5\% and 1.0\%.

% figure: gradcam
\begin{figure*}[t]
    \vspace{-0.75em}
    \centering
    \includegraphics[width=0.98\linewidth]{Figs/fig_app_gradcam.pdf}
    \vspace{-0.5em}
    \caption{
    Visualization of Grad-CAM activation maps of the models trained on ImageNet-1K.}
    \label{fig:app_gradcam}
    % \vspace{-0.5em}
\end{figure*}

\input{Tabs/tab_in1k_app_rsb.tex}


% \section{Discussions}
% \label{app:discussion}
% We discuss the relationship between our proposed MogaNet and prior models and summarize the limitations of MogaNet. We hope MogaNet can be a strong baseline applied to various vision tasks.

% \subsection{Comparison with Prior Art}
% \label{app:relationship}
% \paragraph{Motivations and main challenges.}
% The motivations for our work come from two aspects. As verified in previous work of hybrid architectures~\cite{nips2021coatnet, cvmj2022PVTv2, iclr2022uniformer, pinto2022impartial, iclr2022how}, which combine convolutions and self-attention mechanisms, only using advanced regionality preceptions or context aggregation modules is not enough to learn the optimal visual representation. According to empirical and theoretical analysis~\cite{hermann2020origins, naseer2021intriguing, iclr2022how, cvpr2022replknet, deng2021discovering}, we find that the gap between DNNs and human visions is DNNs have some unnatural bias (\textit{e.g.,} convolution operations are likely to be high-pass filters with the texture bias) and prefer to low-order or high-order interactions. Meanwhile, the efficiency of the model is also  emphasized with the boom of Transformer architectures~\cite{nips2020linformer, aaai2022LIT, iclr2022mobilevit, Lin2022SuperViT, icml2022FLASH, icml2022Flowformer}, which usually require quadratic complexity and might be time-consuming in fine-grained vision tasks.
% Therefore, we summarize the main challenge is how to leverage the advantages of regionality perceptions and context aggregations to learn more multi-order interactions efficiently.

% \paragraph{Relations to Efficient Transformers.}
% pass

% \paragraph{Relations to Modern ConvNets.}
% \cite{han2021demystifying} shows that local Transformer attention is equivalent to inhomogeneous dynamic depthwise convolution.

% \subsection{Limitations and Boarder Impacts}
% \label{app:limitation}
% pass
