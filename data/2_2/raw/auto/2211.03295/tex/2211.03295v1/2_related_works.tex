\section{Related Work}
\label{sec:related_work}
\paragraph{Convolutional Neural Networks}
% History of Pre-ViT ConvNets and modern ConvNets
ConvNets~\cite{lecun1998gradient, Krizhevsky2012ImageNetCW, he2016deep} have dominated a wide scope of computer vision (CV) tasks for decades. 
% VGG~\cite{simonyan2014very} proposes a modular network design strategy, stacking the same type of blocks repeatedly, which simplifies both the design workflow and transfer learning for downstream tasks. 
ResNet~\cite{he2016deep} introduces an identity skip connection and a bottleneck module that alleviates training difficulties (\textit{e.g.,} vanishing gradient). ResNet and its variants~\cite{bmvc2016wrn, xie2017aggregated, hu2018squeeze, cvpr2022resnest} have become the most widely-adopted ConvNet architectures in numerous CV applications. For practical usage, many efficient models~\cite{eccv2018shufflenet, 2017MobileNet, cvpr2018mobilenetv2, iccv2019mobilenetv3, icml2019efficientnet, cvpr2020regnet} are designed for a complexity-accuracy trade-off and hardware devices.
Since the limited reception field of spatial and temporal convolutions struggles to capture global dependency~\cite{Luo2016ERF}, various spatial-wise or channel-wise attention strategies~\cite{iccv2017Deformable, hu2018squeeze, wang2018non, eccv2018CBAM, iccv2019GCNet} are designed.
Recently, taking merits of the macro design of Transformers~\cite{iclr2021vit}, modern ConvNets~\cite{2022convmixer, cvpr2022convnext, cvpr2022replknet, Liu2022SLak, nips2022hornet, nips2022focalnet} show thrilling performance with large depth-wise convolutions~\cite{han2021demystifying} for local context and long-range aggregation.
% Among them, HorNet~\cite{nips2022hornet} and FocalNet~\cite{nips2022focalnet} exploit . However, these methods memorize and contextualize features in the regions with pairwise operations repeatedly in parallel, which leads to high computational complexity.

\vspace{-1.0em}
\paragraph{Vision Transformers}
% VIT相关介绍可以简化
Transformer~\cite{vaswani2017attention} with the self-attention mechanism has become the mainstream choice in the natural language processing (NLP) community~\cite{devlin2018bert, brown2020language}.
Considering that global information is also essential for CV tasks, Vision Transformer (ViT)~\cite{iclr2021vit} is proposed and has achieved promising results on ImageNet~\cite{cvpr2009imagenet}. In particular, ViT splits raw images into non-overlapping fixed-size patches as visual tokens to capture long-range feature interactions among these tokens by self-attention. By introducing regional inductive bias, ViT and its variants have been extended to various vision tasks \cite{carion2020end, zhu2020deformable, chen2021pre, parmar2018image, jiang2021transgan, arnab2021vivit}. Equipped with advanced training strategies~\cite{icml2021deit, touvron2021training, yuan2021tokens, eccv2022deit3} or extra knowledge~\cite{nips2021TL, Lin2022SuperViT, eccv2022tinyvit}, pure ViTs can achieve competitive performance as ConvNets in CV tasks. 
In the literature of \cite{yu2022metaformer}, the MetaFormer architecture substantially influenced the design of vision backbones, and all Transformer-like models~\cite{icml2021deit, 2022convmixer, aaai2022shiftvit} are classified by how they treat the token-mixing approaches, such as relative position encoding~\cite{wu2021rethinking}, local window shifting~\cite{liu2021swin} and MLP layer~\cite{nips2021MLPMixer}, \textit{etc.} 
% Beyond the aspect of macro design, \cite{touvron2021training, yuan2021tokens} introduced knowledge distillation and progressive tokenization to boost training data efficiency. 
% Hybird ViTs: Swin, Uniformer, Next-ViT
Compared to ConvNets banking on the inherent inductive biases (\textit{e.g.,} locality and translation equivariance), the pure ViTs are more over-parameterized and rely on large-scale pre-training~\cite{iclr2021vit, bao2021beit, cvpr2022mae, li2022A2MIM} to a great extent. Targeting this problem, one branch of researchers proposes lightweight ViTs~\cite{nips2021vitc, iclr2022mobilevit, nips2022EfficientFormer, chen2022CFViT} with more efficient self-attentions variants~\cite{nips2020linformer}.
Meanwhile, the incorporation of self-attention and convolution as a hybrid backbone has been vigorously studied~\cite{guo2021cmt, wu2021cvt, nips2021coatnet, d2021convit, iclr2022uniformer, aaai2022LIT, nips2022iformer} for imparting regional priors to ViTs.
