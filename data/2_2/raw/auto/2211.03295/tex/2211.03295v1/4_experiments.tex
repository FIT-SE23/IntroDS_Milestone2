\section{Experiments}
\label{sec:expriments}
To verify the effectiveness of our method, we conduct extensive experiments on ImageNet-1K (IN-1K)~\cite{cvpr2009imagenet} for image classification, COCO~\cite{2014MicrosoftCOCO} for object detection and instance segmentation, and ADE20K~\cite{Zhou2018ADE20k} for semantic segmentation. All experiments are implemented with PyTorch on Ubuntu workstations with NVIDIA A100 GPUs. \textbf{Bold} and \hl{gray} indicate the best performance and our models.

% % table: IN-1K Tiny (5M) & Small (25M)
% \begin{figure*}[t!]
% \vspace{-1.0em}
% \begin{minipage}{0.5\linewidth}
% \centering
%     \input{Tabs/tab_in1k_tiny.tex}
% \end{minipage}
% \begin{minipage}{0.5\linewidth}
% \centering
%     \input{Tabs/tab_in1k_small.tex}
% \end{minipage}
% \vspace{-1.5em}
% \end{figure*}

\subsection{ImageNet Classification}
\label{sec:exp_in1k}
\paragraph{Settings.} 
For classification experiments on ImageNet-1K, we train MogaNet variants following the standard procedure \cite{icml2021deit, liu2021swin} for a fair comparison. Specifically, the models are trained for 300 epochs by AdamW~\cite{iclr2019AdamW} optimizer with $224^2$ or $256^2$ resolutions, a basic learning rate $lr$ = $1\times 10^{-3}$, 5 epochs warmup, and a Cosine scheduler~\cite{loshchilov2016sgdr}. See Appendix~\ref{app:in1k_settings} for implementation details.
We compare four typical architectures: (\romannumeral1) \textbf{Classical ConvNets} include ResNet, SENet, ShuffleNetV2, EfficientNet, MobileNetV3, and RegNet. (\romannumeral2) \textbf{Transformers} include DeiT, Swin, T2T-ViT, PVT, Focal, ViT-C, CSWin, SReT, and LiTV2. (\romannumeral3) \textbf{Hybrid architectures} of attention and convolution include PiT, LeViT, CoaT, BoTNet, ViTAE, Twins, CoAtNet, MobileViT, Uniformer, Mobile-Former, ParC-Net, EfficientFormer, and MaxViT. (\romannumeral4) \textbf{Modern ConvNets} include ConvNeXt, RepLKNet, FocalNet, VAN, SLak, and HorNet.
% We compare four types of popular network architectures: (\romannumeral1) \textbf{Classical CNN} includes ResNet~\cite{he2016deep}, SENet~\cite{hu2018squeeze}, ShuffleNetV2~\cite{eccv2018shufflenet}, EfficientNet~\cite{icml2019efficientnet}, MobileNetV3~\cite{iccv2019mobilenetv3}, and RegNet~\cite{cvpr2020regnet}. (\romannumeral2) \textbf{Transformer} includes DeiT~\cite{icml2021deit}, Swin~\cite{liu2021swin}, T2T-ViT~\cite{iccv2021t2t}, PVT~\cite{iccv2021PVT}, FocalNet~\cite{nips2021Focal}, ViT-C~\cite{nips2021vitc}, CSWin~\cite{cvpr2022CSWin}, SReT~\cite{eccv2022SReT}, and LiTV2~\cite{nips2022hilo}. (\romannumeral3) \textbf{Hybrid} Transformer and CNN architecture includes PiT~\cite{iccv2021pit}, LeViT~\cite{iccv2021levit}, CoaT~\cite{iccv2021coat}, BoTNet~\cite{cvpr2021botnet}, ViTAE~\cite{nips2021vitae}, Twins~\cite{nips2021Twins}, CoAtNet~\cite{nips2021coatnet}, MobileViT~\cite{iclr2022mobilevit}, Uniformer~\cite{iclr2022uniformer}, Mobile-Former~\cite{cvpr2022MobileFormer}, and ParC-Net~\cite{eccv2022edgeformer}. (\romannumeral3) \textbf{Post-ConvNet} includes ConvNeXt~\cite{cvpr2022convnext}, RepLKNet~\cite{cvpr2022replknet}, VAN~\cite{guo2022van}, SLak~\cite{Liu2022SLak}, and HorNet~\cite{nips2022hornet}.

% table: IN-1K Tiny (5M) & Small (25M)
\input{Tabs/tab_in1k_tiny.tex}
\input{Tabs/tab_in1k_small.tex}

% table: IN-1K Base (40M) & Large (80M)
\begin{figure*}[t!]
\vspace{-1.5em}
\begin{minipage}{0.495\linewidth}
\centering
    \input{Tabs/tab_in1k_base.tex}
\end{minipage}
~\begin{minipage}{0.495\linewidth}
\centering
    \input{Tabs/tab_in1k_large.tex}
\end{minipage}
\vspace{-1.25em}
\end{figure*}

\paragraph{Results.}
We compare the image classification performances of four widely adopted model sizes (around 5M, 25M, 45M, and 80M parameters).
As for lightweight models, Table~\ref{tab:in1k_cls_tiny} shows that MogaNet-XT/T significantly outperforms existing lightweight architectures. Using the default training settings, MogaNet-T achieves 79.0\% top-1 accuracy, which improves models with around 5M parameters by at least 1.1\% using $224^2$ resolutions, while outperforming the current best backbone ParC-Net-S by 1.0\% using $256^2$ resolutions. Meanwhile, MogaNet-XT also surpasses models with 3M parameters, \textit{e.g.,} +4.6\% and +1.5\% over T2T-ViT-7 and MobileViT-XS. Particularly, MogaNet-T$^{\S}$ achieves 80.0\% top-1 accuracy using $256^2$ resolutions and the refined settings, which adjusts $lr$ and replaces RandAugment~\cite{cubuk2020randaugment} with 3-Augment~\cite{eccv2022deit3} as detailed in Appendix~\ref{app:advanced_tiny}.
As for small-size models, Table~\ref{tab:in1k_cls_small} shows MogaNet-S achieves 83.4\% top-1 accuracy, which consistently outperforms Transformers, hybrid architectures, and ConvNets, \textit{e.g.,} +2.1\% and +1.2\% over Swin-T and ConvNeXt-T. 
As for 45M and 80M models, we summarize their performances in Table~\ref{tab:in1k_cls_base} and Table~\ref{tab:in1k_cls_large} and MogaNet-B/L still surpass the current state-of-the-art architectures, especially improving Swin-S/B and ConvNeXt-S/B by 1.2\%/ 1.1\% and 1.1\%/ 0.8\%. MogaNet also outperforms recently proposed modern ConvNets, \textit{e.g.,} +0.9\% over RepLKNet-31B and +0.2\%/ 0.3\% over HorNet-S/B$_{7\times 7}$.


\subsection{Dense Prediction Tasks}
\label{sec:exp_det_seg}
\paragraph{Object detection and segmentation on COCO.}
We evaluate MogaNet for object detection and segmentation tasks on the COCO dataset using Mask-RCNN~\cite{2017iccvmaskrcnn} as the detector. Following the training and evaluation settings in \cite{liu2021swin}, we fine-tune the models with AdamW optimizer for $1\times$ training schedule (12-epoch) on the COCO~\textit{train2017} and evaluate on the COCO~\textit{val2017}. We adopt MMDetection~\cite{mmdetection} as the codebase and measure the performance by the box mAP (AP$^{bb}$) and mask mAP (AP$^{mk}$). Refer to Appendix~\ref{app:coco_settings} for more details. Table~\ref{tab:coco} shows that models with MogaNet-T/S/B significantly outperform all previous backbones. Specifically, MogaNet-T gains 3.6\% AP$^{bb}$ and 4.6\% AP$^{mk}$ over ResNet-18; MogaNet-S outperforms Swin-T (Transformers) by 3.9\% AP$^{bb}$ and 2.7\% AP$^{mk}$, and surpasses UniFormer-S (hybrid) by 0.5\% AP$^{bb}$; MogaNet-B outperforms Swin-T and LITV2-M (Transformer) by 2.9\% AP$^{bb}$ and 1.2\% AP$^{mk}$ respectively.

\vspace{-1.0em}
\paragraph{Semantic segmentation on ADE20K.}
We then evaluate MogaNet for semantic segmentation tasks on the ADE20K dataset using Semantic FPN~\cite{cvpr2019semanticFPN} and UperNet~\cite{eccv2018upernet} following the evaluation schemes in \cite{liu2021swin, yu2022metaformer}. All experiments are implemented on MMSegmentation~\cite{mmseg2020} codebase, and the performance is measured by mIoU (single scale). Based on Semantic FPN, the models are fine-tuned for 80K iterations by the AdamW optimizer. In Table~\ref{tab:ade20k}, MogaNet-S consistently outperforms previous architectures, \textit{e.g.,} +6.6\% over Swin-T (Transformer), +1.5\% over Uniformer-S (hybrid). Based on UperNet, the models are fine-tuned 160K by AdamW optimizer. In Table~\ref{tab:ade20k}, the models with MogaNet-S improves backbones of Transformers (+3.1\% over Swin-T), hybrid architectures (+1.6\% over UniFormer-S), and modern ConvNets (+1.1\% over HorNet-T$_{7\times 7}$. Refer to Appendix~\ref{app:ade20k_settings} for more details.

% figure (interaction) & table (ablation)
\begin{figure}[hb]
\vspace{-1.25em}
\centering
\begin{minipage}{0.38\linewidth}
    \vspace{-1.25em}
    \centering
    \input{Tabs/tab_ablation_small.tex}
    \vspace{3pt}
    % \vspace{-0.25em}
\end{minipage}
~\begin{minipage}{0.59\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth,trim= 4 0 0 0,clip]{Figs/fig_ablation_interaction.pdf}
    \vspace{-2.25em}
\end{minipage}
    \caption{
    \textbf{Ablation of the proposed modules on ImageNet-1K.} \textbf{Left}: the table verifies each proposed module based on the baseline of MogaNet-S. \textbf{Right}: the figure plots distributions of the interaction strength $J^{(m)}$ and verifies that $\mathrm{Miga}(\cdot)$ contributes the most to learning multi-order interactions and better performance.
    }
    \label{fig:ablation_interaction}
\vspace{-1.5em}
\end{figure}

% figure: gradcam
\begin{figure}[hb]
    \vspace{-0.75em}
    \centering
    \includegraphics[width=1.0\linewidth,trim= 4 0 0 0,clip]{Figs/fig_analysis_gradcam.pdf}
    \vspace{-1.75em}
    \caption{
    \textbf{Grad-CAM activation maps of models trained on ImageNet-1K.} MogaNet-S shows similar activation maps as local attention architectures (Swin-T), which are located on the semantic targets. Unlike the results of previous ConvNets, which might activate some irrelevant parts, the activation maps of MogaNet-S are more gathered. See more visualizations in Appendix~\ref{app:gradcam}.
    }
    \label{fig:analysis_gradcam}
    \vspace{-1.25em}
\end{figure}

% table: COCO & ADE20K
\begin{figure*}[t!]
\vspace{-1.5em}
\begin{minipage}{0.555\linewidth}
\centering
    \input{Tabs/tab_coco.tex}
\end{minipage}
\begin{minipage}{0.45\linewidth}
\centering
    \input{Tabs/tab_ade20k.tex}
\end{minipage}
\vspace{-1.5em}
\end{figure*}

\subsection{Ablation and Analysis}
\label{sec:exp_ablation}
We first ablate the spatial aggregation module, including \textbf{$\mathrm{FD}(\cdot)$} and $\mathrm{Moga}(\cdot)$, which contains the \textbf{gating branch} and the context branch with \textbf{multi-order DWConv layers}, and the \textbf{channel aggregation} module $\mathrm{CA}(\cdot)$.
% As verified in Table~\ref{tab:ablation}, the proposed modules yield +2.4\% performance gain to the baselines.
As verified in Table~\ref{tab:ablation} and Figure~\ref{fig:ablation_interaction} (left), all proposed modules yield improvements with a few costs. Appendix~\ref{app:ablation} provides more ablation studies.
Furthermore, we empirically verify the multi-order interactions in Figure~\ref{app:ablation_multiorder} (right) and visualize class activation maps (CAM) by Grad-CAM~\cite{cvpr2017grad} in comparison to existing models in Figure~\ref{fig:analysis_gradcam}.
