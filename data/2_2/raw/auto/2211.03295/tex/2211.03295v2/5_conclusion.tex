\vspace{-0.5em}
\section{Conclusion}
\label{sec:conclusion}
In this paper, we present MogaNet, a computationally efficient pure ConvNet architecture from the novel view of multi-order game-theoretic interaction. 
%  We demonstrate that using a spatial aggregation block and a channel aggregation block results in stronger feature interactions of intermediate complexities efficiently, boosting the performance of ConvNet architecture substantially on diverse vision scenarios. 
By paying special attention to multi-order game-theoretic interaction, we design a unified Moga Block, which effectively captures robust multi-order context across spatial and channel spaces. 
%
Extensive experiments verify the consistent superiority of MogaNet in terms of accuracy and computational efficiency compared to representative ConvNets, ViTs, and hybrid architectures on various vision benchmarks.
%
% Overall, we hope this study can prompt people to perceive the importance of multi-order game-theoretic interaction in efficient and effective deep computer vision architecture design.


% for arXiv
\section*{Acknowledgement}
This work was supported by National Key R\&D Program of China (No. 2022ZD0115100), National Natural Science Foundation of China Project (No. U21A20427), and Project (No. WU2022A009) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University.
This work was done during the Zedong Wang and Zhiyuan Chen internship at Westlake University. We thank the AI Station of Westlake University for the support of GPUs. We thank Mengzhao Chen, Zhangyang Gao, Jianzhu Guo, Fang Wu, and the anonymous reviewers for polishing the writing of the manuscript.

%%% 2023.03.15 version of thanking
% This work was supported by National Key R&D Program of China (No. 2022ZD0115100), National Natural Science Foundation of China Project (No. U21A20427), and Project (No. WU2022A009) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University.
%%% 2022.06 version of thanking
% This work is supported by the Science and Technology Innovation 2030- Major Project (No. 2021ZD0150100) and National Natural Science Foundation of China (No. U21A20427), and Westlake University Funded Scientific Research Project (No. WU2022C043).
