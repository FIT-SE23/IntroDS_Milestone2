\section{Related Work}
\label{sec:preliminaries}
\subsection{Vision Transformers}
Since the significant success of Transformer~\cite{vaswani2017attention} in natural language processing (NLP)~\cite{devlin2018bert, brown2020language},
Vision Transformer (ViT)~\cite{iclr2021vit} is proposed and has attained promising results on ImageNet~\cite{cvpr2009imagenet}. %It splits raw images into non-overlapping fixed-size patches as visual tokens to capture long-range interactions among these tokens by self-attention. 
% Hybird ViTs: Swin, Uniformer, Next-ViT
However, compared with ConvNets, pure ViTs are more over-parameterized and rely on large-scale pre-training~\cite{iclr2021vit, bao2021beit, cvpr2022mae, li2022A2MIM}. Targeting this problem, one branch of researchers proposes lightweight ViTs~\cite{nips2021vitc, iclr2022mobilevit, nips2022EfficientFormer, chen2022CFViT} with efficient attention variants~\cite{nips2020linformer}.
Meanwhile, the incorporation of self-attention and convolution as a hybrid backbone has been vigorously studied~\cite{guo2021cmt, wu2021cvt, nips2021coatnet, d2021convit, iclr2022uniformer, aaai2022LIT, nips2022iformer} for imparting regional priors to ViTs.
%
By introducing inductive bias~\cite{carion2020end, zhu2020deformable, chen2021pre, parmar2018image, jiang2021transgan, arnab2021vivit}, advanced training strategies~\cite{icml2021deit, touvron2021training, yuan2021tokens, eccv2022deit3} or extra knowledge~\cite{nips2021TL, Lin2022SuperViT, eccv2022tinyvit}, ViT and its variants can achieve competitive performance as ConvNets and have been extended to various computer vision areas.

MetaFormer~\cite{yu2022metaformer} as shown in Fig. \ref{fig:framework} substantially influenced the principle of deep architecture design, where all ViTs~\cite{icml2021deit, 2022convmixer, aaai2022shiftvit} can be classified by how they treat the token-mixing approaches, such as relative position encoding~\cite{wu2021rethinking}, local window shifting~\cite{liu2021swin} and MLP layer~\cite{nips2021MLPMixer}, \textit{etc.}
%
It primarily comprises three cardinal components: (\romannumeral1) embedding stem, (\romannumeral2) spatial mixing block, and (\romannumeral3) channel mixing block.
The embedding stem downsamples the input image to reduce image-inherent redundancies and computational overload. We assume the input feature $X$ and the output $Z$ are in the same shape $\mathbb{R}^{C\times H\times W}$, we have:

% fig: framework
\begin{figure}[t]
    \vspace{-0.5em}
    \centering
    \includegraphics[width=0.92\linewidth]{Figs/fig_framework.pdf}
    \vspace{-0.75em}
    \caption{\textbf{Illustration of the macro architecture.} It has 4 stages in hierarchical, and $i$-th stage contains an embedding stem and $N_{i}$ blocks of $\mathrm{SMixer}(\cdot)$ and $\mathrm{CMixer}(\cdot)$ with PreNorm~\cite{acl2019Learning} and identical connection~\cite{he2016deep}. The features within the $i$-th stage are in the same shape, except that $\mathrm{CMixer}(\cdot)$ will increase the dimension to $rC_{i}$ with an expand ratio $r$ as an inverted bottleneck \cite{cvpr2018mobilenetv2}.
    }
    \label{fig:framework}
    \vspace{-1.25em}
\end{figure}

\vspace{-0.30em}
\begin{equation}
    Z = \mathrm{Stem}(X),
    \vspace{-0.40em}
\end{equation}
where $Z$ is downsampled features, \textit{e.g.,}. 
%
Then, the feature flows to a stack of residual blocks. In each stage, the network modules can be decoupled into two separate functional components, $\mathrm{SMixer}(\cdot)$ and $\mathrm{CMixer}(\cdot)$ for spatial-wise and channel-wise information propagation~\cite{yu2022metaformer},
\vspace{-0.50em}
\begin{equation}
    \label{eq:smixer}
    Y = X + \mathrm{SMixer}\big(\mathrm{Norm}(X)\big),
    \vspace{-0.50em}
\end{equation}
\begin{equation}
    \label{eq:cmixer}
    Z = Y + \mathrm{CMixer}\big(\mathrm{Norm}(Y)\big),
    \vspace{-0.20em}
\end{equation}
where $\mathrm{Norm}(\cdot)$ is a normalization layer, \textit{e.g.,} Batch Normalization~\cite{ioffe2015batch} (BN). Notice that $\mathrm{SMixer}(\cdot)$ could be various spatial operations (\textit{e.g.,} self-attention~\cite{vaswani2017attention}, convolution), while $\mathrm{CMixer}(\cdot)$ is usually achieved by channel-wise MLP in inverted bottleneck~\cite{cvpr2018mobilenetv2} and an expand ratio of $r$.


\subsection{Post-ViT Modern ConvNets}
%Classical ConvNets~\cite{simonyan2014very, xie2017aggregated} fails to capture long-range interactions constrained by their receptive fields.
By taking the merits of ViT-style macro-level architecture~\cite{yu2022metaformer}, modern ConvNets~\cite{2022convmixer, cvpr2022convnext, cvpr2022replknet, Liu2022SLak, nips2022hornet, nips2022focalnet} show thrilling performance with large depth-wise convolutions~\cite{han2021demystifying} for global context aggregation.
%Images require operations with local and geometrical inductive biases such as convolutions~\cite{cvpr2016inceptionv3, iclr2016dilated, chollet2017xception}, spatial MLP~\cite{nips2021MLPMixer}, or even non-parametric operations like pooling~\cite{yu2022metaformer} and spatial shifting~\cite{aaai2022shiftvit}.  
%
Similar to ViTs, \textit{context aggregation} operations in modern ConvNets can be summerized as a group of components that \textit{adaptively} emphasize contextual information and decrease trivial redundancies in spatial mixing between two embedded features:
\vspace{-0.40em}
\begin{equation}\label{eq:aggregate}
    O = \mathcal{S}\big(\mathcal{F}_{\phi}(X), \mathcal{G}_{\psi}(X)\big),
    \vspace{-0.40em}
\end{equation}
where $\mathcal{F}_{\phi}(\cdot)$ and $\mathcal{G}_{\psi}(\cdot)$ are the aggregation and context branches with parameters $\phi$ and $\psi$. Context aggregation models the importance of each position on $X$ by the aggregation branch $\mathcal{F}_{\phi}(X)$ and reweights the embedded feature from the context branch $\mathcal{G}_{\psi}(X)$ by $\mathcal{S}(\cdot,\cdot)$.

\input{Tabs/tab_attention.tex}

As shown in Table~\ref{tab:attention}, there are mainly two types of context aggregations for modern ConvNets: self-attention mechanism~\cite{vaswani2017attention, wang2018non, iclr2021vit} and gating attention~\cite{dauphin2017language, hu2018squeeze}.
%
The importance of each position on $X$ is calculated by global interactions of all other positions in $\mathcal{F}_{\phi}(\cdot)$ with a dot-product, which results in quadratic computational complexity. To overcome this limitation, attention variants in linear complexity~\cite{nips2021SOFT, iclr2022cosFormer} were proposed to substitute vanilla self-attention, \textit{e.g.,} linear attention~\cite{nips2020linformer, nips2022hilo} in the second line of Table~\ref{tab:attention}, but they might degenerate to trivial attentions~\cite{icml2022Flowformer}.
Unlike self-attention, gating unit employs an element-wise product $\odot$ as $\mathcal{S}(\cdot,\cdot)$ in linear complexity, \textit{e.g.,} gated linear unit (GLU) variants~\cite{Shazeer2020GLU} and squeeze-and-excitation (SE) modules~\cite{hu2018squeeze} in the last two lines of Table~\ref{tab:attention}. % However, they only aggregate the information of each position or the overall context with global average pooling (GAP), which lacks spatial interactions.


% figure: spatial interaction analysis
\begin{figure}[b]  % bottom
\centering
\vspace{-2.0em}
    \subfloat[]{\label{fig:mask}
    \includegraphics[height=0.465\linewidth,trim= 22 0 22 0,clip]{Figs/fig_analysis_mask.pdf}
    \vspace{1pt}}
    \hspace{-0.17cm}
    \subfloat[]{\label{fig:interaction}
    \includegraphics[height=0.500\linewidth,trim= 1 5 0 1,clip]{Figs/fig_analysis_interaction.pdf}
    }
\vspace{-1.0em}
    \caption{%\textbf{Analysis of multi-order interaction.}
    \textbf{(a) Illustrations of representation bottleneck.} Humans can recognize the object from images with around 50\% patches while learning little new information with a few (\textit{e.g.,} 5$\sim$10\%) or finding it too redundant with almost all patches (\textit{e.g.,} 90$\sim$95\%). DNNs are inclined to extract the most information from very few or the most patches but usually have less information gain with 20$\sim$80\% patches.
    \textbf{(b) Distributions of the interaction strength $J^{(m)}$} are plotted for Transformers and ConvNets on ImageNet-1K with $224^2$ resolutions and $n$ = $14\times 14$. Middle-order interactions mean the middle-complex interaction, where a medium number of patches (\textit{e.g.,} 0.2$\sim$0.8n) participate.
    }
    \label{fig:spatial_interaction}
    \vspace{-0.5em}
\end{figure}


\section{Representation Bottleneck from the View of Multi-order Game-theoretic Interaction}
\label{sec:rep_bottleneck}
Recent analysis towards the robustness~\cite{naseer2021intriguing, icml2022FAN, iclr2022how} and generalization ability~\cite{iclr2019shapebias, ancona2019explaining, 2021shapebias, nips2021partial} of DNNs delivers a new perspective to improve deep architectures.
Apart from these efforts, we extend the scope to the investigation of multi-order game-theoretic interaction.
%
As shown in Fig.~\ref{fig:mask}, DNNs can still recognize the target object under extreme occlusion ratios (\textit{e.g.,} only 10$\sim$20\% visible patches) but produce less information gain with intermediate occlusions~\cite{deng2021discovering, naseer2021intriguing}.
Interestingly, our human brains attain the sharpest knowledge upsurge from images with around 50\% patches, which indicates an intriguing cognition gap between human vision and deep models.
%
Formally, it can be explained by $m$-th order game-theoretic interaction $I^{(m)}(i,j)$ and $m$-order interaction strength $J^{(m)}$, as defined in \cite{zhang2020interpreting, deng2021discovering}.
Considering the image with $n$ patches in total, $I^{(m)}(i,j)$ measures the average interaction complexity between the patch pair $i,j$ over all contexts consisting of $m$ patches, where $0\le m \le n-2$ and the order $m$ reflects the scale of the context involved in the game-theoretic interactions between pixels $i$ and $j$. Normalized by the average of interaction strength, the relative interaction strength $J^{(m)}$ with $m\in (0,1)$ measures the complexity of interactions encoded in DNNs. 
%
Notably, low-order interactions tend to encode \textbf{common} or \textbf{widely-shared local texture} and the high-order ones are inclined to forcibly memorize the pattern of \textbf{rare outliers}~\cite{deng2021discovering, cheng2021game}. Refer to Appendix~\ref{app:interaction} for definitions and details.
%
As shown in Fig.~\ref{fig:interaction}, most DNNs are more favored to encode excessively low-order or high-order game-theoretic interactions while typically suppressing the most flexible and discriminative middle-order ones~\cite{deng2021discovering, cheng2021game}.
%
From our perspective, such dilemma in both ConvNets and ViTs may be attributed to the inappropriate composition of convolutional inductive bias and context aggregations~\cite{treisman1980feature, 2021shapebias, deng2021discovering, li2022A2MIM}. 
In particular, a naive implementation of self-attention or convolutions can be intrinsically prone to the strong bias of global shape~\cite{nips2021partial, cvpr2022replknet} or local texture~\cite{hermann2020origins}, infusing spurious extreme-order game-theoretic interaction preference into deep networks.


\vspace{-0.5em}
\section{Methodology}
\label{sec:method}
% In this section, we instantiate the overall framework as MogaNet equipped with spatial and channel aggregation blocks for multi-order context aggregation and channel-wise multi-order features reallocation.
% In this section, we instantiate the overall framework as MogaNet with convolution-based modules that efficiently combine regionality perception and context aggregation.

\vspace{-0.5em}
\subsection{Overview of MogaNet}
\label{sec:overview}
% Fig.~\ref{fig:app_moga_framework} comprehensively illustrates the four-stage MogaNet architecture.
Based on Fig. \ref{fig:framework}, we design the four-stage MogaNet architecture, illustrated in 
Fig.~\ref{fig:app_moga_framework}.
For stage $i$, the input image or feature is first fed into the embedding stem to regulate the feature resolutions and embed into $C_{i}$ dimensions. Assuming the input image in $H\times W$ resolutions, features of the four stages are in $\frac{H}{4}\times\frac{W}{4}$, $\frac{H}{8}\times\frac{W}{8}$, $\frac{H}{16}\times\frac{W}{16}$, and $\frac{H}{32}\times\frac{W}{32}$ resolution respectively.
Then, the embedded feature flows into $N_{i}$ Moga Blocks, consisting of spatial and channel aggregation blocks as presented in Sec.~\ref{sec:moga} and \ref{sec:channel}, for further context extraction and aggregation. 
After the final output, GAP and a linear layer are added for classification tasks. As for dense prediction tasks~\cite{2017iccvmaskrcnn, eccv2018upernet}, the output from four stages can be used through neck modules~\cite{cvpr2017fpn, cvpr2019semanticFPN}.

% figure: spatial Multi-Order
\begin{figure}[b]  % bottom
    \vspace{-1.0em}
    \centering
    \includegraphics[width=1.0\linewidth]{Figs/fig_moga_spatial.pdf}
\vspace{-1.75em}
    \caption{\textbf{Structure of spatial aggregation block $\mathrm{Moga}(\cdot)$.}}
    \label{fig:moga_multiorder}
    \vspace{-0.50em}
\end{figure}

% figure: grad-CAM for Moga ablation
\begin{figure}[t]
    \centering
    \vspace{-0.5em}
    \includegraphics[width=1.0\linewidth]{Figs/fig_ablation_gradcam.pdf}
    \vspace{-1.75em}
    \caption{Grad-CAM visualization of ablations. 1:\ 0:\ 0 and 0:\ 0:\ 1 denote only using $C_l$ or $C_h$ for Multi-order DWConv Layers. Models with extremely low- ($C_l$) or high- ($C_h$) order interactions are sensitive to similar regional textures (1:\ 0:\ 0) or excessive discriminative parts (0:\ 0:\ 1), not localizing semantic parts. Gating can effectively eliminate the disturbing contextual noise (\textit{$\backslash$wo} Gating).}
    \label{fig:ablation_cam}
    \vspace{-1.25em}
\end{figure}

% figure (Channel aggregation & analysis) and table (ablation)
\begin{figure*}[t!]
\vspace{-1.5em}
\centering
\begin{minipage}{0.67\linewidth}
    % \vspace{-0.25em}
    \subfloat[]{\label{fig:channal_moga}
    \includegraphics[height=0.430\linewidth,trim= 0 0 0 0,clip]{Figs/fig_moga_channel.pdf}
    }
    \hspace{-0.18cm}
    \subfloat[]{\label{fig:channal_analysis}
    \includegraphics[height=0.435\linewidth,trim= 1 8 0 1,clip]{Figs/fig_analysis_decomp.pdf}
    }
    \vspace{-0.75em}
    \caption{
\textbf{(a) Structure of channel aggregation block.}
\textbf{(b) Analysis of channel MLP and the channel aggregation module.} Based on MogaNet-S, performances and model sizes of the raw channel MLP, MLP with SE block, and the channel aggregation is compared with the MLP ratio of $\{2,4,6,8\}$ on ImageNet-1K.
    }
\end{minipage}
~\begin{minipage}{0.32\linewidth}
    \vspace{-0.75em}
    \input{Tabs/tab_ablation.tex}
\end{minipage}
\vspace{-1.25em}
\end{figure*}

\subsection{Multi-order Gated Aggregation}
\label{sec:moga}
%However, as discussed in Sec.~\ref{sec:rep_bottleneck}, the sole presence of regionality perception or context aggregation exhibits the inability to learn comprehensive contextual features with multi-order interactions~\cite{iclr2022uniformer, pinto2022impartial, deng2021discovering}.
As discussed in Sec.~\ref{sec:rep_bottleneck}, conventional DNNs with the incompatible composition of locality perception and context aggregation are inclined to concentrate on extreme-order interactions while suppressing the most discriminative middle-order ones~\cite{iclr2022uniformer, pinto2022impartial, deng2021discovering}.
As shown in Fig.~\ref{fig:ablation_cam}, the primary challenge is how to capture contextual representation with balanced multi-order game-theoretic interactions efficiently.
%Fig.~\ref{fig:interaction} shows conventional DNNs tend to focus on extremely low or high-order interactions. They are missing the most informative middle-order interactions. Thus, the primary challenge is how to capture contextual multi-order interactions effectively and efficiently.
%
To this end, we propose a spatial aggregation (SA) block as $\mathrm{SMixer}(\cdot)$ to aggregate multi-order contexts in a unified design, as shown in Fig.~\ref{fig:moga_multiorder}, consisting of two cascaded components. We rewrite Eq.~(\ref{eq:smixer}) as:
\vspace{-0.40em}
\begin{equation}
    \label{eq:moga_block}
    Z = X + \mathrm{Moga}\Big( \mathrm{FD}\big(\mathrm{Norm}(X)\big) \Big),
    \vspace{-0.40em}
\end{equation}
where $\mathrm{FD}(\cdot)$ indicates a feature decomposition module (FD) and $\mathrm{Moga}(\cdot)$ is a multi-order gated aggregation module comprising the gating $\mathcal{F}_{\phi}(\cdot)$ and context branch $\mathcal{G}_{\psi}(\cdot)$.

\paragraph{Multi-order contexts.}
\vspace{-1.0em}
As a pure convolutional structure, we extract multi-order features with both \textit{static} and \textit{adaptive} locality perceptions. 
% Since convolutions are inherently high-pass filters~\cite{iclr2022how, wang2022anti}, we design $\mathrm{FD}(\cdot)$ to adaptively decompose and enhance the high-frequency details to learn more informative features of full-frequency bands, which is formulated as: 
Except for $m$-order interactions, there are two complementary counterparts, $0$-order interaction of common local texture and `$n$-order' interaction covering complex global shape, which are modelled by $\mathrm{Conv}_{1\times 1}(\cdot)$ and $\mathrm{GAP}(\cdot)$ respectively. To force the network focus on balancing interactions of multiple complexities, we propose $\mathrm{FD}(\cdot)$ to dynamically exclude trivial feature interactions, defined as:
\vspace{-0.50em}
\begin{align}
    \label{eq:FD_proj}
    Y &= \mathrm{Conv}_{1\times 1}(X),\\
    \label{eq:FD}
    Z &= \mathrm{GELU}\Big(Y + \gamma_{s}\odot\big( Y-\mathrm{GAP}(Y)\big) \Big),
    \vspace{-0.6em}
\end{align}
where ${\gamma}_{s} \in \mathbb{R}^{C\times 1}$ denotes a scaling factor initialized as zeros. By re-weighting the complimentary
interaction component $Y - \mathrm{GAP}(Y)$, $\mathrm{FD}(\cdot)$ also increases spatial feature diversities~\cite{iclr2022how, wang2022anti}.
Then, we ensemble depth-wise convolutions (DWConv) to encode multi-order features in the context branch of $\mathrm{Moga}(\cdot)$. Unlike previous works that simply combine DWConv with self-attentions to model local and global interactions~\cite{eccv2022edgeformer, nips2022hilo, nips2022iformer, nips2022hornet} , we employ three different DWConv layers with dilation ratios $d\in \{1,2,3\}$ in parallel to capture low, middle, and high-order interactions: given the input feature $X\in \mathbb{R}^{C\times HW}$, $\mathrm{DW}_{5\times 5, d=1}$ is first applied for low-order features; then, the output is factorized into ${X}_l \in \mathbb{R}^{C_l \times HW}$, ${X}_m \in \mathbb{R}^{C_m \times HW}$, and ${X}_h \in \mathbb{R}^{C_h \times HW}$ along the channel dimension, where $C_l + C_m + C_h =C$; afterward, ${X}_m$ and ${X}_h$ are assigned to $\mathrm{DW}_{5\times 5, d=2}$ and $\mathrm{DW}_{7\times 7, d=3}$, respectively, while ${X}_l$ serves as identical mapping; finally, the output of ${X}_l$, ${X}_m$, and ${X}_h$ are concatenated to form multi-order contexts, $Y_{C} = \mathrm{Concat}(Y_{l, 1:C_{l}}, Y_{m}, Y_{h})$. 
Notice that the proposed $\mathrm{FD}(\cdot)$ and multi-order DWConv layers only require a little extra computational overhead and parameters in comparison to $\mathrm{DW}_{7\times 7}$ used in ConvNeXt~\cite{cvpr2022convnext}, \textit{e.g.,} +multi-order and +$\mathrm{FD}(\cdot)$ increase 0.04M parameters and 0.01G FLOPS over $\mathrm{DW}_{7\times 7}$ as shown in Table~\ref{tab:ablation}.

\vspace{-1.0em}
\paragraph{Gating aggregation.}
To aggregate the multi-order features from the context branch, we employ SiLU~\cite{elfwing2018sigmoid} activation in the gating branch, \textit{i.e.,} $x\cdot \mathrm{Sigmoid}(x)$, which can be regarded an advanced version of Sigmoid. As verified in Appendix~\ref{app:ablation_gating}, we find that SiLU owns both the gating effects as Sigmoid and the stable training property.
Taking the output from $\mathrm{FD}(\cdot)$ as the input, we rewrite Eq.~(\ref{eq:aggregate}) as:
\vspace{-0.50em}
\begin{align}
    \label{eq:moga}
    Z &= \underbrace{\mathrm{SiLU}\big( \mathrm{Conv}_{1\times 1}(X) \big)}_{\mathcal{F}_{\phi}} \odot \underbrace{\mathrm{SiLU}\big( \mathrm{Conv}_{1\times 1}(Y_{C}) \big)}_{\mathcal{G}_{\psi}},
    \vspace{-1.5em}
\end{align}
% where $\mathcal{G}_{\psi}(\cdot)$ and $\mathcal{F}_{\phi}(\cdot)$ are defined as $\mathrm{SiLU}(\mathrm{Conv}_{1\times 1}(\cdot))$.
With the proposed SA blocks, MogaNet captures more middle-order interactions, as validated in Fig.~\ref{fig:interaction}. 
The SA block produces discriminative multi-order representations with similar parameters and FLOPs as $\mathrm{DW}_{7\times 7}$ in ConvNeXt, which is well beyond the reach of existing methods without the cost-consuming self-attentions.

\subsection{Multi-order Feature Reallocation by Channel Aggregation}
\vspace{-0.25em}
\label{sec:channel}
Prevailing architectures, as illustrated in Sec.~\ref{sec:preliminaries}, perform channel-mixing $\mathrm{CMixer}(\cdot)$ mainly by two linear projections, \textit{e.g.,} 2-layer channel MLP~\cite{iclr2021vit, liu2021swin, nips2021MLPMixer} with a channel expand ratio $r$ or the MLP with a $3\times 3$ DWConv in between~\cite{cvmj2022PVTv2, aaai2022LIT, nips2022hilo}.
Due to the inherent redundancy cross channels~\cite{eccv2018CBAM, iccv2019GCNet, icml2019efficientnet, cvpr2020Orthogonal}, vanilla MLP requires a number of parameters ($r$ default to 4 or 8) to achieve expected performance, showing low computational efficiency as plotted in Fig.~\ref{fig:channal_analysis}. 
To address this issue, most current methods directly insert a channel enhancement module, \textit{e.g.,} SE module~\cite{hu2018squeeze}, into MLP.
%This problem might arise from the inherent redundancy cross channels~\cite{eccv2018CBAM, iccv2019GCNet, icml2019efficientnet, cvpr2020Orthogonal}, and most current methods try to address this issue by starchedly inserting a channel enhancement module, %\textit{e.g.,} SE module~\cite{hu2018squeeze}, into MLP.
Unlike these designs requiring additional MLP bottleneck, we introduce a lightweight channel aggregation module $\mathrm{CA}(\cdot)$ to conduct adaptive channel-wise reallocation in high-dimensional hidden spaces and further extend it to a channel aggregation (CA) block. 
% Unlike these attempts, we introduce a lightweight channel aggregation module $\mathrm{CA}(\cdot)$ to conduct adaptive channel-wise reallocation in high-dimensional hidden spaces and further extend it to a channel aggregation (CA) block. 
As shown in Fig.~\ref{fig:channal_moga}, we rewrite Eq.~(\ref{eq:cmixer}) for our CA block as:
\vspace{-0.5em}
\begin{equation}
\begin{aligned}
    Y &= \mathrm{GELU}\Big(\mathrm{DW_{3\times 3}}\big(\mathrm{Conv_{1\times 1}}(\mathrm{Norm}(X))\big)\Big),\\
    Z &= \mathrm{Conv_{1 \times 1}}\big(\mathrm{CA}(Y)\big) + X.
\end{aligned}
\vspace{-0.4em}
\end{equation}
Concretely, $\mathrm{CA}(\cdot)$ is implemented by a channel-reducing projection $W_{r}: \mathbb{R}^{C\times HW}\rightarrow \mathbb{R}^{1\times HW}$ and GELU to gather and reallocate channel-wise information:
\vspace{-0.5em}
\begin{equation}
    \mathrm{CA}(X) = X + \gamma_{c}\odot\big(X - \mathrm{GELU}(XW_{r})\big),
\vspace{-0.4em}
\end{equation}
where $\gamma_{c}$ is the channel-wise scaling factor initialized as zeros, which reallocates the complementary channel-wise interactions $X - \mathrm{GELU}(XW_{r})$. As shown in Fig.~\ref{fig:ablation_interaction}, $\mathrm{CA}(\cdot)$ effectively boosts middle-order game-theoretic interactions.
Fig.~\ref{fig:channal_analysis} verifies the superiority of $\mathrm{CA}(\cdot)$ compared with the vanilla MLP and the MLP with SE module in eliminating channel-wise information redundancy. Despite some improvements to the baseline, the MLP $w/$ SE module still requires large MLP ratios (\textit{e.g.,} $r$ = 6) to achieve expected performance while introducing extra parameters and computational overhead. 
In contrast, our $\mathrm{CA}(\cdot)$ with $r$ = 4 brings 0.6\% gain over the baseline at a small extra cost (0.04M extra parameters and 0.01G FLOPs) while achieving the same performance as the baseline with $r$ = 8.
% It is implemented by various attention mechanisms in recent vision Transformer models or spatial Multi-layer Perceptron (MLP) in MLP-like models. Note that the main function of the token mixer is to propagate token information, although some token mixers can also mix channels, like attention. At Stage II, the second sub-block primarily consists of a two-layered MLP with non-linear activation, looking at cross-channel correlations via a set of $1\times 1$ convolutions. 

\subsection{Implementation Details}
\label{sec:details}
Following the network design style of ConvNets~\cite{cvpr2022convnext}, we scale up MogaNet for six model sizes (X-Tiny, Tiny, Small, Base, Large, and X-Large) via stacking the different number of spatial and channel aggregation blocks at each stage, which has similar numbers of parameters as RegNet~\cite{cvpr2020regnet} variants. Network configurations and hyper-parameters are detailed in Table~\ref{tab:app_architecture}. FLOPs and throughputs are analyzed in Appendix \ref{app:flops_throughput}.
We set the channels of the multi-order DWConv layers to $C_l:$ $C_m:$ $C_h$ = 1:3:4 (see 
 Appendix~\ref{app:ablation_multiorder}).
% The embedding stem in canonical ResNet contains a 7$\times$7 convolution layer with stride 2, followed by a max-pooling layer, which results in a 4$\times$ downsampling of the input images.
Similar to \cite{2021patchconvnet, iclr2022uniformer, nips2022EfficientFormer}, the first embedding stem in MogaNet is designed as two stacked 3$\times$3 convolution layers with the stride of 2 while adopting the single-layer version for embedding stems in other three stages.
We select GELU~\cite{hendrycks2016bridging} as the common activation function and only use SiLU in the Moga module as Eq.~(\ref{eq:moga}).
