\section{Introduction}
\label{sec:intro}
% 1. ConvNets
Convolutional Neural Networks (ConvNets) have been the method of choice for computer vision~\cite{he2016deep, tpami2015faster, cvpr2019semanticFPN} since the renaissance of deep neural networks (DNNs)~\cite{Krizhevsky2012ImageNetCW}.
%
By interleaving hierarchical convolutional layers in-between pooling and non-linear operations~\cite{lecun1998gradient, yamins2014performance, sifre2014rigid, simonyan2014very}, ConvNets can encode underlying semantic patterns of observed images with the built-in translation equivariance constraints~\cite{he2016deep, bmvc2016wrn, Luo2016ERF, xie2017aggregated, hu2018squeeze, cvpr2022resnest} and have further become the fundamental infrastructure in today's computer vision systems.
% With the desired properties, ResNet~\cite{he2016deep} and its variants~\cite{bmvc2016wrn, xie2017aggregated, hu2018squeeze, cvpr2022resnest} have become the most widely-adopted ConvNet architectures in numerous computer vision applications. 
% For practical usage, efficient models~\cite{eccv2018shufflenet, 2017MobileNet, cvpr2018mobilenetv2, iccv2019mobilenetv3, icml2019efficientnet, cvpr2020regnet} are designed for a complexity-accuracy trade-off and hardware devices.
%
Nevertheless, representations learned by ConvNets have been proven to have a strong bias on local texture~\cite{2021shapebias}, resulting in a serious detriment of global information~\cite{Baker2018DeepCN, hermann2020origins, cvpr2022replknet}.
Therefore, efforts have been made to upgrade macro-level architectures~\cite{chollet2017xception, xie2017aggregated, cvpr2020regnet, cvpr2022resnest} and context aggregation modules~\cite{iccv2017Deformable, wang2018non, hu2018squeeze, eccv2018CBAM, iccv2019GCNet}.

% figure: Acc vs Param vs GFLOPs
\begin{figure}[t!]  % Top
    \vspace{-0.25em}  % iccv
    % \vspace{-1.0em}  % arxiv
    \centering
    \includegraphics[width=0.94\linewidth]{Figs/fig_acc_param_flops.pdf}
\vspace{-1.25em}
    \caption{\textbf{Performance on ImageNet-1K validation set at $224^2$ resolutions.} MogaNet with pure Convolutions outperforms Transformers (DeiT\cite{icml2021deit} and Swin~\cite{liu2021swin}), ConvNets (RegNetY~\cite{cvpr2020regnet} and ConvNeXt~\cite{cvpr2022convnext}), and hybrid models (CoAtNet~\cite{nips2021coatnet}) at all parameter scales.}
    \label{fig:in1k_acc_param}
    \vspace{-1.25em}
\end{figure}

% 2. ViTs
In contrast, by relaxing local inductive bias, the newly emerged Vision Transformers (ViTs)~\cite{iclr2021vit, liu2021swin, iccv2021PVT, nips2021Twins} have rapidly challenged the long dominance of ConvNets on a wide range of vision benchmarks.
There is an almost unanimous consensus that such superiority of ViTs primarily stems from self-attention mechanism~\cite{bahdanau2014neural, vaswani2017attention}, which facilitates long-term feature interactions regardless of the topological distance.
%
From a practical standpoint, however, quadratic complexity within self-attention modelling prohibitively restricts the computational efficiency of ViTs~\cite{nips2020linformer, icml2022FLASH, icml2022Flowformer} and its application potential to fine-grained scenarios~\cite{cvpr2022VideoSwin, jiang2021transgan, zhu2020deformable} where high-resolution features are required. In addition, the absence of inductive bias shatters the inherent geometric structure of images, thereby inevitably inducing the detriment of neighborhood correlations~\cite{pinto2022impartial}. 
%
To tackle this obstacle, endeavors have been contributed to reintroduce pyramid-like hierarchical layouts~\cite{liu2021swin, fan2021multiscale, iccv2021PVT} and shift-invariant priors~\cite{wu2021cvt, nips2021coatnet, han2021transformer, iclr2022uniformer, cvpr2022MobileFormer} to ViTs, at the expense of model generality and expressiveness. 

% 3. Post-ViT ConvNet Architectures
More recent studies have shown that the representation capability of ViTs should mainly be credited to their macro-level architectures rather than the commonly-conjectured self-attention mechanisims~\cite{nips2021MLPMixer, raghu2021vision, yu2022metaformer}.
More importantly, with advanced training setup and ViT-style architecture modernization, ConvNets can readily deliver excellent scalability and competitive performance \emph{w.r.t.}~well-tuned ViTs across a variety of visual benchmarks~\cite{wightman2021rsb, cvpr2022convnext, cvpr2022replknet, pinto2022impartial}, which heats the debate between ConvNets and ViTs and further alters the roadmap for deep architecture design.
% Nevertheless, there remains a representation bottleneck for existing approaches~\cite{hermann2020origins, iclr2022how, deng2021discovering, wu2022bottleneck}: naive implementation of self-attention or large kernels hampers the modelling of discriminative contextual information and global interactions, leading to a cognition gap between DNNs and human visual system.
% As in feature integration theory~\cite{treisman1980feature}, human brains not only extract local features but simultaneously aggregate these features for global perception, which is more compact and efficient than DNNs~\cite{liu2021swin, cvpr2022convnext}.

Different from previous attempts, we investigate the representation capacity of modern ConvNets through the lens of multi-order game-theoretic interaction \cite{icml2019explaining}, which provides a new view to explain the feature interaction behaviors and effects encoded in a deep architecture based on game theory.
%
As shown in Fig.~\ref{fig:interaction}, most modern DNNs are inclined to encode game-theoretic interaction of extremely low or high complexities rather than the most discriminative intermediate one \cite{deng2021discovering}, which limits their representation abilities and robustness to complex samples.

Under this perspective, we develop a novel pure ConvNet architecture called \textbf{{M}}ulti-\textbf{{o}}rder \textbf{{g}}ated \textbf{{a}}ggregation Network (MogaNet) for balancing the multi-order interaction strength, in order to improve the performance of ConvNets.
%Multi-order interactions among input variables are modeled as a set of learnable \emph{global filters} that are applied to the spectrum of the input features. Since the collection of depthwise convolutions are able to cover all the frequencies, our model can capture both long-term and short-term interactions. The filters are directly learned from the raw data without introducing human priors.
%
Our design encapsulates both low-order locality priors and middle-order context aggregation into a unified spatial aggregation block, where features of balanced multi-order interactions are efficiently congregated and contextualized with the gating mechanism in parallel.
%
From the channel aspect, as existing methods are prone to channel information redundancy~\cite{raghu2021vision, icml2022FLASH}, we tailor a conceptually simple yet
efficient channel aggregation block, which performs adaptive channel-wise feature reallocation to the multi-order input and significantly outperforms prevailing counterparts (\textit{e.g.}, SE module~\cite{hu2018squeeze}) with lower computational cost. 

% Experimental results
Extensive experiments demonstrate the impressive performance and great efficiency of MogaNet at different model scales on various computer vision tasks, including image classification, object detection, semantic segmentation, instance segmentation, pose estimation, \etc.
% We empirically show that interaction complexity can serve as an essential indication, like the receptive field, for high-quality visual architecture design.
As a result, MogaNet attains 83.4\% and 87.8\% with 25M and 181M parameters, which exhibits favorable computational overhead compared with existing small-size models, as shown in Fig.~\ref{fig:in1k_acc_param}. MogaNet-T achieves 80.0\% top-1 accuracy on ImageNet-1K, outperforming the state-of-the-art ParC-Net-S~\cite{eccv2022edgeformer} by 1.0\% with 2.04G lower FLOPs under the same setting. 
Moreover, MogaNet exhibits strong performance gains on various downstream tasks, \textit{e.g.,} surpassing Swin-L~\cite{liu2021swin} by 2.3\% AP$^b$ on COCO detection with fewer parameters and computational budget.
Therefore, the performance gains of MogaNet are not due to increased capacity but rather to more efficient use of model parameters.
