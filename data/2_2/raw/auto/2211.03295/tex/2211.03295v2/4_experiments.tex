\section{Experiments}
\label{sec:expriments}
To verify and compare MogaNet with the leading network architectures, we conduct extensive experiments on popular vision tasks, including image classification, object detection, instance and semantic segmentation, 2D and 3D pose estimation, and video prediction. Experiments are implemented with PyTorch and run on NVIDIA A100 GPUs.
% To verify the effectiveness of our method, we conduct extensive experiments on ImageNet-1K \cite{cvpr2009imagenet} for image classification, COCO~\cite{2014MicrosoftCOCO} for object detection, instance segmentation, and pose estimation and ADE20K~\cite{Zhou2018ADE20k} for semantic segmentation. All experiments are implemented with PyTorch on Ubuntu workstations with NVIDIA A100 GPUs. \textbf{Bold} and \hl{gray} indicate the best performance and our models.

\subsection{ImageNet Classification}
\label{sec:exp_in1k}
\paragraph{Settings.} 
For classification experiments on ImageNet \cite{cvpr2009imagenet}, we train MogaNet variants following the standard procedure \cite{icml2021deit, liu2021swin} on ImageNet-1K (IN-1K) for a fair comparison, training 300 epochs with AdamW~\cite{iclr2019AdamW} optimizer, a basic learning rate of $1\times 10^{-3}$, and a Cosine scheduler~\cite{loshchilov2016sgdr}. To explore the capacities of large models, we pre-trained MogaNet-XL on ImageNet-21K (IN-21K) for 90 epochs and then fine-tuned 30 epochs on IN-1K following \cite{cvpr2022convnext}. Appendix~\ref{app:in1k_settings} and \ref{app:exp_in1k} provide implementation details and more results.
We compare three typical architectures: \textbf{Pure ConvNets} (C), \textbf{Transformers} (T), and \textbf{Hybrid model} (H) with both self-attention and convolution operations.
% We compare four typical architectures: (\romannumeral1) \textbf{Pure ConvNets} (C) include ResNet, ShuffleNetV2, EfficientNet, MobileNetV3, RegNet, ConvNeXt, RepLKNet, FocalNet, SLak, and HorNet. (\romannumeral2) \textbf{Transformers} (T) include DeiT, Swin, T2T-ViT, PVT, PVTV2 Focal, ViT-C, CSWin, SReT, and LiTV2. (\romannumeral3) \textbf{Hybrid architectures} (H) of attention and convolution include PiT, LeViT, CoaT, BoTNet, ViTAE, Twins, CoAtNet, MobileViT, Uniformer, Mobile-Former, ParC-Net, EfficientFormer, and MaxViT.

% table: IN-1K Tiny (5M) & Small (25M)
\input{Tabs/tab_in1k_tiny.tex}
% \input{Tabs/tab_in1k_small.tex}

% table: IN-1K scaling-up
\input{Tabs/tab_in1k}


\vspace{-1.0em}
\paragraph{Results.}
As for lightweight models, Table~\ref{tab:in1k_cls_tiny} shows that MogaNet-XT/T significantly outperforms existing lightweight architectures with efficient usage of parameters and FLOPs. MogaNet-T achieves 79.0\% top-1 accuracy, which improves models with $\sim$5M parameters by at least 1.1 at $224^2$ resolutions. Using $256^2$ resolutions, MogaNet-T outperforms the current SOTA ParC-Net-S by 1.0 while achieving 80.0\% top-1 accuracy with the refined settings. Even with only 3M parameters, MogaNet-XT still surpasses models with around 4M parameters, \textit{e.g.,} +4.6 over T2T-ViT-7. Particularly, MogaNet-T$^{\S}$ achieves 80.0\% top-1 accuracy using $256^2$ resolutions and the refined training settings (detailed in Appendix~\ref{app:advanced_tiny}).
% which adjusts $lr$ and replaces RandAugment~\cite{cubuk2020randaugment} with 3-Augment~\cite{eccv2022deit3}
As for scaling up models in Table~\ref{tab:in1k_cls_scaling}, MogaNet shows superior or comparable performances to SOTA architectures with similar parameters and computational costs.
For example, MogaNet-S achieves 83.4\% top-1 accuracy, outperforming Swin-T and ConvNeXt-T with a clear margin of 2.1 and 1.2. MogaNet-B/L also improves recently proposed ConvNets with fewer parameters, \textit{e.g.,} +0.3/0.4 and +0.5/0.7 points over HorNet-S/B and SLaK-S/B.
% As for 45M and 80M models, we summarize their performances in Table~\ref{tab:in1k_cls_scaling} and MogaNet-B/L still surpass the current state-of-the-art architectures, especially improving Swin-S/B and ConvNeXt-S/B by 1.2\%/ 1.1\% and 1.1\%/ 0.8\%.
When pre-trained on IN-21K, MogaNet-XL is boosted to 87.8\% top-1 accuracy with 181M parameters, saving 169M compared to ConvNeXt-XL. Noticeably, MogaNet-XL can achieve 85.1\% at $224^2$ resolutions without pre-training and improves ConvNeXt-L by 0.8, indicating MogaNets are easier to converge than existing models (also verified in Appendix~\ref{app:exp_in1k}).
% Additionally, we provide fast training results based on RSB A3 100-epoch scheme~\cite{wightman2021rsb} in Appendix~\ref{app:comparison}.


% table: COCO & ADE20K
\input{Tabs/tab_coco.tex}
\input{Tabs/tab_ade20k.tex}

\subsection{Dense Prediction Tasks}
\label{sec:exp_det_seg}
\paragraph{Object detection and segmentation on COCO.}
We evaluate MogaNet for object detection and instance segmentation tasks on COCO~\cite{2014MicrosoftCOCO} with RetinaNet~\cite{iccv2017retinanet}, Mask-RCNN~\cite{2017iccvmaskrcnn}, and Cascade Mask R-CNN~\cite{tpami2019cascade} as detectors. Following the training and evaluation settings in \cite{liu2021swin, cvpr2022convnext}, we fine-tune the models by the AdamW optimizer for $1\times$ and $3\times$ training schedule on COCO~\textit{train2017} and evaluate on COCO~\textit{val2017}, implemented on 
MMDetection~\cite{mmdetection} codebase. The box mAP (AP$^{b}$) and mask mAP (AP$^{m}$) are adopted as metrics. Refer Appendix~\ref{app:coco_det_settings} and \ref{app:exp_det_coco} for detailed settings and full results. Table~\ref{tab:coco} shows that detectors with MogaNet variants significantly outperform previous backbones. It is worth noticing that Mask R-CNN with MogaNet-T achieves 42.6 AP$^{b}$, outperforming Swin-T by 0.4 with 48\% and 27\% fewer parameters and FLOPs. Using advanced training setting and IN-21K pre-trained weights, Cascade Mask R-CNN with MogaNet-XL achieves 56.2 AP$^{b}$, +1.4 and +2.3 over ConvNeXt-L and RepLKNet-31L. 
% MogaNet-T gains 3.6\% AP$^{bb}$ and 4.6\% AP$^{mk}$ over ResNet-18; MogaNet-S outperforms Swin-T (Transformers) by 3.9\% AP$^{bb}$ and 2.7\% AP$^{mk}$, and surpasses UniFormer-S (hybrid) by 0.5\% AP$^{bb}$; MogaNet-B outperforms Swin-T and LITV2-M (Transformer) by 2.9\% AP$^{bb}$ and 1.2\% AP$^{mk}$ respectively.

\vspace{-1.0em}
\paragraph{Semantic segmentation on ADE20K.}
We also evaluate MogaNet for semantic segmentation tasks on ADE20K \cite{Zhou2018ADE20k} with Semantic FPN \cite{cvpr2019semanticFPN} and UperNet \cite{eccv2018upernet} following \cite{liu2021swin, yu2022metaformer}, implemented on MMSegmentation~\cite{mmseg2020} codebase. The performance is measured by single-scale mIoU. Initialized by IN-1K or IN-21K pre-trained weights, Semantic FPN and UperNet are fine-tuned for 80K and 160K iterations by the AdamW optimizer. See Appendix \ref{app:ade20k_seg_settings} and \ref{app:exp_seg_ade20k} for detailed settings and full results.
In Table~\ref{tab:ade20k}, Semantic FPN with MogaNet-S consistently outperforms Swin-T and Uniformer-S by 6.2 and 1.1 points; UperNet with MogaNet-S/B/L improves ConvNeXt-T/S/B by 2.5/1.4/1.8 points. Using higher resolutions and IN-21K pre-training, MogaNet-XL achieves 54.0 SS mIoU, surpassing ConvNeXt-L and RepLKNet-31L by 0.3 and 1.6.
% UperNet with MogaNet-S improves backbones of Transformers (+3.1\% over Swin-T), hybrid architectures (+1.6\% over UniFormer-S), and modern ConvNets (+1.1\% over HorNet-T$_{7\times 7}$. Refer to Appendix~\ref{app:ade20k_seg_settings} for more details.


% table: 2D & 3D Pose
\input{Tabs/tab_coco_pose}
\input{Tabs/tab_3d_vp}

\vspace{-1.0em}
\paragraph{2D and 3D Human Pose Estimation.}
We then evaluate MogaNet for 2D and 3D human pose estimation tasks. As for 2D key points estimation on COCO, we conduct evaluations with SimpleBaseline~\cite{eccv2018simple} following \cite{iccv2021PVT, iclr2022uniformer}, which fine-tunes the model for 210 epoch by Adam optimizer \cite{iclr2014adam}. Table \ref{tab:coco_pose} shows that MogaNet variants yield at least 0.9 AP improvements for $256\times 192$ input, \textit{e.g.,} +2.5 and +1.2 over Swin-T and PVTV2-B2 by MogaNet-S. Using $384\times 288$ input, MogaNet-B outperforms Swin-L and Uniformer-B by 1.0 and 0.6 AP with fewer parameters.
As for 3D face/hand surface reconstruction tasks on Stirling/ESRC 3D \cite{feng2018evaluation} and FreiHAND \cite{iccv2019freihand} datasets, we benchmark backbones with ExPose \cite{eccv2020ExPose}, which fine-tunes the model for 100 epoch by Adam optimizer. 3DRMSE and Mean Per-Joint Position Error (PA-MPJPE) are the metrics. In Table \ref{tab:3d_vp}, MogaNet-S shows the lowest errors compared to Transformers and ConvNets.
We provide detailed implementations and results for 2D and 3D pose estimation tasks in Appendix \ref{app:exp_2d_pose} and \ref{app:exp_3d_pose}.

\vspace{-1.0em}
\paragraph{Video Prediction.}
We further evaluate MogaNet for unsupervised video prediction tasks with SimVP~\cite{cvpr2022simvp} on MMNIST \cite{icml2015mmnist}, where the model predicts the successive 10 frames with the given 10 frames as the input. We train the model for 200 epochs from scratch by the Adam optimizer and are evaluated by MSE and Structural Similarity Index (SSIM). Table \ref{tab:3d_vp} shows that SimVP with MogaNet blocks improves the baseline by 6.58 MSE and outperforms ConvNeXt and HorNet by 1.37 and 4.07 MSE. Appendix \ref{app:mmnist_vp_settings} and \ref{app:exp_vp_mmnist} show more experiment settings and results.


% figure (interaction) & table (ablation)
\begin{figure}[t]
\vspace{-0.5em}
\centering
\begin{minipage}{0.38\linewidth}
    \vspace{-1.25em}
    \centering
    \input{Tabs/tab_ablation_small.tex}
\end{minipage}
~\begin{minipage}{0.59\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth,trim= 4 0 0 0,clip]{Figs/fig_ablation_interaction.pdf}
    \vspace{-2.25em}
\end{minipage}
    \caption{
    \textbf{Ablation of designed modules on ImageNet-1K.} \textbf{Left}: the table ablates the proposed modules by removing each one based on the baseline of MogaNet-S. \textbf{Right}: the figure plots distributions of the interaction strength $J^{(m)}$ and verifies that $\mathrm{Moga}(\cdot)$ contributes the most to learning multi-order interactions and better performance.
    }
    \label{fig:ablation_interaction}
\vspace{-0.5em}
\end{figure}

% figure: gradcam
\begin{figure}[t]
    \vspace{-0.75em}
    \centering
    \includegraphics[width=1.0\linewidth,trim= 4 0 0 0,clip]{Figs/fig_analysis_gradcam.pdf}
    \vspace{-1.75em}
    \caption{
    \textbf{Grad-CAM activation maps} on IN-1K. MogaNet shows similar activation maps as local attention architectures (Swin), which are located on the semantic targets. Unlike the results of previous ConvNets, which might activate some irrelevant parts, the activation maps of MogaNet are more gathered. See more results in Appendix~\ref{app:gradcam}.
    }
    \label{fig:analysis_gradcam}
    \vspace{-1.25em}
\end{figure}


\subsection{Ablation and Analysis}
\label{sec:exp_ablation}
We first ablate the spatial aggregation module in Table~\ref{tab:ablation} and Fig.~\ref{fig:ablation_interaction} (left), including \textbf{FD$(\cdot)$} and \textbf{Moga$(\cdot)$}, which contains the \textbf{gating branch} and the context branch with multi-order DWConv layers \textbf{Multi-DW$(\cdot)$}, and the channel aggregation module \textbf{CA}$(\cdot)$. We found that all proposed modules yield improvements with a few costs. Appendix~\ref{app:ablation} provides more ablation studies.
Furthermore, we empirically show design modules can learn more middle-order interactions in Fig.~\ref{fig:ablation_interaction} (right) and visualize class activation maps (CAM) by Grad-CAM~\cite{cvpr2017grad} compared to existing models in Fig.~\ref{fig:analysis_gradcam}.
