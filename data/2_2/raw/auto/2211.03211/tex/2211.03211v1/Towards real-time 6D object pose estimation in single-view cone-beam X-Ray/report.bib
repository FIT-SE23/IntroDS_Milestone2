
@inproceedings{yolo6D,
      TITLE = {{Real-Time Seamless Single Shot 6D Object Pose Prediction}},       
      AUTHOR = {Tekin, Bugra and Sinha, Sudipta N. and Fua, Pascal},
      BOOKTITLE = {CVPR},
      YEAR = {2018}
}


@INPROCEEDINGS{YOLOv2,
  author={J. {Redmon} and A. {Farhadi}},
  booktitle={CVPR}, 
  title={YOLO9000: Better, Faster, Stronger}, 
  year={2017},
  volume={},
  number={},
  pages={6517-6525},
  doi={10.1109/CVPR.2017.690}}

@InProceedings{Xie_2018_ECCV_Workshops,
        author = {Xie, Yiting and Richmond, David},
        title = {Pre-training on Grayscale ImageNet Improves Medical Image Classification},
        booktitle = {Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
        month = {September},
        year = {2018}
}

@INPROCEEDINGS{xrayPosNet,
      author={M. {Bui} and S. {Albarqouni} and M. {Schrapp} and N. {Navab} and S. {Ilic}},
      booktitle={2017 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
      title={X-Ray PoseNet: 6 DoF Pose Estimation for Mobile X-Ray Devices}, 
      year={2017},
      volume={},
      number={},
      pages={1036-1044}
  }

@article{PnP,
	doi = {10.1088/1742-6596/1087/5/052009},
	url = {https://doi.org/10.1088%2F1742-6596%2F1087%2F5%2F052009},
	year = 2018,
	month = {sep},
	publisher = {{IOP} Publishing},
	volume = {1087},
	pages = {052009},
	author = {Xiao Xin Lu},
	title = {A Review of Solutions for Perspective-n-Point Problem in Camera Pose Estimation},
	journal = {Journal of Physics: Conference Series},
	abstract = {As there is a rapid development of robotics in the field of automation engineering, ego-motion estimation has become a most challenging task. In this review, we presented a model to help describe the PnP problems, and introduced two most common solutions. The P3P solution is the smallest subset of control points that yields a finite number of solutions. The EPnP solution is to reduce the complexity by expressing the n 3D points as a weighted sum of four virtual control points. The former solution is widely applied while there are 3 pairs of corresponding points in the problem. However, in most real cases, the latter is more used.}
}

@article{i3PosNet,
    author = {K{\"u}gler, David and Sehring, Jannik and Stefanov, Andrei and Stenin, Igor and Kristin, Julia and Klenzner, Thomas and Schipper, J{\"o}rg and Mukhopadhyay, Anirban},
    year = {2020},
    title = {i3PosNet: Instrument Pose Estimation from X-Ray in temporal bone surgery},
    url = {https://link.springer.com/article/10.1007/s11548-020-02157-4},
    pages = {1137--1145},
    volume = {15},
    number = {7},
    issn = {1861-6429},
    journal = {International journal of computer assisted radiology and surgery}
}

@incollection{luma,
title = "28 - Luma and colour differences",
editor = "Charles Poynton",
booktitle = "Digital Video and HD (Second Edition)",
publisher = "Morgan Kaufmann",
edition = "Second Edition",
address = "Boston",
pages = "335 - 354",
year = "2012",
series = "The Morgan Kaufmann Series in Computer Graphics",
isbn = "978-0-12-391926-7",
doi = "https://doi.org/10.1016/B978-0-12-391926-7.50028-X",
url = "http://www.sciencedirect.com/science/article/pii/B978012391926750028X"
}
  
@article{DPOD,
      author    = {Sergey Zakharov and
                   Ivan Shugurov and
                   Slobodan Ilic},
      title     = {{DPOD:} Dense 6D Pose Object Detector in {RGB} images},
      journal   = {CoRR},
      volume    = {abs/1902.11020},
      year      = {2019},
      url       = {http://arxiv.org/abs/1902.11020},
      archivePrefix = {arXiv},
      eprint    = {1902.11020},
      timestamp = {Tue, 21 May 2019 18:03:39 +0200},
      biburl    = {https://dblp.org/rec/journals/corr/abs-1902-11020.bib},
      bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{SSD6D,
          author    = {Wadim Kehl and
                       Fabian Manhardt and
                       Federico Tombari and
                       Slobodan Ilic and
                       Nassir Navab},
          title     = {{SSD-6D:} Making RGB-based 3D detection and 6D pose estimation great
                       again},
          journal   = {CoRR},
          volume    = {abs/1711.10006},
          year      = {2017},
          url       = {http://arxiv.org/abs/1711.10006},
          archivePrefix = {arXiv},
          eprint    = {1711.10006},
          timestamp = {Mon, 13 Aug 2018 16:48:58 +0200},
          biburl    = {https://dblp.org/rec/journals/corr/abs-1711-10006.bib},
          bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{BB8,
      author    = {Mahdi Rad and
                   Vincent Lepetit},
      title     = {{BB8:} {A} Scalable, Accurate, Robust to Partial Occlusion Method
                   for Predicting the 3D Poses of Challenging Objects without Using Depth},
      journal   = {CoRR},
      volume    = {abs/1703.10896},
      year      = {2017},
      url       = {http://arxiv.org/abs/1703.10896},
      archivePrefix = {arXiv},
      eprint    = {1703.10896},
      timestamp = {Mon, 13 Aug 2018 16:48:42 +0200},
      biburl    = {https://dblp.org/rec/journals/corr/RadL17.bib},
      bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Hatt2016,
abstract = {In recent years, registration between x-ray fluoroscopy (XRF) and transesophageal echocardiography (TEE) has been rapidly developed, validated, and translated to the clinic as a tool for advanced image guidance of structural heart interventions. This technology relies on accurate pose-estimation of the TEE probe via standard 2D/3D registration methods. It has been shown that latencies caused by slow registrations can result in errors during untracked frames, and a real-time ( {\textgreater} 15 hz) tracking algorithm is needed to minimize these errors. This paper presents two novel similarity metrics designed for accurate, robust, and extremely fast pose-estimation of devices from XRF images: Direct Splat Correlation (DSC) and Patch Gradient Correlation (PGC). Both metrics were implemented in CUDA C, and validated on simulated and clinical datasets against prior methods presented in the literature. It was shown that by combining DSC and PGC in a hybrid method (HYB), target registration errors comparable to previously reported methods were achieved, but at much higher speeds and lower failure rates. In simulated datasets, the proposed HYB method achieved a median projected target registration error (pTRE) of 0.33 mm and a mean registration frame-rate of 12.1 hz, while previously published methods produced median pTREs greater than 1.5 mm and mean registration frame-rates less than 4 hz. In clinical datasets, the HYB method achieved a median pTRE of 1.1 mm and a mean registration frame-rate of 20.5 hz, while previously published methods produced median pTREs greater than 1.3 mm and mean registration frame-rates less than 12 hz. The proposed hybrid method also had much lower failure rates than previously published methods.},
author = {Hatt, Charles R. and Speidel, Michael A. and Raval, Amish N.},
doi = {10.1016/j.media.2016.04.008},
file = {:C$\backslash$:/Users/320088652/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hatt, Speidel, Raval - 2016 - Real-time pose estimation of devices from x-ray images Application to x-rayecho registration for cardiac i.pdf:pdf},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {2D/3D registration,Cardiac interventions,GPGPU,Real-time},
mendeley-groups = {Pose Estimation},
month = {dec},
pages = {101--108},
pmid = {27179366},
publisher = {Elsevier B.V.},
title = {{Real-time pose estimation of devices from x-ray images: Application to x-ray/echo registration for cardiac interventions}},
volume = {34},
year = {2016}
}

@techreport{EfficientPose,
abstract = {In this paper we introduce EfficientPose, a new approach for 6D object pose estimation. Our method is highly accurate , efficient and scalable over a wide range of computational resources. Moreover, it can detect the 2D bounding box of multiple objects and instances as well as estimate their full 6D poses in a single shot. This eliminates the significant increase in runtime when dealing with multiple objects other approaches suffer from. These approaches aim to first detect 2D targets, e.g. keypoints, and solve a Perspective-n-Point problem for their 6D pose for each object afterwards. We also propose a novel augmentation method for direct 6D pose estimation approaches to improve performance and generalization, called 6D augmentation. Our approach achieves a new state-of-the-art accuracy of 97.35{\%} in terms of the ADD(-S) metric on the widely-used 6D pose estimation benchmark dataset Linemod using RGB input, while still running end-to-end at over 27 FPS. Through the inherent handling of multiple objects and instances and the fused single shot 2D object detection as well as 6D pose estimation, our approach runs even with multiple objects (eight) end-to-end at over 26 FPS, making it highly attractive to many real world scenarios. Code will be made publicly available at https://github.com/ybkscht/EfficientPose.},
archivePrefix = {arXiv},
arxivId = {2011.04307v2},
author = {Bukschat, Yannick and Vetter, Marcus},
eprint = {2011.04307v2},
year = {2020},
file = {::},
mendeley-groups = {Pose Estimation},
title = {{EfficientPose: An efficient, accurate and scalable end-to-end 6D multi object pose estimation approach}},
url = {https://github.com/ybkscht/EfficientPose.}
}

@incollection{pinhole,
abstract = {The pinhole camera model describes the mathematical relationship between the coordinates of a 3D point and its projection onto the image plane of an ideal pinhole camera, where the camera aperture is described as a point and no lenses are used to focus light. The model does not include, for example, geometric distortions or blurring of unfocused objects caused by lenses and finite sized apertures. It also does not take into account that most practical cameras have only discrete image coordinates. This means that the pinhole camera model can only be used as a first order approximation of the mapping from a 3D scene to a 2D image. Its validity depends on the quality of the camera and, in general, decreases from the center of the image to the edges as lens distortion effects increase.},
author = {Sturm, Peter},
booktitle = {Computer Vision},
doi = {10.1007/978-0-387-31439-6_472},
file = {::},
mendeley-groups = {Pose Estimation},
pages = {610--613},
publisher = {Springer US},
title = {{Pinhole Camera Model}},
url = {http://www.usa.canon.com/},
year = {2014}
}

@inproceedings{crysi,
title = "Feature-based depth estimation for monoplane X-ray imaging with limited C-arm motion",
abstract = "During interventional procedures, surgical guidance is provided through live X-ray imaging. Depth cues from 2D imaging can enhance the insight to the true position of important structures. In this paper, we examine a novel application where a small motion of the C- arm is used to create disparity between views for defining a sparse depth map of generic interest points. Feature points are detected and then matched across multiple views, and the depth of the points is estimated using multi-view geometry. Specific adaptations for our case are (1) the matching using geometric constraints for locally dense searching and (2) outlier rejection for robust feature tracks in multiple views. Evaluation using phantom images gave an accuracy in the mm-range for up to 20 views (or _ 300), and a sufficiently rich set of robustly tracked features. This creates a valid option for depth estimation of static points of clinical interest.",
author = "C. Papalazarou and P.M.J. Rongen and {With, de}, P.H.N.",
year = "2009",
language = "English",
pages = "1--8",
booktitle = "MICCAI 2009 Workshop on Geometric Accuracy in Image Guided Interventions, London, U.K., September 2009",
}

@InProceedings{Brachmann_2016_CVPR,
author = {Brachmann, Eric and Michel, Frank and Krull, Alexander and Yang, Michael Ying and Gumhold, Stefan and Rother, carsten},
title = {Uncertainty-Driven 6D Pose Estimation of Objects and Scenes From a Single RGB Image},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@techreport{Presenti2020,
abstract = {3D X-ray Computed Tomography (CT) is increasingly being used for non-destructive inspection of objects. Conventional CT inspection requires many projections, typically spanning 360 â€¢ to reconstruct a 3D image of the object, which is then segmented and subsequently compared with the reference computer-aided design (CAD) model. Such an inspection flowchart, however, is a time inefficient procedure, not suitable for inline inspection. To overcome this problem, we directly compare the measured projections with simulated ones from the CAD model. To do so, the simulated projections need to be created with the same acquisition geometry as the measured ones. When an object is inserted on a scanning system, its orientation may vary with respect to the default CAD model orientation. For this reason, 2D/3D registration between the CAD model and the measured projections of the real object is necessary. In this paper, we present a deep learning based method to accurately estimate the 3D orientation of an object from one projection image.},
author = {Presenti, Alice and Bazrafkan, Shabab and Sijbers, Jan and {De Beenhouwer}, Jan},
file = {::},
keywords = {ResNet-50,X-ray CT,deep learning,pose estimation},
mendeley-groups = {Pose Estimation},
title = {{Deep learning-based 2D-3D sample pose estimation for X-ray 3DCT}},
url = {http://www.ndt.net/?id=25117},
year = {2020}
}

@book{Papalazarou2012,
author = {Papalazarou, Chrysi},
doi = {10.6100/IR735445},
file = {:C$\backslash$:/Users/320088652/OneDrive - Philips/Philips/Pose Estimation/3D object reconstruction in image guided interventions using multiview X ray.pdf:pdf},
isbn = {9789461913579},
mendeley-groups = {Pose Estimation},
title = {{3D object reconstruction in image-guided interventions using multi-view X-ray}},
year = {2012}
}

@article{Slagowski2017,
abstract = {{\textcopyright} The Authors. Published by SPIE under a Creative Commons Attribution 3.0 Unported License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI. Accurate and artifact-free reconstruction of tomographic images requires precise knowledge of the imaging system geometry. A projection matrix-based calibration method to enable C-arm inverse geometry CT (IGCT) is proposed. The method is evaluated for scanning-beam digital x-ray (SBDX), a C-arm mounted inverse geometry fluoroscopic technology. A helical configuration of fiducials is imaged at each gantry angle in a rotational acquisition. For each gantry angle, digital tomosynthesis is performed at multiple planes and a composite image analogous to a cone-beam projection is generated from the plane stack. The geometry of the C-arm, source array, and detector array is determined at each angle by constructing a parameterized three-dimensional-to-two-dimensional projection matrix that minimizes the sum-of-squared deviations between measured and projected fiducial coordinates. Simulations were used to evaluate calibration performance with translations and rotations of the source and detector. The relative root-mean-square error in a reconstruction of a numerical thorax phantom was 0.4{\%} using the calibration method versus 7.7{\%} without calibration. In phantom studies, reconstruction of SBDX projections using the proposed method eliminated artifacts present in noncalibrated reconstructions. The proposed IGCT calibration method reduces image artifacts when uncertainties exist in system geometry.},
author = {Slagowski, Jordan M. and Dunkerley, David A. P. and Hatt, Charles R. and Speidel, Michael A.},
doi = {10.1117/1.jmi.4.1.013506},
file = {::},
issn = {2329-4302},
journal = {Journal of Medical Imaging},
keywords = {C-arm calibration,computed tomography Paper 16241PR,inverse geometry,scanning-beam digital x-ray},
mendeley-groups = {Pose Estimation},
month = {mar},
number = {1},
pages = {013506},
publisher = {SPIE-Intl Soc Optical Eng},
title = {{Single-view geometric calibration for C-arm inverse geometry CT}},
url = {/pmc/articles/PMC5358550/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5358550/},
volume = {4},
year = {2017}
}

@inproceedings{ADD,
abstract = {We propose a framework for automatic modeling, detection, and tracking of 3D objects with a Kinect. The detection part is mainly based on the recent template-based LINEMOD approach [1] for object detection. We show how to build the templates automatically from 3D models, and how to estimate the 6 degrees-of-freedom pose accurately and in real-time. The pose estimation and the color information allow us to check the detection hypotheses and improves the correct detection rate by 13{\%} with respect to the original LINEMOD. These many improvements make our framework suitable for object manipulation in Robotics applications. Moreover we propose a new dataset made of 15 registered, 1100+ frame video sequences of 15 various objects for the evaluation of future competing methods. {\textcopyright} 2013 Springer-Verlag.},
author = {Hinterstoisser, Stefan and Lepetit, Vincent and Ilic, Slobodan and Holzer, Stefan and Bradski, Gary and Konolige, Kurt and Navab, Nassir},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-37331-2_42},
file = {::},
isbn = {9783642373305},
issn = {03029743},
mendeley-groups = {Pose Estimation},
pages = {548--562},
publisher = {Springer, Berlin, Heidelberg},
title = {{Model based training, detection and pose estimation of texture-less 3D objects in heavily cluttered scenes}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-37331-2{\_}42},
volume = {7724 LNCS},
year = {2013}
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@incollection{Sim2real,
title = {Sim2real transfer learning for 3D human pose estimation: motion to the rescue},
author = {Doersch, Carl and Zisserman, Andrew},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {12949--12961},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9454-sim2real-transfer-learning-for-3d-human-pose-estimation-motion-to-the-rescue.pdf}
}
