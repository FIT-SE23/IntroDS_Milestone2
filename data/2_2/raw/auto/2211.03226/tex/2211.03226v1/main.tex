\documentclass[notitlepage,twocolumn,pre,tightenlines,superscriptaddress,showpacs,floatfix]{revtex4}

\usepackage{amsmath,amssymb,amsfonts,latexsym,mathrsfs}
\usepackage{comment}
\usepackage{svg}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue]{hyperref}
%\usepackage[mathcal]{euscript}
\usepackage{graphicx}
%\usepackage{epsfig}
\usepackage{color}
\usepackage{booktabs}
%\usepackage[activate=normal]{pdfcprot}  % richtiges character protruding

%\usepackage{psfrag}
%\usepackage{txfonts,pxfonts,wasysym,}
%\usepackage{txfonts}
%\usepackage{wasysym}
%\usepackage{pifont}

\usepackage[caption = false]{subfig}
%\usepackage{subcaption}

%\usepackage{soul}  %% allows for the command \st{}  to cross some text.

%%\bibliographystyle{apsrev}
%%\setlength\parindent{0pt} % sets indent to zero
%%\setlength{\parskip}{8pt} % changes vertical space between paragraphs

%% == Adresses =======================

% == Shortcuts =======================
\newcommand{\ie}{\textit{i.e.~} }
\newcommand{\etal}{\textit{et~al.~} }
\newcommand{\eg}{\textit{e.g.~} }

\newcommand{\md}[1]{\left|#1\right|}
\renewcommand{\d}{{\rm d}}
\renewcommand{\d}{\ensuremath{\mathrm{d}}}
%%\addtolength{\parskip}{1em}
\newcommand{\moy}[1]{\left\langle  #1 \right\rangle }
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}

\newcommand{\tauaffil}{Université Paris-Saclay, CNRS, INRIA, Laboratoire Interdisciplinaire des Sciences du Numérique, TAU team,  91190 Gif-sur-Yvette, France}


\begin{document}


\title{SE(3)-equivariant Graph Neural Networks for Learning Glassy Liquids Representations\!\!\!}
\author{Francesco Saverio Pezzicoli}
\affiliation{\tauaffil}
\author{Guillaume Charpiat}
\affiliation{\tauaffil}
\author{Fran\c{c}ois P. Landes}
\affiliation{\tauaffil}

\begin{abstract}
Within the glassy liquids community, the use of Machine Learning (ML) to model particles' static structure in order to predict their future dynamics is currently a hot topic. 
The actual state of the art consists in Graph Neural Networks (GNNs) \cite{bapst_unveiling_2020} which, beside having a great expressive power, are heavy models with numerous parameters and lack interpretability.
Inspired by recent advances \cite{thomas_tensor_2018},
we build a GNN 
that learns a robust representation of the glass' static structure by constraining it to preserve the roto-translation (SE(3)) equivariance.
We show that this constraint not only significantly improves the predictive power but also allows to reduce the number of parameters while improving the interpretability. 
Furthermore, we relate our learned equivariant features to  well-known invariant expert features, 
which are easily expressible with a single layer of our network.
\end{abstract}

%Recently, there have been many Machine Learning (ML) based attempts at predicting future mobility of glassy liquids from their static structure. 

%explaining the difference in expressive power between these two choices.
%explain the difference between equivariance and invariance and why each level of representation ought to be equivariant with respect to the input, for maximum expressivity.

\maketitle

\section*{Introduction}
\label{Introduction}

%\textbf{FL: TODO: less talk about DH - CHECK - and more about the general problem of high-D input hard to map to a single scalar value (order parameter)}
Understanding the nature of the dynamical glass transition is one of the most intriguing open problems in condensed matter. 
As a glass-forming liquid is cooled down towards its characteristic temperature ($T_g$), its viscosity increases by several orders of magnitude, while its structure, the spatial arrangement of the particles, does not show any obvious change.
%as measured by the two-point correlation functions. 
Finding the subtle changes in the structure that explain the huge variations in the dynamics (increased viscosity) seems like an obvious target for Machine Learning, which is a great tool for finding hidden patterns in data.

%Independently from this, a number of works identify patterns using expert hand-crafted features that seem quite specific to glasses where these descriptors are shown to directly correlate with the dynamics \cite{tong_structural_2019, lerbinger_relevance_2021}, without any actual use of Machine Learning.
Aside from Machine Learning, historically and up to the present day, physicists have inspected how some well-defined physical quantities correlate with glassy dynamics \cite{Rottler2014, golde_correlation_2016, tong_revealing_2018, tanaka_revealing_2019, lerbinger2020local, lerbinger2022relevance}, in a manner akin to expert features (but without any learning).
There are also direct pattern-recognition techniques \cite{Coslovich2011,keys_characterizing_2011,Royall2017,Turci2017,royall_dynamical_2020} and a few unsupervised approaches \cite{boattini_autonomously_2020,paret_assessing_2020}, but the typical strategy is to learn a representation using supervised learning, i.e.~using ML to map a given local structure to a target label that represents the local dynamics (i.e.~a ``local viscosity'' measure). 
Since glasses are very heterogeneous when approaching the transition (a phenomenon called Dynamic Heterogeneity \cite{Candelier2010a, arceri2021statistical}), %% TODO: here we should cite a review paper on DH ! 
a single temperature point already displays a broad variety of labels' values, and is generally considered to be enough data to learn a good representation.
%In this context, the notion of ``good'' can be summarized crudely as ``yielding a high prediction accuracy for the target label''.


%Even more intriguing is the emergence of Dynamical Heterogeneities (DH): upon cooling, separate regions of high  and low mobility arise and the characteristic size of these regions (which is a dynamical length-scale) diverges as the temperature is decreased. (\textbf{FL: Do you have a reference ?? I am NOT sure it has been proven numerically ?!}) 
% The problem lies in finding a suitable structural order parameter pointing at the transition: a function only of the geometric structure of the system which varies significantly from the liquid to the solid state, featuring a length scale comparable to the one of the DH.




The quest for this good representation has prompted many works, relying on various techniques.
The idea of using ML for glasses was introduced by the pioneering works of Liu \etal~\cite{Schoenholz2014, Cubuk2015,Schoenholz2016,Schoenholz2016a}, where a rather generic set of isotropic features (describing the density around the target particle) is fed to a binary classifier (a linear SVM) to predict the target particle's mobility.
After considering only isotropic features, they marginally improved accuracy by using angular-aware expert features, measuring bond-angles around the target particle. 
This simple yet robust technology alone yielded a number of physics-oriented works  \cite{Sharp2017a,Schoenholz2017,Definiujemy2012,sussman_disconnecting_2017,landes_attractive_2020}, 
%TODO: also cite the step model and more recent stuff from Andrea's gorup.
which provide  physical interpretations of the model's output value (named Softness).
%The insights from these works could quantitatively benefit from more accurate ML predictions.
%Other expert-features approaches then introduced the use of Spherical Harmonics (before Boattini, in glasses?? I could not find the trace) and [other serious stuff done before SE3-GNNs, especially the not-Deep approaches] to deal with the angular, non-isotropic aspects of the input data.
These and the other shallow learning approaches all rely on features that describe the neighborhood of a single particle to then predict this single particle's future: in all cases, there is no interaction between neighborhoods.

Independently from the glass literature, within other application fields dealing with particles living in 3D space, Deep Learning frameworks and in particular Graph Neural Networks (GNNs) were developed, with some success, quite early on (2016).
Applications range from molecular properties predictions~\cite{kearnes_molecular_2016}, to  bulk materials properties prediction (MegNet)~\cite{chen_graph_2019}, or Density Functional Theory approximations~\cite{hu_forcenet_2021}, and most importantly for us, for glassy dynamics~\cite{bapst_unveiling_2020}.
%Independently and around 2020, Deep Learning entered the game, with some early attempts using Convolutional Neural Networks (CNNs), which are designed to handle 2D or 3D picture-like data, and most prominently using Graph Neural Networks (GNNs) \cite{bapst_unveiling_2020}, which we detail in this paper.
Indeed, designing geometrical features to capture the slight variations in the geometry of a particle's neighborhood is a daunting task, and the lesson learned from Computer Vision and the advent of Convolutional Neural Networks (CNNs) is that 
meta design works better than design, that is, expert features designed by hand are not as informative as the ones learned by a neural network with a suitable architecture designed by an expert.
%things work better when people stop designing features by hand, and instead work at the higher level of architecture, i.e.~when they start designing the architectures that will automatically find the proper features (meta design).
This is the point of the Deep approach, and more specifically GNNs, that are designed to build aggregate representations of the nodes' neighborhoods.
%The point of the Deep approach is to model the potentially intricate correlation between neighboring particles while avoiding the use of expert features: it requires minimal physical insight.
Although they are very effective, standard GNNs lack interpretability due to their structure and their high number of learnable parameters.
%% 

The coarse-graining effectively performed by GNNs was then mimicked (or distilled) in an expert-features approach, with the impressive result by Boattini \etal \cite{boattini_averaging_2021} where it is shown that a simple Ridge Regression with $\sim O(1000)$ of rather generic, rotational-invariant basic features, perform equally well as the state of the art (GNNs, \cite{bapst_unveiling_2020}).
The features consist for a part of node-wise rotational-invariant descriptors, and for another, of these elementary features but averaged over neighboring nodes, over a small radius (mimicking a GNN aggregation step).
%At the same time they study different types of glass forming systems: hard spheres/disks, 2D and 3D binary mixtures and poly-disperse ones. 
% \textbf{FL: ok your original sentence wasn't bad too:}
% On the other side Boattini \etal \cite{boattini_autonomously_2020} recently showed that mimicking the local averaging property of GNNs on single particle descriptors manages to match the state of the art at significantly reduced complexity. Both attempts rely on the interaction over the neighborhood of scalar quantities which are invariant to roto-translation. 
There is currently some debate \cite{boattini_averaging_2021, alkemade_comparing_2022} over whether Deep Learning can achieve better results than such expert features, and part of the community leans on this expert side.

%\textbf{FL: idea of paragraphs around here: give clearly the feeling that literature has 2 opposite sides: there are agnostic work(s), mostly GNN, that dont rely on any expertise and use Deep ; versus ; various expert approaches, which are not Deep but are interpretable. No one mixes them, they are sort of in opposition. (and we come to bridge the gap, sort of, or combine them)}

Here, inspired by the fast-growing field of SE(3)-equivariant networks \cite{thomas_tensor_2018,batzner_e3-equivariant_2021,weiler_3d_2018, brandstetter_geometric_2022}, we build a GNN that produces directional hidden representations that are rotation-equivariant, i.e.~respect the symmetries of atom packings.
In other words, we take the best of both worlds: we combine the basic idea of symmetry-aware features, already used with some success \cite{boattini_averaging_2021}, with the combinatorial or expressive power of Deep Learning.
In practice, we surpass the state of the art for prediction of the dynamical propensity of 3D Kob-Anderson mixtures, at a reduced number of parameters and increased interpretability. 


In the next section (sec.~\ref{sec:task}) we define the type and amount of input and output data, and the task to be solved.
We then introduce all the necessary tools to build a SE(3)-GNN, explaining how they apply to our specific case (sec.~\ref{sec:buildSE3GNN}).
Equipped with these tools, we then describe the main characteristics of our network's architecture, in sec.~\ref{sec:arch}.
In section \ref{sec:results}, we compare the performance of our network with the previously known state of the art results, showing it consistently outperforms previous methods, at a reduced cost.
In the discussion (sec.~\ref{sec:discussion}) we put our work in a broader perspective and outline directions for future work.


\section{Dataset and task}
\label{sec:task}

%\textbf{FL idea: define the task early, so we can refer to how the general ideas apply to our case later in the text. For now I cut paste the data part here.}

%\textit{Dataset} \hspace{.3cm} 
To probe the ability of our model to predict mobility, we adopt the dataset built by Bapst \etal in \cite{bapst_unveiling_2020}. It is obtained from molecular dynamics simulations of an 80:20 Kob-Andersen mixture of $N=4096$ particles in a three-dimensional box with periodic boundary conditions, at number densities of $\rho\simeq1.2$.
Four state points (temperatures) are analyzed: $T=0.44, 0.47, 0.50, 0.56$ (in dimensionless units). For each point, 800 independent configurations $\{ \mathbf{x}_i \}_{i=1...N}$ are available, i.e. $800$ samples (each sample represents $N$ particles' positions).

Ground truth labels, i.e. the quantities to be predicted, are the dynamical propensity \cite{berthier_structure_2007, berthier_spontaneous_2007} of particles: for each initial configuration, $30$ micro-canonical simulations are run independently, each with initial velocities sampled from the Maxwell-Boltzmann distribution. 
The propensity of particle $i$ over a timescale $\tau$ is then defined as the average displacement over the $30$ runs.
%\textbf{FL:send formula to the Appendix ?} 
%$m_i(\tau) = 1/n_\alpha \sum_\alpha||\mathbf{x}_{i,\alpha}(t=\tau)-\mathbf{x}_{i}(t=0)||$. 

For each sample to be processed through the GNN, an input graph is built by taking particles as nodes and connecting them when the inter-atomic distance is less than $d_c = 2$ (in atomic potential units). 
%% NO - we don't use these features in the end ! A node has features $\mathbf{x}_i \in \mathbb{R}^3$ (its associated particle's coordinates) and 
The node features \footnote{``Node features'' is Machine Learning Vocabulary for  ``set of values associated to the node. Similarly for edge features.} only encode the particle type, here A or B. 
We use one-hot encoding, such that node features consist of $n_{type}=2$ boolean variables. This generalizes trivially to mixtures with $n_{type}>2$.
The edges are directed, and edge $(i,j)$ has for feature $\mathbf{a}_{ij} = (\mathbf{x}_j -\mathbf{x}_i)$, i.e. it stores the relative position of the particles (nodes) it connects. We show a sketch of  our input graph with its node and edge features in Fig.~\ref{fig:graph_info}.

Our task is the node-wise regression of the particles' propensity $m_i\in \mathbb{R}$ (node label), restricted to the type-A particles (nodes). 


\begin{figure}
    \centering
    \includegraphics[width=0.35\textwidth]{embedded_graph.pdf}
    \caption{\textbf{Input Graph with its input features.} Node features are the one-hot encoded particle types (invariant features, $l=0$), and edge attributes $\mathbf{a_{ij}}$ are split: the direction is embedded in Spherical Harmonics $Y(\mathbf{\hat{a}_{ij}})$ and the norm is retained separately.}
    \label{fig:graph_info}
\end{figure}


\section{How to build a Graph-convolution equivariant layer?}
\label{sec:buildSE3GNN}

\textit{Graph Neural Networks}\hspace{.3cm} Consider a graph $\mathcal{G} = (\mathcal{V},\mathcal{E})$, where $\mathcal{V} = \{1,\dots,n_v\}$ is the set of nodes $v_i$ and $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$ is the set of edges $e_{ij}$, endowed with node features $\mathbf{h}_i\in\mathbb{R}^{c_v}$ and edge features $\mathbf{a}_{ij}\in\mathbb{R}^{c_e}$. GNNs operate on such graphs by updating node (and possibly edge) features through local operations on the neighborhood of each node. 
These operations are designed to be adaptive to different kinds of neighborhoods and respect node-index permutation invariance, which are the two key features of GNNs, as opposed to CNNs (for which the learned kernels must have fixed, grid-like geometry, and for which each neighboring pixel is located at a fixed relative position).
In this work we deal with Graph Convolutional Networks (GCN), a subclass of GNNs. 
A GCN layer acts on node features as follows: 
\begin{align}
    \mathbf{h}'(\mathbf{x}_i) = \sum_{j\in \mathcal{N}(i)} \kappa (\mathbf{x}_j-\mathbf{x}_i) \mathbf{h}(\mathbf{x}_j)
    \label{eq:GCNKernel}
\end{align}
where $\mathcal{N}(i)$ is the neighborhood of node $i$. Here a position $\mathbf{x}_i \in \mathbb{R}^3$ is associated to each node and $\kappa$ is a continuous convolution kernel which only depends on relative nodes' positions. In this case, as for CNNs, the node update operation is translation-equivariant by construction. 
It is however not automatically rotation-equivariant.

% One of the most general ways to describe GNNs is regarding them as message passing networks (most of GNNs if not all can be described in this framework). In a forward pass, node features are updated as follow. Message from node $v_i$ to node $v_j$ is computed:
% \begin{align}
%     \mathbf{m}_{ij} = \phi_m \left(\mathbf{f}_i, \mathbf{f}_j, \mathbf{a}_{ij}\right)
% \end{align}
% different messages are aggregated over the neighborhood to update $\mathbf{f}_i$:
% \begin{align}
%     \mathbf{f}'_i = \phi_f \left(\mathbf{f}_i, \sum_{j\in\mathcal{N}(i)} \mathbf{m}_{ij} \right)
% \end{align}
% $\phi_m$ and $\phi_f$ are commonly implemented as MLPs whose weights are shared respectively over edges and nodes and are learned by back-propagation.

% In this work we deal with graphs embedded in a 3-dimensional space: each node may be associated with a position $\mathbf{x}_i \in \mathbb{R}^3$, but it's the edge features that carry the information about nodes' relative positions. 
% In this case, the node update operation 
% %(reminiscent of weight sharing of CNNs) 
%  is translation-equivariant by construction, since only relative positions are used. It is however not automatically rotation-equivariant.
%  %[cite proof]

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{equivariance.pdf}
    \caption{\textbf{Equivariance to 2D rotations.} Simple case in which the input and the output fields have the same dimension. $\pi(r)$ represents the action of the rotation operator on the input field, $\pi'(r)$ on the output one. The mapping $\mathcal{K}$ acts in an equivariant way, indeed it commutes with the rotation. More generally, the output field may be of a different dimension, hence the presence of $\pi'(r)$, which in the above case is equal to $\pi(r)$.}
    \label{fig:equivariance}
\end{figure}

\medskip
\textit{Equivariance} \hspace{.3cm} A layer of a network is said to be equivariant with respect to a group $G$ if 
%upon transformation of the input by the action of $G$, 
upon group action on the input,
the output is transformed accordingly. 
One simple example is shown in Figure~\ref{fig:equivariance}: it depicts a 2D vector field $\mathbf{f}(\mathbf{x})$ and a mapping $\mathcal{K}$ that acts on it,  $\mathcal{K}(\mathbf{f}) = \cos{(||\mathbf{f}||)}\cdot \hat{\mathbf{f}}$ (where $\hat{\mathbf{f}} = \mathbf{f} / ||\mathbf{f}||$), generating the output field $\mathbf{f}'(\mathbf{x})$, which happens to live in the same space.
$\mathcal{K}$ is equivariant to 2D rotations: it operates only on the norm of the vectors, thus it commutes with the rotation operator.

As introduced in previous the example, we can represent the point cloud processed by the GNN as a vector field 
$\mathbf{h}(\mathbf{x}) = \sum_{i\in\mathcal{V}} \delta(\mathbf{x}-\mathbf{x}_i)\mathbf{h}_i$
with values in the vector space $H$, and the action of a layer as a mapping $\mathcal{K}$ from one field to the updated one. 
To require that $\mathcal{K}$ fulfils equivariance, we need to define how the group of interest acts on the vector space $H$ through representations.
% Feature maps form a functional space $\mathcal{F}$, then the action of a layer is just a mapping from a functional space to another one: $\mathcal{K}: \mathbf{h}(\mathbf{x})\in\mathcal{F}\rightarrow \mathbf{h}'(\mathbf{x})\in\mathcal{F}'$ with $\mathbf{h}(\mathbf{x}) \in H$ and $\mathbf{h}'(\mathbf{x}) \in H'$. Note that the input and the output of one layer don't need to inhabit the same space.
Given a group $G$, a representation $\rho$ is a mapping from group elements $g$ to square matrices $\mathbf{D}^H(g)$ that respect the group structure. Essentially it tells how $G$ acts on a specific space $H$.
%% FL: couldn't we discard the end of this paragraph ?
For example, the representation of the group of three-dimensional rotations $SO(3)$ on the 3-D Cartesian space is the usual rotation matrix $\mathbf{R}(r_{\alpha,\beta,\gamma}) = \mathbf{R}_x(\alpha)\mathbf{R}_y(\beta)\mathbf{R}_z(\gamma)$. Then if we consider an element $tr\in\nolinebreak SE(3)$ which is composed of a translation and a rotation, it will act on a vector field as follows: 
\begin{align}
    \mathbf{h}(\mathbf{x}) \xrightarrow[]{\pi(tr)} \mathbf{D}^H(r) \mathbf{h}(\mathbf{R}^{-1}(r)(\mathbf{x}-\mathbf{t}))
\end{align}
The codomain (output domain) is transformed by the representation of the rotation while the domain by the one of the inverse roto-translation. See Fig. \ref{fig:equivariance} top left to bottom left for an example with 2D rotations. 
For more details and further examples, see \cite{weiler_3d_2018}.

Let us define equivariance. 
Let there be a mapping $\mathcal{K}: \mathbf{h}(\mathbf{x}) \rightarrow \mathbf{h}'(\mathbf{x})$ and $\mathbf{h} \in H$, $\mathbf{h}' \in H'$ with $H,H'$ two vector spaces, $\mathcal{K}$ is equivariant with respect to $G$ if 
\begin{align}
    \forall g\in G \;\; \mathcal{K} \circ \pi(g) =  \pi'(g) \circ \mathcal{K}
\end{align}

The input and output codomains $H,H'$ do not need to be the same, and this is taken into account by the group representations $\mathbf{D}^H(g) $ and $ \mathbf{D}^{H'}(g)$. A direct consequence of this definition is that invariance is a particular case of equivariance where $\mathbf{D}^{H'}(g) = \mathbb{I} \;\; \forall g\in G$. 

When dealing with $SE(3)$, the \textit{only} invariant quantities are \textit{scalar}, thus considering only invariant features significantly reduces the model's expressivity.


\medskip
\textit{Equivariant features} \hspace{.3cm} To enforce equivariance of layers, we work with equivariant features (also called \textit{steerable features}), following the schema of steerable group convolutions \cite{weiler_3d_2018,thomas_tensor_2018,kondor_clebsch-gordan_2018} (for theoretical insight, see Appendix-B of \cite{brandstetter_geometric_2022}). 
These features inhabit the space of irreducible representations of $SO(3)$, which is factorized into sub-spaces: $V_{SO(3)} = V_{l_1}\oplus V_{l_2}\oplus V_{l_3}\oplus \dots$. Each subspace, indexed by $l\geq 0$, is of size $2l+1$ and transforms independently under rotations thanks to the action of Wigner-D matrices $\mathbf{D}^{(l)} \in \mathbb{R}^{(2l+1)\times(2l+1)}$. 
Coming to implementation, a feature is just a concatenation of different $l$\mbox{-}vectors: scalars ($l=0$), 3-D vectors ($l=1$), 5-D vectors ($l=2$) and so on. Multiple pieces with the same $l$ are also allowed, we address this multiplicity by referring to channels. 
For example we can have two $l=0$ channels, and a single $l=1$ channel, likewise for $l=2$:
\begin{align}
    \mathbf{h}(\mathbf{x}) = \left(h^{(l=0)}_{c=0}(\mathbf{x}),h^{(l=0)}_{c=1}(\mathbf{x}),\mathbf{h}^{(l=1)}_{c=0}(\mathbf{x}),\mathbf{h}^{(l=2)}_{c=0}(\mathbf{x})\right)
    \label{eq:h_ell}
\end{align}
where $\mathbf{h}: \mathbb{R}^3 \rightarrow \mathbb{R}^k$ with $k = \sum_{l} n^{(l)}_{c} \cdot (2l+1) =2\cdot 1 + 3 + 5 = 10$ with $n^{(l)}_{c}$ number of channels of type $l$. Rotation of these features is straightforward: 
\begin{align}
    \mathbf{D}(r) \mathbf{h} = \left[\begin{array}{cccc}
   D^{(0)}&&&\\
   &D^{(0)}&&\\
   &&\mathbf{D}^{(1)}&\\
   &&&\mathbf{D}^{(2)}\\
   \end{array}\right]
   \left[ \begin{array}{c}
        h^{(l=0)}_{c=0}\\
        h^{(l=0)}_{c=1}\\
        \mathbf{h}^{(l=1)}_{c=0}\\
        \mathbf{h}^{(l=2)}_{c=0}
   \end{array}\right]
\end{align}
The representation matrix is block-diagonal thanks to $SO(3)$ being decomposed in a direct sum, and scalars ($l=0$) are invariant with respect to rotation ($D^{(0)} = 1$). 
The wording \textit{steerable} becomes clear: upon rotation of the input coordinates, these features rotate accordingly, as when one steers the steering wheel, thus turning the wheels.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{update_and_cm.pdf}
    \caption{
\textbf{
Overview of the convolution layer, summarizing Eqs.~(\ref{eq:convolution},\ref{eq:self_int}).}
For each neighboring node, the node and edge features are combined (with C-G product) and multiplied by the learned radial filter $\varphi$.
Before performing this operation, the one-hot encoded particle, are concatenated to $\mathbf{h_i}$ by adding 2 $l=0$ channels, but this is not shown in the figure for simplicity.
Because multiple triplets come out of the C-G product, we obtain a much larger representation (left part of the inset at the right). This intermediate representation is narrowed down using a linear layer (one for each $l_O$).
% The convolution (i) takes as input equivariant node features $\mathbf{h}_i$, (ii) embeds edge features in SH basis, (iii) makes them interact with C-G equivariance-preserving tensor product, to output (iv) an equivariant representation $\mathbf{h'}$
    \label{fig:aggregating_and_mixing}
    }
\end{figure*}

\medskip
\textit{Spherical Harmonics (SH)} \hspace{.3cm} 
To embed the three dimensional node and edge input data in an equivariant form, we use Real Spherical Harmonics $Y^l_m:\mathbb{S}^2 \rightarrow \mathbb{R}$.
They can be though of as the generalization of Fourier modes (circular harmonics) 
% $\mathcal{F}^k:\mathbb{S}^1 \rightarrow \mathbb{R}$ 
 to the sphere.
Spherical Harmonics are indexed by the rotational order $l\geq 0$, which is reminiscent of the 1-D frequency, and by $m = -l,\dots,l$, which determines the spatial orientation. 
They form an orthonormal basis of $\mathbf{L}^2(\mathbb{S}^2)$, i.e.~any real-valued function on the sphere $f: \mathbb{S}^2 \rightarrow \mathbb{R}$ can be Fourier Transformed to this SH basis: 
\begin{align}
    f(\mathbf{n})  &= \sum_{l=0}^\infty \sum_{m=-l}^l \hat{f}^l_{m} Y^l_m (\mathbf{n})\\
     \mathscr{F}(f)(l,m) &= \hat{f}^l_{m} = \int_{\mathbb{S}_2} f(\mathbf{n})  Y^l_m (\mathbf{n}) \text{d}\mathbf{n}
\end{align}
where $\mathbf{n} = (\theta,\phi)$ represents a generic direction on the sphere.
Here the coefficients $\hat{f}^l$ are not real values but are $(2l+1)$-dimensional vectors (with components $\hat{f}^l_{m}$). 
The set of all coefficients $(\hat{f}^l)_{l=0,\ldots}$ plays the same role as $ \mathbf{h}(\mathbf{x})$ in Eq.(\ref{eq:h_ell}), and each coefficient $\hat{f}^l$ transforms according to a Wigner-D matrix $\mathbf{D}^{(l)}$: the SH embedding is thus equivariant. 
%
% \textbf{FL: next sentence is a bit unclear/confusing to read:}
% Every directional information can be embedded in a steerable space trough the use of SH, considering $f(\mathbf{n}) = \delta(\mathbf{n}-\mathbf{n}_0)$: 
% \begin{align}
%     \mathscr{F}(\mathbf{n}_0) = \left(Y^0_0 (\mathbf{n}_0),Y^1_{-1} (\mathbf{n}_0),Y^1_0 (\mathbf{n}_0),Y^1_1 (\mathbf{n}_0),\dots \right)
% \end{align}



In particular, the density of neighbor particles at a fixed distance $r$ from the central one, $\rho_r(\mathbf{n})=\sum_{j \in \mathcal{N}(i)} \delta(\mathbf{n}-\mathbf{n}_{ij})\delta(r-r_{ij})$, 
 is a real-valued function (distribution) on the sphere and can be decomposed into Spherical Harmonics.
Furthermore, summing such decompositions at a number of $r \in [0,d_c]$, one obtains an equivariant representation of the field of density around a target particle in the ball of radius $d_c$ (this is what our very first convolution performs, see lower).
%This is what we do in Eq.~(\ref{eq:convolution}).

Note that to make a fixed-$r$ representation finite-dimensional, we need to choose a high-frequency cutoff value for the rotational order, $l=l_{max}$.
Analogously to Fourier transforms on the circle, this way of decomposing and filtering out high-frequencies preserves the input signal better than most naive schemes (as e.g. a discretization of solid angles).

% \textbf{FL: this last paragraph is really obscure to me. It sounds like we repeat the same thing several times. To be discussed IRL. If you just want to say that learned stuff applies to each ell differently, but to all m's of the same ell equally, then let's say that/ show a block diagonal matrix or sthg more telling.}
% We adopt SH also to parameterize convolution kernels as in \cite{thomas_tensor_2018}. We consider functions $\hat{\mathbf{k}}: \mathbb{R}^3 \rightarrow V_{SO(3)}$ as for feature maps, and define them directly in the steerable space as: 
% \begin{align}
%     \mathbf{k}(\mathbf{x}) = \left(w^0(||\mathbf{x}||) Y^0(\hat{\mathbf{x}}), w^1(||\mathbf{x}||) \mathbf{Y}^1(\hat{\mathbf{x}}),w^2(||\mathbf{x}||) \mathbf{Y}^2(\hat{\mathbf{x}}),\dots\right)
% \end{align}
% where $\hat{\mathbf{x}} = \frac{\mathbf{x}}{||\mathbf{x}||}$ and the weights incorporate the radial dependence of the filer. Note that they only depend on $l$ this means that for different $m$ the weight is the same and the filter inherits the rotation properties of SH. 
% if we keep this kind of kernel approach, then do the proper split, K(phi_16 , Y lf) * wlflilo_16 * h(li)





\medskip
\textit{Clebsh-Gordan tensor product} \hspace{.3cm} 
As said above, we do not want to restrict ourselves to invariant features, but to equivariant ones. 
For this, we need a way to combine feature vectors together, other than the dot product (which would produce invariant scalar features).

Analogously to the outer product for vectors of $\mathbb{R}^3$, which is a bilinear operator  $\otimes: \mathbb{R}^3\times \mathbb{R}^3 \rightarrow \mathbb{R}^3$, 
the Clebsh-Gordan tensor product $\otimes_{l_1,l_2}^{l_O}: V_{l_1}\times V_{l_2} \rightarrow V_{l_O}$ is a bilinear operator that combines two $SO(3)$ steerable features of type $l_1$ and $l_2$ and returns another steerable vector of type $l_O$. 
It allows to maintain equivariance when combining equivariant features: consider $\mathbf{h}_1(\mathbf{x}) \in V_{l_{1}}$, $\mathbf{h}_2(\mathbf{x}) \in V_{l_{2}}$ and their C-G tensor product $\mathbf{h}'(\mathbf{x}) = (\mathbf{h}_1(\mathbf{x})\otimes_{l_{1},l_{2}}^{l_O}\mathbf{h}_2(\mathbf{x})) \in V_{l_{o}}$; if we apply a rotation $r$, inputs will be transformed by $\mathbf{D}^{(l_{1})}(r)$, $\mathbf{D}^{(l_{2})}(r)$ and the output by $\mathbf{D}^{(l_{o})}(r)$, i.e.~equivariance is fulfilled. 
Concretely, the tensor product is computed using Clebsh-Gordan coefficients $C^{(l_O,m_O)}_{(l_1,m_1),(l_2,m_2)}$ as
\begin{align}
    h'^{l_O}_{m_O} = \sum_{m_1=-l_1}^{l_1}\sum_{m_2=-l_2}^{l_2} C^{(l_O,m_O)}_{(l_1,m_1),(l_2,m_2)} h_{1,m_1}^{l_1}h_{2,m_2}^{l_2}
\end{align}
We have $C^{(l_O,m_O)}_{(l_1,m_1),(l_2,m_2)}\neq 0 $ only for $l_0 \in [ |l_1-l_2|,l_1+l_2]$, thus it is a sparse tensor product. In a more concise form we write: 
\begin{align}
    \mathbf{h}'^{l_O} = \mathbf{h}_1^{l_1}C^{l_O}_{l_1 l_2}\mathbf{h}_2^{l_2}
\end{align}
where $C^{l_O}_{l_1 l_2}$ is a $(2l_O+1)\times (2l_1+1)\times (2l_2+1)$ tensor.



\medskip
\textit{SE(3)-equivariant Graph Convolution Layer} \hspace{.3cm} 
Using all the concepts introduced above, we can now define the explicit form of the convolution kernel $\kappa$ of Eq.~\ref{eq:GCNKernel}.
Denoting the edge features $\mathbf{a}_{ij} = (\mathbf{x}_j - \mathbf{x}_i)$, the kernel factorizes the effect from the radial part  $||\mathbf{a}_{ij}||$ and the direction $\mathbf{\hat{a}}_{ij}$ of the input edge feature.
Each Input rotational order $l_I$ interacts with the rotational order of the Filter $l_F$ to Output various rotational orders $l_O$. 
We have
$\kappa(\mathbf{a}_{ij}) = \varphi(||\mathbf{a}_{ij}||)^{l_O}_{l_I l_F,c} \mathbf{Y}^{l_F}(\mathbf{\hat{a}}_{ij})$.
The radial filters $\varphi^{l_O}_{l_I l_F,c}$ are implemented as Multi-Layer Perceptrons (MLPs, to be learned) that share some weights among triplets $l_O,l_I,l_F$ and channels (details in Appendix \ref{appendix:MLP}).
%Convolution is performed independently for each channel $c$: a set of filters is defined, one for each triplet $l_O,l_I,l_F$, which share the same angular part but have different radial functions $\varphi$ which are learnt.
%%
%In this case we deal with 
%discrete feature map/
% a discrete vector fields 
% $\mathbf{h}(\mathbf{x}) = \sum_{i\in\mathcal{V}} \delta (\mathbf{x}-\mathbf{x}_i) \mathbf{h}_i$ and we update them trough a convolution operation on the neighborhood defined by graph connections:
Expliciting the C-G tensor product, Eq.~\ref{eq:GCNKernel} now reads:
\begin{align}
\label{eq:convolution}
    \mathbf{h}_{i,c,l_I l_F}^{'l_O} = \sum_{j\in\mathcal{N}(i)} \varphi(||\mathbf{a}_{ij}||)^{l_O}_{l_I l_F,c} \mathbf{Y}^{l_F}(\mathbf{\hat{a}}_{ij}) C^{l_O}_{l_I l_F} \mathbf{h}_{j,c}^{l_I}
\end{align}
This operation is depicted in Fig.~\ref{fig:aggregating_and_mixing} (left part).
%The updated feature $\mathbf{h}_{i,c,l_I l_F}^{'l_O}$ at node $i$ is indexed by the channels (which are in one-to-one correspondence with the input ones) and by the triplet of $l$'s. 
At this stage, operations are performed channel-wise, but $\mathbf{h}'_{i,c}$ is a concatenation of all possible triplets, and as multiple combinations of $l_I,l_F$ can contribute to a given $l_O$, it is larger than the original feature $\mathbf{h}_{i,c}$.
For $l_{max}=3$, there are $34$ different triplets (instead of just $4$ different values of $l$).
%Since multiple combinations of $l_I,l_F$ can give a certain $l_O$, the size of the updated feature will be larger than the original one: $\mathbf{h}'$ is just a concatenation of $l$ features resulting from different triplets and channels.
%%this is because you're going from a function on S2 to a function on SO(3), but we want to compress it back.

To go back to a reduced representation, we mix together the triplets that share the same output $l_O$ with a linear layer. 
However, to let the various channel interact, we also perform channel mixing (also called self-interaction) with a linear layer.
As linear layers combine linearly, this can be expressed as a single linear layer (see right part of Fig.~\ref{fig:aggregating_and_mixing}):
\begin{align}
\label{eq:self_int}
    \mathbf{h}_{i,c}^{\text{out},l_O} = \sum_{l_I l_F,c'} w^{l_O}_{l_I l_F,cc'} \mathbf{h}_{i,c',l_I l_F}^{'l_O}
\end{align}
where all operations are now performed node-wise and independently for each $l_O$. Note that this operation fulfills equivariance since only features with the same $l_O$ are combined with weights that don't depend on $m$ (all elements inside a vector are multiplied by the same factor) thus preserving rotation properties. 
At this point we are back to our expected node feature shape, and the convolution layer can be repeated (up to a few technical details like using Batch-Norm and adding the previous layer's representation to the newly computed one, see Appendix \ref{appendix:overall_arch}).


\medskip
\textit{Interpretation} \hspace{.3cm}
Here we want to insist that the first layer especially, and to some extent the next ones, have a very clear interpretation.
Let us consider the first convolution, for which the input node features consist in two $l=0$ channels, namely the one-hot encoding of the particle type.
Since $l_I=0$, the only non-zero C-G coefficients will be the ones at $l_O=l_F=0,1,2,3$.
For simplicity of view, let us imagine that the radial filters $\varphi^{l_O=l_F}_{l_I=0,l_F, c}$, are not learned anymore, but consist of a Gaussian Radial Basis, with numerous ``soft bins'' (i.e.~numerous channels).
Then, the first layer's $(L=0)$ action is to convolve the particle density with our kernel, i.e.~to build a high-dimensional embedding of the density field in the ball of radius $d_c$ (each particle type is associated to a given density field).
Then the channel mixing step Eq.~(\ref{eq:self_int}) does not have to mix any triplet (since $l_O=l_F$) and thus simply mixes channels.
Taking the norm of this representation, one qualitatively obtains the expert descriptors described in \cite{boattini_averaging_2021} (for an exact and more detailed derivation, see Appendix \ref{appendix:Boattini}).
For a quantitative match however, one would need to use a much larger cutoff radius $d_c=5$ for building the graph and a maximum rotational order of $l_{max}=12$.
%, and mix the channels (in this simplified case, channels only match the radial basis) \textit{after} taking the norm. 

In our network, we build analogous intermediate features but we do not rush to compute the norm (the invariant feature), instead we remain at the equivariant level to combine them, keeping the computation of invariants as the last step of our network.
We believe that this difference significantly contributes to making our model much more expressive than direct invariant expert features.

%% FL: not sure I want to put this in the end...
%% Independently from these expert features, our radial filters are learnt, and we do not retain too many of them (only $4$ channels). Compared to a Gaussian Basis, we learn 4 modes and retain only that compressed information.



%Furthermore, we can weight such decompositions with  weights $\varphi(r)$, summing  from $r=0$ to $r=d_c$,  to obtain an equivariant representation of the field of density around a target particle (in a ball of radius $d_c$). This is what we do in Eq.~(\ref{eq:convolution}).



\section{Combining equivariant layers}
\label{sec:arch}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{global_arch.pdf}
    \caption{\textbf{Overall Architecture.} Arrows connecting the embedded graph features to each convolution block show that the initial information (one-hot particle types and relative positions in SH  and radial basis) is fed to each layer.}
    \label{fig:global_arch}
\end{figure}

\textit{Network} \hspace{.3cm} Our network is composed of embedding blocks for nodes and edges features followed by a series of SE(3)-equivariant convolutional layers interspersed with batch normalization connected in a Res-Net fashion and one output block, as shown in Fig. \ref{fig:global_arch}.

Here we provide a few insights on some key parts that are specific to SE(3)-aware networks.
Further details about the architecture and the training procedure are available in Appendix \ref{appendix:training_and_stuff}.


\textit{Batch Normalization} \hspace{.3cm} As often in Neural Networks, we sometimes need to perform Batch Normalization to avoid the neuron's activation to take overly large values.
However, using a usual batch normalization layer \cite{ioffe_batch_2015} separately on each entry of the hidden representations $\mathbf{h}$ would kill the equivariance property. 
Thus a modified version is implemented and applied to node features \cite{weiler_3d_2018, e3nn}. The $l=0$ features are invariant and thus can be processed as usual:

%%I cited the library, we should stress that also somewhere else.
    \begin{align}
        \frac{h^{0} - \bar{h}}{\sigma} \beta + \gamma
    \end{align}
where $\bar{h} = \langle h^{0}\rangle$ and $\sigma^2 = \langle{h^{0}}^2\rangle - \langle h^{0}\rangle^2$ with $\langle\cdot\rangle$ batch average computed with $0.5$ momentum  (keeping memory of previous batches) and $\beta,\gamma$ are learned parameters. For each piece of feature with $l\geq 0$, only the norm can be modified: 
\begin{align}
  \frac{||\mathbf{h}^{l}||}{\sigma^{l}} \beta^{l}
\end{align}
with $\sigma^{l}=\sqrt{\langle||\mathbf{h}^l||^2\rangle}/\sqrt{2l+1}$ and $\beta^{l}$ learned parameter.
In Fig.~\ref{fig:global_arch} we show where this Batch Norm is used.


\textit{Output Block} \hspace{.3cm} After the last convolution layer, the ultimate output block performs two node-wise operations.
First it extracts SE(3)-invariant features from the hidden representations. 
The norm of each directional ($l>0$) feature $h^{l} = \sqrt{\sum_{m} {h^{l}_m}^2}$ is computed, then they are all concatenated together with the 
$l=0$ features (already invariant).
At this stage the network's representation of the input is a vector of invariant features.
The second operation is to feed this vector into a linear layer 
%with weights shared over nodes 
that outputs one real value, to predict the mobility.

We note that all the layers act linearly on the node features. 
The only non-linearities of the network are hidden in the implementation of the radial part of filters. 
This limited scope of non-linearities is unusual, and is needed to preserve equivariance (as pointed out above when we describe Batch Norm).


\section{Experiments, results}
\label{sec:results}

Here we report the performance of our architecture on the glass data set mentioned above, and compare it with the previous State Of The Art (SOTA) performance model, namely the GNN presented in \cite{bapst_unveiling_2020}, and the expert approach of \cite{boattini_averaging_2021}.

\medskip
\textit{Experimental setup} \hspace{.3cm}
For each state point (4 different temperatures), we have 10 different tasks that correspond to the prediction of mobility at 10 different times. %(geometrically increasing times).
As in the works we compare with, we use the Pearson correlation coefficient as performance metric, which is invariant under shift and scale of the test labels distribution.
Thus when comparing different tasks, we at least discount the trivial effect of the dispersion of labels' values (the Loss, by comparison, is sensitive to the scale's of labels' distribution).


The network architecture and all hyper-parameter choices were optimized for a single task ($T=0.44$ and longest timescale $\tau=3\times 10^4$), using only the train and validation sets.
The resulting choices were applied straightforwardly to other tasks, thus preventing over-tuning of the hyper-parameters.
The main architecture's and hyper-parameters choices are the following. 
The number of convolution layers is $n_{(L)} = 8$.
In the input layer, the visible representation has 2 channels of $l=0$ features (the encoded particle type). At each layer $L>0$ the internal or hidden representation $\mathbf{h}_{i}^{(L)}$ (for particle $i$ at layer $(L)$) has 4 channels, with a maximum rotational order $l_{max} = 3$. 
These choices arise from striking a balance between over- and under-fitting, under our compute-time and memory budget constraints.

Note that we perform a different train-test split with respect to \cite{bapst_unveiling_2020}, which does not explicitly use a test set.
Here, for each state point, $400$ configurations are used for training, $320$ for validation and $80$ for the final test. 
% In all state points others than the one where we performed the architecture search, the validation set is only used to retain the best model over all epochs, and we note that the validation loss remains stable for a large amount of epochs (see Fig.~\ref{fig:loss_vs_epoch}, in the Appendix) 

%actually the last epoch's model does not differ much from it, in validation accuracy (see Fig.~\ref{fig:loss_vs_epoch}, in the Appendix).
%This way we prove that reported results don't show any performance bias due to overfitting of the validation set in the hyper-parameter optimization.
In Appendix \ref{appendix:training_and_stuff}, we provide more details about the training of the model.
%performance and loss as function of the epochs.
% TODO? as well as some qualitative observation on the predicted vs. ground truth data (scatter plots, heat-maps).
%we show a scatter plot of $y^{predicted}$ vs $y^{Ground Truth}$, to give a better sense of what $\rho=0.71$ means.
%More details of the training of the model (performance and loss as function of the epoch) are also shown in Appendix.



\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Time_Temp.pdf}
    \caption{
    \textbf{SE(3)-GNN outperforms conventionnal GNN across time scales and temperatures.}
    (top) Correlation coefficient $\rho$ as a function of the time scale probed, for $T=0.44$.
    We observe a dip at intermediate times, but less pronounced than SOTA.
    (bottom) Dependence on Temperature, for each state point $\tau=\tau_\alpha (T)$. Here the comparison with \cite{boattini_averaging_2021} is not available.
    Overall, at all points studied our model has a higher correlation coefficient than the baseline, always by a significant margin.
    %(larger than fluctuations) and sometimes rather impressive (for early times especially).
    }
    \label{rho_vs_time_and_temp}
\end{figure}



\begin{table*}
\begin{tabular}{|l |p{0.09\textwidth} p{0.07\textwidth} p{0.07\textwidth} p{0.07\textwidth} p{0.07\textwidth} p{0.07\textwidth} p{0.15\textwidth} |l|}
\hline
\textbf{Model} & $\tau = 1\cdot 10^3$ & $ 3\cdot 10^3$ & $ 5\cdot 10^3$ & $ 6\cdot 10^3$ & $ 9\cdot 10^3$ & $ 2\cdot 10^4$  & $ 3\cdot 10^4$ & \textbf{Num. parameters}\\
\hline
GNN (Bapst 2020) & $0.584$ & $0.613$ & $0.633$ & $0.639$ &  $0.647$ & $0.653$ & $0.656$ & $70\,721$\\
SE(3)-GNN $l_{max} = 3$ & $\mathbf{0.683}$ & $\mathbf{0.706}$ & $\mathbf{0.713}$ & $\mathbf{0.725}$ &  $\mathbf{0.725}$ & $\mathbf{0.709}$ & $\mathbf{\in[0.7095, 0.7169]}$ & $23\,177$\\
SE(3)-GNN $l_{max} = 1$ & $0.635$ & $0.660$ & $0.672$ & $0.690$ & $0.696$ & $0.689$ & $0.676$ & $5\,381$\\
Ridge (Boattini 2021) & $0.566$ & $0.601$ & $0.615$ & $0.623$ & $0.627$ & $0.630$ & $0.625$ & $\mathbf{\sim1000}$\\
\hline
\end{tabular}
\caption{\textbf{SE(3)-GNN outperforms SOTA despite having a smaller number of parameters.} 
Numbers display the Pearson correlation coefficient $\rho$ between predicted and true mobilities for different timescales $\tau$ at $T=0.44$. 
For comparison, ridge regression from \cite{boattini_averaging_2021} is also reported. 
Due to lack of time, the confidence interval is only available for one task
(see Appendix \ref{appendix:errors} for details).
% Due to time limitations, the error bar 
% %(not visible in the plot due to small scale of fluctuations depending on initialization of weights) 
% was computed only at one point, $\tau=3\times 10^4$
Notice that our model is able to beat the SOTA also with $l_{max} = 1$ at a significantly reduced number of learnable parameters.
\label{performance_table}
}
\end{table*}



\medskip
\textit{Results} \hspace{.3cm}
In Fig.~\ref{rho_vs_time_and_temp} and table \ref{performance_table}, we show our main result, i.e.~that our model consistently outperforms the SOTA \cite{bapst_unveiling_2020} by $5$ to $10\%$ depending on the task, in terms of test accuracy, despite having much fewer learnable parameters.
For comparison, we also report results from the expert approach \cite{boattini_averaging_2021}, which nearly matches the GNN baseline.


The convolutional layers that we stack, as described in section \ref{sec:arch}, effectively build a \textit{representation} describing the environment of each particle, that is then regressed to our target label, the mobility.
To probe the robustness (generalization ability) of this representation, we perform a transfer learning experiment across temperatures: we train the full model at a single temperature, then  freeze the parameters in all layers except from the very last one. %(which performs node-wise linear regression). 
At all other temperatures we then re-train (the last layer only) and test the model. 
Let's recall that the operation performed just before our last layer is to compute the norm of the representation $\mathbf{h}^{(L=7)}$ to obtain a set of invariant features: for each $l=1,2,3$ and each  $c=1,2,3,4$ we compute the norm of $\mathbf{h}_{i,c}^{l,(L=7)}$, while for $l=0$ the feature $\mathbf{h}_{i,c}^{l=0,(L=7)}$ is already invariant, so taking the norm is useless.
Thus, the representation which summarizes a particle's environment and is transferred across temperatures consists in exactly $16$ values.
The final (linear) layer simply performs linear regression between these 16 values and the target label, here the mobility.
%In other words, the function  $\Vec{f}_{i,\theta}(\{\mathbf{x}_j\}_{j=1,\dots,N})$ that describe the structure around one particle $i$ with parameters $\theta$ learned once and fixed while at different temperatures we train only the linear regression task from these features to the scalar mobility of each particle. 
%If the features $\Vec{f}$ are robust, the model generalizes well across state points. 
In Fig.~\ref{transfer_learning} we show that the model generalizes well across temperatures (full lines), with an accuracy that decreases when the difference in train and test temperatures increases.

Furthermore we perform an even simpler check: without re-learning anything, we take a model trained at a temperature and test it directly at the other temperatures (dashed lines). 
Since the performance metric $\rho$ is insensitive to global shift and scale of the labels' distribution, there is no need to perform such global re-shift and re-scale when comparing values of $\rho$ (when comparing output mobilities, such a re-scaling would be needed however).
Here again we still obtain good performances, significantly better than the SOTA: compare with the Fig.~3d of \cite{bapst_unveiling_2020}.
Overall, this shows that our learned representation is not strongly training-temperature-dependent, a desirable property in the perspective of learning a structural order parameter.



\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{Temp_generalization.pdf}
    \caption{\textbf{Transfer-learning between different temperatures.} Each model is trained once at one state point and tested on the remaining ones. The timescale of mobilities is fixed at $\tau = \tau_\alpha(T)$. Best generalization performances are observed for model trained at $T=0.50$. }
    \label{transfer_learning}
\end{figure}

%We have shown that the model performs well across temperatures and time scales, i.e. across physical choices, representing different tasks.
All our results rely on the embedding of the input data into the Spherical Harmonics basis and on the built-in equivariance of convolution layers. 
So one may expect the choice of the cutoff rotational order $l_{max}$ to be crucial.
%that constrains internal representations to respect the symmetries of the system.
In Fig.~\ref{rho_vs_ellmax} we show it is indeed the case: we build architectures that vary only by their $l_{max}$ value and measure the performance $\rho$ in each.
The biggest gap in performance is indeed observed between purely isotropic, scalar features ($l_{max} = 0$) versus directional ones ($l_{max} = 1$). 
Further increasing $l_{max}$ provides a finer angular resolution, but we observe that the accuracy tends to saturate. 
We cannot go above $l_{max} = 3$ due to memory constraints: as the rotational order increases, the number of activations of neurons to keep track of grows exponentially with $l_{max}$ (while it grows linearly with the number of edges, with the batch size, and the with size of the hidden layer in the radial MLP).

%However, the choice of architecture is not everything, and to learn a good representation, the network needs to be expressive enough (big enough, in a sense).
%Here we study how the network size affects performance, by considering the dependence on $l_{max}$ (the highest rotational order used in our Spherical Harmonics projection),
%or on the number of layers in the network $N_{layer}$,
%or on the number of channels, $N_{channels}$.
%These ablation studies are performed for the task at $T=0.44$, $\tau=taumax$.

%In Fig.~\ref{rho_vs_ellmax}, we show that the performance strongly depends on the level of $\ell_{max}$.
%We could not go any further due to memory constraints: as the rotational order increases, the number of activations of neurons to keep track of grows exponentially with $\ell_{max}$ (and linearly with the number of edge, with the batch size, and the last hidden layer of the radial MLP).
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{Lmax.pdf}
    \caption{
    \textbf{Rotational order matters.} Performance $\rho$ as a function of $l_{max}$, the maximum rotational order retained in the Spherical Harmonics embedding of the edge feature. Results shown for $T=0.44$ and $\tau=\tau_\alpha$}
    \label{rho_vs_ellmax}
\end{figure}



\section{Discussion}
\label{sec:discussion}

As is well known in physics, enforcing symmetries is key to reducing the number of degrees of freedom in the description of a physical problem.
In the ML vocabulary, this translates as building features that respect the proper symmetries. 
We achieve this by combining two ingredients.
First, the proper embedding by Spherical Harmonics builds a description of the neighborhood that is both easy-to-rotate and compact, since it is a decomposition in rotational order, akin to frequency decomposition, a much more efficient representation than directly discretizing angles.
Second, layers combine the hidden representations $\mathbf{h}_i^{(L)}$ in a way that preserves equivariance up to the last layer, a choice that compared with rushing to invariant features, guarantees maximum expressivity for the network.
We believe that these two ingredients are key to building a good representation: we surpass the SOTA and achieve better generalization across tasks, while using much fewer learnable parameters.
%[and in terms of building a glass order parameter, it would be key]

% Left for future work: checking the physical-ness of our predicted field: does it have the proper space-time correlations ?

%Additionally our architecture doesn't need data augmentation by random rotations, as done in \cite{bapst_unveiling_2020}, which is costly and inefficient.
While we cannot claim our network to be fully interpretable, our first hidden feature $\mathbf{h}_i^{(0)}$ is clearly interpreted as a representation of the field of density around node $i$, in a way that relates with the rotation-invariant features introduced in \cite{boattini_averaging_2021}.


%Equivariant output, like Sylvain's: possible in our archi.



The present work focuses on building a representation for glassy materials, but we would like to stress that progress in this area is intimately connected to progress made in other application areas, whenever the input data consists in particles living in 3D space (as in \textit{ab initio} or effective potential approximations, crystalline materials or amorphous materials' properties prediction), regardless of the precise task or output label.
%%FL: note: I wanted to start with CNNs dealing with materials but actually most of the early good works are GNNs, not CNNs.
%Simultaneously, Graph Neural Networks (GNNs) entered the game, and are obviously an effective way to deal with node-index permutation equivariance.
While each of these application tasks may need fine-tuning of the architecture to perform well, we believe that they are essentially different facets of the same problem, that is, efficient learning of representations for sets of particles in space. 
%Taking inspiration from novel architectures developed for different tasks can only foster progress in one's area.
%More concretely, a possibility for future work is to train a backbone model on various tasks, e.g. amorphous and crystalline materials prediction (as is already done in molecular properties predictions).


Although we performed neural architecture search by hand somehow to achieve these results, we did not yet explore the space of possibilities thoroughly, and there is ample room for improvement.
For instance, one could include attention mechanisms, or bottleneck-styled architectures (varying $l_{max}^{(L)}$ across layers), or simply using larger $l_{max}$ (would require multi-GPU training).
Besides, one could mix different tasks, beyond the obvious multi-time and multiple particle-type regression and mixing of temperatures of the same glass-forming liquids:
one could mix tasks between various glass-forming liquids and crystalline materials, together with other amorphous material's input, thus requiring the backbone of the network to generalize even more strongly. This kind of pre-training strategy has been shown \cite{liu2021pre, fang2022geometry,zhang2022protein} to be effective to improve robustness. 

% Left for future work: perform multi-variate regression on time scales, improve transfer-learning across temperatures.
% Regress also the B's, simultaneously to the A's.

%Left for future work: Use data augmentation for robustness (e.g. perturbing the input positions).

% Left for future work: Boost the tails of P(y) for more physical predictions ? (or we keep  this idea for us in a future-soon paper)

Whatever improvements one could think of, we believe that the SE(3)-GNN is the right framework to be developed.
Indeed, it seems in line with the recent history of neural architecture design: while the CNN architecture has been a game-changer thanks to its enforcement of translation-equivariance by design, the GNNs then enforced the node permutation-equivariance by construction, and SE(3)-GNNs now additionally enforce the rotation-equivariance, leveraging all the symmetries at hand.


\section*{Acknowledgments}

We are thankful to \cite{bapst_unveiling_2020} for sharing their dataset publicly, a good practice that should be fostered in the physics' community.
\\
We are thankful to the e3nn library developpers \cite{e3nn} for developing their E(3)-GNN library, which should give a huge boost to work in this area.
\\
This work is supported by a public grant overseen by the French National Research Agency (ANR) through the program UDOPIA, project funded by the ANR-20-THIA-0013-01.

%\tableofcontents


\bibliographystyle{alpha}
\bibliography{GNN.bib, MLglass.bib, ManualEntries.bib}



\appendix

\section{Training details, Training curve and time/energy cost}
\label{appendix:training_and_stuff}

Here we provide more details about our architecture, hyper-parameters and how the model is trained.


%Embedding: node features are one-hot encoded, thus the first node representation $\mathbf{h}^0_i \in \{[0,1],[1,0]\}$ has $2$ channels at $l=0$. 
%The edge features $\mathbf{a_{ij}}$ are factorized between their radial part and their angular part which is embedded in an equivariant basis (Spherical Harmonics). % to build the convolution filters. 

\subsection{Radial MLP}
\label{appendix:MLP}

As mentioned in the main text, the radial MLPs are the only part of the network with non-linearities. They implement the radial dependence of convolution filters $\varphi^{l_O}_{l_I l_F,c}(||\mathbf{a}_{ij}||)$, thus they take as input the norms of relative node positions. Before being fed to the MLPs each norm is expanded from a real value to an array through an embedding.
%in the radial basis. 
Here we use a Bessel basis embedding:
\begin{align}
    B_n(r) = \sqrt{\frac{2}{r_c}} \frac{\sin{(n \pi\frac{r}{r_c})}}{r}
\end{align}
with $r_c = d_c=2$ and $n=1,2,\dots,N_b=10$ is the number of roots of each basis function. 
Other embeddings could be for instance a Gaussian basis (with cutoff), which would act as a kind of smooth one-hot encoding of the value $r$. In practice, the Bessel basis has better generalization properties.%, but performs a less intuitively interpretable but similar role.

% We denote $\mathbf{d}_{ij} = (B_1(||\mathbf{a}_{ij}||),\dots,B_{N_b}(||\mathbf{a}_{ij}||))$ this  radial embedding \textbf{but since we never use notation d, we do not introduce this notation}
%and it is fed to the $\varphi$. The direction $\hat{\mathbf{a}}_{ij} = \mathbf{a}_{ij}/||\mathbf{a}_{ij}||$ is projected on a SH basis giving the $\mathbf{Y}^{l_F}$ of eq. \ref{eq:convolution}.

%Convolution: this block implements the operations of equations \ref{eq:convolution} and \ref{eq:self_int}. 
 
The embedded input is processed through an MLP with layers sizes $(N_b,16,n_{comb})$ and ReLu non linearities. 
The output size $n_{comb}$ is the number of possible triplets, times the number of channels.
%\textbf{FL: don't we learn each MLP independently in each channel ? I would erase ``times the number of channels''}. 
%Each output neuron represents a different radial function, one for each $(l_O,l_I,l_F,c)$ combination. 
We also use BatchNorm (BN) \cite{ioffe_batch_2015} and Dropout (with rate $p=0.5$) in this MLP to stabilize training and reduce overfitting.
In summary, for each combination of triplets and channels $(l_O,l_I,l_F,c)$, we have a real output $\varphi^{l_O}_{l_I l_F,c} = \sigma(W_{n_{comb}, 16} Dropout(BN(\sigma(W_{16,10} B(||\mathbf{a}_{ij}||)))))$, where the $W$'s are weight matrices and $\sigma(z)=max(0,z)$.
There are bias parameters, which are not displayed here.
Note that up to the layer of $16$ neurons, the MLP is the same for all triplets and channels, only the last linear layer introduces different weights for each combination.
In total, the MLPs of our network (across all layers) account for a number of $18\,936$ learnable parameters: in each layer $L>0$ we have one radial MLP of size $(10,16,144)$ with $2656$ parameters, for the layer $L=0$ the MLP is of size $(10,16,2\times4)$ with $344$ parameters. 
The other main source of learnable parameters in the Network is the part of mixing the channels (right part of fig.~\ref{fig:aggregating_and_mixing}), which accounts for $4064$ learnable parameters: $144\times4 = 576$ for $L>0$ layers and $8 \times 4 = 32$ for the $L=0$ layer.  

% In this kind of block reside most of the learned parameters of the model: the weights of the MLP which are shared edge-wise and the ones of the linear combination in the self-interaction step which are shared node-wise.

\subsection{Overall Architecture}
\label{appendix:overall_arch}

Steerable group convolution blocks are connected in a way similar to the widespread Res-Net \cite{he_deep_2015} structure, which is known to speed up the training. The output of each layer is added to its own input and it is passed through one batch-normalization block to the next layer: see Fig. \ref{fig:global_arch}. 
After the final convolution, node features are passed to the output block acts where invariants are computed and combined through a linear layer. 
This architecture choice is found empirically to be the most stable at train time. Our architecture is built and trained in the framework of PyTorch Geometric \cite{Fey/Lenssen/2019} which handles all the generic graph operations. All the SE(3)-related operations (SH embedding, C-G tensor product, equivariant batch-normalization) are integrated in this framework thanks to the \textit{e3nn} library \cite{e3nn}.

\subsection{Training strategy}
\label{appendix:training}


\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{loss.pdf}
    \caption{\textbf{Loss and $\rho$ vs epoch.} Training of our main model performed at $T=0.44$, $\tau = 3\times10^4$ (longest timescale available). Vertical dashed lines locate the epochs at which the learning rate is divided by $2$. }
    \label{fig:loss_vs_epoch}
\end{figure}


In Fig.~\ref{fig:loss_vs_epoch} we display one learning curve (as function of iterations, epochs).
Each epoch is a sweep over the entire 400 samples dataset (each sample represents $N=4096$ atoms).

For training, we use Adam optimizer with initial learning rate $\gamma = 10^{-3}$, moments $\beta_1 = 0.95, \beta_2 = 0.999$ and weight decay $\lambda = 10^{-7}$. We also add a learning rate scheduler that divides $\gamma$ by 2 at several epochs as shown by the vertical dashed lines in Fig. \ref{fig:loss_vs_epoch}. 

Each training of our model takes approximately 20 hours on a A100 Nvidia GPU. %The Global memory (on GPU) consumption peaks at ??GB. 
This represents approximately $4$ kWh per training, and in France, an equivalent of $300$ gCO2 (we use a number of $75$ gCO2/kWh, after considering quickly EDF's optimistic figure of $25$ g/kWh or RTE's more detailed daily numbers, which oscillate mostly between $50$ and $100$, depending on the season and time of day). 
We did not include the footprint of manufacturing of the GPU and other infrastructure, which is generally estimated to be one half of the IT-related CO2 emissions (the other half being running the infrastructure).


\section{Measures of the errors}
\label{appendix:errors}

There are two sources of uncertainty when measuring the accuracy of a given model (a model is a ML architecture).

The first, that one usually reports, comes from varying the initialization of the weights in the Neural Network (NN). Indeed, as gradient descent only finds a local minimum, the learned weights depend on the (random) initialization. 
To account for this variability, one typically re-trains the network multiple times, each time with a different initial random seed.
Due to lack of time, we do not compute these error bars thoroughly at all timescales and temperatures, but we did a focused study at $T=0.44$ and $\tau=3\times 10^4$ (longest timescale available), see Table \ref{table:seeds}. From this, we conclude that the true accuracy lies approximately within the range $[0.7095, 0.7169]$ (a spread of $0.0074$), i.e. the accuracy is $0.713 \pm 0.004$


\begin{table*}
\begin{tabular}{|l |p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} |}
\hline
Seed number & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ & $10$\\
\hline
 $\rho$ & $0.7122$ & $\mathbf{0.7095}$ & $0.7135$ & $0.7099$ &  $0.7128$ & $0.7120$ &  $\mathbf{0.7169}$ & $0.7117$ & $0.7128 $ & $0.7165 $\\
 %% we could sort the values for convenience, no ?
% 0.7095 
% 0.7099 
% 0.7117 
% 0.7120 
% 0.7122 
% 0.7128 
% 0.7128 
% 0.7135 
% 0.7165
% 0.7169 
\hline
\end{tabular}
\caption{\textbf{Accuracy across random initializations.} Each seed of the random number generator determines a different initial set of weights for the model. We report the test accuracy of each model independently trained at $T=0.44$, $\tau =3\times10^4$. Best and worst performance are highlighted.
%, corresponding to a spread of $0.0074$
\label{table:seeds}
}
\end{table*}



The second source of uncertainty can come from the test set itself (being too small). A way to assess the test set robustness, i.e. how confident we can be that our accuracy would be in the same range, for an ideal infinite test set, is to bootstrap the test set. We do not recall the classic bootstrap idea here.
The test set is made of $80$ configurations of $N=4096$ atoms each, for which $80\%$ are A particles (for which we regress). this results in $n=80 * 4096 * 0.80= 262144$ samples. We sample with replacement $n$ out of these $n$ samples, and measure $\rho$ on this ``new'' test set. Repeating this operation 5000 times, we obtain the empirical distribution $P(\rho)$ shown in Fig.~\ref{fig:bootstrappedTestSet}.
From this, we conclude that for this particular seed the true accuracy is $0.712 \pm 0.001$

We find that the fluctuations coming initialization of the Network are very much comparable (same order of magnitude) to those due to the finite-size test set fluctuations (evaluated by bootstrap).


\begin{figure}
    \centering
    \includegraphics[width= 0.45\textwidth]{boot_p.pdf}
    \caption{\textbf{Distribution of accuracy over bootstrap samples.} Train/Test performed at $T=0.44$ and $\tau=3\times 10^4$. Number of re-samplings $N_{res}=5000$.}
    \label{fig:bootstrappedTestSet}
\end{figure}


\section{Reproducing expert features (Boattini 2021)}
\label{appendix:Boattini}

Here we show how a network with one SE(3)-convolution layer is able to reproduce some good expert features  used for glassy liquids. We start by considering embedded node features $h_{i,c} = \delta_{t_i,(1-c)}$ where $t_i$ is the type of particle $i$: we have only two channels at $l=0$ that we extend from $n_{ch} = 2$ $\rightarrow$ $n_{ch} = 2*N_{b}$ just by replicating them $N_{b}$ times. We will denote the replicated  $h_{i,c}$ as $h_{i,c,r}$ since in spirit, each copy of the two channels (one-hot of the particle type) will correspond to one radial basis function. Then the convolution operation reads: 
\begin{align}
    \mathbf{h}_{i,c,r,l_I=0,l_F}^{'l_F} = \sum_{j\in\mathcal{N}(i)} \varphi(||\mathbf{a}_{ij}||)^{l_F}_{l_I=0, l_F,c,r} \mathbf{Y}^{l_F}(\mathbf{\hat{a}}_{ij}) h_{j,c,r}
\end{align}
We choose a Gaussian radial basis function $B$ (instead of Bessel): $\varphi(||\mathbf{a}_{ij}||)^{l_F}_{l_I=0 l_F,c,r} = B(||\mathbf{a_{ij}}||)_r$.
%where $B$ is the radial embedding in a Gaussian basis.
Then if we focus on $l_F = 0$, since $Y^{0}(\mathbf{\hat{a}}_{ij}) = 1$ we have: 
\begin{align}
    h_{i,c,r}^{'0} = \sum_{j\in\mathcal{N}(i)}  e^{-\frac{(||\mathbf{a}_{ij}||-r)^2}{2\delta}} \delta_{t_j,(1-c)}
\end{align}
which correspond to $G_i^{(0)}(r,\delta,s)$ in \cite{boattini_averaging_2021}. Note that we require also no channel mixing at $l=0$. For $l_F > 0$: 
\begin{align}
    h_{i,c,r}^{'l_F} = \sum_{j\in\mathcal{N}(i)} \mathbf{Y}^{l_F}(\mathbf{\hat{a}}_{ij}) e^{-\frac{(||\mathbf{a}_{ij}||-r)^2}{2\delta}} \delta_{t_j,(1-c)}
\end{align}
which, after a channel mixing step that sums over $c$ (mixing different particle types), correspond to $q_i^{(0)}(l,m,r,\delta)$ in \cite{boattini_averaging_2021}. By computing invariants from these features through their norm, we recover exactly $q_i^{(0)}(l,r,\delta)$ from \cite{boattini_averaging_2021}. By contrast, in our model we do not compute invariants after one layer, we keep equivariant features and let them interact over multiple layers in order to increase the expressivity.

% \subsection{Further ablation studies (FL: very optional !!)}

% As often in ML works, one wants to check whether architecture or hyper-parameter choices are optimal, and whether results are robust against small changes in these choices.

% Here we report on two main components of our architecture: number of graph-convolutional layers, and number of channels.


% In Fig.~\ref{rho_vs_Nlayer}, we show that GNNs are useful, beyond the initial lifting convolution: indeed, a single-layer or 2-layer GNN significantly underperforms compared to our 7-layer model.
% However, we see that the gain from 5 to 7 layers is not significant, indicating that adding more layers would likely not improve results.
% Note that the number of network parameters grows essentially linearly with the number of layers.
% \begin{figure}
%     \centering
%     \includesvg[width=0.4\textwidth]{equivariance.svg}
%     \caption{Performance saturates with Network Depth. Performance $\rho$ as a function of $N_{layer}$, the number of layers in the GNN (not counting the lifting convolution). The first few layers are critical, but as we add more, their marginal added value shrinks to 0.
%     }
%     \label{rho_vs_Nlayer}
% \end{figure}


% In Fig.~\ref{rho_vs_Nchannel}, we show that it is critical to have more than 1 channel, but that as we reach 4 channels, the gain from adding more becomes negligible.
% Note that the number of network parameters grows with the number of channels, with a linear and a quadratic term: the linear one comes from the radical MLP, where operations happen channel-wise; while in the mixing of channels, the number of weights grows as $N_{channel}^2$.
% \begin{figure}
%     \centering
%     \includesvg[width=0.4\textwidth]{equivariance.svg}
%     \caption{Performance saturates with number of Channels. Performance $\rho$ as a function of $N_{channels}$, the number of channels in the features $h$. The first few channels are useful, but as we add more, their marginal added value shrinks to 0.
%     }
%     \label{rho_vs_Nchannel}
% \end{figure}




\end{document}

\subsection{OPTIONAL - Qualitative glimpse at the results}


% TODO: later !! (another month !) Compute correlations ($\chi_4$) on the field of predicted mobilities

OPTIONAL: ?? qualitative comparison of snapshots? Extract the growing length-scale?


%\begin{comment}

% \section{OPTIONAL - Regular vs Steerable G-Conv}

% \label{appendix_RegularVsSteerable}
% Given two feature maps: $f^{in} \in \mathbb{L}_2(H)$ and $f^{out} \in \mathbb{L}_2(H')$ with $H,H'$ homogeneous spaces of $G$, the only linear operator $\mathcal{K}: \mathbb{L}_2(H) \rightarrow \mathbb{L}_2(H')$ mapping between the two maps to be equivariant with respect to $G$ is of the Group-Convolution form as showed in \cite{bekkers_b-spline_2021}. Without going to deep into the formalism of this theorem, let's look at how it reads in some cases of interest. If $H=\mathbb{R}^3$ and $H'=SE(3)=\mathbb{R}^3 \times SO(3)$ then:
% \begin{align}
%     [\mathcal{K}f](\mathbf{x},\mathbf{R}) = \int_{\mathbb{R}^3} k(\mathbf{R}^{-1} (\mathbf{x}-\mathbf{x}')) f(\mathbf{x}')\text{d}\mathbf{x}'
% \end{align}

% with $\mathbf{x}\in \mathbb{R}^3$ and $\mathbf{R}$ operator of rotations in $SO(3)$. This is what is commonly referred to as ``Lifting convolution'' because it lifts the map domain $\mathbb{R}^3$ to the group $SE(3)$. It consists just in convolving the input map with a kernel which is rotated and translated thus resulting in an output domain which has 3 additional dimensions (parametrized by different angles of rotations). Usually the lifting convolution is followed by a series of regular group convolutions where input and output domain are the same and coincide with the group: $H=H'=SE(3)=G$
% \begin{align}
% \label{eq:regular_g_conv}
%     [\mathcal{K}f](g) = \int_{SE(3)} k(g^{-1}g') f(g')\text{d}g'
% \end{align}

% with $g=(\mathbf{x},\mathbf{R})$.
% The major drawback of this approach is that it requires discretization over the space of rotations, thus it results in a costly and not exact procedure. [cite works with regular G-Conv]

% An alternative way consists in expanding the co-domain of the features maps, instead of the domain, through a Fourier Transform (FT) on the space of the group:
% \begin{align}
%     \hat{f}(\mathbf{x}) = \mathcal{F}_{SO(3)}[f(\mathbf{x},\mathbf{R})]
% \end{align}

% with $\hat{f}: \mathbb{R}^3 \rightarrow V_{SO(3)}$. Thanks to Fourier Shift Theorem, the operator $\mathcal{K}$ in this new space becomes much simpler, there's no more need to convolve on $SO(3)$ since $\widehat{(k\star f)}_{SO(3)} = \hat{f} \cdot \hat{k}^T$. Thus Equation~\ref{eq:regular_g_conv} becomes:
% \begin{align}
%      [\hat{\mathcal{K}}\hat{f}](\mathbf{x}) = \int_{\mathbb{R}^3} \hat{f}(\mathbf{x}') \hat{k}^T(\mathbf{x}-\mathbf{x}')\text{d}\mathbf{x}'
% \end{align}

% This method allows to have exact equivariance at the price of reducing the angular frequency (fixed number of Fourier components, see next section).
% %\end{comment}


The present work focuses on building a representation for glassy materials, but we would like to stress that progress in this area is intimately connected to progress made in other applications where the input data consists in particles living in 3D space, regardless of the precise task or output label.
Indeed we recall that initially, Deep Convolutional Neural Networks (CNNs) were developed (for \textit{ab initio} or effective potential approximations \cite{zhang_deep_2018}, for crystalline materials' prediction \cite{li_convolutional_2021} or amorphous materials' prediction \cite{swanson_deep_2020}).
%%FL: note: I wanted to start with CNNs dealing with materials but actually most of the early good works are GNNs, not CNNs.
Some of these approaches already considered the issue of defining rotational symmetry-aware features, with the notable SchNet \cite{schutt2017schnet, schutt_schnet_2018} that already considers rotation-equivariant features. The problem of particle permutation-equivariance was also addressed early in the point cloud learning literature \cite{ravanbakhsh_deep_2016, zaheer_deep_2018}.
Within the Convolutional Neural Networks approach (CNNs), which was the ``mainstream'' Deep Learning, the search for CNN architectures dealing with 3D particle data fully matured with Spherical CNNs in 2018 \cite{cohen_spherical_2018,  weiler_3d_2018}, that define generic rotation-equivariant features and allow the network to combine them efficiently.
Simultaneously, Graph Neural Networks (GNNs) entered the game, and are obviously an effective way to deal with node-index permutation equivariance.
While each application task mentioned above may need fine-tuning of the architecture to perform well, we believe that these various tasks are essentially different facets of the same problem, that is, efficiently learning representations for sets of particles in space. 
Taking inspiration from novel architectures developed for different tasks can only foster progress in one's area.
%More concretely, a possibility for future work is to train a backbone model on various tasks, e.g. amorphous and crystalline materials prediction (as is already done in molecular properties predictions).

