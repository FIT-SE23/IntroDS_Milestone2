\documentclass[notitlepage,twocolumn,prx,tightenlines,superscriptaddress,showpacs,floatfix]{revtex4}

\usepackage{amsmath,amssymb,amsfonts,latexsym,mathrsfs}
\usepackage{comment}
\usepackage{svg}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue]{hyperref}
%\usepackage[mathcal]{euscript}
\usepackage{graphicx}
%\usepackage{epsfig}
\usepackage{color}
\usepackage{booktabs}
%\usepackage[activate=normal]{pdfcprot}  % richtiges character protruding

%\usepackage{psfrag}
%\usepackage{txfonts,pxfonts,wasysym,}
%\usepackage{txfonts}
%\usepackage{wasysym}
%\usepackage{pifont}

\usepackage[caption = false]{subfig}
%\usepackage{subcaption}

%\usepackage{soul}  %% allows for the command \st{}  to cross some text.

%%\bibliographystyle{apsrev}
%%\setlength\parindent{0pt} % sets indent to zero
%%\setlength{\parskip}{8pt} % changes vertical space between paragraphs

%% == Adresses =======================

% == Shortcuts =======================
\newcommand{\ie}{\textit{i.e.~} }
\newcommand{\etal}{\textit{et~al.~} }
\newcommand{\eg}{\textit{e.g.~} }

\newcommand{\md}[1]{\left|#1\right|}
\renewcommand{\d}{{\rm d}}
\renewcommand{\d}{\ensuremath{\mathrm{d}}}
%%\addtolength{\parskip}{1em}
\newcommand{\moy}[1]{\left\langle  #1 \right\rangle }
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}

\newcommand{\tauaffil}{Université Paris-Saclay, CNRS, INRIA, Laboratoire Interdisciplinaire des Sciences du Numérique, TAU team,  91190 Gif-sur-Yvette, France}


\begin{document}


\title{Rotation-equivariant Graph Neural Networks for Learning Glassy Liquids Representations}
\author{Francesco Saverio Pezzicoli}
\affiliation{\tauaffil}
\author{Guillaume Charpiat}
\affiliation{\tauaffil}
\author{Fran\c{c}ois P. Landes}
\affiliation{\tauaffil}

\begin{abstract}
Within the glassy liquids community, the use of Machine Learning (ML) to model particles' static structure is currently a hot topic. 
The state of the art consists in Graph Neural Networks (GNNs), which have a great expressive power
but are heavy models with numerous parameters and lack interpretability.
%% remark: the current SOTA is still GNN based, but is maybe Shiba rather than bapst..
Inspired by recent advances in the field of Machine Learning group-equivariant representations,
we build a GNN that learns a robust representation of the glass' static structure by constraining it to preserve the roto-translation (SE(3)) equivariance.
We show that this constraint not only
significantly improves the predictive power 
but also 
improves the ability to generalize to unseen temperatures 
while allowing to reduce the number of parameters.
Furthermore, interpretability is improved, as we can relate the action of our basic convolution layer to well-known rotation-invariant expert features. % which are easily expressible with a single layer of our network. 
Through transfer-learning experiments we demonstrate that our network learns a robust representation,
which allows us to push forward the idea of a learned glass structural order parameter.
\end{abstract}


% Abstracts should concisely summarize the subjects, conclusions, and results of the manuscript. In the abstracts of experimental papers, specify the quantities measured and objects studied and clearly describe the experimental conditions. Avoid coined words and unexplained acronyms. Abstracts should be self-contained and should not include footnotes or citations to references, as abstracts are reprinted in abstracting journals and databases where such information is not useful. Note that Comments and Replies submitted to PRL do not require abstracts.

%Recently, there have been many Machine Learning (ML) based attempts at predicting future mobility of glassy liquids from their static structure. 

%explaining the difference in expressive power between these two choices.
%explain the difference between equivariance and invariance and why each level of representation ought to be equivariant with respect to the input, for maximum expressivity.

\maketitle

% \section*{Popular summary} % TODO


% Popular Summary: Physical Review X requires authors to submit a nontechnical summary that conveys the context, the essential message(s), and the significance of the work to all readers. The summary should be concise (approximately 250 words), readable, objective, and have broad appeal. Please avoid including mathematical expressions.

% When writing the Popular Summary, imagine how you would explain your work to a junior physics undergrad: someone familiar with physics fundamentals but likely unfamiliar with your area of research. Step back from the details (expert readers can always read the paper) and focus more on the essence of your work. Depending on the paper, that essence may be a new paradigm, the discovery of some phenomenon, a new technical milestone, or a new methodological capability. The tone can be casual and conversational. Be careful with jargon—avoid it if possible, but if a technical word or phrase is essential, please make a conscious effort to explain it using plain language.

% We recommend a three-paragraph structure. The first paragraph provides the “big picture” context and tells the reader why your investigation is worth doing. The reader should come away knowing what problem you’re trying to solve and the main point of what you found. Keep the wording concise—don’t make the reader wade through lots of background before revealing the central point of the paper. Think of this paragraph as a summary within a summary—a reader could read just this paragraph and have a sense of what you did and why it’s useful.

% The second paragraph fleshes out the most essential details. This is a good place to add extra background and context and to tell the reader how you went about your work and, when appropriate, the new qualitative physical insights you gained from the detailed results. But stay focused on the essentials: The reader doesn’t need to know everything you did, just a “taste” of how you arrived at your results and what your results teach all readers.

% The third paragraph provides the reader with an outlook to the future. In just a sentence or two, tell the reader where you see this research going or what a logical next step might be.

% Submit the popular summary when you first submit the manuscript to minimize any processing delay, although it is acceptable to submit it at a later stage. 

%% TODOs : over the whole text:
% l or \ell ?
% more italics here and there in the body of the text
% grey scale compatibility: For color-online-only figures, a .ps or .eps file is required for production. Ensure color online figures are intelligible in grey scale. To achieve this goal, use colors that have clearly distinguished grey-scale values. To assist in differentiating colored curves, use different line styles (dashed, solid, etc.) and give a description of the lines in the caption. See Guide to Acceptable use of Color in Color Online Figures for more information.


\section*{Introduction}
\label{Introduction}

Understanding the nature of the dynamical glass transition is one of the most intriguing open problems in condensed matter. 
As a glass-forming liquid is cooled down towards its characteristic temperature ($T_g$), its viscosity increases by several orders of magnitude, while its structure, the spatial arrangement of the particles, does not show any obvious change.
Finding the subtle changes in the structure that explain the huge variations in the dynamics (increased viscosity) seems like an obvious target for Machine Learning, which is great at finding hidden patterns in data.

Independently from this, a number of works identify patterns using expert hand-crafted features that seem quite specific to glasses where these descriptors are shown to directly correlate with the dynamics \cite{tong_structural_2019, lerbinger2022relevance}, without any actual use of Machine Learning.
Aside from Machine Learning, historically and up to the present day, physicists have inspected how some well-defined physical quantities correlate with glassy dynamics \cite{Rottler2014, golde_correlation_2016, tong_revealing_2018, tanaka_revealing_2019, tong_structural_2019,lerbinger2020local, lerbinger2022relevance}, in a manner akin to expert features (but without any learning).
There are also direct pattern-recognition techniques \cite{Coslovich2011,keys_characterizing_2011,Royall2017,Turci2017,royall_dynamical_2020} 
and a few unsupervised approaches \cite{boattini_autonomously_2020,paret_assessing_2020, coslovich_dimensionality_2022, ronhovde2011detecting, ronhovde2012detection}, 
but the typical strategy is to learn a representation using supervised learning, \ie use ML to map a given local structure to a target label that represents the local dynamics (\ie a ``local viscosity'' measure).
See the review \cite{richard2020predicting} for a comparison or various machine-learned of physical quantities and how they correlate with dynamics.
Since glasses are very heterogeneous when approaching the transition (a phenomenon called Dynamic Heterogeneity \cite{Candelier2010a, berthier2011dynamical,chacko2021elastoplasticity,arceri2022glasses}),
a single temperature point already displays a broad variety of labels' values, and is generally considered to be enough data to learn a good representation.
%In this context, the notion of ``good'' can be summarized crudely as ``yielding a high prediction accuracy for the target label''.
We further articulate the relation between the physics of glasses and learning a representation in section \ref{sec:transferability}.
%% XXX: note: we could do better than training on a single state point. This was mentionned in future directions, too.


The quest for this good representation has prompted many works, relying on various techniques.
The idea of using ML for glasses was introduced by the pioneering works of Liu \etal~\cite{Schoenholz2014, Cubuk2015,Schoenholz2016,Schoenholz2016a}, where a rather generic set of isotropic features (describing the density around the target particle) is fed to a binary classifier (a linear Support Vector Machine (SVM)) to predict the target particle's mobility.
After considering only isotropic features, they marginally improved accuracy by using angular-aware expert features, measuring bond-angles around the target particle. 
This simple yet robust technology alone yielded a number of physics-oriented works  \cite{Sharp2017a,Schoenholz2017,Definiujemy2012,sussman_disconnecting_2017,landes_attractive_2020,tah2022fragility, zhang2022structuro}, 
which provide  physical interpretations of the model's output value (named Softness).
These and the other shallow learning approaches all rely on features that describe the neighborhood of a single particle to then predict this single particle's future: in all cases, there is no interaction between neighborhoods.

Independently from the glass literature, within other application fields dealing with particles living in 3D space, Deep Learning frameworks and in particular Graph Neural Networks (GNNs) were developed, with some success, quite early on (2016).
Applications range from molecular properties predictions~\cite{kearnes_molecular_2016}, to  bulk materials properties prediction (MegNet)~\cite{chen_graph_2019}, or Density Functional Theory approximations~\cite{hu_forcenet_2021}, and most importantly for us, for glassy dynamics~\cite{bapst_unveiling_2020}.
%Independently and around 2020, Deep Learning entered the game, with some early attempts using Convolutional Neural Networks (CNNs), which are designed to handle 2D or 3D picture-like data, and most prominently using Graph Neural Networks (GNNs) \cite{bapst_unveiling_2020}, which we detail in this paper.
Indeed, designing geometrical features to capture the slight variations in the geometry of a particle's neighborhood is a daunting task, and the lesson learned from Computer Vision and the advent of Convolutional Neural Networks (CNNs) is that 
meta design works better than design, that is, expert features designed by hand are not as informative as the ones learned by a neural network with a suitable architecture designed by an expert.
%things work better when people stop designing features by hand, and instead work at the higher level of architecture, \ie when they start designing the architectures that will automatically find the proper features (meta design).
This is the point of the Deep approach, and more specifically GNNs, that are designed to build aggregated representations of the nodes' neighborhoods.
%The point of the Deep approach is to model the potentially intricate correlation between neighboring particles while avoiding the use of expert features: it requires minimal physical insight.
Indeed in 2020 Bapst \etal \cite{bapst_unveiling_2020} substantially redefined the state of the art using a GNN.
Although they are very effective, standard GNNs lack interpretability due to their structure and their high number of learnable parameters.
%% 

The coarse-graining effectively performed by GNNs was then mimicked (or distilled) in an expert-features oriented approach, with the impressive result by Boattini \etal \cite{boattini_averaging_2021} where it was shown that a simple Ridge Regression with $\sim O(1000)$ rather generic, rotation-invariant features, performs equally well as the reference of that time, \ie GNNs \cite{bapst_unveiling_2020}).
The features consist for a part of node-wise rotation-invariant descriptors, and for another, of these elementary features but averaged over neighboring nodes, over a small radius (mimicking a GNN aggregation step).
%At the same time they study different types of glass forming systems: hard spheres/disks, 2D and 3D binary mixtures and poly-disperse ones. 
% \textbf{FL: ok your original sentence wasn't bad too:}
% On the other side Boattini \etal \cite{boattini_autonomously_2020} recently showed that mimicking the local averaging property of GNNs on single particle descriptors manages to match the state of the art at significantly reduced complexity. Both attempts rely on the interaction over the neighborhood of scalar quantities which are invariant to roto-translation. 
There is currently some debate \cite{boattini_averaging_2021, coslovich_dimensionality_2022, shiba_predicting_2022, jung2023predicting, alkemade_improving_2023, jiang_geometry-enhanced_2022} over whether Deep Learning
 %can achieve better results than
 is the right was to do better than
such expert features, and part of the community leans on this expert side.

Here, inspired by the fast-growing field of SE(3)-equivariant networks \cite{thomas_tensor_2018,batzner_e3-equivariant_2021,weiler_3d_2018, brandstetter_geometric_2022, batatia2022mace,liao2023equiformerv2, ruhe2023clifford, ruhe2023geometric}, % ,hutchinson2021lietransformer
we build a GNN with hidden representations that are translation and rotation-equivariant (SE(3)-symmetry),
 \ie enforce the symmetries present in atom packings.
In other words, we take the best of both worlds: we combine the basic idea of symmetry-aware features, already used with some success \cite{boattini_averaging_2021}, with the combinatorial and expressive power of Deep Learning.
%\textbf{TODO: add more citations of people using BO or SOAP here}
In practice, for the task of predicting the dynamical propensity of 3D Kob-Anderson mixtures,
we significantly surpass the historical state of the art \cite{bapst_unveiling_2020}, at a reduced number of parameters and increased interpretability, 
while we perform comparably as well or better than other approaches (the details depends on the timescale considered).
Importantly, the representation we learn generalizes very well across temperatures.

In the next section (sec.~\ref{sec:task}) we define the task to be solved: input and output data.
We then introduce all the necessary theoretical tools to build the basic SE(3)-GNN layer, explaining how they apply to our specific case (sec.~\ref{sec:buildSE3GNN}).
We explain how to combine these layers into a netwrok in sec.~\ref{sec:arch}.
In section \ref{sec:results}, we study the impact of various pre-processing choices on performance and we compare with other recent works.
%, showing it consistently outperforms previous methods, at a reduced cost.
We open on interpretating the learned representation as an order parameter and
experiment on the robustness of our representation in section \ref{sec:transferability}.
We outline directions for future work in section \ref{sec:futureDirections}.
We summarize the main outcomes of this work in the conclusion, section \ref{sec:discussion}.


\section{Dataset and task}
\label{sec:task}
% \textbf{FP: introduce also Shiba here?} -> ok we refer to it but really explain it in appendix
To probe the ability of our model to predict mobility, we adopt the dataset built by Bapst \etal in \cite{bapst_unveiling_2020}. It is obtained from molecular dynamics simulations of an 80:20 Kob-Andersen mixture of $N=4096$ particles in a three-dimensional box with periodic boundary conditions, at number densities of $\rho\simeq1.2$.
Four state points (temperatures) are analyzed: $T=0.44, 0.47, 0.50, 0.56$. For each point, 800 independent configurations $\{ \mathbf{x}_i \}_{i=1...N}$ are available, \ie  $800$ samples (each sample represents $N$ particles' positions).


\begin{figure}
    \centering
    \includegraphics[width=0.35\textwidth]{embedded_graph.pdf}
    \caption{\textbf{Input Graph with its input features.} Node features are the one-hot encoded particle types (invariant features, $l=0$), and edge attributes $\mathbf{a_{ij}}$ are split: the direction is embedded in Spherical Harmonics $Y(\mathbf{\hat{a}_{ij}})$ and the norm is retained separately.
    Throughout this paper, we depict each rotational order with a given color: $l=0$ (red), $l=1$ (green), $l=2$ (blue). The relative length of each is a reminder that each requires $2l+1$ real values to be stored on the machine.
    % \textbf{OPTIONAL: maybe also write i inside the central node and j in one of the others}
    }
    \label{fig:graph_info}
\end{figure}

The quantity to predict (Ground Truth label) is the individual mobility of each particle, measured as the dynamical propensity \cite{berthier_structure_2007, berthier_spontaneous_2007}: for each initial configuration, $30$ micro-canonical simulations are run independently, each with initial velocities independently sampled from the Maxwell-Boltzmann distribution.
The propensity of particle $i$ over a timescale $\tau$ is then defined as the average displacement over the $30$ runs.
%$m_i(\tau) = 1/n_\alpha \sum_\alpha||\mathbf{x}_{i,\alpha}(t=\tau)-\mathbf{x}_{i}(t=0)||$. 
Propensity is available at $n_{times}=10$ different timescales that span the log scale, \textit{a priori} resulting in  $n_{times}$ different tasks.
For some experiments we also use another similar dataset as provided by Shiba \etal \cite{shiba_predicting_2022}, which models the same glass-former yet differs from that of Bapst \etal on a couple of points, that we detail in Appendix \ref{appendix:Shiba}.


For each sample to be processed through the GNN, the input graph is built by taking particles as nodes and connecting them when the inter-atomic distance between positions $\mathbf{x}_i$ and $\mathbf{x}_j$ is less than $d_c = 2$ (in atomic potential units).
The node features encode the particle type, here A or B
(``Node features'' is machine learning vocabulary for  ``set of values associated to the node'', and similarly for edge features).
% , and optionally the potential energy of each particle.
We use one-hot encoding, such that node features consist of $n_{type}=2$ boolean variables. 
This generalizes trivially to mixtures with $n_{type}>2$.
Optionally, we also include the value of the potential energy of particle $i$ as node feature, which brings their number to $3$ (2 boolean and a real).  
The edges are directed, and edge $(i,j)$ has for feature $\mathbf{a}_{ij} = (\mathbf{x}_j -\mathbf{x}_i)$, \ie  it stores the relative position of the particles (nodes) it connects. We show a sketch of our input graph with its node and edge features in Fig.~\ref{fig:graph_info}.

The task is then the node-wise regression of the particle's propensity $m_i\in \mathbb{R}$ (node label).
Notably, here we simultaneously regress both particle types, meaning that all nodes contribute to the computation of the loss function. 
We also introduce a new task, referred to as \textit{multi-variate regression}, in which the $n_{times}$ timescales are regressed at once, as opposed to as the usual \textit{uni-variate} approach.
%In this work we use mainly two approaches: 1) Uni-variate regression: at fixed temperature $T$ the regression of mobility at each timescale is performed separately (a different model is trained for each task). 2) Multi-variate regression: the last linear layer is modified to have an output size $n_{times} = n_{\tau} ??$. The $n_{times}$ timescales are regressed at once in this case, thus they share the convolution layers and we have a separate set of weights only for the last linear layer.



\section{How to build a Graph-convolution equivariant layer?}
\label{sec:buildSE3GNN}

\subsection{Graph Neural Networks} 
Consider a graph $\mathcal{G} = (\mathcal{V},\mathcal{E})$, where $\mathcal{V} = \{1,\dots,n_v\}$ is the set of vertices or nodes $v_i$ and $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$ is the set of edges $e_{ij}$, respectively endowed with node features $\mathbf{h}_i\in\mathbb{R}^{c_v}$ and edge features $\mathbf{a}_{ij}\in\mathbb{R}^{c_e}$. GNNs operate on such graphs by updating node (and possibly edge) features through local operations on the neighborhood of each node.
These operations are designed to adapt to different kinds of neighborhoods and respect node-index permutation equivariance, which are the two key features of GNNs, as opposed to CNNs (for which the learned kernels must have fixed, grid-like geometry, and for which each neighboring pixel is located at a fixed relative position).
In this work we deal with Graph Convolutional Networks (GCN), a subclass of GNNs. 
A GCN layer acts on node features as follows: 
\begin{align}
    \mathbf{h}'(\mathbf{x}_i) = \sum_{j\in \mathcal{N}(i)} \kappa (\mathbf{x}_j-\mathbf{x}_i) \mathbf{h}(\mathbf{x}_j)
    \label{eq:GCNKernel}
\end{align}
where $\mathcal{N}(i)$ is the neighborhood of node $i$. Here a position $\mathbf{x}_i \in \mathbb{R}^3$ is associated to each node and $\kappa$ is a continuous convolution kernel which only depends on relative nodes' positions. In this case, as for CNNs, the node update operation is translation-equivariant by construction. 
It is however not automatically rotation-equivariant.

% One of the most general ways to describe GNNs is regarding them as message passing networks (most of GNNs if not all can be described in this framework). In a forward pass, node features are updated as follow. Message from node $v_i$ to node $v_j$ is computed:
% \begin{align}
%     \mathbf{m}_{ij} = \phi_m \left(\mathbf{f}_i, \mathbf{f}_j, \mathbf{a}_{ij}\right)
% \end{align}
% different messages are aggregated over the neighborhood to update $\mathbf{f}_i$:
% \begin{align}
%     \mathbf{f}'_i = \phi_f \left(\mathbf{f}_i, \sum_{j\in\mathcal{N}(i)} \mathbf{m}_{ij} \right)
% \end{align}
% $\phi_m$ and $\phi_f$ are commonly implemented as MLPs whose weights are shared respectively over edges and nodes and are learned by back-propagation.

% In this work we deal with graphs embedded in a 3-dimensional space: each node may be associated with a position $\mathbf{x}_i \in \mathbb{R}^3$, but it's the edge features that carry the information about nodes' relative positions. 
% In this case, the node update operation 
% %(reminiscent of weight sharing of CNNs) 
%  is translation-equivariant by construction, since only relative positions are used. It is however not automatically rotation-equivariant.
%  %[cite proof]

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{equivariance.pdf}
    \caption{
    \textbf{Equivariance to 2D rotations.} Simple case in which the input and output fields have the same dimension, $2$. $\pi(r)$ represents the action of the rotation operator on the input field, $\pi'(r)$ on the output one.
    In general they can be different, here since input and output are in the same space, they are equal.
    The mapping $\mathcal{K}$ acts in an equivariant way, indeed it commutes with the rotation.
    %More generally, the output field may be of a different dimension, hence the presence of $\pi'(r)$, which in the above case is equal to $\pi(r)$.
    }
    \label{fig:equivariance}
\end{figure}

\subsection{Equivariance}
A layer of a network is said to be equivariant with respect to a group $G$ if 
%upon transformation of the input by the action of $G$, 
upon group action on the input,
the output is transformed accordingly. 
One simple example is shown in Figure~\ref{fig:equivariance}: it depicts a 2D vector field $\mathbf{f}(\mathbf{x})$ and a mapping $\mathcal{K}$ that acts on it,  $\mathcal{K}(\mathbf{f}) = \cos{(||\mathbf{f}||)}\cdot \hat{\mathbf{f}}$ (where $\hat{\mathbf{f}} = \mathbf{f} / ||\mathbf{f}||$), generating the output field $\mathbf{f}'(\mathbf{x})$, which happens to live in the same space.
$\mathcal{K}$ is equivariant to 2D rotations: it operates only on the norm of the vectors, thus it commutes with the rotation operator.

As introduced in the previous example, we can represent the point cloud processed by the GNN as a vector field
$\mathbf{h}(\mathbf{x}) = \sum_{i\in\mathcal{V}} \delta(\mathbf{x}-\mathbf{x}_i)\mathbf{h}_i$
with values in some vector space $H$, and the action of a layer as a mapping $\mathcal{K}$ from one field $\mathbf{h}_i$ to the updated one $\mathbf{h}_i'$.
To require that $\mathcal{K}$ fulfils equivariance, we need to define how the group of interest acts on the vector space $H$ through representations.
% Feature maps form a functional space $\mathcal{F}$, then the action of a layer is just a mapping from a functional space to another one: $\mathcal{K}: \mathbf{h}(\mathbf{x})\in\mathcal{F}\rightarrow \mathbf{h}'(\mathbf{x})\in\mathcal{F}'$ with $\mathbf{h}(\mathbf{x}) \in H$ and $\mathbf{h}'(\mathbf{x}) \in H'$. Note that the input and the output of one layer do not need to inhabit the same space.
Given a group $G$, a representation $\rho$ is a mapping from group elements $g$ to square matrices $\mathbf{D}^H(g): H \to H$ that respect the group structure. Essentially it tells how $G$ acts on a specific space $H$.
%% FL: couldn't we discard the end of this paragraph ?
For example, the representation of the group of three-dimensional rotations $SO(3)$ on the 3-D Cartesian space is the usual rotation matrix $\mathbf{R}(r_{\alpha,\beta,\gamma}) = \mathbf{R}_x(\alpha)\mathbf{R}_y(\beta)\mathbf{R}_z(\gamma)$. Then if we consider an element $tr\in\nolinebreak SE(3)$ which is composed of a translation $t$ and a rotation $r$, it will act on a vector field as follows:
\begin{align}
    \mathbf{h}(\mathbf{x}) \xrightarrow[]{\pi(tr)} \mathbf{D}^H(r) \mathbf{h}(\mathbf{R}^{-1}(r)(\mathbf{x}-\mathbf{t}))
\end{align}
The codomain (output domain) is transformed by the representation of the rotation while the domain is transformed by the one of the inverse roto-translation.
See Fig. \ref{fig:equivariance}, top left to bottom left, for an example with 2D rotations.
For further explanations and examples, see \cite{weiler_3d_2018}.

Let us define equivariance. 
Let there be a mapping $\mathcal{K}: \mathbf{h}(\mathbf{x}) \rightarrow \mathbf{h}'(\mathbf{x})$ and $\mathbf{h} \in H$, $\mathbf{h}' \in H'$ with $H,H'$ two vector spaces.
The kernel $\mathcal{K}$ is equivariant with respect to $G$ if
\begin{align}
    \forall g\in G \;\; \mathcal{K} \circ \pi(g) =  \pi'(g) \circ \mathcal{K}
\end{align}

The input and output codomains $H,H'$ do not need to be identical, and this is taken into account by the group representations $\mathbf{D}^H(g) $ and $ \mathbf{D}^{H'}(g)$. A direct consequence of this definition is that invariance is a particular case of equivariance where $\mathbf{D}^{H'}(g) = \mathbb{I} \;\; \forall g\in G$.

When dealing with $SE(3)$, the \textit{only} invariant quantities are \textit{scalars}, thus considering only invariant features would significantly reduce the model's expressivity.


\subsection{Equivariant features} 
 To enforce equivariance of layers, we work with equivariant features (also called \textit{steerable features}), following the schema of steerable group convolutions \cite{weiler_3d_2018,thomas_tensor_2018,kondor_clebsch-gordan_2018} (for theoretical insight, see Appendix-B of \cite{brandstetter_geometric_2022}). 
These features inhabit the space of irreducible representations of $SO(3)$, which is factorized into sub-spaces: $V_{SO(3)} = V_{l_1}\oplus V_{l_2}\oplus V_{l_3}\oplus \dots$. Each subspace, indexed by $l\geq 0$, is of size $2l+1$ and transforms independently under rotations thanks to the action of Wigner-D matrices $\mathbf{D}^{(l)} \in \mathbb{R}^{(2l+1)\times(2l+1)}$. 
Coming to implementation, a feature is just a concatenation of different $l$\mbox{-}vectors: scalars ($l=0$), 3-D vectors ($l=1$), 5-D vectors ($l=2$) and so on. Multiple pieces with the same $l$ are also allowed, we address this multiplicity by referring to channels. 
For example we can have two $l=0$ channels, a single $l=1$ channel and a single $l=2$ channel:
\begin{align}
    \mathbf{h}(\mathbf{x}) = \left(h^{(l=0)}_{c=0}(\mathbf{x}),h^{(l=0)}_{c=1}(\mathbf{x}),\mathbf{h}^{(l=1)}_{c=0}(\mathbf{x}),\mathbf{h}^{(l=2)}_{c=0}(\mathbf{x})\right)
    \label{eq:h_ell}
\end{align}
where $\mathbf{h}: \mathbb{R}^3 \rightarrow \mathbb{R}^k$ with $k = \sum_{l} n^{(l)}_{c} \cdot (2l+1) =2\cdot 1 + 3 + 5 = 10$ with $n^{(l)}_{c}$ number of channels of type $l$. Rotation of these features is straightforward: 
\begin{align}
    \mathbf{D}(r) \mathbf{h} = \left[\begin{array}{cccc}
   D^{(0)}&&&\\
   &D^{(0)}&&\\
   &&\mathbf{D}^{(1)}&\\
   &&&\mathbf{D}^{(2)}\\
   \end{array}\right]
   \left[ \begin{array}{c}
        h^{(l=0)}_{c=0}\\
        h^{(l=0)}_{c=1}\\
        \mathbf{h}^{(l=1)}_{c=0}\\
        \mathbf{h}^{(l=2)}_{c=0}
   \end{array}\right]
\end{align}
The representation matrix is block-diagonal thanks to $SO(3)$ being decomposed in a direct sum, and scalars ($l=0$) being invariant with respect to rotation ($D^{(0)} = 1$).
The wording \textit{steerable} becomes clear: upon rotation of the input coordinates, these features rotate accordingly, as when one steers the steering wheel, thus turning the wheels.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{update_and_cm.pdf}
    \caption{
\textbf{Overview of the convolution layer, summarizing Eqs.~(\ref{eq:convolution},\ref{eq:self_int}).}
For each neighboring node, the node and edge features are combined (with C-G product) and multiplied by the learned radial filter $\varphi$.
Before performing this operation, the one-hot encoded particle type is concatenated to $\mathbf{h_i}$ by adding 2 $l=0$ channels (not shown, for simplicity).
Because multiple triplets come out of the C-G product, we obtain a much larger representation (left part of inset). This intermediate representation is narrowed down using a linear layer (one for each $l_O$ and each channel).
% The convolution (i) takes as input equivariant node features $\mathbf{h}_i$, (ii) embeds edge features in SH basis, (iii) makes them interact with C-G equivariance-preserving tensor product, to output (iv) an equivariant representation $\mathbf{h'}$
    \label{fig:aggregating_and_mixing}
    }
\end{figure*}

\subsection{Spherical Harmonics (SH)} 

To embed the three dimensional node and edge input data in an equivariant form, we use Real Spherical Harmonics $Y^l_m:\mathbb{S}^2 \rightarrow \mathbb{R}$.
They can be though of as the generalization of Fourier modes (circular harmonics) 
% $\mathcal{F}^k:\mathbb{S}^1 \rightarrow \mathbb{R}$ 
 to the sphere.
Spherical Harmonics are indexed by the \textit{rotational order} $l\geq 0$, which is reminiscent of the 1-D frequency, and by $m = -l,\dots,l$, which determines the spatial orientation.
They form an orthonormal basis of $\mathbf{L}^2(\mathbb{S}^2)$, \ie any real-valued function on the sphere $f: \mathbb{S}^2 \rightarrow \mathbb{R}$ can be Fourier Transformed to this SH basis:
\begin{align}
\mathscr{F}(f)(l,m) &= \hat{f}^l_{m} = \int_{\mathbb{S}_2} f(\mathbf{n})  Y^l_m (\mathbf{n}) \text{d}\mathbf{n}
\\
f(\mathbf{n})  &= \sum_{l=0}^\infty \sum_{m=-l}^l \hat{f}^l_{m} Y^l_m (\mathbf{n}) \quad \text{(inverse transform)}
\end{align}
where $\mathbf{n} = (\theta,\phi) \in \mathbb{S}$ represents a generic direction or point on the sphere.
Here the coefficients $\hat{f}^l$ are not real values but are $(2l+1)$-dimensional vectors (with components $\hat{f}^l_{m}$). 
The set of all coefficients $(\hat{f}^l)_{l=0,\ldots}$ plays the same role as $ \mathbf{h}(\mathbf{x})$ in Eq.(\ref{eq:h_ell}), and each coefficient $\hat{f}^l$ transforms according to a Wigner-D matrix $\mathbf{D}^{(l)}$: the SH embedding is thus equivariant. 
%
% \textbf{FL: next sentence is a bit unclear/confusing to read:}
% Every directional information can be embedded in a steerable space trough the use of SH, considering $f(\mathbf{n}) = \delta(\mathbf{n}-\mathbf{n}_0)$: 
% \begin{align}
%     \mathscr{F}(\mathbf{n}_0) = \left(Y^0_0 (\mathbf{n}_0),Y^1_{-1} (\mathbf{n}_0),Y^1_0 (\mathbf{n}_0),Y^1_1 (\mathbf{n}_0),\dots \right)
% \end{align}



In particular, the density of neighbor particles at a fixed distance $r$ from the central one, $\rho_r(\mathbf{n})=\sum_{j \in \mathcal{N}(i)} \delta(\mathbf{n}-\mathbf{n}_{ij})\delta(r-r_{ij})$, 
 is a real-valued function (distribution) on the sphere and can be decomposed into Spherical Harmonics.
Furthermore, summing such decompositions at a number of radii  $r \in [0,d_c]$, one obtains an equivariant representation of the density field around a target particle in the ball of radius $d_c$ (this is what our very first convolution performs, see below).
%This is what we do in Eq.~(\ref{eq:convolution}).

Note that to make a fixed-$r$ representation finite-dimensional, we need to choose a high-frequency cutoff for the rotational order, $l=l_{max}$.
Analogously to Fourier transforms on the circle, this way of decomposing and filtering out high-frequencies preserves the input signal better than most naive schemes (as \eg a discretization of solid angles).

% \textbf{FL: this last paragraph is really obscure to me. It sounds like we repeat the same thing several times. To be discussed IRL. If you just want to say that learned stuff applies to each ell differently, but to all m's of the same ell equally, then let's say that/ show a block diagonal matrix or sthg more telling.}
% We adopt SH also to parameterize convolution kernels as in \cite{thomas_tensor_2018}. We consider functions $\hat{\mathbf{k}}: \mathbb{R}^3 \rightarrow V_{SO(3)}$ as for feature maps, and define them directly in the steerable space as: 
% \begin{align}
%     \mathbf{k}(\mathbf{x}) = \left(w^0(||\mathbf{x}||) Y^0(\hat{\mathbf{x}}), w^1(||\mathbf{x}||) \mathbf{Y}^1(\hat{\mathbf{x}}),w^2(||\mathbf{x}||) \mathbf{Y}^2(\hat{\mathbf{x}}),\dots\right)
% \end{align}
% where $\hat{\mathbf{x}} = \frac{\mathbf{x}}{||\mathbf{x}||}$ and the weights incorporate the radial dependence of the filer. Note that they only depend on $l$ this means that for different $m$ the weight is the same and the filter inherits the rotation properties of SH. 
% if we keep this kind of kernel approach, then do the proper split, K(phi_16 , Y lf) * wlflilo_16 * h(li)





\subsection{Clebsh-Gordan tensor product} 

As said above, we do not want to restrict ourselves to invariant features, but to equivariant ones. 
For this, we need a way to combine feature vectors together other than the dot product (which produces only invariant scalar features).

Analogously to the outer product for vectors of $\mathbb{R}^3$, which is a bilinear operator  $\otimes: \mathbb{R}^3\times \mathbb{R}^3 \rightarrow \mathbb{R}^3$, 
the Clebsh-Gordan tensor product $\otimes_{l_1,l_2}^{l_O}: V_{l_1}\times V_{l_2} \rightarrow V_{l_O}$ is a bilinear operator that combines two $SO(3)$ steerable features of type $l_1$ and $l_2$ and returns another steerable vector of type $l_O$. 
It allows to maintain equivariance when combining equivariant features: consider $\mathbf{h}_1(\mathbf{x}) \in V_{l_{1}}$, $\mathbf{h}_2(\mathbf{x}) \in V_{l_{2}}$ and their C-G tensor product $\mathbf{h}'(\mathbf{x}) = (\mathbf{h}_1(\mathbf{x})\otimes_{l_{1},l_{2}}^{l_O}\mathbf{h}_2(\mathbf{x})) \in V_{l_O}$; if we apply a rotation $r$, inputs will be transformed by $\mathbf{D}^{(l_{1})}(r)$, $\mathbf{D}^{(l_{2})}(r)$ and the output by $\mathbf{D}^{(l_O)}(r)$, \ie equivariance is fulfilled.
Concretely, the tensor product is computed using Clebsh-Gordan coefficients $C^{(l_O,m_O)}_{(l_1,m_1),(l_2,m_2)}$ as
\begin{align}
    h'^{l_O}_{m_O} = \sum_{m_1=-l_1}^{l_1}\sum_{m_2=-l_2}^{l_2} C^{(l_O,m_O)}_{(l_1,m_1),(l_2,m_2)} h_{1,m_1}^{l_1}h_{2,m_2}^{l_2}
\end{align}
We have $C^{(l_O,m_O)}_{(l_1,m_1),(l_2,m_2)}\neq 0 $ only for $l_O \in [ |l_1-l_2|,l_1+l_2]$, thus it is a sparse tensor product. In a more concise form we write:
\begin{align}
    \mathbf{h}'^{l_O} = \mathbf{h}_1^{l_1}C^{l_O}_{l_1 l_2}\mathbf{h}_2^{l_2}
\end{align}
where each ``coefficient'' $C^{l_O}_{l_1 l_2}$ is actually a $(2l_O+1)\times (2l_1+1)\times (2l_2+1)$ tensor.



\subsection{SE(3)-equivariant Graph Convolution Layer} 

Using all the concepts introduced above, we can now define the explicit form of the convolution kernel $\kappa$ of Eq.~\ref{eq:GCNKernel}.
Denoting the input edge attributes $\mathbf{a}_{ij} = (\mathbf{x}_j - \mathbf{x}_i)$, the kernel factorizes the effect from its radial part  $||\mathbf{a}_{ij}||$ and its directional part $\mathbf{\hat{a}}_{ij}$.
Each Input rotational order $l_I$ interacts with the rotational order of the Filter $l_F$ to Output various rotational orders $l_O$. 
We decide to use, in a similar spirit as in \cite{batzner20223}:
\begin{align}
\kappa(\mathbf{a}_{ij}) = \varphi(||\mathbf{a}_{ij}||)^{l_O}_{l_I l_F,c} \mathbf{Y}^{l_F}(\mathbf{\hat{a}}_{ij})
\end{align}
The radial filters $\varphi^{l_O}_{l_I l_F,c}$ are implemented as Multi-Layer Perceptrons (MLPs, to be learned) that share some weights among triplets $l_O,l_I,l_F$ and channels $c$ (details in Appendix \ref{appendix:MLP}).
%Convolution is performed independently for each channel $c$: a set of filters is defined, one for each triplet $l_O,l_I,l_F$, which share the same angular part but have different radial functions $\varphi$ which are learnt.
%%
%In this case we deal with 
%discrete feature map/
% a discrete vector fields 
% $\mathbf{h}(\mathbf{x}) = \sum_{i\in\mathcal{V}} \delta (\mathbf{x}-\mathbf{x}_i) \mathbf{h}_i$ and we update them trough a convolution operation on the neighborhood defined by graph connections:
Expliciting the C-G tensor product, Eq.~\ref{eq:GCNKernel} now reads:
\begin{align}
\label{eq:convolution}
    \mathbf{h}_{i,c,l_I l_F}^{'l_O} = \sum_{j\in\mathcal{N}(i)} \varphi(||\mathbf{a}_{ij}||)^{l_O}_{l_I l_F,c} \mathbf{Y}^{l_F}(\mathbf{\hat{a}}_{ij}) C^{l_O}_{l_I l_F} \mathbf{h}_{j,c}^{l_I}
\end{align}
This operation is depicted in Fig.~\ref{fig:aggregating_and_mixing} (left part).
%The updated feature $\mathbf{h}_{i,c,l_I l_F}^{'l_O}$ at node $i$ is indexed by the channels (which are in one-to-one correspondence with the input ones) and by the triplet of $l$'s. 
At this stage, operations are performed channel-wise, but $\mathbf{h}'_{i,c}$ is a concatenation of all possible triplets, and as multiple combinations of $l_I,l_F$ can contribute to a given $l_O$, it is larger than the original feature $\mathbf{h}_{i,c}$.
For $l_{max}=3$, there are $34$ different triplets (instead of just $4$ different values of $l$).
%Since multiple combinations of $l_I,l_F$ can give a certain $l_O$, the size of the updated feature will be larger than the original one: $\mathbf{h}'$ is just a concatenation of $l$ features resulting from different triplets and channels.
%%this is because you're going from a function on S2 to a function on SO(3), but we want to compress it back.

To go back to a reduced representation, we mix together the triplets that share the same output $l_O$, with a linear layer.
However, to let the various channels interact, we also perform channel mixing (also called self-interaction) with a linear layer.
As linear layers combine linearly, this can be expressed as a single linear layer (right part of Fig.~\ref{fig:aggregating_and_mixing}):
\begin{align}
\label{eq:self_int}
    \mathbf{h}_{i,c}^{\text{out},l_O} = \sum_{l_I l_F,c'} w^{l_O}_{l_I l_F,cc'} \mathbf{h}_{i,c',l_I l_F}^{'l_O}
\end{align}
where $c'$ is the input channel's index and $c$ is the output one.
Note that all operations are now performed node-wise, and independently for each $l_O$.
Note that this operation fulfills equivariance because only features with the same $l_O$ are combined together, with weights that do not depend on $m$ (all elements inside a vector are multiplied by the same factor). % thus preserving rotation properties.
At this point we are back to our expected node feature shape, and the convolution layer can be repeated (up to a few technical details like using Batch-Norm and adding the previous layer's representation to the newly computed one, see next section, sec.~\ref{sec:arch}). % and Appendix \ref{appendix:overall_arch}).
See Appendix \ref{appendix:overall_arch} for the counting of the number of weights of the MLP and of this mixing layer.

\subsection{Interpretation} 

Here we want to insist that the first layer at least has a very clear interpretation.
In the first convolution, for which the input node features consist in two $l=0$ channels, namely the one-hot encoding of the particle type.
Since $l_I=0$, the only non-zero C-G triplets will be the ones at $l_O=l_F=0,1,2,3$.
To simplify, let us first imagine that the radial filters $\varphi^{l_O=l_F}_{l_I=0,l_F, c}(||\mathbf{a_{ij}}||)$ are not learned anymore but are replaced with a Gaussian Radial Basis $B(||\mathbf{a_{ij}}||)_r$ ($r$ is the basis index), where each element of the basis (a ``soft bin'') is attributed to a given channel.
Then, the first layer's $(L=0)$ action is to convolve the particle density $\mathbf{h}_{j,c=A,B}^{l_I=0, L=0}$ with the kernel $B(||\mathbf{a_{ij}}||)_{r=c} \mathbf{Y}^{l_F=0,1,2,3}(\mathbf{\hat{a}}_{ij})$,
\ie to project the density fields (one for A's and one for B's) on the direct product
% build a high-dimensional embedding of the density fields in the ball of radius $d_c$ (each particle type is associated to a given density field).
%Concretely, this embedding corresponds to expressing the density fields on the direct product
 of the chosen Radial Basis and chosen Spherical Harmonics.
 This projection can be seen as an embedding.
%% in Eq.~(\ref{eq:convolution}), we have $l_I=0$, $l_O=l_F$ and the channel index can be called $r$ to simplify 
Taking the norm of this representation, one would qualitatively obtain the expert descriptors described in \cite{boattini_averaging_2021} (for an exact and more detailed derivation, see Appendix \ref{appendix:Boattini}).

In our case, the channel mixing step Eq.~(\ref{eq:self_int}) actually comes in before taking the norm, and since it does not have to mix any triplet (since $l_O=l_F$), it only mixes channels, \ie  performs a linear combination of the density fields for different particle types.
Furthermore, we actually learn the functions  $\varphi(||\mathbf{a_{ij}}||)$, so that we linearly combine the density fields measured at different radii early on, before mixing channels or taking the norm of the representation.
%Furthermore, we can weight such decompositions with  weights $\varphi(r)$, summing  from $r=0$ to $r=d_c$,  to obtain an equivariant representation of the field of density around a target particle (in a ball of radius $d_c$). This is what we do in Eq.~(\ref{eq:convolution}).
% FL: possibility: put the attempt with 1 layer, 40 channels, no phi, only Radial Basis, rc=5. ellmax=12 in the appendix, even if it is nearly as good as ours. Also the same with 2 layers, maybe (? then rc=2? Or if Philippe can manage to make it work, we do 1 layer rc=5,ell=12 and then next 2 layers rc=1.5 or 2,ell=3).
To conclude, the features $\mathbf{h}_{i,c}^{\text{out},l_O, L=1}$ computed by our first layer correspond to various linear combinations (1 per output channel) of the density fields at all radii $d<d_c$ and all particle types.
Each SH decomposition $\mathbf{h}_{i,c}^{\text{out}, L=1} = \left( \mathbf{h}_{i,c}^{\text{out},l_O, L=1} \right)_{l_O=0,\ldots,l_{max}}$ corresponds to a function on the sphere, which is interpreted as the projection on a unit sphere of the densities inside the ball of radius $d_c$, weighted according to their distance from the center and particle type.
In our network, we build these intermediate features  $\mathbf{h}_{i,c}^{\text{out},l_O}$  but we do not rush to compute their norm (the invariant features), instead we remain at the equivariant level to combine them, keeping the computation of invariants as the last step of our network.
This difference significantly improves our ability to predict mobility: see section \ref{sec:results} for our discussion on the key elements that increase performance, or directly Appendix \ref{appendix:ablation} for the full ablation study.

For the next layers, although the interpretation is harder to explicit as much, the spirit is the same. The representation at any layer $L$, $\mathbf{h}_{i,c}^{\text{out},l_O,L}$ can be seen as SH decompositions of functions on the sphere. The next representation $\mathbf{h}_{i,c}^{\text{out},l_O,L+1}$ 
is then the weighted density field of these functions.
For instance,  $\mathbf{h}_{i,c}^{\text{out},l_O,L=1}$ is an aggregate field of the local density fields $\mathbf{h}_{i,c}^{\text{out},l_O,L=0}$.


\section{Combining equivariant layers}
\label{sec:arch}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{global_arch.pdf}
    \caption{\textbf{Overall Architecture.}
    Top: node and edge features are fed to each convolution layer.
    Each SE-convolution layer $L=0,\ldots, 7$ refines the output $\mathbf{h}_i^{(L)}$.
    %Arrows connecting the embedded graph features to each convolution block show that the initial information (one-hot particle types and relative positions in SH  and radial basis) is fed to each layer.
    }
    \label{fig:global_arch}
\end{figure}

\subsection{Network} 
 Our network is composed of embedding blocks for nodes and edges features followed by a series of SE(3)-equivariant convolutional layers interspersed with batch normalization and connected in a Res-Net fashion \cite{he_deep_2015}, and one output block (decoder), as shown in Fig. \ref{fig:global_arch}.

Here we provide a few insights on some key parts that are specific to SE(3)-equivariant networks.
Further details about the architecture and the training procedure are available in Appendix \ref{appendix:training_and_stuff}.

Our code will be made available on Zenodo upon publication, to allow full reproducibility. We will also provide the trained model so as to allow further inference. 
%% TODO: format the code and pack it, maybe into a docker image, along with a few samples, for showcasing - in a way that anyone could strat training, or doing inference.

\subsection{Batch Normalization} 
 As often in Neural Networks, we sometimes need to perform Batch Normalization to avoid the neuron's activation to take overly large values.
However, using a usual batch normalization layer \cite{ioffe_batch_2015} separately on each entry of the hidden representations $\mathbf{h}$ would kill the equivariance property. 
Thus a modified version is implemented and applied to node features \cite{weiler_3d_2018, e3nn}. The $l=0$ features are invariant and can be processed as usual:

%%I cited the library, we should stress that also somewhere else.
    \begin{align}
        h^{0}_{BN} = \frac{h^{0} - \bar{h}}{\sigma} \beta + \gamma
    \end{align}
where $\bar{h} = \langle h^{0}\rangle$ and $\sigma^2 = \langle{h^{0}}^2\rangle - \langle h^{0}\rangle^2$ with $\langle\cdot\rangle$ batch average computed with $0.5$ momentum  (keeping memory of previous batches) and $\beta,\gamma$ are learned parameters. For each piece of feature with $l\geq 0$, only the norm can be modified: 
\begin{align}
    \mathbf{h}^{l}_{BN} =   \mathbf{h}^{l}  \frac{||\mathbf{h}^{l}||}{\sigma^{l}} \beta^{l}
\end{align}
where $\sigma^{l}=\sqrt{\langle||\mathbf{h}^l||^2\rangle}/\sqrt{2l+1}$ and $\beta^{l}$ are learnable parameters.
In Fig.~\ref{fig:global_arch} we show where this Batch Norm is used.

%\textbf{FL: merge this witht the decoder explanations given in the temperature generalization second part. At the end, make sure we don't make to many repetitions between here and the temperature generalization, that will be way below. Temperature gener. could rerfer to this part here instead of repeating it}

\subsection{Decoder} 
 After the last convolution layer, the ultimate output block performs two node-wise operations to decode the last layer's output into a mobility prediction.
First it computes SE(3)-invariant features from the hidden representation $\mathbf{h}^{(L_{max})}$: for each channel $c=1,\dots,8$, the norm of each directional ($l\geq 1$) feature  is computed: $||\mathbf{h}^{l}||_2 = \sqrt{\sum_{m} {h^{l}_m}^2}$, and all these norms are concatenated together with the $l=0$ features (already invariant).
%Taking its norm, we obtain an invariant representation: for each $l=1,2,3$ (and each channel $c=1,\dots,8$) we compute the norm $||\mathbf{h}_{i,c}^{l,(L_{max})}||_2$, while for $l=0$ the feature $\mathbf{h}_{i,c}^{l=0,(L_{max})}$ is already invariant and may be signed, so it is directly copied.
Thus, we obtain an invariant representation 
%%which summarizes a particle's environment (and is transferred across temperatures) consists
 of exactly $l_{max}+1$ ($l$ values) $\times 8 $ (channels) $= 32$ (components), which we denote $|\mathbf{h}^{(L_{max})}|$ for simplicity, despite the fact that the $l=0$ components can be negative.
 %% OPTIONAL: write $|\mathbf{h}^{(L_{max})}|$ using align env, to show in 4 lines, the 4 lists of 8 values : for l=0 and then for l=1,2,3 (1 line per ell)
The second operation is to feed this representation into a decoder, which we chose to be a linear layer, it outputs one real value, which is the predicted mobility for a given timescale and given particle type.
For instance, at the timescale $\tau_\alpha$ and for particles of type $A$, the model writes:
\begin{align}
y_{A,\tau_\alpha}
= \mathbf{w}_{A,\tau_\alpha}  |\mathbf{h}^{(L_{max})} (\{\mathbf{x}\})|,
\label{eq:decoder}
\end{align}
where $y_{A,\tau_\alpha}$ is the mobility label and $\mathbf{w}_{A,\tau_\alpha}$ is a set of weights to be regressed (32 real values).
In the multi-variate setup we regress mobilities at all timescales at once, using one linear decoder (set of weights $\mathbf{w}$) per timescale and per particle type (20 different decoders for the Bapst dataset).
%so that we have twice as many independent decoders as there are timescales to be regressed (twice because there are two particle types).
%For the Bapst dataset, this means $20$ decoders are applied to this $32$-values representation $|\mathbf{h}^{(L_{max})}|$.
%Our multi-time regression framework then decodes this single invariant representation into mobilities using linear layers: one linear decoder is learned for each timescale and for each particle type (20 different decoders).


\subsection{Non-linearities}
We note that all the layers act linearly on the node features. 
The only non-linearities of the network are hidden in the implementation of the radial part of the filters $\varphi$ (MLPs).
This limited scope of non-linearities is unusual, and is needed to preserve equivariance (as pointed out above when we describe Batch Norm).
We have explored other forms of non linearities, like Gate-activation, without observing significant improvement.


\section{Experiments, results}
\label{sec:results}

Here we report on the performance of our architecture, %on the glass data set introduced above,
discuss the role of the task, input choices and architecture choices,
and compare with recent works that tackle the same problem.

%previous State Of The Art (SOTA) performance model, namely the GNN presented in \cite{bapst_unveiling_2020}, and the expert approach of \cite{boattini_averaging_2021}.

\subsection{Experimental setup} 

To increase our model's robustness, we simultaneously predict the mobility both for A and B particles, instead of focusing only on the A's.
The accuracy turns out to be similar for the two types.
Here we show results only for one type, A, which is the reference most other works are also using.
As in the works we compare with, we use the Pearson correlation coefficient as performance metric, which is invariant under shift and scale of the test labels distribution.
%%%%
The network architecture and hyper-parameter choices were optimized for a single task ($T=0.44$ and $\tau=\tau_\alpha$ for uni-variate and $T=0.44$ for multi-variate), using only the train and validation sets.
The resulting choices were applied straightforwardly to other tasks, thus preventing over-tuning of the hyper-parameters.
%%% The main architecture's and hyper-parameters choices are the following.
The number of convolution layers is $8$, thus the last representation is indexed $L=L_{max} = 7$ (representation $\mathbf{h}_i^{(L=0)}$ at $L=0$ is the input, before any convolution).
%In the input layer, the visible representation has 2 channels of $l=0$ features (the encoded particle type).
At each layer $L>0$ the internal or hidden representation $\mathbf{h}_{i}^{(L)}$ %%(for particle $i$ at layer $(L)$)
has a maximum rotational order $l_{max} = 3$ and a number $n_c^{(l)}=n_c^{(0)}=n_c^{(1)}=n_c^{(2)}=n_c^{(3)}$ of channels,
$n_c=4$ for uni-variate and
$n_c=8$ for multi-variate.
These choices arise from striking a balance between over- and under-fitting, under our compute-time and memory budget constraints.


Note that we perform a different train-test split with respect to \cite{bapst_unveiling_2020}, which does not explicitly use a test set.
Here, for each state point, $400$ configurations are used for training, $320$ for validation and $80$ for the final test. 
% In all state points others than the one where we performed the architecture search, the validation set is only used to retain the best model over all epochs, and we note that the validation loss remains stable for a large amount of epochs (see Fig.~\ref{fig:loss_vs_epoch}, in the Appendix) 

%actually the last epoch's model does not differ much from it, in validation accuracy (see Fig.~\ref{fig:loss_vs_epoch}, in the Appendix).
%This way we prove that reported results do not show any performance bias due to overfitting of the validation set in the hyper-parameter optimization.
In Appendix \ref{appendix:training_and_stuff}, we provide more details about the training of the model.
%performance and loss as function of the epochs.
% OPTIONAL as well as some qualitative observation on the predicted vs. ground truth data (scatter plots, heat-maps).
%we show a scatter plot of $y^{predicted}$ vs $y^{Ground Truth}$, to give a better sense of what $\rho=0.71$ means.
%More details of the training of the model (performance and loss as function of the epoch) are also shown in Appendix.


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{svg_figures/Multivariate.pdf}
    \caption{
    \textbf{Multi-variate vs uni-variate and influence of inputs.} 
    %\textbf{DONE/not really needed: ennoying suggestion : use similar symbols in the lower plot as in the upper plot. That is, large time, use the marker of the top plot (upper blue triangle filled, with black contour, and red diamond with balck contour); and for small times, use also filled symbols with black contours, but slightly dffferent, like a square (rotated diamond) and a lower or right-poiting triangle for the other T)}
    (top) Correlation $\rho$ between the true and the predicted propensity for A particles at temperature $T=0.44$ as function of timescale. Marker shapes distinguish multi-variate and uni-variate approaches. 
    Colors picture the input type: red for thermal positions ($\{\mathbf{x}_i^{th}\}$), blue for quenched (Inherent Structures, IS) positions ($\{\mathbf{x}_i^{IS}\}$) and light-blue for combined ($\{\mathbf{x}_i^{th}\} + E_{pot}^{IS}$).
    Error-bars represent the best and the worst $\rho$ for ten identical models trained independently with different random seed initialisation, and are comparable with marker's sizes. 
    (bottom) Correlation $\rho$ as function of training (and testing) temperature. Two timescales are shown: $\tau = \tau_\alpha$ (full markers) and $\tau = 0.5 \tau_{LJ}$ (empty markers). 
    Color code and marker code identical to that of the top plot. 
    The multi-variate, thermal positions + $E_{pot}(IS)$ choice is a good compromise to maintain high performance across timescales.
    }
    \label{fig:many_models}
\end{figure}

    % (top) Pearson correlation coefficient ($\rho$) between the true and the predicted propensity for A particles at temperature $T=0.44$ as function of timescale. Triangle markers are used for multi-variate approach where all the timescales are regressed at once while diamond markers are used for the uni-variate that consists in training one separate model for each timescales. Colors are used to represent different types of information present in the input graph: red markers stand for thermal information ($\{\mathbf{x}_i^{th}\}$), blue for IS information ($\{\mathbf{x}_i^{IS}\}$) and light-blue for a combination of thermal and IS information ($\{\mathbf{x}_i^{th}\} + E_{pot}^{IS}$). Error-bars represent the best and the worst of ten identical models independently trained with different random seed initialisation, and are comparable with marker's sizes. 
    % (bottom) Correlation $\rho$ between the true and the predicted propensity as function of the temperature. Two timescales are shown: $\tau = \tau_\alpha$ represented with full markers and $\tau = 0.5$ L-J time units represented with empty markers. The color code and marker shapes are the same as in the top plot. 
    % The multi-variate thermal positions + $E_{pot}(IS)$ choice is a good compromise to maintain high performance across timescales.



\subsection{Uni-variate or Multi-variate}

In Fig. \ref{fig:many_models} we compare the performances of various choices for our model, in particular uni-variate and multi-variate approach (red triangle and red diamonds).
We see that we get almost the same prediction accuracy by training only one model instead of ten models, at the moderate cost of increasing the number of parameters for that single model (because we are doubling the number of channels in the multi-variate case, from $4$ to $8$).
In other setups we even see slightly increased performance when comparing multi-variate multi-particle regression with uni-variate, A particles only regression.
%Actually, also keeping the number of parameters constant yields competitive results (not shown). 
In any case, we observe that the multi-variate choice slightly improves the robustness of our representation: it generalizes better to other temperatures.
Beyond performance considerations, it is very advantageous when considering generalization to other temperatures, since all timescales are encompassed in the same representation  $|\mathbf{h}^{(L_{max})}|$.
% Temp_generalization_MultivariateVsSingle
% [OPTIONAL]: is it useful to say this here?: 
% \textit{We stress that when performing multi-time regression, the representation built by the convolution layers $|\bf{h}_i^{(L)}|$ (the description of the density around each particle) is exactly the same for all timescales. }  %%%which is rather compact: only $32$ numbers (4 invariants when using $\ell_{max}=3$ for each of the $8$ channels). 


\subsection{Role of Inherent Structures} 

It has been observed several times that pre-processing the input positions by quenching them to their corresponding Inherent Structures (IS) helps most Machine Learning models in predicting long-time mobility measures \cite{schoenholz_structural_2016, jung2023predicting, alkemade_improving_2023}.
Such a quench is performed using the FIRE algorithm: temperature is set to 0 (velocities set to 0), and positions adjust gradually so as to converge to a local minimum of the potential energy, typically close to the original configuration.
This can be seen as a mere pre-processing step (for which the knowledge of the interaction potentials is needed) or as a new task, \ie predicting the propensities $\{m_i\}$ from the quenched positions $\{\mathbf{x}_i^{IS}\}$.
We note that the quench, while intuitively cleaning some noise related to thermal motion, destroys information too: one cannot recover the thermal positions from the quenched one (the mapping thermal to quenched is non injective).
% since it is not trivial that a network can learn to predict these quenched positions.

We observe that for our network, this new task is harder at short timescales, while it's easier at long timescales (in Fig.~\ref{fig:many_models}, compare the red diamonds and the dark blue downward-pointing triangles).
We interpret this result by noting that the quench destroyed the information about the relative location of each particle within its cage, thus making it much harder to predict short-time displacements.
Our experiment and its interpretation explain why some models, based on quenched positions alone, have very low performance at short timescales \cite{jung2023predicting}.
Their low performance should not be attributed to the machine learning models themselves, but rather to their input data.
About mobility at long times, there it is not much of a surprise that quenched positions reveal an underlying slowly-evolving pattern in the structure and thus help at prediction (although in principle all the information was contained in the original thermal positions). 


\begin{table}
    \centering
    \begin{tabular}{c|p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}|}
        \cline{2-5}&\multicolumn{2}{|c|}{$t = 0.5 LJ$} & \multicolumn{2}{|c|}{$t = 1 \tau_\alpha$}\\
        \cline{2-5} & ${\{\mathbf{x}_i^{th}\}}$ & ${\{\mathbf{x}_i^{IS}\}}$ & ${\{\mathbf{x}_i^{th}\}}$ & ${\{\mathbf{x}_i^{IS}\}}$ \\
        \hline
        \multicolumn{1}{|c|}{no $E_{pot}$} & $0.676^{+0.005}_{-0.006}$ & $0.274^{+0.001}_{-0.002}$ & $0.718^{+0.007}_{-0.003}$  & $0.815^{+0.003}_{-0.003}$ \\
        \hline
        \multicolumn{1}{|c|}{$E_{pot}^{th}$} & $0.678^{+0.006}_{-0.014}$ & $0.334^{+0.002}_{-0.001}$ & $0.728^{+0.005}_{-0.003}$ & $0.816^{+0.003}_{-0.001}$ \\
        \hline
        \multicolumn{1}{|c|}{$E_{pot}^{IS}$} & $0.677^{+0.005}_{-0.013}$ & $0.273^{+0.001}_{-0.002}$ & $0.798^{+0.005}_{-0.002}$ & $0.822^{+0.003}_{-0.001}$ \\
        \hline
    \end{tabular}
    \caption{\textbf{Influence of IS at low temperature.} For each combination of inputs, a multi-time model is trained at temperature $T=0.44$. We repeat the training $10$ times with variable parameter initialization and report the test set correlation coefficient (median, best and worst values).
    %\textbf{FL: why 10 and not an odd number, to take the median ? :) -> ok it takes the average of the two values that are in the middle of the range, it makes sense}  np.median([-20,3,4.1,500])==3.55
    } 
    \label{table:IS}
\end{table}


Ideally, one would like to combine both the complete information from thermal positions and the de-noised information from the quenched positions.
For GNNs, this could be done by building the graph from either the thermal or quenched relative positions, but using as edge features a concatenation of both.
However this would be quite costly in terms of memory and would increase the number of parameters needlessly.
%% IDEA: actually it's not so hard to do it like this. I'd be curious to see how it works, but then there are more interesting things in life.
Instead, inspired by the findings of \cite{jung2023predicting}, we compute the local potential energy (in either the thermal or the IS positions)  for each particle $E_{pot,i}= \sum_{j\neq i} V_{LJ}(\mathbf{x}_i,\mathbf{x}_j)$ and use it as a new scalar ($l=0$) input node feature.
This can be seen as a compressed version of the positional information.
Note that the first layer remains very interpretable: this new channel represents the field of potential energies surrounding a given particle, expressed in the spherical harmonics basis.
In Table  \ref{table:IS} we compare performances obtained for all combinations of input positions (thermal or quenched), with all possible $E_{pot}$ inputs (none, thermal or quenched), resulting in 6 combinations, that we study at two timescales: $0.5 \tau_{LJ}$ and $\tau_\alpha$.
We summarize the key results from this table:
\begin{itemize}
\item Adding the information about $E_{pot}^{IS}$ to $\{\mathbf{x}_i^{IS}\}$ is irrelevant. Indeed we observed that we could easily regress $E_{pot}^{IS}$ from a network with $\{\mathbf{x}_i^{IS}\}$  input with very high precision ($\rho\approx 0.9$). %, and in the table we see that the 1st and 3rd row of the 1st and 3rd column are very much equal: it does not help.
\item Similarly for thermal positions and thermal potential: adding $E_{pot}^{th}$ to $\{\mathbf{x}_i^{th}\}$ is basically useless, the increase from $\rho=0.718$ to $0.728$ is barely statistically significant.
% we can regress Epot-th anyway (?is it true? I dont remember), and the increase of perf by adding Epot-th ot the x-th is almost zero (\textbf{to be confirmed with the new results, that will have error bars} -- if indeed it's not significant this bullet point is then merge with the previous one
\item Adding $E_{pot}^{th}$ to  $\{\mathbf{x}_i^{IS}\}$ helps only at short timescales (from $\rho=0.27$ to $0.33$) and it's not sufficient to fill the gap with thermal positions.
\item Adding $E_{pot}^{IS}$ to  $\{\mathbf{x}_i^{th}\}$ helps,  but at long timescales only (from $\rho=0.72$ to $0.80$)
\item For predicting short times, thermal positions work much better than quenched ones: 1st column shows consistently larger performance than the 2nd one, by up to $0.4$ more in correlation.
\item For predicting long times, quenched positions work better than thermal ones: 4th column shows consistently larger performance than the 3rd one, by up to $0.1$ more in correlation.
\item A good compromise for maintaining performance at all timescales is to combine $E_{pot}^{IS}$ to  $\{\mathbf{x}_i^{th}\}$.  %  best of both worlds -> was nice.
\end{itemize}
In the table we focus on two timescales for clarity, and in Figure  \ref{fig:many_models} (top) we report results for 3 out of the 6 combinations but at all times.
% \textbf{TODO: move figure 5(bottom) here - which requires to first split it :(  ... to do during resubmissions...}
%
%
%Our claims are supported also by Figure \ref{fig:many_models} where multi-time models trained on different input data are compared: when thermal positions are coupled with potential energy computed in the IS the prediction accuracy is almost optimal on all the timescales. 
In Figure \ref{fig:many_models} (bottom) we study the effect of adding$E_{pot}^{IS}$ to the thermal positions (red to blue symbols) as a function of temperature, for two timescales. 
We verify that for the long timescale (full symbols) the addition of $E_{pot}^{IS}$ helps especially for the lower temperatures, where the potential energy landscape is expected to be more relevant, 
while for the short timescale (open symbols) there's no improvement at all, at any temperature.
%Error bars are not computed at all points, they are typically of the marker's size.


We can compare these observations with the findings of Alkemade \etal \cite{alkemade_improving_2023}.
They identify three physical quantities, each being relevant in a given time range:
\begin{enumerate}
\item In the ballistic regime, the forces $\mathbf{F}_i = - \nabla_{\mathbf{x}_i} E_{pot,i}$ are most relevant 
\item In the early caging time, the distance between the thermal position and the IS one $\Delta r^{IS}$ is most relevant
\item At later times, the quenched configurations are most relevant
\end{enumerate}
For the ballistic regime, our results perfectly match theirs: our model is likely to be aware of information equivalent to the forces, since it's able to regress the local potential energy with very high accuracy ($\rho\approx 0.9$). This explains our good performances in the very early regime (see also Fig.~\ref{fig:comparison}).
For the early caging regime, we tried to introduce $\Delta r^{IS}$ as a further $l=0$ node feature but were not able to see any significant improvement in the caging regime. This may be due to improper encoding of this information, or to a deeper shortcoming of our architecture, or also to the datasets being slightly different (see Appendix \ref{appendix:Shiba}).
For the long times, our performances are indeed high thanks to the use of $E_{pot}^{IS}$: they are slightly higher if we use $\{\mathbf{x}_i^{IS}\}$ (see Table \ref{table:IS} or Figure \ref{fig:many_models} (top)).


\subsection{Comparison with recent works}

Often, comparison with previous works can not be done rigorously, for two reasons: use of different datasets and different input data. As mentioned in the previous section and already pointed out in \cite{alkemade_improving_2023} the two main datasets~\cite{bapst_unveiling_2020,shiba_predicting_2022} differ in many features (see Appendix \ref{appendix:Shiba} for details), although being built from MD simulations of the same system (3D Kob-Andersen mixture).
A detailed comparison at fixed input dataset will be published by other authors and some of us in a Roadmap paper soon\cite{jung_2023_roadmap}.
A further difference is introduced by the choice of input data. For instance we have shown that the introduction of Inherent Structures helps, especially for low temperatures and long timescales.
Thus better performances for works that rely on IS do not directly imply that the machine learning architecture is better (or vice versa for works that are limited to thermal inputs).


\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{svg_figures/Comparison_otherWorks_v2.pdf}
    \caption{\textbf{Comparison with recent works.} Correlation $\rho$ between true and predicted propensity for A particles at temperature $T=0.44$, as a function of timescale, for several recent works. 
    Color indicates dataset choice: dark purple for Bapst' (NPT equilibration),
    %+ dynamics sampled at constant $S(\mathbf{q},t)$),
    orange for simulations using NVT equilibration.
    %and fixed timescales for sampling the dynamics. 
    Line-styles indicate input choices: thermal data only (dotted lines),  IS data (dashed lines) or a combination of both (solid line). 
    Markers describe the type of model: upper triangles refer to GNNs, diamonds for MLPs and squares for Linear Regression.
    Note how curves computed for a given dataset barely cross each other, indicating rather consistent ranking between models.
    }
    \label{fig:comparison} 
\end{figure}


Despite these limitations, in Figure \ref{fig:comparison} we provide a qualitative comparison of methods by considering each as a whole, regardless of the details of dataset and input choice. 
Thus we compare our model trained on thermal positions + $E_{pot}(IS)$ at temperature $T=0.44$, with recent works in the field \cite{boattini_averaging_2021, alkemade_improving_2023, jung2023predicting, bapst_unveiling_2020, shiba_predicting_2022}.
\\
Boattini \etal \cite{boattini_averaging_2021} and Alkemade \etal \cite{alkemade_improving_2023} apply linear regression techniques on expert rotation-invariant features: structural descriptors that capture the local density and express it in terms of spherical harmonics. They also perform coarsening of these descriptors by averaging over the neighborhood of each particle, in a manner reminiscent of the aggregation step of a GNN.
In particular \cite{alkemade_improving_2023} emphasizes the significance of IS inputs for long timescale predictions and that of using the estimated cage size as input for short timescales.
\\
Jung \etal \cite{jung2023predicting} apply an MLP on coarse-grained structural indicators computed from quenched positions including local potential energy and estimated dynamical fluctuations and introduce a carefully designed loss function to capture the spatial correlation of the original mobility field.
\\
Bapst \etal \cite{bapst_unveiling_2020} and Shiba \etal \cite{shiba_predicting_2022} apply standard Graph Neural Networks (GNN) on raw inputs, as we do here.
The latter study introduces an auxiliary task of edge-regression:
to enhance the accuracy of mobility predictions,
they regress the change in particles' relative positions.
\\
Our proposed approach outperforms all previous methods for timescales approaching the structural relaxation time ($\tau_\alpha$), while demonstrating competitive results in other regimes. Notably, our model achieves comparable performance to other GNN approaches on short timescales (ballistic motion), despite being the first to regress all timescales simultaneously. 
For the early caging regime,
 we do not perform as well as Alkemade \etal \cite{alkemade_improving_2023}
although it is important to note that they incorporate early-times related information as an input feature.
To be fair, we overperform Shiba \etal \cite{shiba_predicting_2022} only when using the quenched input. We do not know about their performances when using the quenched input too.
\\
Jiang \etal \cite{jiang_geometry-enhanced_2022} 
(not shown)  % \textbf{TODO: show it?}
use a GNN that computes angles between nearest-neighbors (\ie  it provides geometric information, reminiscent of our equivariant approach), and introduce a self-attention mechanism that is designed to capture the spatial heterogeneity of the dynamics, referred to as smoothness in their work. It is not clear to us whether their network is partly equivariant or not, and it is rather obviously heavier than ours. At most timescales we perform a bit better.


Since the first version of this paper (preprint of Nov. 2022), we tried to include these recent works' ideas to improve performance.
Our use of $E_{pot}$, inspired by \cite{jung2023predicting}, was indeed successful.
However, when we tried to mimick \cite{shiba_predicting_2022} by regressing the edge relative elongation as an additional (edge) target label, 
or when we tried to reproduce the results of  \cite{alkemade_improving_2023}, using as input node feature the distance to the local cage center (estimated as the quenched position),
or when we introduced equivariant attention schemes (inspired by \cite{jiang_geometry-enhanced_2022} but technically as in \cite{liao2022equiformer, hutchinson2021lietransformer}),
our attempts did not yield any significant improvement (nor deteriorated the performance).




% \textbf{FL: suggestion: we could separate two kinds of comparison here. The first one, that is done now, compares methods as a whole, including variable input type. Another form of comparison is to compare methods from the ML perspective, keeping inputs \textit{(but not necessarily tasks, which include output, which is data..?)} constant.}
To compare Machine Learning architectures in a fair way, one should work at fixed task (fixed dataset and input data). 
We now respect this constraint to obtain two precise results, that we deem significant.
\\
Firstly, going back to using only the thermal positions as input, we perform an ablation study on the choice of $l_{max}$, to compared fairly with Bapst \etal,  % or with Shiba !! and then we lose -but he uses more labels- but ok it's not the point we make here
and notice that: (i) restricted to $l_{max}=0$, we reach the same accuracy, (ii) increasing $l_{max}$ notably improves results, especially up to $l_{max}=2$.
We conclude that the equivariant nature of a network can be key to its performance, compared to a standard GNN approach. Numerical proof is provided in Appendix \ref{appendix:ablation}, figure \ref{fig:ablation_ellmax_noEpot}.
%We're still the first ones to do a GNN (not a simple LR) that combines invariant features with several layers: Bapst is not even invariant (as Shiba, for Jiang it's not clear) 
\\
Secondly, using the same kind of (invariant) inputs as non-GNN methods \cite{boattini_averaging_2021, alkemade_improving_2023, jung2023predicting}, \ie  thermal positions combined with $E_{pot}(IS)$, we study the impact of the network's depth.
We noticed already in Fig.~\ref{fig:comparison} that we perform better than those methods, at most timescales. 
Here we want to stress that the network's depth plays a crucial role (more so that the rotational order $l_{max}$): varying the number of convolution layers from $L_{max}=1$ to $L_{max}=7$, we noticed that performance does not even saturate. 
We conclude that although using invariant features (and ideally, equivariant ones) is helpful, the combinatorial power of deep learning architectures is also key to performance.
Numerical proof is provided in Appendix \ref{appendix:ablation}, figures \ref{fig:ablation_ellmax} and  \ref{fig:ablation_nLayers}.

A side result of these ablation studies is that the short timescales seem to be the ones that benefit the most from increased $l_{max}$, while they also benefit from increased depth ($L_{max}$).
%\textit{especially at short timescales, while at long timescales invariant features ($l_{max}=0$) seem to play the major role, although they benefit a lot from increasing the depth of the network (number of layers).}
We conjecture that directional features are key to computing instantaneous forces, itself a key element for predicting short-time dynamics.

\begin{figure}
    \includegraphics[width=0.45\textwidth]{svg_figures/temp_generalization/Bapst_Chi4.pdf}
    \includegraphics[width=0.45\textwidth]{svg_figures/temp_generalization/Shiba_Chi4.pdf}
    \caption{\textbf{Fluctuations of the Self-overlap function.} Time evolution of the fluctuations as measured by $\chi_4^s(t)$. 
    Top: On Bapst's dataset;
    Bottom: Shiba's dataset.
    MD is short for Molecular Dynamics and refers to the ground truth.
    ``Separate training'' indicates a new model was trained at each temperature (but dealt with all timescales at once), while ``transfer-learned'' refers to sec.~\ref{sec:transferability}: we apply a single model trained at a given temperature to all other temperatures (top: $T_{train}=0.50$, bottom: $T_{train}=0.56$).
    }
    \label{fig:chi4_transfer_learning}
\end{figure}

\subsection{Spatio-temporal correlations}
%% statistical properties of our predicted mobility: chi4 and G4
The particle-wise correlation coefficient between ground truth mobility and predicted one is not everything, it's good to also measure whether the statistical properties of our predicted mobility match those of the true one.
%A further assessment of the robustness of our learned representation is shown 
Defining $c_i(t) =  \tanh{(20(m_i(t)-0.44)+1)}/2$ a pseudo-binarized mobility measure,
$Q^s(t) = \frac{1}{N_A}\sum_{i\in A} c_i(t) $ its sample average (also called self-overlap function),
one defines a four-point correlation function $\chi_4^s(t) = N_A \left[ \left<Q^s(t)^2\right> - \left<Q^s(t)\right>^2 \right]$,
the fluctuations of the Self-overlap function,
that we report in Fig.~\ref{fig:chi4_transfer_learning}
(we use the same specifications as in \cite{jung2023predicting}).
This measure of the sample-to-sample fluctuations of mobility is often interpreted as a volume of correlation (as it can be re-written as the integral of a correlation function).
Our estimated $\chi_4^s$ (``separate training'') is generally smaller than the ground-truth (MD) but tracks variations over time fairly well and much better so than the initial GNN of \cite{bapst_unveiling_2020}.
Furthermore it is comparable to the performance of \cite{jung2023predicting}, which however incorporates information about fluctuations in theirs model's loss.
One may notice that the amplitude of fluctuations is smaller in the first dataset (Baspt's): this is due to the peculiar sampling choice, in which samples at a given ``timescale'' are actually taken at different times but equal value of the self-intermediate scattering function $F_k(t)$,
%% TODO: write also the explicit formula for Fk(t) here, inline.
a choice which by definition reduces the variance between samples.
%% TODO: also show a heatmap, qualitative comparison of the predicted and true field, for a slice of Shiba's or Bapst's data, and refer to it here.


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{svg_figures/G4_transfer.pdf}
    \caption{\textbf{Spatial dynamical correlations.} 
    The function $G_4$ is computed on the true labels (MD, solid line) or on our predictions.
    Same color and marker coding as previous plot.
%    The label ``MD'' indicates the ground truth correlations, and markers are computed from our predicted mobility fields.
    Our models reproduce $G_4$ remarkably well (``separate training''), especially at low temperatures (blue), 
    while the transfer-learned fields track the trends and orders of magnitude correctly as well.
    }
    \label{fig:G4}
\end{figure}
%% REMARK: actually there is not much to comment here, it works well, that's it. We could also move this to supp mat.
A complementary measure of the statistical quality of the predicted mobility field is given by the spatial correlation function of the mobility-related quantity $c_i(t)$: $ G_4(\mathbf{r},t) = \frac{V}{N_A} \left< \sum_{i,j \in A}\Tilde{c}_i(t) \Tilde{c}_j(t) \delta (\mathbf{r}-\mathbf{r}_i(0)+\mathbf{r}_j(0))  \right>$ where $\Tilde{c}_i(t) = c_i(t)-\left<c(t)\right>$.
Our predictions reproduce it almost perfectly (see Fig.~\ref{fig:G4}).


\section{Temperature generalization}
\label{sec:transferability}

\begin{figure}
    \includegraphics[width=0.45\textwidth]{svg_figures/temp_generalization/Bapst_timIdx=9.pdf}
    \includegraphics[width=0.45\textwidth]{svg_figures/temp_generalization/Shiba_timIdx=6.pdf}
    \caption{\textbf{Transfer-learning between different temperatures.} Each model is fully trained once at one state point (T) and tested (``transfer test'')  or fine-tuned on the remaining ones (``transfer learn'') . The timescale of mobilities showed in the plot is $\tau = \tau_\alpha(T)$, but multi-times models were used. (top) Bapst' dataset, (bottom) Shiba's dataset.
    The transfer learned-generalization on Shiba's dataset is almost indistinguishable from direct training, indicating excellent generalization power of our learnt representation.
    }
    \label{fig:transfer_learning}
\end{figure}

%Here we open on interpretating the learned representation as an order parameter and
%experiment on the robustness of our representation in section \ref{sec:transferability}.

Here we want to push forward the idea that one can use deep learning to define a structural order parameter.
We start by developing further the arguments briefly evoked in the introduction.

Although it is not clear yet whether the glass transition is indeed a thermodynamic phase transition or a crossover, an avoided transition, there is no doubt that a dynamical phase transition exists \cite{royall_dynamical_2020}. Under cooling through the critical temperature, a sample goes from a fully liquid to an almost fully solid state (provided the measurement timescale for defining liquid/solid is set constant) and around the transition temperature, one observes a phase coexistence of mobile (liquid-like) parts and non mobile (solid-like) parts. Swap-Monte Carlo methods have allowed to measure this effect while remaining in equilibrium, which confirms that this scenario is not an effect of falling out of equilibrium \cite{scalliet2022thirty}.
This coexistence phenomenon is commonly referred to as Dynamic Heterogeneities, which are dynamic because they relate to the local dynamical state and also because this dynamical state slowly fluctuates over time.
The deep learning program for glasses is then rather obvious: define a machine learning model, \ie  a function $f_\theta(\{\mathbf{x} \})$
%% more complete but heavier notation:
%% ($\Vec{f}_{i,\theta}(\{\mathbf{x}_j\}_{j=1,\dots,N})$)
that solely depends on the structure as input, and train it to predict the local mobility (such as propensity, which acts as a proxy for the dynamical order parameter, which has a transition).
Thus one obtains a formally purely structure-based function  $f_\theta(\{\mathbf{x} \})$ that has a transition at the critical temperature, \ie a structural order parameter.
A counter-argument to this line of thought is that by definition such an order parameter is not strictly structure-based, because it uses mobility as training data and since ``neural networks can overfit'', this function $f_\theta(\{\mathbf{x} \})$ simply tracks what it was trained to fit, \ie mobility: in a sense, regressing mobility would be like ``cheating''.
Admittedly, it is conceivable that a heavy network, with millions of parameters, specialized for a given temperature and timescale, could associate mobility variations to tiny peculiarities of the physics of that particular temperature and timescale, which do not generalize to other temperatures or timescales.
The underlying idea is also that if the function $f_\theta$ is so heavy and complicated that it cannot be interpreted, we do not learn anything about the physics, or at least it is not a satisfying order parameter.
In this view, a network that would reach a correlation of $\rho=1$ would be seen as a simple computational shortcut to compute the iso-configurational displacement faster than Molecular Dynamics (which in itself would already be quite useful, \eg for designing effective glass models, as \cite{zhang2022structuro}).

However, here we argue that a Deep Learning model $f_\theta$ should be seen as a microscope that magnifies the subtle structural variations present in the structure.
%, which are expected to somehow correlate with the dynamics.
Training a model to predict mobility is just a recipe to extract the relevant structural variations, but the details of this training are irrelevant. To reconcile our view with the previous one, self-supervised learning is a promising route.
We recall that an order parameter $f(\{ \mathbf{x}\})$ must be defined uniquely for a given system, regardless of temperatures, which translates into applying the same (trained) model $f_\theta$ to all temperatures, at least for a given glass-former.
Then, an indication that this order parameter does track relevant structural changes is its ability to predict mobility and its spatial and temporal correlations, especially at temperatures other than the training one. 
This kind of simple transfer-test performance measure was already introduced a few years ago~\cite{landes_attractive_2020, bapst_unveiling_2020},  %% FL: I don't think people did this before my 2019 arxiv ? I could be wrong? In a sense the arrhenius law of andrea is also that, hence I mention it here:
while the idea of applying the trained model to other temperatures dates back from the original works on Machine Learning applied to glasses \cite{Schoenholz2014}.
% This is what we test in Fig.~\ref{fig:transfer_learning}.  %%NOTE: not exactly because we don't do simple test, we do transfer-learning.
%To probe the robustness (generalization ability) of our representation, we perform a transfer learning experiment across temperatures: we train the full model at a single temperature, then  freeze the parameters in all layers except from the very last one. %(which performs node-wise linear regression). 
%At all other temperatures we then re-train (the last layer only) and test the model. 


\subsection{Transfer-testing}
Here we repeat this experiment and observe better temperature-generalization abilities of our network, as compared with the original work of Bapst~\cite{bapst_unveiling_2020}, as shown in Fig.~\ref{fig:transfer_learning} (top part, label ``Transfer test'').
We also perform the same experiment using the more recent Shiba's dataset~\cite{shiba_predicting_2022}, showing even better temperature-generalization.
This is a strong indication that our network learns the relevant subtle structural signatures rather than ``overfitting'' the dynamics separately at each temperature.
%%%% The main differences between the two datasets are that the statistical ensemble used for equilibration (NPT in Bapst's, leading to variable density among samples, and NVT in Shiba's), and the sampling of dynamics being performed at constant Structure Factor, resulting in different samples of the same temperature having dynamic propensities evaluated at different times (while in Shiba's, the timescale is fixed).
%%% 
%Comparison with previous works is not always straightforward due mainly to two reasons: use of different datasets and different input data. As pointed out in \cite{alkemade_improving_2023}, the Bapst dataset is obtained through constant pressure simulations meaning that density varies among samples. Furthermore the dynamics is sampled not at fixed times but at constant Structure Factor value, thus different samples have as target the dynamic propensity at different timescales (but equal decay of the Structure Factor). In other works (\cite{alkemade_improving_2023,shiba_predicting_2022}) the size of the simulation box is fixed (and thus the density) and also the timescales for sampling the dynamics are fixed. In the appendix we show the performance of our model on Shiba dataset. 
%%%
%% Clearly our generalization ability is improved in the second case.
We note that performance at a given temperature decreases as the training temperature goes further away from the test temperature (reading markers vertically). This can be attributed either to an increasing dissimilarity in the structures present, or also to a change in how these structures correlate with the dynamics at different temperatures.
We also note an asymmetry in the performance drop between training at high temperature, testing at low (red line) or vice versa (blue line): the training at high temperature generalizes better, comparatively. In works based on SVM, the opposite was observed, and attributed to the noisy nature of high-temperature data. Here we do not seem to suffer from this noise, and attribute the increased generalizability to
the larger diversity in input structures observed
 and
 the broader range of propensities observed
 when training on high temperature data.

%% (already recycled) In Fig.~\ref{fig:transfer_learning} we show that the model generalizes well across temperatures, with an accuracy that decreases when the spread between train and test temperatures increases. On the right, we study the fluctuations of the overlap function, \ie the statistical properties of our predicted mobility.


%Here we go further, embracing the Deep Learning idea of learning representations, and introduce the idea of a multi-dimensional order parameter.
%Concretely, we speculate that our ``order parameter'' need not be a scalar: since glasses are complex materials, it may need to be a vector $\mathbf{f}_\theta(\{\mathbf{x} \}) \in \mathbf{R}^D$, with $D>1$. Then, this vector ought to correlate with the dynamical variables (like propensity evaluated at various timescales).

\subsection{Transfer-learning}
Here we go further, embracing the Deep Learning notion of learning \textit{representations}, and comment on the properties of our learned representation.
Indeed, the convolution layers that we stack, as described in section \ref{sec:arch}, effectively build an equivariant feature $\mathbf{h}^{(L_{max})}$ that describes the local structure around each particle. 
%This feature can be interpreted as functions on the sphere (8 functions if we have 8 channels), and is equivariant.
As explained in sec.~\ref{sec:arch} (\textit{decoder}), the norm $|\mathbf{h}^{(L_{max})}|$ of these features is a list of $32$ numbers (8 channels times 4 possible $l$ values, $l=0,1,2,3$) that is decoded into mobility by 20 independent decoders.
% (in the multi-variate setting where we regress $n_{times}=10$ timescales and both particle types at once).
Thus, for any training temperature the model has to somehow pack the information about these 20 (non-independent) scalar values into the 32 components of $|\mathbf{h}^{(L_{max})}|$.


From such a representation, one can consider doing various things.
For instance, one can perform clustering to study the number of different local structures, or perform dimensional reduction (\eg PCA) to visualize their distribution.
%In a sense,
A cluster of points $|\mathbf{h}_i^{(L_{max})}|$ in the 32-dimensional space could then be seen as a Machine-Learned Locally Favored Structure (LFS).
It is then physically informative to track the evolution of the population of each cluster \cite{Coslovich2007, Coslovich2011} (type of local structure) for instance as a function of temperature or time since a quench.
%1)  speculation about stuff to do : Daniels's unsupervised (PCA, clustering of h, etc), or even self-supervise this thing, or, transfer learn by fine-tuning the decoder.
Although here we use labels (mobility) to learn our representation, it is in principle possible to adopt a self-supervised learning strategy to learn a representation $|\mathbf{h}^{(L_{max})}|$ only from input data (spatial configurations), using a pretext task as \eg de-noising particles' positions.
%% other stuff one could do with representation ? put it here if you have more ideas.
Note that the original works of Liu \etal were also focusing on a representation rather than a model's prediction, when using the continuous output of the SVM as a physical quantity of interest (named Softness), instead of using its thresholded, binarized version (which is the actual prediction of an SVM classifier), although in that case it is a one-dimensional representation.


Here we further probe the robustness of our model by testing the generalization ability of its underlying representation, $|\mathbf{h}^{(L_{max})}|$.
If we consider $|\mathbf{h}^{(L_{max})}|$ to be a generic structural descriptor, a sort of multi-dimensional structural order parameter, then it must be relevant at all temperatures.
A simple way to evaluate whether this structural measure captures the glass transition is to see whether it tracks dynamics correctly, especially in temperatures different from the training one. 
Concretely, we train a representation  $|\mathbf{h}^{(L_{max})}|$ by regressing labels at a given temperature, and then fine-tune only the decoders at other temperatures. The part of the network responsible for computing  $|\mathbf{h}^{(L_{max})}|$ (most of it) is frozen, so the fine-tuning reduces exactly to linear regressions (we need to learn the weights $\mathbf{w}$ of the decoders as in Eq.~\ref{eq:decoder}, \ie only $32$ values per timescale and per particle type).
%%%%%%%%%%
%For instance, at the timescale $\tau_\alpha$ and for particles of type $A$, the model writes: $y_{A,\tau_\alpha}= \mathbf{w}_{A,\tau_\alpha} \cdot |\mathbf{h}^{(L_{max})} (\{\mathbf{x}\})|$, where $y_{A,\tau_\alpha}$ is the mobility label and $\mathbf{w}_{A,\tau_\alpha}$ is a set of weights to be regressed (32 real values).
%%%%%%%%%%
%\textbf{IDEA: start with a pedagogical analogy with Vision task and what transfer learning measns and does over there, then explain what it measn for physics. OR, start wiht the stuff below and then explain analogy ? Priobavbly, analogy first.}
This idea of transfer learning is central to Machine Learning, and has shown great success \eg in computer vision.
For instance, a Convolutional Neural Network (CNN) is trained on a first task (\eg ImageNet data, with 1000 classes). Then the backbone of the network (all convolution layers) is frozen, and the last 2 layers that decode this representation into labels are re-trained, but for another task (\eg CIFAR10, or any other kind of natural images). This transfer-learning experiment can show improved performance compared to directly training the network on the final task, especially when there are less data available there \cite{bengio2012deep}.
Most importantly, the fact that transfer learning can perform well is an indication that the representation learnt by the network is more generic than one could think: the backbone is good at extracting image's features, it can be thought of as an advanced image pre-processing tool.
An application of transfer learning is few-shots learning \cite{wang2020generalizing}: 
having build a good representation from a first large dataset (either with labels or with self-supervised learning), one then trains a classifier using only a handful (1 to 5) examples (per class).
In our case, $|\mathbf{h}^{(L_{max})}|$ is good at extracting structural features, or more precisely at detecting patterns that correlate with the mobility.

We report the results of our own transfer-learning experiment between temperatures in  Fig.~\ref{fig:transfer_learning}. As expected, the performance (dashed lines) is improved compared to transfer-testing  (full lines).
Since the predictions are computed as a linear combination of the components of $|\mathbf{h}^{(L_{max})}|$, the correlation coefficient $\rho(y_{A,\tau_\alpha}, y_{A,\tau_\alpha}^{GT})$ shown here can be seen as an aggregated measure of the correlations between the ground-truth labels $y_{A,\tau_\alpha}^{GT}$ and the individual components of $|\mathbf{h}^{(L_{max})}|$.
%More future work: study how the weights learned at each transfer temperature depend on T.
Going further into the direction of considering $|\mathbf{h}^{(L_{max})}|$ as a (multi-dimensional) equivalent of a structural order parameter, one could then study how the coefficients $\mathbf{w}_{A,\tau_\alpha}$ depend on the target temperature, and \eg attempt to fit them with interpretable functions of the temperature. % (as ${\rm exp}(-a.T)$).
%We report the temperature dependence of the 32 components of $\mathbf{w}_{A,\tau_\alpha}$ in the Supplemental Material, and
We note that among the components of $\mathbf{w}_{A,\tau_\alpha}$, most of them vary monotonously with temperature, and in particular all the $\ell=0$ components do (those dominate the total). We leave deeper study of these coefficients for future works.


As a further check of our representation's robustness, we also report the transfer-learned estimated $\chi_4^s$'s in Fig.~\ref{fig:chi4_transfer_learning}, which show larger discrepancies than those trained at each temperature, but still track the trends seen in the data (similarly in Fig.~\ref{fig:G4}).
Note that this transfer-learned $\chi_4^s$ measures the structural heterogeneity, since our input is purely structural, using a single set of descriptors (the representation $|\mathbf{h}^{(L_{max})}|$).
To our knowledge, this is the first time a unique set of descriptor is shown to display such large structural fluctuations across temperatures and times.
Here we do not show the transfer-test results for clarity (they are typically a bit worse).
% \textbf{OPTIONAL: maybe actually show the transfer-test chi4 rather than the transfer-learned ? Because what I wrote here is not correct: in the transfer-learned exp, we used different decoders at different T (and for different timescales too, but ok).}


%In Fig.~\ref{rho_vs_time_and_temp} and table \ref{performance_table}, we show our main result, \ie that our model consistently outperforms the SOTA \cite{bapst_unveiling_2020} by $5$ to $10\%$ depending on the task, in terms of test accuracy, despite having much fewer learnable parameters.
%For comparison, we also report results from the expert approach \cite{boattini_averaging_2021}, which nearly matches the GNN baseline.



\section{Future directions}
\label{sec:futureDirections}
%Although we performed neural architecture search by hand somehow to achieve these results, we did not yet explore the space of possibilities thoroughly, and there is ample room for improvement.
Here we performed non-exhaustive but rather thorough architecture search, and found no performance gain when trying rather obvious improvements for the network, such as increasing $l_{max}$, increasing the decoders' complexity, introducing bottleneck layers (reduced $l_{max}$ or channel number), using attention mechanisms as in \cite{liao2022equiformer, liao2023equiformerv2,hutchinson2021lietransformer}, or attributing channels to specific bond-types. 
% \textbf{OPTIONAL: (complete the list of stuff we tried to find basically no change -- if I missed some?)}
%Late into this study we realized performance could be improved a bit 
% when using up to $20$ channels (possibly lowering $l_{max}$ to 1 without much damage, to spare GPU memory), --> CAREFUL THIS WAS WITH 5 EPOCHS !
%or increasing the number of layers to $L_{max}>7$, but do not include these results here, by lack of time.
%%%%%%%%
This list of negative results does not prevent us from formulating further suggestions for improved performance, which can apply to our model or other's, and are left for future work.
This includes:
\begin{itemize}
    \item regressing not the iso-configurational average displacement, but single trajectories-specific displacements (in norm). From a computational perspective, using an average (the propensity) to train, when 30 individual realizations are available, appears as throwing away some of the training data. With the mean squared error loss, training on single instances will converge to a model that is good at predicting the average.
    \item fully using the equivariant nature of the network to predict the vector of the displacement (3 components in 3D) instead of a scalar (its norm); this needs to be combined to the first idea.
    % OK - included ajust above - Other supervised tasks, that could be considered to increase robustness could be: predict the particle's new position (iso-conf average) or predict single-instance positions (total displacement or vectorial displacement). 
    \item performing a sort of data augmentation by adding noise to the input. In practice, a very good ``noise'' would be to sample the positions at short timescales around $t=0$, \eg at $t=0.5\tau_{LJ}, 1\tau_{LJ}, 1.5\tau_{LJ}, 2\tau_{LJ}, \ldots$. For predicting timescales $\tau \gg \tau_{LJ}$, this is a negligible perturbation and would allow to teach the network what are similar configurations.  %%-> self supervised learning !! contrastive learning
    \item decode various timescales with a single decoder that would be timescale-aware, in a fashion akin to that introduced in FiLM \cite{perez2018film} for instance (conditioning the decoder to a proper embedding of the timescale as in \cite{gupta2022towards}, so as to use a single final decoder).
    %The advantage of our approach is that all predictions are derived from a single representation.
    \item training the backbone on several temperatures at once, using separate decoders for separate temperatures (possibly using the previous idea also for decoding temperatures, so as to have a single decoder, that would be timescale-aware and temperature-aware).
    % \item use more physical features as inputs or as pretext-task outputs: using both thermal and quenched positions, or both thermal and quenched $E_{pot}$ (not expected to bring improvement), cage size or distance to cage center (this we tried without success)
    \item in the spirit of \cite{jung2023predicting}, use non-local quantities as additional target labels (additional terms in the loss), such as the global value of correlation functions evaluated at a few lengths (label computed for the whole sample, resulting in a graph-wide target), or more simply the local variance of the mobility (variance of target label for a node's neighborhood). This is expected to increase the quality of the prediction in terms of spatio-temporal correlations, \ie decrease over-smoothing, a known issue in GNNs.
    \item use more expressive equivariant architectures, such as those recently introduced in \cite{batatia2022mace}.
\end{itemize}
Self-supervised learning is a possible way around the ontological issue that our structural features are trained using dynamical data (as labels).
Here we outline a few possible self-supervised strategies: 
\begin{itemize}
    \item Contrastive learning: a network could be made to identify when 2 configurations are almost the same (input configurations differing by a couple of $\tau_{LJ}$) as opposed to independent configurations.
    \item Denoising: adding nonphysical noise to thermal (or quenched) positions, ask the network to denoise the input.
    \item Predict only known quantities such as $E_{pot}$, or the quenched/thermal positions, from the input thermal/quenched positions.
    %\item (weaker proposition) train using short timescales as labelled data, then extrapolate to longer times)
\end{itemize}
% item: predict other quantities of interest, as the direction of minimal yield stress \cite{patinet} 

%\textbf{either delete this, or include it above when talking about pre-training. mixing various glass formers is not really in the spirit of a structural order param wiht a simple model, so... reduce this a lot, at least.}
A much more ambitious (and debatable) idea would be to use a very heavy backbone (with attention for instance) and mix tasks between various glass-forming liquids, various crystalline materials, together with other amorphous material's input, to require the backbone to generalize even more strongly. This kind of pre-training strategy has been shown \cite{liu2021pre, fang2022geometry,zhang2022protein} to be effective to improve robustness. 

% \textbf{F: perfect final paragraph, let's  keep it as it is !}
Whatever improvements one could think of, we believe that the SE(3)-GNN framework is the right one to be developed.
Indeed, it seems in line with the recent history of neural architecture design \cite{bronstein2021geometric}: while the CNN architecture has been a game-changer thanks to its enforcement of translation-equivariance by design, the GNNs then enforced the node permutation-equivariance by construction, and SE(3)-GNNs now additionally enforce the rotation-equivariance, leveraging all the symmetries at hand.


% \textbf{DONE ! Well done :}
% \\
% \textit{Left for future work: perform multi-variate regression on timescales, improve transfer-learning across temperatures.
% Regress also the B's, simultaneously to the A's.}

\section{Conclusion}
\label{sec:discussion}

% Summary of the key added values present in the paper:
% PEDAGOGICAL: -> recycle what is already there in conclusion:
% OK - introduction of the Se3-equivariant nets framework, new to the physics community
% OK - introduction of an original Se3-equiv model, adapted to glasses, using Epot, th and IS for improved perf, [and regressing all particle types and times]
% OK - clarifiy the role of IS/th inputs:
%   IS > th at long times
%   th > IS at short times
%   th+IS: best of both worlds
% OK - usefulness of Epot: if using Epot of the same ensemble (th/IS), it does not help, the network can compute it basically.
In this paper we first provide a pedagogical introduction to the general theoretical framework of rotation-equivariant Graph Neural Networks, then present an original Neural Network design specifically tailored to model glassy liquids.
In particular, 
inspired by recent works on Machine Learning glasses \cite{jung2023predicting,alkemade_improving_2023}, 
we combine information from thermal positions and their quenched counterpart, using the local potential energy of quenched positions as input, thus boosting our network's performance.
We disentangle the role of thermal and quenched positions: the former are necessary to predict dynamics at short times, the latter are helpful at long times. 
The potential energy itself is useless to us, it can be predicted accurately from positions alone%
% by our SE(3)-equivariant GNN
, we only use it as a shortcut to pass information about the quenched positions.

As is well known in physics, finding symmetries and enforcing them in a model is key to reducing 
the %that model's 
number of degrees of freedom. 
In the machine learning vocabulary this translates to building representations that respect the proper symmetries \cite{bronstein2021geometric}.  % building features or  building representations ? or  building networks ?
%Here we achieve this by combining two ingredients.
SE(3)-equivariant networks achieve this by combining two ingredients.
First, the proper embedding by Spherical Harmonics builds a description of the neighborhood that is both easy-to-rotate and compact since it is a decomposition in rotational order, akin to frequency decomposition, a much more efficient representation than directly discretizing angles.
Second, layers combine the hidden representations $\mathbf{h}_i^{(L)}$ in a way that preserves equivariance up to the last layer, a choice that guarantees maximum expressivity for the network as opposed to rushing to invariant features.
These two ingredients are key to building a good representation: our overall performance is above that of other approaches at most timescales and we achieve better generalization across tasks, while using fewer learnable parameters.


% Performance / Machine Learning technicality: 1 sentence for each point.
% OK - overall GOOD PERFORMANCE (fig 6), and although we are GNN, we don't use that many parameters, and don't need too much data to do well (although the results would still improve with more data!)
%   +We have less or as many parameters as Shiba, or Bapst. (in the single-time case; for multi-time we have 50.000 parameters, bapst is 70.000)
%   +We can do as good as LR using as many or fewer params. For MLP I don't know the number of params.
%   +Probably we could use only ellMax=1 and get similar results, with many less params.
% OK - comparison with GNN or LR: compare perf and interpretability.
%     OK- (already 1 phrase in v1, keep 1 phrase for this) equivariant GNN (or at least invariant one, with ellmax=0) is better than naive one (Bapst, not even invariant) 
%                 [indeed, using same input exactly, we beat Bapst iff we use ell>0, at ell=0 we just recover their rho=0.65 performance]
%     OK- (1 new sentence) deep net (L>1) is better than SOAP/Boattini (even remaining purely invariant, ell=0, we do better/as good)  
%                 [note: if we do ell=0 from the start, we are purely isotropic, much less expressive than SOAP, but somehow still do good]
More precisely, we compare well with two families of architectures.
On the one hand, compared to Deep Learning and in particular GNN models \ie  models~that are not equivariant, nor even invariant~\cite{bapst_unveiling_2020, shiba_predicting_2022}, our SE(3)-equivariant architecture %% I say architecture becasue here we restrict to no-Epot input.
performs better with much fewer parameters, as soon as we use strictly equivariant features ($l_{max}>0$), \ie we prove the usefulness of equivariance.
%% For lmax=0, we obtain similar perf as Bapst, at a much reduced number of parameters (maybe using more channels we could do slightly better than Bapst).
On the other hand, compared to shallow learning techniques~\cite{boattini_averaging_2021, alkemade_improving_2023, jung2023predicting}
%(Linear Regression or rather small MLPs) 
that use expert features as input (invariant features and the local potential), our deep network performs better (when enriched with the combined information from thermal and quenched positions),
and the deeper it is the better it performs, \ie we prove the usefulness of deep architectures (as embodied by GNNs).
%% in that case, using Epot it seems that invariant features are enough ??? ok that's another story.. not for today.


% - improved interpretability, compared to Naive GNN:   
%   + at least we can relate to SOAP and interpret the action of the first layer, 
%   + and somehow also of the second one if using ell=0 in the second layer for instance. Then any layer's channel can be seen as a function on the sphere, but it doesn't help much..
% - [NOT DONE] Bridge the gap between expert-features oriented works and GNNs: by a series of steps, within our framework, one can express models from Filion, to us.
In terms of interpretability, while we cannot claim our network to be fully interpretable, we show that our first hidden feature $\mathbf{h}_i^{(L=1)}$ corresponds to representing the field of density locally, around node $i$, \ie it relates directly
%%%
%\textbf{OPTIONAL: do some work in pen-and-paper, then put it in appendix, to show the precise connection between our net, the net without phi, and these guys, then mention it in main text (interpretation), then mention it even more concisely here.}
%%
% to the  Bond Order parameter rotation-invariant features 
% or more precisely to the smooth overlap of atomic positions (SOAP) ones 
 to the Bond Order parameter (BO) variant introduced in \cite{boattini_averaging_2021}).
The next layer is a field of that local density field representation, \ie much less intuitive to grasp, yet much easier to describe with explicit formula than the representations typically built by usual GNNs (which rely on fully connected layers to compute representations, thus completely entangling the inputs).


Last but not least, in this paper we emphasize the importance of building a robust representation: as explained in sec.~\ref{sec:transferability}, the pure performance measured by the correlation of our predictions with the ground truth mobility is a means to an end, not an end in itself. 
What truly matters is for our representation of the local structure to allow to deduce physical facts.
Our good correlation $\rho$, the very good fit of $G_4$ and acceptable trends of predicted $\chi_4$'s are all clues that we built a decent representation: we are able to capture the mobility field locally as well as its spatial and temporal correlations.
But most crucially, the fact that a representation learnt at a given temperature can readily generalize to other temperatures is what makes our predicted field an acceptable structural order parameter.
This %unrivalled 
generalization power is due mostly our use of an equivariant representation, and is reinforced by our idea of regressing all particle types and all timescales at once: we use a single backbone representation, the various predictions differing only in the final decoder.
Furthermore, inspired by recent success in machine learning, we introduce a new way to think about the network's output: rather than focusing on the scalar prediction, we discuss the role of the representation  $|\mathbf{h}^{(L_{max})}|$.
We present an example use of  $|\mathbf{h}^{(L_{max})}|$: it can be correlated linearly to the target mobility, performing almost equally well as a fully retrained network for one dataset, thus showing the generalization power of this representation.
Further physical study of this representation is left for future work.
% ROBUST REPRESENTATION: 1 sentence to say 
% OK (1) transferability between T is important, and we do well ; 
% OK - chi4: good-ish, G4: very good fit. So we capture the spatial correlations :)
% OK - (maybe not mentionnned in conclusion) prove (?) that regressing 10 times, 2 particles , generalizes better than 1,1
%   Note: also shiba added a pretext task, and it worked (for him)
% OK - transfer-test: we perform much better than Bapst (at least)
% OK - [pedagogically explain that what matters is transferability to other T.] -> was done in the section, here we jsut say we emphasize it, it's enough for a conclusion
% OK [(2) we introduce a new way to think about this stuff.]
% OK - transfer-learning idea: the representation can be the object to focus on, to do clustering or whatever, \eg transfer-learning (fine-tune the decoders only, just to prove it's good, but use h as the physical object of interest).



The present work focuses on building a representation for glassy materials, but we would like to stress that progress in this area is intimately connected to progress made in other application areas, whenever the input data consists in particles living in 3D space (as in \textit{ab initio} or effective potential approximations, crystalline materials or amorphous materials' properties prediction), regardless of the precise task or output label.
%%FL: note: I wanted to start with CNNs dealing with materials but actually most of the early good works are GNNs, not CNNs.
While each of these application tasks may need fine-tuning of the architecture to perform well, we believe that they are essentially different facets of the same problem, that is, efficient learning of representations for sets of particles in space. 
%Taking inspiration from novel architectures developed for different tasks can only foster progress in one's area.
%More concretely, a possibility for future work is to train a backbone model on various tasks, \eg amorphous and crystalline materials prediction (as is already done in molecular properties predictions).


\section*{Acknowledgments}

We are thankful to \cite{bapst_unveiling_2020} for sharing their dataset publicly, a good practice that should be fostered in the physics' community.
\\
We are thankful to the e3nn library developpers \cite{e3nn} for developing their E(3)-GNN library, which should give a huge boost to work in this area.
\\
We thank Erik Bekkers for his notes on Group Equivariant Deep Learning \url{https://uvagedl.github.io/}
\\
We are thankful to the pyTorch Geometric developpers \cite{Fey/Lenssen/2019} (GNN library).
This work is supported by a public grant overseen by the French National Research Agency (ANR) through the program UDOPIA, project funded by the ANR-20-THIA-0013-01.

%\tableofcontents

%
%\bibliographystyle{apsrev}
%\bibliography{GNN.bib, MLglass.bib, ManualEntries.bib}

\input{main.bbl}
%% TODO: update some arxiv citations into their corresponding paper ?


\appendix

\section{Training details, Training curve and time/energy cost}
\label{appendix:training_and_stuff}

Here we provide more details about our architecture, hyper-parameters and how the model is trained.


%Embedding: input node attributes are one-hot encoded, thus the first node representation $\mathbf{h}^0_i \in \{[0,1],[1,0]\}$ has $2$ channels at $l=0$.
%The edge features $\mathbf{a_{ij}}$ are factorized between their radial part and their angular part which is embedded in an equivariant basis (Spherical Harmonics). % to build the convolution filters. 

\subsection{Radial MLP}
\label{appendix:MLP}

As mentioned in the main text, the radial MLPs are the only part of the network with non-linearities. They implement the radial dependence of convolution filters $\varphi^{l_O}_{l_I l_F,c}(||\mathbf{a}_{ij}||)$, thus they take as input the norms of relative node positions. Before being fed to the MLP, each norm $||a_{ij}||$ is expanded from a real value to an array through an embedding.
%in the radial basis. 
Here we use a Bessel basis embedding:
\begin{align}
    B_n(r) = \sqrt{\frac{2}{r_c}} \frac{\sin{(n \pi\frac{r}{r_c})}}{r}
\end{align}
where $n$ is the number of roots of each basis function: $n=1,2,\dots,N_b$ and we use $N_b=10$, $r_c = d_c=2$.
Other embeddings could be for instance a Gaussian basis (with cutoff), which would act as a kind of smooth one-hot encoding of the value $r$. In practice, the Bessel basis (which is orthogonal) has better generalization properties.%, but performs a less intuitively interpretable but similar role.

% We denote $\mathbf{d}_{ij} = (B_1(||\mathbf{a}_{ij}||),\dots,B_{N_b}(||\mathbf{a}_{ij}||))$ this  radial embedding \textbf{but since we never use notation d, we do not introduce this notation}
%and it is fed to the $\varphi$. The direction $\hat{\mathbf{a}}_{ij} = \mathbf{a}_{ij}/||\mathbf{a}_{ij}||$ is projected on a SH basis giving the $\mathbf{Y}^{l_F}$ of eq. \ref{eq:convolution}.

%Convolution: this block implements the operations of equations \ref{eq:convolution} and \ref{eq:self_int}. 
 
The embedded input is processed through an MLP with layers sizes $(N_b,16,n_{comb})$ and ReLu non linearities. 
The output size $n_{comb}$ is the number of possible triplets (combinations), times the number of channels.
%\textbf{FL: don't we learn each MLP independently in each channel ? I would erase ``times the number of channels''}. 
%Each output neuron represents a different radial function, one for each $(l_O,l_I,l_F,c)$ combination. 
We also use BatchNorm (BN) \cite{ioffe_batch_2015} and Dropout (with rate $p=0.3$) in this MLP to stabilize training and reduce overfitting.
In summary, for each combination of triplets and channels $(l_O,l_I,l_F,c)$, we have a real output
\begin{align}
\varphi^{l_O}_{l_I l_F,c} = \sigma(W_{n_{comb}, 16} Dropout(BN(\sigma(W_{16,N_b} B(||\mathbf{a}_{ij}||)))))
\end{align}
 where the $W$'s are weight matrices and $\sigma(z)=max(0,z)$.
There are also bias parameters, which are not displayed here.
Note that up to the layer of $16$ neurons, the MLP is the same for all triplets and channels, only the last linear layer introduces different weights for each combination.

\subsection{Number of parameters}

This counting refers to the early (light) version of our network, with 4 channels, no $E_{pot}$ in input, and uni-variate regression (of only the A's).
In total, the MLPs of our network (across all layers) account for a number of $35\,664$ learnable parameters: in each layer $L>0$ we have one radial MLP of size $(10,16,284)$ with $5036$ parameters, for the layer $L=0$ the MLP is of size $(10,16,3\times4)$ with $412$ parameters. 
The other main source of learnable parameters in the Network is the part of mixing the channels (right part of fig.~\ref{fig:aggregating_and_mixing}), which accounts for $16000$ learnable parameters: $2272$ for each $L>0$ layer and $12 \times 8 = 96$ for the $L=0$ layer.
%%TODO: the total number of parameters is thus XXX. And for the heavier network, with 8 channels and 20 decoders (multi-variate setup), it is of XXX (XXX in MLPs, XXX in mixing channels and XXXX in decoders)

% In this kind of block reside most of the learned parameters of the model: the weights of the MLP which are shared edge-wise and the ones of the linear combination in the self-interaction step which are shared node-wise.

\subsection{Overall Architecture}
\label{appendix:overall_arch}

We do not repeat here what is written in the main text, section \ref{sec:arch}
%% Steerable group convolution blocks are connected in a way similar to the widespread Res-Net \cite{he_deep_2015} structure, which is known to speed up the training: the output of each layer is added to its own input and is passed through a batch-normalization block before being sent to the next layer: see Fig. \ref{fig:global_arch}.
Note that our Res-Net style of update is possible when two consecutive layers have the same number of channels and same $l_{max}$.
%After the final convolution, node features are passed to the output block where invariants are computed and combined through a decoder (a linear layer for A and another one for B particles). In the case of multi-variate regression the number of decoders amounts to $2\times n_{times}$ where  $n_{times}$ is the number of timescales to regress at once.
Our architecture choice is found empirically to be the stable at train time.
Our architecture is built and trained in the framework of PyTorch Geometric \cite{Fey/Lenssen/2019} which handles all the generic graph operations.
All the SE(3)-related operations (SH embedding, C-G tensor product, equivariant batch-normalization) are integrated in this framework thanks to the \textit{e3nn} library \cite{e3nn}.


\subsection{Training strategy}
\label{appendix:training}


\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{svg_figures/loss_multivariate.pdf}
    \caption{\textbf{Loss and $\rho$ vs epoch.} Training of multi-time model performed at $T=0.44$. The loss curves (full lines) correspond to the total loss of the multi-variate regression setting: sum over all the timescales of the mean squared error per timescale. The correlation curves show the Pearson correlation coefficient between predicted mobility and the ground-truth for a single timescale $\tau = \tau_\alpha$. Vertical dashed lines locate the epochs at which the learning rate is divided by $2$.}
    \label{fig:loss_vs_epoch}
\end{figure}


In Fig.~\ref{fig:loss_vs_epoch} we display one learning curve (as function of iterations, epochs).
Each epoch is a sweep over the entire 400 samples dataset (each sample represents $N=4096$ atoms).

For training, we use the Adam optimizer with initial learning rate $\gamma = 10^{-3}$, moments $\beta_1 = 0.99, \beta_2 = 0.999$ and weight decay $\lambda = 10^{-7}$. We also add a learning rate scheduler that divides $\gamma$ by 2 at several epochs as shown by the vertical dashed lines in Fig. \ref{fig:loss_vs_epoch}. Most of the results shown in the main text are obtained with a number of epochs $n_{epochs} = 100$, this choice results from several tests and strikes the balance between accuracy and training time. As it can be seen in Fig. \ref{fig:loss_vs_epoch}, each training stops before any serious overfit kicks in.
% and nothing more can be gained in terms of generalization accuracy.

Each training of our model takes approximately 10 hours on a A100 Nvidia GPU. %The Global memory (on GPU) consumption peaks at ??GB. 
This represents approximately $2$ kWh per training, and in France, an equivalent of $150$ gCO2 (we use a number of $75$ gCO2/kWh, after considering quickly EDF's optimistic figure of $25$ g/kWh or RTE's more detailed daily numbers, which oscillate mostly between $50$ and $100$, depending on the season and time of day). 
We did not include the footprint of manufacturing the GPU and other infrastructure, which is generally estimated to be one half of the IT-related CO2 emissions (the other half being running the infrastructure).


\section{Ablation studies}
\label{appendix:ablation}

Here we display the ablation studies, that outline which are the key elements of our model. We also report the learning curve (ablation on training set size).

\subsection{Ablation of $l_{max}$.}
All our results rely on the embedding of the input data into the Spherical Harmonics basis and on the built-in equivariance of convolution layers. 
%So one may expect the choice of the cutoff rotational order $l_{max}$ to be crucial.
% In Fig.~\ref{fig:ablation_ellmax} we show it is indeed the case: 
One may expect that a large cutoff rotational order $l_{max}$ is needed.
Here we show that actually, going from $l_{max}=0$ to $l_{max}=1$ is the most critical step.
We build architectures that vary only by their $l_{max}$ value and measure the performance $\rho$ in each, as shown in
 Fig.~\ref{fig:ablation_ellmax_noEpot} and ~\ref{fig:ablation_ellmax}.
The biggest gap in performance is indeed observed between purely isotropic, scalar features ($l_{max} = 0$) versus directional ones ($l_{max} = 1$). We notice as well that short timescales require higher rotational order and the performance indeed has not saturated for them. One possible interpretation is that the network has to learn interatomic forces to describe the dynamics at short times, and that directional information is more relevant in that case.
Further increasing $l_{max}$ provides a finer rotational order resolution, but we observe that the accuracy tends to saturate. 
We cannot go above $l_{max} = 3$ due to memory constraints: as the rotational order increases, the number of activations of neurons to keep track of grows exponentially with $l_{max}$ (while it grows linearly with the number of edges, with the batch size, and with the size of the hidden layer in the radial MLP).


\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{svg_figures/Ablation_lmax_noEpot.pdf}
    \caption{
    \textbf{$l_{max}$ ablation using thermal positions and no $E_{pot}$ input.} 
    A separate model was trained at $T=0.44$, for each value of $l_{max}$. The colors correspond to the $n_{times}=10$ timescales of mobility, with $t_{idx}$ ranging from $0$ to $9$. For clarity, only some of these timescales are displayed here.
    The GNN of \cite{bapst_unveiling_2020} obtains $\rho\approx 0.65$ for the timescale 6, we are in this range when using only invariant features ($l_{max}=0$).
    When using higher orders, we outperform it.
    }
    \label{fig:ablation_ellmax_noEpot}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{svg_figures/Ablation_lmax.pdf}
    %\includegraphics[width=0.4\textwidth]{svg_figures/Ablation_lmax_noEpot.pdf}
    \caption{
    \textbf{$l_{max}$ ablation} 
    Same setup as previous plot, but here the model is using thermal positions combined with $E_{pot}(IS)$.
    Performance is overall higher, the relative gain from using $l>0$ is less pronounced, probably because $E_{pot}(IS)$ already provides equivalent information.
    }
    \label{fig:ablation_ellmax}
\end{figure}

% \textit{FL - Note for ourselves:} without epot, and using thermal positions, we are in the same setup as Bapst, and using ell=0 only, we get exactly just 0.65 performance.
% Using ell=1 we go to >0.7, a significant gain.
% (and slightly more with 2, and for 3 it's negligible)
% So, architecture-wise, using ell>0 is useful



\subsection{Ablation of $L_{max}$ (depth).}
In Figure \ref{fig:ablation_nLayers} we present the performance of our multi-time model trained at temperature $T=0.44$ for an increasing number  $L_{max}$ of equivariant convolution layers stacked in the architecture.
%For certain timescales the accuracy has already converged with just $4$ layers.
While the short time scales seem to be saturated, the longer ones seem not: indeed, we'd expect increased accuracy if we increased  $L_{max}$ further.
%However, for other timescales, it remains unclear whether introducing more layers could potentially result in performance enhancement.
Note that there is a possibility of encountering over-smoothing effects with an increased number of layers.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{svg_figures/Ablation_nLayers.pdf}
    \caption{
    \textbf{$L_{max}$ ablation.} Color-code is the same as for the other ablation studies, but here we vary the number of convolution layers applied.
    $L_{max}=1$ corresponds to a very interpretable model, similar to using a Bond Order parameter and linear regression (hidden in $\varphi$ and in the decoder). 
    Performance does not seem to saturate: one expects increased performance with more layers.}
    \label{fig:ablation_nLayers}
\end{figure}


\subsection{Ablation of $N_{train}$ (Learning curve).}
In Figure \ref{fig:Learning_curve}, we present learning curves that illustrate the model's performance (multi-variate setting) as a function of the number of samples in the training set for different timescales.
%It is important to note that only one multi-time model was trained for each dataset configuration, and all the timescales were regressed simultaneously from the same representation.
The choice of the samples to include in the training set is performed in an incremental way: for each point of the curve new samples are added to the others already present, while the test set is kept constant.
%%Each time new training samples are added, the previous ones being kept, while the test set remains always the same.
We emphasize that competitive performances are achieved already by using less than $1/4$ of the available training set and meaningful prediction are obtained also when training the model on a single sample, contrary to what one would expect for a ``deep'' model like ours.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{svg_figures/Learning_curve.pdf}
    \caption{
    \textbf{Learning curve} 
     Color-code is the same as for the other ablation studies, but here we vary the number of training samples.
     Performance is already very high when training on a single sample: our network seems to resist well to overfitting.
     Here again, performance does not seem to saturate, more precisely it seems to increase logarithmically with train set size.
     }
    \label{fig:Learning_curve}
\end{figure}

% \section{Measures of the errors}
% \label{appendix:errors}

% There are two sources of uncertainty when measuring the accuracy of a given model (a model is a ML architecture).

% The first, that one usually reports, comes from varying the initialization of the weights in the Neural Network (NN). Indeed, as gradient descent only finds a local minimum, the learned weights depend on the (random) initialization. 
% To account for this variability, one typically re-trains the network multiple times, each time with a different initial random seed.
% Due to lack of time, we do not compute these error bars thoroughly at all timescales and temperatures, but we did a focused study at $T=0.44$ and $\tau=3\times 10^4$ (longest timescale available), see Table \ref{table:seeds}. From this, we conclude that the true accuracy lies approximately within the range $[0.7095, 0.7169]$ (a spread of $0.0074$), \ie  the accuracy is $0.713 \pm 0.004$


% \begin{table*}
% \begin{tabular}{|l |p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} p{0.06\textwidth} |}
% \hline
% Seed number & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ & $10$\\
% \hline
%  $\rho$ & $0.7122$ & $\mathbf{0.7095}$ & $0.7135$ & $0.7099$ &  $0.7128$ & $0.7120$ &  $\mathbf{0.7169}$ & $0.7117$ & $0.7128 $ & $0.7165 $\\
%  %% we could sort the values for convenience, no ?
% % 0.7095 
% % 0.7099 
% % 0.7117 
% % 0.7120 
% % 0.7122 
% % 0.7128 
% % 0.7128 
% % 0.7135 
% % 0.7165
% % 0.7169 
% \hline
% \end{tabular}
% \caption{\textbf{Accuracy across random initializations.} Each seed of the random number generator determines a different initial set of weights for the model. We report the test accuracy of each model independently trained at $T=0.44$, $\tau =3\times10^4$. Best and worst performance are highlighted.
% %, corresponding to a spread of $0.0074$
% \label{table:seeds}
% }
% \end{table*}



% The second source of uncertainty can come from the test set itself (being too small). A way to assess the test set robustness, \ie  how confident we can be that our accuracy would be in the same range, for an ideal infinite test set, is to bootstrap the test set. We do not recall the classic bootstrap idea here.
% The test set is made of $80$ configurations of $N=4096$ atoms each, for which $80\%$ are A particles (for which we regress). this results in $n=80 * 4096 * 0.80= 262144$ samples. We sample with replacement $n$ out of these $n$ samples, and measure $\rho$ on this ``new'' test set. Repeating this operation 5000 times, we obtain the empirical distribution $P(\rho)$ shown in Fig.~\ref{fig:bootstrappedTestSet}.
% From this, we conclude that for this particular seed the true accuracy is $0.712 \pm 0.001$

% We find that the fluctuations coming initialization of the Network are very much comparable (same order of magnitude) to those due to the finite-size test set fluctuations (evaluated by bootstrap).


% \begin{figure}
%     \centering
%     \includegraphics[width= 0.45\textwidth]{boot_p.pdf}
%     \caption{\textbf{Distribution of accuracy over bootstrap samples.} Train/Test performed at $T=0.44$ and $\tau=3\times 10^4$. Number of re-samplings $N_{res}=5000$.}
%     \label{fig:bootstrappedTestSet}
% \end{figure}

% \textbf{OPTIONAL: have a 2D slice with our prediction and the true y, side by side, for a slice that has an average correlation close to the bulk one ?
% We could even show the GT one one side, and the difference between truth and predicted on the other, to make the comparison easier.
% }


\section{Reproducing expert features (Boattini 2021)}
\label{appendix:Boattini}

Here we relate the SE(3)-equivariant formalism with the expert features such as those used in \cite{boattini_averaging_2021}.
%Here we show how a network with one SE(3)-convolution layer is able to reproduce some good expert features  used for glassy liquids.
We start by considering embedded node features $h_{i,c} = \delta_{t_i,c}$ where $t_i$ is the type of particle $i$: we have only two channels at $l=0$.
We extend them from $n_{ch} = 2$ $\rightarrow$ $n_{ch} = 2*N_{b}$ just by replicating them $N_{b}$ times.
We will denote the replicated  $h_{i,c}$ as $h_{i,c,r}$ since in spirit, each copy of the two channels (one-hot of the particle type) will correspond to one radial basis function. Then the convolution operation reads (remember that for $l_I=0$, only $l_O=l_F$ is allowed):
\begin{align}
    \mathbf{h}_{i,c,r,l_I=0,l_F}^{'l_F} = \sum_{j\in\mathcal{N}(i)} \varphi(||\mathbf{a}_{ij}||)^{l_F}_{l_I=0, l_F,c,r} \mathbf{Y}^{l_F}(\mathbf{\hat{a}}_{ij}) h_{j,c,r}
\end{align}
We may choose a Gaussian radial basis function $B$ (instead of Bessel): $\varphi(||\mathbf{a}_{ij}||)^{l_F}_{l_I=0 l_F,c,r} = B(||\mathbf{a_{ij}}||)_r$.
%where $B$ is the radial embedding in a Gaussian basis.
Then if we focus on $l_F = 0$, since $Y^{0}(\mathbf{\hat{a}}_{ij}) = 1$ we have: 
\begin{align}
    h_{i,c,r}^{'0} = \sum_{j\in\mathcal{N}(i)}  e^{-\frac{(||\mathbf{a}_{ij}||-r)^2}{2\delta}} \delta_{t_j,c}
\end{align}
which correspond to $G_i^{(0)}(r,\delta,s)$ in \cite{boattini_averaging_2021}. Note that we require also no channel mixing at $l=0$. For $l_F > 0$: 
\begin{align}
    h_{i,c,r}^{'l_F} = \sum_{j\in\mathcal{N}(i)} \mathbf{Y}^{l_F}(\mathbf{\hat{a}}_{ij}) e^{-\frac{(||\mathbf{a}_{ij}||-r)^2}{2\delta}} \delta_{t_j,c}
\end{align}
which, after a channel mixing step that sums over $c$ (mixing different particle types), correspond to $q_i^{(0)}(l,m,r,\delta)$ in \cite{boattini_averaging_2021}. By computing invariants from these features through their norm, we recover exactly $q_i^{(0)}(l,r,\delta)$ from \cite{boattini_averaging_2021}.

By contrast, in our model we do not compute invariants after one layer, we keep equivariant features and let them interact over multiple layers in order to increase the expressivity.

Although this architecture qualitatively reproduces these expert features, for a quantitative match one would need to use a much larger cutoff radius $d_c=5$ for building the graph, and a maximum rotational order of $l_{max}=12$.

%%\textbf{OPTIONAL: and ALMOST ALREADY DONE in main text, now  :D  
%%-- as in current main text, explain step by step: using no phi, and many radial basis, and several channels, if we do not mix channels, or only between particle types and not radii, we get Boattini. BUT this is not what we do, we mix radii with phi, thus building an aggregate representation of the full ball (not spheres), and then we mix channels on top of that - but all that is still linear combinations (before taking the norm, though)}
%\textbf{OPTIONAL: if time allows: also precisely compare with SOAP - it's almost the same I think}


\section{Shiba vs Bapst dataset comparison}

In Bapst's dataset \cite{bapst_unveiling_2020}, a timescale actually corresponds to a fixed value of the self-intermediate scattering function $F_k(t)$, so that different samples are measured at slightly different times.
Equilibration is performed under an NPT thermostat, \ie  at constant pressure and temperature, \ie  the volume is varying (and thus the density as well).

In Shiba's dataset \cite{shiba_predicting_2022}, equilibration is performed at constant volume and temperature (NVT) so that the density is exactly $\rho=1.2$ in all samples and at all temperatures. Furthermore, sampling of the trajectories is performed at fixed times, not fixed $F_k(t)$.

In Figure. \ref{fig:rho_vs_time_BapstAndShiba} we report the performance of our main model (thermal positions + $E_{pot}(IS)$ inputs) for these two Kob-Anderson 3D datasets.
Performances are shifted in time but otherwise rather comparable (a bit better on Shiba's dataset).

\label{appendix:Shiba}
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{svg_figures/Comparison_ShibaBapst.pdf}
    \caption{\textbf{Comparison of datasets.} The same models trained at temperature $T=0.44$ on each dataset. }
    \label{fig:rho_vs_time_BapstAndShiba}
\end{figure}




% \textbf{FL: do we keep the simple test in the main text? It depends a bit on the quality of the result.}

% \textbf{FL: for now I cut-paste the leftover of the simple-test explanation here.}
% Furthermore we perform an even simpler check: without re-learning anything, we take a model trained at a temperature and test it directly at the other temperatures (dashed lines). 
% Although we may want to correct predictions with a global shift and scale of the predictions, since the performance metric $\rho$ is insensitive to 
% such a change, 
% %global shift and scale of the labels' distribution, 
% there is no need to perform such a correction  
% % global re-shift and re-scale 
% when comparing values of $\rho$ (when comparing output mobilities, such a re-scaling would be needed however).
% %Here again we still obtain good performances, significantly better than the SOTA: compare with the Fig.~3d of \cite{bapst_unveiling_2020}.
% Overall, this shows that our learned representation is not strongly training-temperature-dependent, a desirable property in the perspective of learning a structural order parameter. This is supported also by the increase in Dynamical Heterogeneities that you can observe in Figure \ref{fig:transfer_learning}










% \subsection{Further ablation studies (FL: very optional !!)}

% As often in ML works, one wants to check whether architecture or hyper-parameter choices are optimal, and whether results are robust against small changes in these choices.

% Here we report on two main components of our architecture: number of graph-convolutional layers, and number of channels.


% In Fig.~\ref{rho_vs_Nlayer}, we show that GNNs are useful, beyond the initial lifting convolution: indeed, a single-layer or 2-layer GNN significantly underperforms compared to our 7-layer model.
% However, we see that the gain from 5 to 7 layers is not significant, indicating that adding more layers would likely not improve results.
% Note that the number of network parameters grows essentially linearly with the number of layers.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.4\textwidth]{equivariance.pdf}
%     \caption{Performance saturates with Network Depth. Performance $\rho$ as a function of $N_{layer}$, the number of layers in the GNN (not counting the lifting convolution). The first few layers are critical, but as we add more, their marginal added value shrinks to 0.
%     }
%     \label{rho_vs_Nlayer}
% \end{figure}


% In Fig.~\ref{rho_vs_Nchannel}, we show that it is critical to have more than 1 channel, but that as we reach 4 channels, the gain from adding more becomes negligible.
% Note that the number of network parameters grows with the number of channels, with a linear and a quadratic term: the linear one comes from the radical MLP, where operations happen channel-wise; while in the mixing of channels, the number of weights grows as $N_{channel}^2$.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.4\textwidth]{equivariance.pdf}
%     \caption{Performance saturates with number of Channels. Performance $\rho$ as a function of $N_{channels}$, the number of channels in the features $h$. The first few channels are useful, but as we add more, their marginal added value shrinks to 0.
%     }
%     \label{rho_vs_Nchannel}
% \end{figure}




\end{document}


%\begin{comment}

% \section{OPTIONAL - Regular vs Steerable G-Conv}

% Given two feature maps: $f^{in} \in \mathbb{L}_2(H)$ and $f^{out} \in \mathbb{L}_2(H')$ with $H,H'$ homogeneous spaces of $G$, the only linear operator $\mathcal{K}: \mathbb{L}_2(H) \rightarrow \mathbb{L}_2(H')$ mapping between the two maps to be equivariant with respect to $G$ is of the Group-Convolution form as showed in \cite{bekkers_b-spline_2021}. Without going to deep into the formalism of this theorem, let's look at how it reads in some cases of interest. If $H=\mathbb{R}^3$ and $H'=SE(3)=\mathbb{R}^3 \times SO(3)$ then:
% \begin{align}
%     [\mathcal{K}f](\mathbf{x},\mathbf{R}) = \int_{\mathbb{R}^3} k(\mathbf{R}^{-1} (\mathbf{x}-\mathbf{x}')) f(\mathbf{x}')\text{d}\mathbf{x}'
% \end{align}

% with $\mathbf{x}\in \mathbb{R}^3$ and $\mathbf{R}$ operator of rotations in $SO(3)$. This is what is commonly referred to as ``Lifting convolution'' because it lifts the map domain $\mathbb{R}^3$ to the group $SE(3)$. It consists just in convolving the input map with a kernel which is rotated and translated thus resulting in an output domain which has 3 additional dimensions (parametrized by different angles of rotations). Usually the lifting convolution is followed by a series of regular group convolutions where input and output domain are the same and coincide with the group: $H=H'=SE(3)=G$
% \begin{align}
% \label{eq:regular_g_conv}
%     [\mathcal{K}f](g) = \int_{SE(3)} k(g^{-1}g') f(g')\text{d}g'
% \end{align}

% with $g=(\mathbf{x},\mathbf{R})$.
% The major drawback of this approach is that it requires discretization over the space of rotations, thus it results in a costly and not exact procedure. [cite works with regular G-Conv]

% An alternative way consists in expanding the co-domain of the features maps, instead of the domain, through a Fourier Transform (FT) on the space of the group:
% \begin{align}
%     \hat{f}(\mathbf{x}) = \mathcal{F}_{SO(3)}[f(\mathbf{x},\mathbf{R})]
% \end{align}

% with $\hat{f}: \mathbb{R}^3 \rightarrow V_{SO(3)}$. Thanks to Fourier Shift Theorem, the operator $\mathcal{K}$ in this new space becomes much simpler, there's no more need to convolve on $SO(3)$ since $\widehat{(k\star f)}_{SO(3)} = \hat{f} \cdot \hat{k}^T$. Thus Equation~\ref{eq:regular_g_conv} becomes:
% \begin{align}
%      [\hat{\mathcal{K}}\hat{f}](\mathbf{x}) = \int_{\mathbb{R}^3} \hat{f}(\mathbf{x}') \hat{k}^T(\mathbf{x}-\mathbf{x}')\text{d}\mathbf{x}'
% \end{align}

% This method allows to have exact equivariance at the price of reducing the angular frequency (fixed number of Fourier components, see next section).
% %\end{comment}


The present work focuses on building a representation for glassy materials, but we would like to stress that progress in this area is intimately connected to progress made in other applications where the input data consists in particles living in 3D space, regardless of the precise task or output label.
Indeed we recall that initially, Deep Convolutional Neural Networks (CNNs) were developed (for \textit{ab initio} or effective potential approximations \cite{zhang_deep_2018}, for crystalline materials' prediction \cite{li_convolutional_2021} or amorphous materials' prediction \cite{swanson_deep_2020}).
%%FL: note: I wanted to start with CNNs dealing with materials but actually most of the early good works are GNNs, not CNNs.
Some of these approaches already considered the issue of defining rotational symmetry-aware features, with the notable SchNet \cite{schutt2017schnet, schutt_schnet_2018} that already considers rotation-equivariant features. The problem of particle permutation-equivariance was also addressed early in the point cloud learning literature \cite{ravanbakhsh_deep_2016, zaheer_deep_2018}.
Within the Convolutional Neural Networks approach (CNNs), which was the ``mainstream'' Deep Learning, the search for CNN architectures dealing with 3D particle data fully matured with Spherical CNNs in 2018 \cite{cohen_spherical_2018,  weiler_3d_2018}, that define generic rotation-equivariant features and allow the network to combine them efficiently.
Simultaneously, Graph Neural Networks (GNNs) entered the game, and are obviously an effective way to deal with node-index permutation equivariance.
While each application task mentioned above may need fine-tuning of the architecture to perform well, we believe that these various tasks are essentially different facets of the same problem, that is, efficiently learning representations for sets of particles in space. 
Taking inspiration from novel architectures developed for different tasks can only foster progress in one's area.
%More concretely, a possibility for future work is to train a backbone model on various tasks, \eg amorphous and crystalline materials prediction (as is already done in molecular properties predictions).

