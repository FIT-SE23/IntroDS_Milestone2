\begin{table}[h!]
    \caption{\textit{Models vs. guidance quality.} Performance improves regardless of the exact large pretrained language model. Strong improvements can even be achieved through large-scale pretrained word embeddings such as FastText \cite{fasttext} and GloVe \cite{glove}. However, using less transferable word hierarchies falls short in comparison.}
 \footnotesize
  \setlength\tabcolsep{1.4pt}
  \centering
  %\begin{tabular}{l|c|ccc|c}
  \begin{tabular}{l || c | c || c | c }
     \toprule
     \multicolumn{1}{l}{\textsc{Benchmarks}$\rightarrow$} & \multicolumn{2}{c}{\textsc{CUB200-2011}} & \multicolumn{2}{c}{\textsc{CARS196}} \\
     \midrule
     \multirow{2}{*}{\textsc{Models} $\downarrow$} & \multirow{2}{*}{R@1} & mAP & \multirow{2}{*}{R@1} & mAP\\
      &  & @1000 & & @1000\\
    \midrule
    \rowcolor{vlightgray}
    \textbf{Baseline} & 62.8 $\pm$ 0.2 & 31.1 $\pm$ 0.3 & 81.6 $\pm$ 0.3 & 31.7 $\pm$ 0.1\\ 
    \rowcolor{vvlightgray}    
    + CLIP-L \cite{clip} & 67.3 $\pm$ 0.2 & \textbf{34.8 $\pm$ 0.2} & 85.3 $\pm$ 0.1 & \textbf{32.7 $\pm$ 0.2} \\
    % \hline
    % + $\mathbb{S}^{N-1}\left(\mathcal{N}(0,1)\right)$ & - $\pm$ - & - $\pm$ - & - $\pm$ - & - $\pm$ -\\
    \midrule        
    \multicolumn{5}{>{\columncolor[gray]{.9}}l}{\textbf{(a) Language Models}} \\
    \hline
    + BERT \cite{bert} & 66.9 $\pm$ 0.3 & 33.5 $\pm$ 0.2 & 84.9 $\pm$ 0.1 & 32.3 $\pm$ 0.1\\
    + DistBert \cite{distilbert} & 66.7 $\pm$ 0.1 & 33.4 $\pm$ 0.2 & 85.4 $\pm$ 0.4 & 32.4 $\pm$ 0.1\\    
    + Roberta-B \cite{roberta} & 67.0 $\pm$ 0.2 & 33.8 $\pm$ 0.2 & 84.9 $\pm$ 0.1 & 32.3 $\pm$ 0.3\\
    + Roberta-L \cite{roberta} & 67.3 $\pm$ 0.2 & 33.9 $\pm$ 0.3 & 85.1 $\pm$ 0.2 & 32.4 $\pm$ 0.2\\
    + DistRoberta \cite{huggingface} & 66.0 $\pm$ 0.2 & 32.2 $\pm$ 0.2 & 85.0 $\pm$ 0.3 & 32.1 $\pm$ 0.2\\
    + Reformer \cite{Kitaev2020Reformer} & 66.7 $\pm$ 0.1 & 33.1 $\pm$ 0.1 & 85.5 $\pm$ 0.2 & 32.0 $\pm$ 0.2\\
    + MPNet \cite{mpnet} & 66.2 $\pm$ 0.3 & 32.3 $\pm$ 0.2 & 85.4 $\pm$ 0.2 & 32.3 $\pm$ 0.3\\
    + GPT2 \cite{gpt2} & 67.0 $\pm$ 0.3 & 33.7 $\pm$ 0.1 & 84.8 $\pm$ 0.4 & 32.4 $\pm$ 0.1\\
    \hline
    + Top 3 & \textbf{67.5 $\pm$ 0.2} & 34.5 $\pm$ 0.3 & \textbf{85.6 $\pm$ 0.3} & 32.5 $\pm$ 0.3\\   
    \bottomrule
    \end{tabular}
    \label{supp:tab:choice_of_language_models}
 \end{table}