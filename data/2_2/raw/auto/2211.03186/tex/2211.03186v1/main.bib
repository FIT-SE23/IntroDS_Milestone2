############################# OTHERS #############################

# PYTORCH
@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{rev_1,
 author = {Zhang, Michael and Lucas, James and Ba, Jimmy and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Lookahead Optimizer: k steps forward, 1 step back},
 url = {https://proceedings.neurips.cc/paper/2019/file/90fd4f88f588ae64038134f1eeaa023f-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{rev_2,
 author = {Lee, Sang-Woo and Kim, Jin-Hwa and Jun, Jaehyun and Ha, Jung-Woo and Zhang, Byoung-Tak},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Overcoming Catastrophic Forgetting by Incremental Moment Matching},
 url = {https://proceedings.neurips.cc/paper/2017/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf},
 volume = {30},
 year = {2017}
}



# SGD
@InProceedings{pmlr-v28-shamir13,
  title = 	 {Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes},
  author = 	 {Shamir, Ohad and Zhang, Tong},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {71--79},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/shamir13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/shamir13.html},
  abstract = 	 {Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required non-trivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines.  In this paper, we investigate the performance of SGD \emphwithout such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after T rounds, the suboptimality of the \emphlast SGD iterate scales as O(\log(T)/\sqrtT) for non-smooth convex objective functions, and O(\log(T)/T) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in \citetRakhShaSri12arxiv is not as simple to implement). Finally, we provide some experimental illustrations.}
}


# CLIP
@InProceedings{pmlr-v139-radford21a,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

# CIFAR
@TECHREPORT{Krizhevsky09learningmultiple,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}


############################# NEURO #############################
@Article{pmid7624455,
   Author="McClelland, J. L.  and McNaughton, B. L.  and O'Reilly, R. C. ",
   Title="{{W}hy there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory}",
   Journal="Psychol Rev",
   Year="1995",
   Volume="102",
   Number="3",
   Pages="419--457",
   Month="Jul"
}

############################# FLATNESS-BASED #############################
# Noise
@inproceedings{NEURIPS2021_357cfba1,
    author = {SHI, Guangyuan and CHEN, JIAXIN and Zhang, Wenlong and Zhan, Li-Ming and Wu, Xiao-Ming},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
    pages = {6747--6761},
    publisher = {Curran Associates, Inc.},
    title = {Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning by Finding Flat Minima},
    url = {https://proceedings.neurips.cc/paper/2021/file/357cfba15668cc2e1e73111e09d54383-Paper.pdf},
    volume = {34},
    year = {2021}
}

# Hyperparams
@inproceedings{NEURIPS2020_518a38cc,
    author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Pascanu, Razvan and Ghasemzadeh, Hassan},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
    pages = {7308--7320},
    publisher = {Curran Associates, Inc.},
    title = {Understanding the Role of Training Regimes in Continual Learning},
    url = {https://proceedings.neurips.cc/paper/2020/file/518a38cc9a0173d0b2dc088166981cf8-Paper.pdf},
    volume = {33},
    year = {2020}
}

# SWA 
@article{izmailov2018averaging,
    title={Averaging Weights Leads to Wider Optima and Better Generalization},
    author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
    journal={arXiv preprint arXiv:1803.05407},
    year={2018}
}

# SAM (CL)
@article{mehta2021empirical,
    title={An empirical investigation of the role of pre-training in lifelong learning},
    author={Mehta, Sanket Vaibhav and Patil, Darshan and Chandar, Sarath and Strubell, Emma},
    journal={arXiv preprint arXiv:2112.09153},
    year={2021}
}

# SAM
@inproceedings{foret2021sharpnessaware,
    title={Sharpness-aware Minimization for Efficiently Improving Generalization},
    author={Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=6Tm1mposlrM}
}

# SAM vs SWA
@article{DBLP:journals/corr/abs-2202-00661,
    author    = {Jean Kaddour and
                 Linqing Liu and
                 Ricardo Silva and
                 Matt J. Kusner},
    title     = {Questions for Flat-Minima Optimization of Modern Neural Networks},
    journal   = {CoRR},
    volume    = {abs/2202.00661},
    year      = {2022},
    url       = {https://arxiv.org/abs/2202.00661},
    eprinttype = {arXiv},
    eprint    = {2202.00661},
    timestamp = {Wed, 09 Feb 2022 15:43:34 +0100},
    biburl    = {https://dblp.org/rec/journals/corr/abs-2202-00661.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

###########f################## REGULARIZATION-BASED #############################
# EWC
@article{
    doi:10.1073/pnas.1611835114,
    author = {James Kirkpatrick  and Razvan Pascanu  and Neil Rabinowitz  and Joel Veness  and Guillaume Desjardins  and Andrei A. Rusu  and Kieran Milan  and John Quan  and Tiago Ramalho  and Agnieszka Grabska-Barwinska  and Demis Hassabis  and Claudia Clopath  and Dharshan Kumaran  and Raia Hadsell },
    title = {Overcoming catastrophic forgetting in neural networks},
    journal = {Proceedings of the National Academy of Sciences},
    volume = {114},
    number = {13},
    pages = {3521-3526},
    year = {2017},
    doi = {10.1073/pnas.1611835114},
    URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1611835114},
    eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1611835114},
    abstract = {Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially. The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.}
}

# oEWC
@InProceedings{pmlr-v80-schwarz18a,
    title = 	 {Progress and Compress: A scalable framework for continual learning},
    author =       {Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
    booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
    pages = 	 {4528--4537},
    year = 	 {2018},
    editor = 	 {Dy, Jennifer and Krause, Andreas},
    volume = 	 {80},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {10--15 Jul},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v80/schwarz18a/schwarz18a.pdf},
    url = 	 {https://proceedings.mlr.press/v80/schwarz18a.html},
    abstract = 	 {We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance onpreviously encountered tasks while accelerating learning progress on subsequent problems. This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connectedto an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously acquired skills. This cycle of activelearning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. We demonstrate the progress &amp; compress approach onsequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.}
}

# SI
@InProceedings{pmlr-v70-zenke17a,
    title = 	 {Continual Learning Through Synaptic Intelligence},
    author =       {Friedemann Zenke and Ben Poole and Surya Ganguli},
    booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
    pages = 	 {3987--3995},
    year = 	 {2017},
    editor = 	 {Precup, Doina and Teh, Yee Whye},
    volume = 	 {70},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {06--11 Aug},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v70/zenke17a/zenke17a.pdf},
    url = 	 {https://proceedings.mlr.press/v70/zenke17a.html},
    abstract = 	 {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.}
}

# RW 
@InProceedings{Chaudhry_2018_ECCV,
    author = {Chaudhry, Arslan and Dokania, Puneet K. and Ajanthan, Thalaiyasingam and Torr, Philip H. S.},
    title = {Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence},
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    month = {September},
    year = {2018}
}

@ARTICLE{8107520,
    author={Li, Zhizhong and Hoiem, Derek},
    journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
    title={Learning without Forgetting}, 
    year={2018},
    volume={40},
    number={12},
    pages={2935-2947},
    doi={10.1109/TPAMI.2017.2773081}
}



############################# REHEARSAL-BASED #############################

# ER 1
@Article{pmid2186426,
    Author="Ratcliff, R. ",
    Title="{{C}onnectionist models of recognition memory: constraints imposed by learning and forgetting functions}",
    Journal="Psychol Rev",
    Year="1990",
    Volume="97",
    Number="2",
    Pages="285--308",
    Month="Apr"
}

# ER 2
@article{doi:10.1080/09540099550039318,
    author = { ANTHONY   ROBINS },
    title = {Catastrophic Forgetting, Rehearsal and Pseudorehearsal},
    journal = {Connection Science},
    volume = {7},
    number = {2},
    pages = {123-146},
    year  = {1995},
    publisher = {Taylor & Francis},
    doi = {10.1080/09540099550039318},
    URL = { https://doi.org/10.1080/09540099550039318},
    eprint = {https://doi.org/10.1080/09540099550039318}
}


# MER
@inproceedings{MER,
    title={Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference},   
    author={Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},    
    booktitle={In International Conference on Learning Representations (ICLR)}, 
    year={2019}   
}

# HAL
@misc{chaudhry2020using,
    title={Using Hindsight to Anchor Past Knowledge in Continual Learning},
    author={Arslan Chaudhry and Albert Gordo and David Lopez-Paz and Puneet K. Dokania and Philip Torr},
    year={2020},
    url={https://openreview.net/forum?id=Hke12T4KPS}
}

# GEM
@inproceedings{NIPS2017_f8752278,
    author = {Lopez-Paz, David and Ranzato, Marc\textquotesingle Aurelio},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Gradient Episodic Memory for Continual Learning},
    url = {https://proceedings.neurips.cc/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf},
    volume = {30},
    year = {2017}
}

# A-GEM
@inproceedings{
    chaudhry2018efficient,
    title={Efficient Lifelong Learning with A-{GEM}},
    author={Arslan Chaudhry and Marc’Aurelio Ranzato and Marcus Rohrbach and Mohamed Elhoseiny},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=Hkf2_sC5FX},
}

# DER
@inproceedings{NEURIPS2020_b704ea2c,
    author = {Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and CALDERARA, SIMONE},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
    pages = {15920--15930},
    publisher = {Curran Associates, Inc.},
    title = {Dark Experience for General Continual Learning: a Strong, Simple Baseline},
    url = {https://proceedings.neurips.cc/paper/2020/file/b704ea2c39778f07c617f6b7ce480e9e-Paper.pdf},
    volume = {33},
    year = {2020}
}

# GDUMB
@inproceedings{prabhu2020greedy,
    title={GDumb: A Simple Approach that Questions Our Progress in Continual Learning},
    author={Prabhu, Ameya and Torr, Philip and Dokania, Puneet},
    booktitle={The European Conference on Computer Vision (ECCV)},
    month={August},
    year={2020}
}

# DualNet
@inproceedings{NEURIPS2021_86a1fa88,
    author = {Pham, Quang and Liu, Chenghao and Hoi, Steven},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
    pages = {16131--16144},
    publisher = {Curran Associates, Inc.},
    title = {DualNet: Continual Learning, Fast and Slow},
    url = {https://proceedings.neurips.cc/paper/2021/file/86a1fa88adb5c33bd7a68ac2f9f3f96b-Paper.pdf},
    volume = {34},
    year = {2021}
}

# ICARL
@InProceedings{Rebuffi_2017_CVPR,
    author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H.},
    title = {iCaRL: Incremental Classifier and Representation Learning},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {July},
    year = {2017}
}


# CO2L
@InProceedings{Cha_2021_ICCV,
    author    = {Cha, Hyuntak and Lee, Jaeho and Shin, Jinwoo},
    title     = {Co2L: Contrastive Continual Learning},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {9516-9525}
}




### ----------------- OLD -----------------

############################# MULTI-TASK LEARNING #####################
@InProceedings{mtl_pu,
author="Pu, Jian
and Jiang, Yu-Gang
and Wang, Jun
and Xue, Xiangyang",
title="Which Looks Like Which: Exploring Inter-class Relationships in Fine-Grained Visual Categorization",
booktitle="Proceedings of the IEEE European Conference on Computer Vision (ECCV)",
year="2014",
pages="425--440",
}

@INPROCEEDINGS{mtl_bhattarai, 
author={B. Bhattarai and G. Sharma and F. Jurie}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={CP-mtML: Coupled Projection Multi-Task Metric Learning for Large Scale Face Retrieval},
year={2016}, 
pages={4226-4235}
}

@inproceedings{
milbich2021characterizing,
title={Characterizing Generalization under Out-Of-Distribution Shifts in Deep Metric Learning},
author={Timo Milbich and Karsten Roth and Samarth Sinha and Ludwig Schmidt and Marzyeh Ghassemi and Bj{\"o}rn Ommer},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=_KqWSCu566}
}

@inproceedings{
dullerud2022is,
title={Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning},
author={Natalie Dullerud and Karsten Roth and Kimia Hamidieh and Nicolas Papernot and Marzyeh Ghassemi},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=js62_xuLDDv}
}

@misc{roth2021total,
    title={Towards Total Recall in Industrial Anomaly Detection},
    author={Karsten Roth and Latha Pemula and Joaquin Zepeda and Bernhard Schölkopf and Thomas Brox and Peter Gehler},
    year={2021},
    eprint={2106.08265},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@Inbook{shared_properties,
author="Torralba, Antonio
and Murphy, Kevin P.
and Freeman, William T.",
title="Shared Features for Multiclass Object Detection",
year="2006",
publisher="Springer Berlin Heidelberg",
pages="345--361"
}

@article{fasttext,
  title={Enriching Word Vectors with Subword Information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.04606},
  year={2016}
}

@inproceedings{devise,
 author = {Frome, Andrea and Corrado, Greg S and Shlens, Jon and Bengio, Samy and Dean, Jeff and Ranzato, Marc\textquotesingle Aurelio and Mikolov, Tomas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {DeViSE: A Deep Visual-Semantic Embedding Model},
 url = {https://proceedings.neurips.cc/paper/2013/file/7cce53cf90577442771720a370c3c723-Paper.pdf},
 volume = {26},
 year = {2013}
}


@InProceedings{hiermatch2,
author = {Barz, Bjorn and Denzler, Joachim},
title = {Deep Learning on Small Datasets without Pre-Training using Cosine Loss},
booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
month = {March},
year = {2020}
}

@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}

@inproceedings{hier_lit_1,
title = {Integrating Domain Knowledge: Using Hierarchies to Improve Deep Classifiers},
booktitle = {Asian Conference on Pattern Recognition (ACPR)},
author = {Clemens-Alexander Brust and Joachim Denzler},
year = {2019},
}

@INPROCEEDINGS{hier_lit_2,
  author={Deselaers, Thomas and Ferrari, Vittorio},
  booktitle={CVPR 2011}, 
  title={Visual and semantic similarity in ImageNet}, 
  year={2011},
  volume={},
  number={},
  pages={1777-1784},
  doi={10.1109/CVPR.2011.5995474}}

@InProceedings{hier_lit_3,
author = {Bertinetto, Luca and Mueller, Romain and Tertikas, Konstantinos and Samangooei, Sina and Lord, Nicholas A.},
title = {Making Better Mistakes: Leveraging Class Hierarchies With Deep Networks},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@INPROCEEDINGS{hier_lit_4,
  author={Brust, Clemens-Alexander and Barz, Björn and Denzler, Joachim},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Making Every Label Count: Handling Semantic Imprecision by Integrating Domain Knowledge}, 
  year={2021},
  volume={},
  number={},
  pages={6866-6873},
  doi={10.1109/ICPR48806.2021.9413283}}

@article{hier_lit_5,
  title={YOLO9000: Better, Faster, Stronger},
  author={Joseph Redmon and Ali Farhadi},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={6517-6525}
}

@article{hiermatch,
  title={Hierarchy-Based Image Embeddings for Semantic Image Retrieval},
  author={Bj{\"o}rn Barz and Joachim Denzler},
  journal={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year={2019},
  pages={638-647}
}

@Book{wordnet,
    title = {WordNet: An Electronic Lexical Database},
    author = {Christiane Fellbaum},
    year = {1998},
    publisher = {Bradford Books},
  }
  
@inproceedings{glove,
  added-at = {2016-02-18T12:02:38.000+0100},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  biburl = {https://www.bibsonomy.org/bibtex/2a6e77a38c13e374ab250e13ae22993ec/thoni},
  booktitle = {EMNLP},
  interhash = {29813227df1eea94efa14c7df2b5553a},
  intrahash = {a6e77a38c13e374ab250e13ae22993ec},
  keywords = {deeplearning deepwiki glove semantic},
  pages = {1532--1543},
  timestamp = {2016-09-06T08:23:07.000+0200},
  title = {Glove: Global Vectors for Word Representation.},
  volume = 14,
  year = 2014
}



@inproceedings{
radam,
title={On the Variance of the Adaptive Learning Rate and Beyond},
author={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgz2aEKDr}
}
@inproceedings{
Kitaev2020Reformer,
title={Reformer: The Efficient Transformer},
author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgNKkHtvB}
}

############################## METRIC LEARNING #######################
@article{musgrave2020metric,
  author    = {Kevin Musgrave and
               Serge J. Belongie and
               Ser{-}Nam Lim},
  title     = {A Metric Learning Reality Check},
  journal   = {CoRR},
  volume    = {abs/2003.08505},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.08505},
  archivePrefix = {arXiv},
  eprint    = {2003.08505},
  timestamp = {Tue, 24 Mar 2020 16:42:29 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-08505.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{fehervari2019unbiased,
    title={Unbiased Evaluation of Deep Metric Learning Algorithms},
    author={Istvan Fehervari and Avinash Ravichandran and Srikar Appalaraju},
    year={2019},
    eprint={1911.12528},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{rothgithub,
  author = {Karsten Roth and Biagio Brattoli},
  title = {Deep-Metric-Learning-Baselines},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Confusezius/Deep-Metric-Learning-Baselines}}
}

### ICML2019
@inproceedings{ala,
  title={Addressing the Loss-Metric Mismatch with Adaptive Loss Alignment},
  author={Chen Huang and Shuangfei Zhai and Walter Talbott and Miguel {\'A}ngel Bautista and Shih-Yu Sun and Carlos Guestrin and Josh Susskind},
  booktitle={ICML},
  year={2019}
}

@InProceedings{learn2rank,
author = {Cakir, Fatih and He, Kun and Xia, Xide and Kulis, Brian and Sclaroff, Stan},
title = {Deep Metric Learning to Rank},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@misc{cosface,
    title={CosFace: Large Margin Cosine Loss for Deep Face Recognition},
    author={Hao Wang and Yitong Wang and Zheng Zhou and Xing Ji and Dihong Gong and Jingchao Zhou and Zhifeng Li and Wei Liu},
    year={2018},
    eprint={1801.09414},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

### ICCV2019
@InProceedings{softriple,
author = {Qian, Qi and Shang, Lei and Sun, Baigui and Hu, Juhua and Li, Hao and Jin, Rong},
title = {SoftTriple Loss: Deep Metric Learning Without Triplet Sampling},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}


@inproceedings{mic,
  title={MIC: Mining Interclass Characteristics for Improved Metric Learning},
  author={Roth, Karsten and Brattoli, Biagio and Ommer, Bjorn},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={8000--8009},
  year={2019}
}


@article{alex2016deep,
  author    = {Alexander A. Alemi and
               Ian Fischer and
               Joshua V. Dillon and
               Kevin Murphy},
  title     = {Deep Variational Information Bottleneck},
  journal   = {CoRR},
  volume    = {abs/1612.00410},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.00410},
  archivePrefix = {arXiv},
  eprint    = {1612.00410},
  timestamp = {Mon, 13 Aug 2018 16:46:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AlemiFD016.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{milbich2020diva,
  author    = {Timo Milbich and
               Karsten Roth and
               Homanga Bharadhwaj and
               Samarth Sinha and
               Yoshua Bengio and
               Bj{\"{o}}rn Ommer and
               Joseph Paul Cohen},
  title     = {DiVA: Diverse Visual Feature Aggregation for Deep Metric Learning},
  journal   = {CoRR},
  volume    = {abs/2004.13458},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.13458},
  archivePrefix = {arXiv},
  eprint    = {2004.13458},
  timestamp = {Sat, 02 May 2020 19:17:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-13458.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{milbich2020sharing,
  author={T. {Milbich} and K. {Roth} and B. {Brattoli} and B. {Ommer}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Sharing Matters for Generalization in Deep Metric Learning}, 
  year={2020},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TPAMI.2020.3009620}}
  
@InProceedings{roth2020pads,
author = {Roth, Karsten and Milbich, Timo and Ommer, Bjorn},
title = {PADS: Policy-Adapted Sampling for Visual Similarity Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}


### CVPR2019
@InProceedings{horde,
author = {Jacob, Pierre and Picard, David and Histace, Aymeric and Edouard Klein},
title = {Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2019}
}

@InProceedings{Sanakoyeu_2019_CVPR,
author = {Sanakoyeu, Artsiom and Tschernezki, Vadim and Buchler, Uta and Ommer, Bjorn},
title = {Divide and Conquer the Embedding Space for Metric Learning},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2019}
}

@article{hardness-aware,
  title={Hardness-Aware Deep Metric Learning},
  author={Zheng, Wenzhao and Chen, Zhaodong and Lu, Jiwen and Zhou, Jie},
  journal={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}

@article{signal2noise,
  title={Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning},
  author={Yuan, Tongtong and Deng, Weihong and Tang, Jian and Tang, Yinan and Chen, Binghui},
  journal={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}

@article{rankedlist,
  title={Ranked List Loss for Deep Metric Learning},
  author={Wang, Xinshao and Hua, Yang and Kodirov, Elyor and Hu, Guosheng and Garnier, Romain and Robertson, Neil M.},
  journal={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}

### CLASSICAL
@inproceedings{quadtruplet,
  title={Beyond triplet loss: a deep quadruplet network for person re-identification},
  author={Chen, Weihua and Chen, Xiaotang and Zhang, Jianguo and Huang, Kaiqi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2017}
}

@inproceedings{abe,
    title = {Attention-Based Ensemble for Deep Metric Learning}, 
    author = {Kim, Wonsik and Goyal, Bhavya and Chawla, Kunal and Lee, Jungmin and Kwon, Keunjoo},
    booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
    year = {2018},
}

@inproceedings{yu2018correcting,
  title={Correcting the Triplet Selection Bias for Triplet Loss},
  author={Yu, Baosheng and Liu, Tongliang and Gong, Mingming and Ding, Changxing and Tao, Dacheng},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={71--87},
  year={2018}
}

@inproceedings{lifted,
  title={Deep metric learning via lifted structured feature embedding},
  author={Oh Song, Hyun and Xiang, Yu and Jegelka, Stefanie and Savarese, Silvio},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4004--4012},
  year={2016}
}

@inproceedings{facilitylocation,
  title={Deep metric learning via facility location},
  author={Oh Song, Hyun and Jegelka, Stefanie and Rathod, Vivek and Murphy, Kevin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5382--5390},
  year={2017}
}

@inproceedings{smartmining,
  title={Smart mining for deep metric learning},
  author={Harwood, Ben and Kumar, BG and Carneiro, Gustavo and Reid, Ian and Drummond, Tom and others},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2821--2829},
  year={2017}
}

@InProceedings{Parkhi15,
  author       = "Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew",
  title        = "Deep Face Recognition",
  booktitle    = "British Machine Vision Conference",
  year         = "2015",
}

@inproceedings{npairs,
  title={Improved deep metric learning with multi-class n-pair loss objective},
  author={Sohn, Kihyuk},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1857--1865},
  year={2016}
}

@inproceedings{stochastic,
  title={Stochastic variational deep kernel learning},
  author={Wilson, Andrew G and Hu, Zhiting and Salakhutdinov, Ruslan R and Xing, Eric P},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2586--2594},
  year={2016}
}

@InProceedings{dvml,
author = {Lin, Xudong and Duan, Yueqi and Dong, Qiyuan and Lu, Jiwen and Zhou, Jie},
title = {Deep Variational Metric Learning},
booktitle = {The European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@inproceedings{daml,
author = {Duan, Yueqi and Zheng, Wenzhao and Lin, Xudong and Lu, Jiwen and Zhou, Jie},
title = {Deep Adversarial Metric Learning},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@inproceedings{histogram,
  title={Learning deep embeddings with histogram loss},
  author={Ustinova, Evgeniya and Lempitsky, Victor},
  booktitle={Advances in Neural Information Processing Systems},
  year={2016}
}

@inproceedings{angular,
  title={Deep metric learning with angular loss},
  author={Wang, Jian and Zhou, Feng and Wen, Shilei and Liu, Xiao and Lin, Yuanqing},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2593--2601},
  year={2017}
}

@article{zhai2018classification,
  author    = {Andrew Zhai and
               Hao{-}Yu Wu},
  title     = {Making Classification Competitive for Deep Metric Learning},
  journal   = {CoRR},
  volume    = {abs/1811.12649},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.12649},
  archivePrefix = {arXiv},
  eprint    = {1811.12649},
  timestamp = {Mon, 03 Dec 2018 07:50:28 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-12649.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{diversifieddml,
      title={Diversified Mutual Learning for Deep Metric Learning}, 
      author={Wonpyo Park and Wonjae Kim and Kihyun You and Minsu Cho},
      year={2020},
      eprint={2009.04170},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{mutual_learning,
author = {Zhang, Ying and Xiang, Tao and Hospedales, Timothy M. and Lu, Huchuan},
title = {Deep Mutual Learning},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@article{selfreference,
  author    = {Xu Lan and
               Xiatian Zhu and
               Shaogang Gong},
  title     = {Self-Referenced Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1811.07598},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.07598},
  archivePrefix = {arXiv},
  eprint    = {1811.07598},
  timestamp = {Sun, 25 Nov 2018 18:57:12 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-07598.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{beyourownteacher,
author = {Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
title = {Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@article{MetaDistiller,
  author    = {Benlin Liu and
               Yongming Rao and
               Jiwen Lu and
               Jie Zhou and
               Cho{-}jui Hsieh},
  title     = {MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation},
  journal   = {CoRR},
  volume    = {abs/2008.12094},
  year      = {2020},
  url       = {https://arxiv.org/abs/2008.12094},
  archivePrefix = {arXiv},
  eprint    = {2008.12094},
  timestamp = {Tue, 15 Sep 2020 20:52:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2008-12094.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{htl,
  title={Deep metric learning with hierarchical triplet loss},
  author={Ge, Weifeng},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={269--285},
  year={2018}
}

@inproceedings{htg,
  title={An Adversarial Approach to Hard Triplet Generation},
  author={Zhao, Yiru and Jin, Zhongming and Qi, Guo-jun and Lu, Hongtao and Hua, Xian-sheng},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={501--517},
  year={2018}
}

@inproceedings{proxynca,
  title={No fuss distance metric learning using proxies},
  author={Movshovitz-Attias, Yair and Toshev, Alexander and Leung, Thomas K and Ioffe, Sergey and Singh, Saurabh},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={360--368},
  year={2017}
}

@inproceedings{proxyncapp,
  author    = {Eu Wern Teh and
               Terrance DeVries and
               Graham W. Taylor},
  editor    = {Andrea Vedaldi and
               Horst Bischof and
               Thomas Brox and
               Jan{-}Michael Frahm},
  title     = {ProxyNCA++: Revisiting and Revitalizing Proxy Neighborhood Component
               Analysis},
  booktitle = {Computer Vision - {ECCV} 2020 - 16th European Conference, Glasgow,
               UK, August 23-28, 2020, Proceedings, Part {XXIV}},
  series    = {Lecture Notes in Computer Science},
  volume    = {12369},
  pages     = {448--464},
  publisher = {Springer},
  year      = {2020},
  url       = {https://doi.org/10.1007/978-3-030-58586-0\_27},
  doi       = {10.1007/978-3-030-58586-0\_27},
  timestamp = {Mon, 30 Nov 2020 17:54:05 +0100},
  biburl    = {https://dblp.org/rec/conf/eccv/TehDT20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{margin,
  title={Sampling matters in deep embedding learning},
  author={Wu, Chao-Yuan and Manmatha, R and Smola, Alexander J and Krahenbuhl, Philipp},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2840--2848},
  year={2017}
}

@ARTICLE{ann_1,
  author={Malkov, Yu A. and Yashunin, D. A.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs}, 
  year={2020},
  volume={42},
  number={4},
  pages={824-836},
  doi={10.1109/TPAMI.2018.2889473}}

@InProceedings{ann_2,
author = {Baranchuk, Dmitry and Babenko, Artem and Malkov, Yury},
title = {Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@inproceedings{dreml,
  title={Deep Randomized Ensembles for Metric Learning},
  author={Xuan, Hong and Souvenir, Richard and Pless, Robert},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={723--734},
  year={2018}
}

@inproceedings{miningmanifold,
  title={Mining on manifolds: Metric learning without labels},
  author={Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\v{r}}ej},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7642--7651},
  year={2018}
}

@inproceedings{chopra2005learning,
  title={Learning a similarity metric discriminatively, with application to face verification},
  author={Chopra, Sumit and Hadsell, Raia and LeCun, Yann},
  pages={539--546},
  year={2005},
  organization={IEEE}
}

@inproceedings{shi2016embedding,
  title={Embedding deep metric for person re-identification: A study against large variations},
  author={Shi, Hailin and Yang, Yang and Zhu, Xiangyu and Liao, Shengcai and Lei, Zhen and Zheng, Weishi and Li, Stan Z},
  booktitle={European conference on computer vision},
  year={2016},
  organization={Springer}
}

@INPROCEEDINGS{face_verfication_inthewild, 
author={J. {Hu} and J. {Lu} and Y. {Tan}}, 
booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Discriminative Deep Metric Learning for Face Verification in the Wild}, 
year={2014}
}

@article{sphereface,
  title={SphereFace: Deep Hypersphere Embedding for Face Recognition},
  author={Weiyang Liu and Yandong Wen and Zhiding Yu and Ming Li and Bhiksha Raj and Le Song},
  journal={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017}
}

@inproceedings{contrastive,
 author = {Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
 title = {Dimensionality Reduction by Learning an Invariant Mapping},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 year = {2006}
}

############################# Unsupervised/Feature Learning ##########

@article{deepcluster,
  author    = {Mathilde Caron and
               Piotr Bojanowski and
               Armand Joulin and
               Matthijs Douze},
  title     = {Deep Clustering for Unsupervised Learning of Visual Features},
  journal   = {European Conference on Computer Vision},
  year      = {2018}
}

@inproceedings{cliquecnn,
  title={Cliquecnn: Deep unsupervised exemplar learning},
  author={Bautista, Miguel A and Sanakoyeu, Artsiom and Tikhoncheva, Ekaterina and Ommer, Bjorn},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3846--3854},
  year={2016}
}

@inproceedings{jigsaw,
  title={Unsupervised learning of visual representations by solving jigsaw puzzles},
  author={Noroozi, Mehdi and Favaro, Paolo},
  booktitle={European Conference on Computer Vision},
  pages={69--84},
  year={2016},
  organization={Springer}
}

@inproceedings{jigsaw++,
  title={Boosting self-supervised learning via knowledge transfer},
  author={Noroozi, Mehdi and Vinjimoor, Ananth and Favaro, Paolo and Pirsiavash, Hamed},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9359--9367},
  year={2018}
}

@inproceedings{Feng_2019_CVPR,
    title = {Self-Supervised Representation Learning by Rotation Feature Decoupling},
    author = {Feng, Zeyu and Xu, Chang and Tao, Dacheng},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2019}
}

@inproceedings{buechler_ECCV_2018,
	title = {Improving Spatiotemporal Self-Supervision by Deep Reinforcement Learning},
	author = {B{\"u}chler, U. and Brattoli, B. and Bj{\"o}rn Ommer},
	booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
	year = {2018}
}

@inproceedings{milbich2017unsupervised,
  title={Unsupervised video understanding by reconciliation of posture similarities},
  author={Milbich, Timo and Bautista, Miguel and Sutter, Ekaterina and Ommer, Bj{\"o}rn},
  booktitle={Proc. ICCV},
  year={2017}
}


############################## Classification ##################

@article{Zhe2018DirectionalSD,
  title={Directional statistics-based deep metric learning for image classification and retrieval},
  author={Xuefei Zhe and Shifeng Chen and Hong Yan},
  journal={Pattern Recognition},
  year={2018},
  volume={93}
}


############################## ENSEMBLES #######################
@inproceedings{hdc,
  title={Hard-aware deeply cascaded embedding},
  author={Yuan, Yuhui and Yang, Kuiyuan and Zhang, Chao},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={814--823},
  year={2017}
}


@inproceedings{bier,
  title={Bier-boosting independent embeddings robustly},
  author={Opitz, Michael and Waltner, Georg and Possegger, Horst and Bischof, Horst},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={5189--5198},
  year={2017}
}

@article{abier,
  title={Deep metric learning with BIER: Boosting independent embeddings robustly},
  author={Opitz, Michael and Waltner, Georg and Possegger, Horst and Bischof, Horst},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2018},
  publisher={IEEE}
}

@article{freund1997decision,
  title={A decision-theoretic generalization of on-line learning and an application to boosting},
  author={Freund, Yoav and Schapire, Robert E},
  journal={Journal of computer and system sciences},
  volume={55},
  number={1},
  pages={119--139},
  year={1997},
  publisher={Elsevier}
}

############################## DATASETS #######################
@inproceedings{cars196,
  title={3d object representations for fine-grained categorization},
  author={Krause, Jonathan and Stark, Michael and Deng, Jia and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={554--561},
  year={2013}
}


@techreport{cub200-2011,
	Title = {The Caltech-UCSD Birds-200-2011 Dataset},
	Author = {Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie, S.},
	Year = {2011},
	Institution = {California Institute of Technology},
	Number = {CNS-TR-2011-001}
}


@inproceedings{inshop,
 author = {Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang},
 title = {DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations},
 booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2016} 
 }
 
 
@inproceedings{pku,
 author = {H. Liu, Y. Tian, Y. Wang, L. Pang, and T. Huang},
 title = {Deep relative distance learning: Tell the difference between similar vehicles},
 booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2016},
 pages = {2167-2175}
 }
 
@article{nmi,
  title={Introduction to information retrieval},
  author={Manning, Christopher and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
  journal={Natural Language Engineering},
  volume={16},
  number={1},
  pages={100--103},
  year={2010},
  publisher={Cambridge university press}
}

@article{recall,
  title={Product quantization for nearest neighbor search},
  author={Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={1},
  pages={117--128},
  year={2011},
  publisher={IEEE}
}

@inproceedings{semihard,
  title={Facenet: A unified embedding for face recognition and clustering},
  author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={815--823},
  year={2015}
}

############################## OTHERS #######################
@article{googlenetv2,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={International Conference on Machine Learning},
  year={2015}
}

@inproceedings{googlenet,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{pytorch,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle={NIPS-W},
  year={2017}
}

@inproceedings{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{umap,
  title={UMAP: Uniform Manifold Approximation and Projection},
  author={McInnes, Leland and Healy, John and Saul, Nathaniel and Grossberger, Lukas},
  journal={The Journal of Open Source Software},
  volume={3},
  number={29},
  pages={861},
  year={2018}
}

@article{tsne,
  title={Visualizing data using t-SNE},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}

@article{kldiv2mle,
  title={Notes on Kullback-Leibler Divergence and Likelihood},
  author={Jonathon Shlens},
  journal={CoRR},
  year={2014},
  volume={abs/1404.2000}
}

@article{gradreverse,
 author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran\c{c}ois and Marchand, Mario and Lempitsky, Victor},
 title = {Domain-adversarial Training of Neural Networks},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2016},
 volume = {17},
 number = {1},
 month = jan,
 year = {2016},
 issn = {1532-4435},
 pages = {2096--2030},
 numpages = {-65},
 url = {http://dl.acm.org/citation.cfm?id=2946645.2946704},
 acmid = {2946704},
 publisher = {JMLR.org},
 keywords = {deep learning, domain adaptation, image classification, neural network, person re-identification, representation learning, sentiment analysis, synthetic data},
} 

@inproceedings{GlorotB10,
  author = {Glorot, Xavier and Bengio, Yoshua},
  series = {JMLR Proceedings},
  title = {Understanding the difficulty of training deep feedforward neural networks.},
  year = {2010}
}

@InProceedings{grouping,
author = {Bouchacourt, Diane and Tomioka, Ryota and Nowozin, Sebastian},
title = {Multi-Level Variational Autoencoder: Learning Disentangled Representations from Grouped Observations},
booktitle = {AAAI 2018},
year = {2018}
}

@inproceedings{vae,
  author = {Kingma, Diederik P and Welling, Max},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  title = {Auto-encoding variational bayes},
  year = 2013
}

@InProceedings{nat,
  title = 	 {Unsupervised Learning by Predicting Noise},
  author = 	 {Piotr Bojanowski and Armand Joulin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  year = 	 {2017}
}

@inproceedings{inceptionv1,
title	= {Going Deeper with Convolutions},
author	= {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
year	= {2015},
booktitle	= {Computer Vision and Pattern Recognition (CVPR)}
}

@inproceedings{batchnorm,
title	= {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
author	= {Sergey Ioffe and Christian Szegedy},
year	= {2015},
URL	= {http://jmlr.org/proceedings/papers/v37/ioffe15.pdf},
pages	= {448-456}
}


@misc{magnet,
    title={Metric Learning with Adaptive Density Discrimination},
    author={Oren Rippel and Manohar Paluri and Piotr Dollar and Lubomir Bourdev},
    year={2015},
    eprint={1511.05939},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}



###### Memory Bank #######
@inproceedings{MetaAug,
  title={Meta-Learning with Memory-Augmented Neural Networks},
  author={Adam Santoro and Sergey Bartunov and Matthew M Botvinick and Daan Wierstra and Timothy P. Lillicrap},
  booktitle={ICML},
  year={2016}
}


@misc{OneshotAug,
    title={One-shot Learning with Memory-Augmented Neural Networks},
    author={Adam Santoro and Sergey Bartunov and Matthew Botvinick and Daan Wierstra and Timothy Lillicrap},
    year={2016},
    eprint={1605.06065},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{assoBank,
    title={Unsupervised Learning using Pretrained CNN and Associative Memory Bank},
    author={Qun Liu and Supratik Mukhopadhyay},
    year={2018},
    eprint={1805.01033},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{parDiscr,
    title={Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination},
    author={Zhirong Wu and Yuanjun Xiong and Stella Yu and Dahua Lin},
    year={2018},
    eprint={1805.01978},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@InProceedings{moco,
author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
title = {Momentum Contrast for Unsupervised Visual Representation Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@misc{rainbow,
    title={Rainbow: Combining Improvements in Deep Reinforcement Learning},
    author={Matteo Hessel and Joseph Modayil and Hado van Hasselt and Tom Schaul and Georg Ostrovski and Will Dabney and Dan Horgan and Bilal Piot and Mohammad Azar and David Silver},
    year={2017},
    eprint={1710.02298},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@misc{knowledgedistill,
    title={Distilling the Knowledge in a Neural Network},
    author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
    year={2015},
    eprint={1503.02531},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}


@misc{fishergan,
    title={Fisher GAN},
    author={Youssef Mroueh and Tom Sercu},
    year={2017},
    eprint={1705.09675},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{savareseactive,
    title={Active Learning for Convolutional Neural Networks: A Core-Set Approach},
    author={Ozan Sener and Silvio Savarese},
    year={2017},
    eprint={1708.00489},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}


@inproceedings{wei_markov,
    title = "Using Document Summarization Techniques for Speech Data Subset Selection",
    author = "Wei, Kai  and
      Liu, Yuzong  and
      Kirchhoff, Katrin  and
      Bilmes, Jeff",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N13-1086",
    pages = "721--726",
}

@inproceedings{pretextmisra,
  author    = {Ishan Misra and
               Laurens van der Maaten},
  title     = {Self-Supervised Learning of Pretext-Invariant Representations},
  booktitle = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2020, Seattle, WA, USA, June 13-19, 2020},
  pages     = {6706--6716},
  publisher = {{IEEE}},
  year      = {2020},
  url       = {https://doi.org/10.1109/CVPR42600.2020.00674},
  doi       = {10.1109/CVPR42600.2020.00674},
  timestamp = {Tue, 11 Aug 2020 16:59:49 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/MisraM20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{amazondml,
    title={Unbiased Evaluation of Deep Metric Learning Algorithms},
    author={Istvan Fehervari and Avinash Ravichandran and Srikar Appalaraju},
    year={2019},
    eprint={1911.12528},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{smith2017don,
  title={Don't decay the learning rate, increase the batch size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  journal={arXiv preprint arXiv:1711.00489},
  year={2017}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{sinha2019small,
  title={Small-GAN: Speeding Up GAN Training Using Core-sets},
  author={Sinha, Samarth and Zhang, Han and Goyal, Anirudh and Bengio, Yoshua and Larochelle, Hugo and Odena, Augustus},
  journal={arXiv preprint arXiv:1910.13540},
  year={2019}
}

@article{coresets,
  title={Geometric approximation via coresets},
  author={Agarwal, Pankaj K and Har-Peled, Sariel and Varadarajan, Kasturi R},
  journal={Combinatorial and computational geometry},
  volume={52},
  pages={1--30},
  year={2005},
  publisher={Cambridge University Press, New York}
}

@inproceedings{bouthillier2019unreproducible,
  title={Unreproducible Research is Reproducible},
  author={Bouthillier, Xavier and Laurent, C{\'e}sar and Vincent, Pascal},
  booktitle={International Conference on Machine Learning},
  pages={725--734},
  year={2019}
}

@article{kaya,
author = {Kaya, Mahmut and Bilge, H.s},
year = {2019},
month = {08},
pages = {1066},
title = {Deep Metric Learning: A Survey},
volume = {11},
journal = {Symmetry},
doi = {10.3390/sym11091066}
}

@misc{d2coreset,
    title={Practical Coreset Constructions for Machine Learning},
    author={Olivier Bachem and Mario Lucic and Andreas Krause},
    year={2017},
    eprint={1703.06476},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{emd,
author = {Rubner, Yossi and Tomasi, Carlo and Guibas, Leonidas J.},
title = {The Earth Mover’s Distance as a Metric for Image Retrieval},
year = {2000},
issue_date = {November 2000},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {40},
number = {2},
issn = {0920-5691},
url = {https://doi.org/10.1023/A:1026543900054},
doi = {10.1023/A:1026543900054},
journal = {Int. J. Comput. Vision},
month = nov,
pages = {99–121},
numpages = {23},
keywords = {color, texture, image retrieval, perceptual metrics, Earth Mover’s Distance}
}


@misc{fid,
    title={GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
    author={Martin Heusel and Hubert Ramsauer and Thomas Unterthiner and Bernhard Nessler and Sepp Hochreiter},
    year={2017},
    eprint={1706.08500},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@INPROCEEDINGS{arcface,
  author={J. {Deng} and J. {Guo} and N. {Xue} and S. {Zafeiriou}},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={ArcFace: Additive Angular Margin Loss for Deep Face Recognition}, 
  year={2019},
  volume={},
  number={},
  pages={4685-4694},
  doi={10.1109/CVPR.2019.00482}}
  

@misc{manifoldmixup,
    title={Manifold Mixup: Better Representations by Interpolating Hidden States},
    author={Vikas Verma and Alex Lamb and Christopher Beckham and Amir Najafi and Ioannis Mitliagkas and Aaron Courville and David Lopez-Paz and Yoshua Bengio},
    year={2018},
    eprint={1806.05236},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{ben-david,
author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
title = {A Theory of Learning from Different Domains},
year = {2010},
volume = {79},
journal = {Mach. Learn.}
}

@InProceedings{multisimilarity,
author = {Wang, Xun and Han, Xintong and Huang, Weilin and Dong, Dengke and Scott, Matthew R.},
title = {Multi-Similarity Loss With General Pair Weighting for Deep Metric Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{BigGAN,
  author    = {Andrew Brock and
               Jeff Donahue and
               Karen Simonyan},
  title     = {Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
  journal   = {CoRR},
  volume    = {abs/1809.11096},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.11096},
  archivePrefix = {arXiv},
  eprint    = {1809.11096},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1809-11096},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{
coresetoptim,
title={Coresets for Accelerating Incremental Gradient Methods},
author={Baharan Mirzasoleiman and Jeff Bilmes and Jure Leskovec},
year={2020},
url={https://openreview.net/forum?id=SygRikHtvS}
}


@inproceedings{imagenet,
        author = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        title = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
        year = {2009}
}

@incollection{weight_decay,
    title = {A Simple Weight Decay Can Improve Generalization},
    author = {Anders Krogh and John A. Hertz},
    booktitle = {Advances in Neural Information Processing Systems},
    year = {1992},
}



@misc{tishby2015deep,
    title={Deep Learning and the Information Bottleneck Principle},
    author={Naftali Tishby and Noga Zaslavsky},
    year={2015},
    eprint={1503.02406},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@misc{shwartzziv2017opening,
    title={Opening the Black Box of Deep Neural Networks via Information},
    author={Ravid Shwartz-Ziv and Naftali Tishby},
    year={2017},
    eprint={1703.00810},
    archivePrefix={arXiv},
    primaryClass={cs.LG}}


@misc{belghazi2018mutual,
    title={MINE: Mutual Information Neural Estimation},
    author={Mohamed Ishmael Belghazi and Aristide Baratin and Sai Rajeswar and Sherjil Ozair and Yoshua Bengio and Aaron Courville and R Devon Hjelm},
    year={2018},
    eprint={1801.04062},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{dai2019diagnosing,
  author    = {Bin Dai and
               David P. Wipf},
  title     = {Diagnosing and Enhancing {VAE} Models},
  journal   = {CoRR},
  volume    = {abs/1903.05789},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.05789},
  archivePrefix = {arXiv},
  eprint    = {1903.05789},
  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-05789.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{goyal2019infobot,
    title={InfoBot: Transfer and Exploration via the Information Bottleneck},
    author={Anirudh Goyal and Riashat Islam and Daniel Strouse and Zafarali Ahmed and Matthew Botvinick and Hugo Larochelle and Yoshua Bengio and Sergey Levine},
    year={2019},
    eprint={1901.10902},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}


@misc{alex2016information,
    title={Information Dropout: Learning Optimal Representations Through Noisy Computation},
    author={Alessandro Achille and Stefano Soatto},
    year={2016},
    eprint={1611.01353},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}



@article{Bellet_2015,
   title={Robustness and generalization for metric learning},
   volume={151},
   ISSN={0925-2312},
   url={http://dx.doi.org/10.1016/j.neucom.2014.09.044},
   DOI={10.1016/j.neucom.2014.09.044},
   journal={Neurocomputing},
   publisher={Elsevier BV},
   author={Bellet, Aurélien and Habrard, Amaury},
   year={2015},
   month={Mar},
   pages={259–267}
}


@inproceedings{fcn_gen,
  title     = {Deep Metric Learning: The Generalization Analysis and an Adaptive Algorithm},
  author    = {Huai, Mengdi and Xue, Hongfei and Miao, Chenglin and Yao, Liuyi and Su, Lu and Chen, Changyou and Zhang, Aidong},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {2535--2541},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/352},
  url       = {https://doi.org/10.24963/ijcai.2019/352},
}



@InProceedings{Zantedeschi_2016_CVPR,
author = {Zantedeschi, Valentina and Emonet, Remi and Sebban, Marc},
title = {Metric Learning as Convex Combinations of Local Models With Generalization Guarantees},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@misc{bellet2013supervised,
    title={Supervised Metric Learning with Generalization Guarantees},
    author={Aurélien Bellet},
    year={2013},
    eprint={1307.4514},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}



@inproceedings{
generalisation_measures,
title={Fantastic Generalization Measures and Where to Find Them},
author={Yiding Jiang* and Behnam Neyshabur* and Dilip Krishnan and Hossein Mobahi and Samy Bengio},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJgIPJBFvH}
}


@incollection{importance_sampling,
title = {Training Deep Models Faster with Robust, Approximate Importance Sampling},
author = {Johnson, Tyler B and Guestrin, Carlos},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {7265--7275},
year = {2018},
publisher = {Curran Associates, Inc.}
}


@InProceedings{roth2020revisiting, title = {Revisiting Training Strategies and Generalization Performance in Deep Metric Learning}, author = {Roth, Karsten and Milbich, Timo and Sinha, Samarth and Gupta, Prateek and Ommer, Bjorn and Cohen, Joseph Paul}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {8242--8252}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/roth20a/roth20a.pdf}, url = { http://proceedings.mlr.press/v119/roth20a.html }, abstract = {Deep Metric Learning (DML) is arguably one of the most influential lines of research for learning visual similarities with many proposed approaches every year. Although the field benefits from the rapid progress, the divergence in training protocols, architectures, and parameter choices make an unbiased comparison difficult. To provide a consistent reference point, we revisit the most widely used DML objective functions and conduct a study of the crucial parameter choices as well as the commonly neglected mini-batch sampling process. Under consistent comparison, DML objectives show much higher saturation than indicated by literature. Further based on our analysis, we uncover a correlation between the embedding space density and compression to the generalization performance of DML models. Exploiting these insights, we propose a simple, yet effective, training regularization to reliably boost the performance of ranking-based DML models on various standard benchmark datasets. Code and a publicly accessible WandB-repo are available at https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch.} }


@misc{genlifted,
    title={In Defense of the Triplet Loss for Person Re-Identification},
    author={Alexander Hermans and Lucas Beyer and Bastian Leibe},
    year={2017},
    eprint={1703.07737},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}


@InProceedings{kim2020proxy,
author = {Kim, Sungyeon and Kim, Dongwon and Cho, Minsu and Kwak, Suha},
title = {Proxy Anchor Loss for Deep Metric Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}


@article{faiss,
  title={Billion-scale similarity search with GPUs},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:1702.08734},
  year={2017}
}


@article{kmeans,
  title={Least squares quantization in PCM},
  author={Stuart P. Lloyd},
  journal={IEEE Trans. Information Theory},
  year={1982},
  volume={28},
  pages={129-136}
}


############ Few-Shot Learning #############

% Few-Shot
@misc{snell2017prototypical,
    title={Prototypical Networks for Few-shot Learning},
    author={Jake Snell and Kevin Swersky and Richard S. Zemel},
    year={2017},
    eprint={1703.05175},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{sung2017learning,
    title={Learning to Compare: Relation Network for Few-Shot Learning},
    author={Flood Sung and Yongxin Yang and Li Zhang and Tao Xiang and Philip H. S. Torr and Timothy M. Hospedales},
    year={2017},
    eprint={1711.06025},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{vinyals2016matching,
    title={Matching Networks for One Shot Learning},
    author={Oriol Vinyals and Charles Blundell and Timothy Lillicrap and Koray Kavukcuoglu and Daan Wierstra},
    year={2016},
    eprint={1606.04080},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{Ravi2017OptimizationAA,
  title={Optimization as a Model for Few-Shot Learning},
  author={Sachin Ravi and Hugo Larochelle},
  booktitle={ICLR},
  year={2017}
}


@misc{finn2017modelagnostic,
    title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
    author={Chelsea Finn and Pieter Abbeel and Sergey Levine},
    year={2017},
    eprint={1703.03400},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{raghu2019rapid,
  author    = {Aniruddh Raghu and
               Maithra Raghu and
               Samy Bengio and
               Oriol Vinyals},
  title     = {Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness
               of {MAML}},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=rkgMkCEtPB},
  timestamp = {Thu, 07 May 2020 17:11:47 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/RaghuRBV20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{gidaris2018dynamic,
    title={Dynamic Few-Shot Visual Learning without Forgetting},
    author={Spyros Gidaris and Nikos Komodakis},
    year={2018},
    eprint={1804.09458},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{hariharan2016lowshot,
    title={Low-shot Visual Recognition by Shrinking and Hallucinating Features},
    author={Bharath Hariharan and Ross Girshick},
    year={2016},
    eprint={1606.02819},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{wang2018lowshot,
    title={Low-Shot Learning from Imaginary Data},
    author={Yu-Xiong Wang and Ross Girshick and Martial Hebert and Bharath Hariharan},
    year={2018},
    eprint={1801.05401},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{tian2020rethinking,
  title={Rethinking few-shot image classification: a good embedding is all you need?},
  author={Tian, Yonglong and Wang, Yue and Krishnan, Dilip and Tenenbaum, Joshua B and Isola, Phillip},
  journal={arXiv preprint arXiv:2003.11539},
  year={2020}
}

############ Knowledge Distillation ############# 

@inproceedings{bucilu2006model,
  title={Model compression},
  author={Buciluǎ, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={535--541},
  year={2006}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{chen2020simple,
title	= {A Simple Framework for Contrastive Learning of Visual Representations},
author	= {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Everest Hinton},
year	= {2020},
URL	= {https://arxiv.org/abs/2002.05709}
}


@article{tian2019contrastive,
  author    = {Yonglong Tian and
               Dilip Krishnan and
               Phillip Isola},
  title     = {Contrastive Representation Distillation},
  journal   = {CoRR},
  volume    = {abs/1910.10699},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.10699},
  archivePrefix = {arXiv},
  eprint    = {1910.10699},
  timestamp = {Fri, 25 Oct 2019 14:59:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-10699.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{romero2014fitnets,
  author    = {Adriana Romero and
               Nicolas Ballas and
               Samira Ebrahimi Kahou and
               Antoine Chassang and
               Carlo Gatta and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {FitNets: Hints for Thin Deep Nets},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6550},
  timestamp = {Thu, 25 Jul 2019 14:25:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RomeroBKCGB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zagoruyko2016paying,
  author    = {Sergey Zagoruyko and
               Nikos Komodakis},
  title     = {Paying More Attention to Attention: Improving the Performance of Convolutional
               Neural Networks via Attention Transfer},
  journal   = {CoRR},
  volume    = {abs/1612.03928},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.03928},
  archivePrefix = {arXiv},
  eprint    = {1612.03928},
  timestamp = {Mon, 13 Aug 2018 16:47:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZagoruykoK16a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Park_2019,
   title={Relational Knowledge Distillation},
   ISBN={9781728132938},
   url={http://dx.doi.org/10.1109/CVPR.2019.00409},
   DOI={10.1109/cvpr.2019.00409},
   journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
   year={2019},
   month={Jun}
}

@misc{huang2017like,
    title={Like What You Like: Knowledge Distill via Neuron Selectivity Transfer},
    author={Zehao Huang and Naiyan Wang},
    year={2017},
    eprint={1707.01219},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{Heo_2019,
   title={Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons},
   volume={33},
   ISSN={2159-5399},
   url={http://dx.doi.org/10.1609/aaai.v33i01.33013779},
   DOI={10.1609/aaai.v33i01.33013779},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
   author={Heo, Byeongho and Lee, Minsik and Yun, Sangdoo and Choi, Jin Young},
   year={2019},
   month={Jul},
   pages={3779–3787}
}

@article{Ahn_2019,
   title={Variational Information Distillation for Knowledge Transfer},
   ISBN={9781728132938},
   url={http://dx.doi.org/10.1109/CVPR.2019.00938},
   DOI={10.1109/cvpr.2019.00938},
   journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Ahn, Sungsoo and Hu, Shell Xu and Damianou, Andreas and Lawrence, Neil D. and Dai, Zhenwen},
   year={2019},
   month={Jun}
}


@article{surez2018tutorial,
  author    = {Juan{-}Luis Su{\'{a}}rez and
               Salvador Garc{\'{\i}}a and
               Francisco Herrera},
  title     = {A Tutorial on Distance Metric Learning: Mathematical Foundations,
               Algorithms and Software},
  journal   = {CoRR},
  volume    = {abs/1812.05944},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.05944},
  archivePrefix = {arXiv},
  eprint    = {1812.05944},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-05944.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@misc{hypersphere,
    title={Hypersphere},
    author={Eric W. Weisstein},
    year={2002},
    journal={MathWorld--A Wolfram Web Resource},    
    url = https://mathworld.wolfram.com/Hypersphere.html,
}

@article{wang2020understanding,
  title={Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere},
  author={Wang, Tongzhou and Isola, Phillip},
  journal={arXiv preprint arXiv:2005.10242},
  year={2020}
}


@misc{khosla2020supervised,
    title={Supervised Contrastive Learning},
    author={Prannay Khosla and Piotr Teterwak and Chen Wang and Aaron Sarna and Yonglong Tian and Phillip Isola and Aaron Maschinot and Ce Liu and Dilip Krishnan},
    year={2020},
    eprint={2004.11362},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{rajasegaran2020selfsupervised,
  author    = {Jathushan Rajasegaran and
               Salman Khan and
               Munawar Hayat and
               Fahad Shahbaz Khan and
               Mubarak Shah},
  title     = {Self-supervised Knowledge Distillation for Few-shot Learning},
  journal   = {CoRR},
  volume    = {abs/2006.09785},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.09785},
  archivePrefix = {arXiv},
  eprint    = {2006.09785},
  timestamp = {Tue, 23 Jun 2020 17:57:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-09785.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{chen2020new,
    title={A New Meta-Baseline for Few-Shot Learning},
    author={Yinbo Chen and Xiaolong Wang and Zhuang Liu and Huijuan Xu and Trevor Darrell},
    year={2020},
    eprint={2003.04390},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{furlanello2018born,
  author    = {Tommaso Furlanello and
               Zachary Chase Lipton and
               Michael Tschannen and
               Laurent Itti and
               Anima Anandkumar},
  editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {Born-Again Neural Networks},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {1602--1611},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/furlanello18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/FurlanelloLTIA18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@incollection{metric_1,
title = {Metric Learning by Collapsing Classes},
author = {Globerson, Amir and Sam T. Roweis},
booktitle = {Advances in Neural Information Processing Systems 18},
editor = {Y. Weiss and B. Sch\"{o}lkopf and J. C. Platt},
pages = {451--458},
year = {2006},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2947-metric-learning-by-collapsing-classes.pdf}
}

@incollection{metric_2,
title = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},
author = {Weinberger, Kilian Q and John Blitzer and Lawrence K. Saul},
booktitle = {Advances in Neural Information Processing Systems 18},
editor = {Y. Weiss and B. Sch\"{o}lkopf and J. C. Platt},
pages = {1473--1480},
year = {2006},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf}
}

@article{metric_3,
title = "Supervised distance metric learning through maximization of the Jeffrey divergence",
journal = "Pattern Recognition",
volume = "64",
pages = "215 - 225",
year = "2017",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2016.11.010",
url = "http://www.sciencedirect.com/science/article/pii/S0031320316303600",
author = "Bac Nguyen and Carlos Morell and Bernard {De Baets}",
keywords = "Distance metric learning, Nearest neighbor, Linear transformation, Jeffrey divergence"
}

@article{metric_4,
  author       = {Nguyen Cong, Bac and Morell, Carlos and De Baets, Bernard},
  issn         = {0950-7051},
  journal      = {KNOWLEDGE-BASED SYSTEMS},
  keywords     = {Ordinal classification,Ordinal regression,Distance metric learning,Nearest neighbor,Semidefinite programming,REJECTIVE MULTIPLE TEST,NEAREST-NEIGHBOR,RELATIVE COMPARISONS,AGE ESTIMATION,REGRESSION,OPTIMIZATION,MINIMIZATION,DIVERGENCE,RANKING,TESTS},
  language     = {eng},
  pages        = {17--28},
  title        = {Distance metric learning for ordinal classification based on triplet constraints},
  url          = {http://dx.doi.org/10.1016/j.knosys.2017.11.022},
  volume       = {142},
  year         = {2018},
}


@incollection{chen2019curvilinear,
title = {Curvilinear Distance Metric Learning},
author = {Chen, Shuo and Luo, Lei and Yang, Jian and Gong, Chen and Li, Jun and Huang, Heng},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {4223--4232},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8675-curvilinear-distance-metric-learning.pdf}
}

@paper{chen2017darkrank,
	author = {Yuntao Chen and Naiyan Wang and Zhaoxiang Zhang},
	title = {DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2018},
	keywords = {Knowledge distillation; Model acceleration; Metric learning},
	abstract = {We have witnessed rapid evolution of deep neural network architecture design in the past years. These latest progresses greatly facilitate the developments in various areas such as computer vision and natural language processing. However, along with the extraordinary performance, these state-of-the-art models also bring in expensive computational cost. Directly deploying these models into applications with real-time requirement is still infeasible. Recently, Hinton et al. have shown that the dark knowledge within a powerful teacher model can significantly help the training of a smaller and faster student network. These knowledge are vastly beneficial to improve the generalization ability of the student model. Inspired by their work, we introduce a new type of knowledge---cross sample similarities for model compression and acceleration. This knowledge can be naturally derived from deep metric learning model. To transfer them, we bring the "learning to rank" technique into deep metric learning formulation. We test our proposed DarkRank method on various metric learning tasks including pedestrian re-identification, image retrieval and image clustering. The results are quite encouraging. Our method can improve over the baseline method by a large margin. Moreover, it is fully compatible with other existing methods. When combined, the performance can be further boosted.},

	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17147}
}

@InProceedings{Brattoli_2020_CVPR,
author = {Brattoli, Biagio and Tighe, Joseph and Zhdanov, Fedor and Perona, Pietro and Chalupka, Krzysztof},
title = {Rethinking Zero-Shot Video Classification: End-to-End Training for Realistic Applications},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@misc{yi2014bindev,
    title={Deep Metric Learning for Practical Person Re-Identification},
    author={Dong Yi and Zhen Lei and Stan Z. Li},
    year={2014},
    eprint={1407.4979},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@InProceedings{epshn,
author = {Xuan, Hong and Stylianou, Abby and Pless, Robert},
title = {Improved Embeddings with Easy Positive Triplet Mining},
booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
month = {March},
year = {2020}
}

@misc{alex2019revisiting,
    title={Revisiting Self-Supervised Visual Representation Learning},
    author={Alexander Kolesnikov and Xiaohua Zhai and Lucas Beyer},
    year={2019},
    eprint={1901.09005},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{budnik2020asymmetric,
    title={Asymmetric metric learning for knowledge transfer},
    author={Mateusz Budnik and Yannis Avrithis},
    year={2020},
    eprint={2006.16331},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{laskar2020dataefficient,
  author    = {Zakaria Laskar and
               Juho Kannala},
  title     = {Data-Efficient Ranking Distillation for Image Retrieval},
  journal   = {CoRR},
  volume    = {abs/2007.05299},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.05299},
  archivePrefix = {arXiv},
  eprint    = {2007.05299},
  timestamp = {Mon, 20 Jul 2020 14:20:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-05299.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Yu_2019,
   title={Learning Metrics From Teachers: Compact Networks for Image Embedding},
   ISBN={9781728132938},
   url={http://dx.doi.org/10.1109/CVPR.2019.00302},
   DOI={10.1109/cvpr.2019.00302},
   journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Yu, Lu and Yazici, Vacit Oguz and Liu, Xialei and van de Weijer, Joost and Cheng, Yongmei and Ramisa, Arnau},
   year={2019},
   month={Jun}
}

@article{Han2019DeepDM,
  title={Deep Distillation Metric Learning},
  author={Jiaxu Han and Tianyu Zhao and Changqing Zhang},
  journal={Proceedings of the ACM Multimedia Asia},
  year={2019}
}

@article{self_distill_1,
  author    = {Linfeng Zhang and
               Jiebo Song and
               Anni Gao and
               Jingwei Chen and
               Chenglong Bao and
               Kaisheng Ma},
  title     = {Be Your Own Teacher: Improve the Performance of Convolutional Neural
               Networks via Self Distillation},
  journal   = {CoRR},
  volume    = {abs/1905.08094},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.08094},
  archivePrefix = {arXiv},
  eprint    = {1905.08094},
  timestamp = {Tue, 28 May 2019 12:48:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-08094.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{self_distill_2,
  author    = {Samira Abnar and
               Mostafa Dehghani and
               Willem H. Zuidema},
  title     = {Transferring Inductive Biases through Knowledge Distillation},
  journal   = {CoRR},
  volume    = {abs/2006.00555},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.00555},
  archivePrefix = {arXiv},
  eprint    = {2006.00555},
  timestamp = {Mon, 08 Jun 2020 15:48:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-00555.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{self_distill_3,
author = {Yun, Sukmin and Park, Jongjin and Lee, Kimin and Shin, Jinwoo},
title = {Regularizing Class-Wise Predictions via Self-Knowledge Distillation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}




########### New Related Works
@misc{cohen2021singlenoun,
      title={The Single-Noun Prior for Image Clustering}, 
      author={Niv Cohen and Yedid Hoshen},
      year={2021},
      eprint={2104.03952},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{dogan2020labelsimilarity,
      title={Label-similarity Curriculum Learning}, 
      author={Urun Dogan and Aniket Anand Deshmukh and Marcin Machura and Christian Igel},
      year={2020},
      eprint={1911.06902},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

########### Explainable DML


########### New DML
@article{darkrank, title={DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11783}, abstractNote={ &lt;p&gt; We have witnessed rapid evolution of deep neural network architecture design in the past years. These latest progresses greatly facilitate the developments in various areas such as computer vision and natural language processing. However, along with the extraordinary performance, these state-of-the-art models also bring in expensive computational cost. Directly deploying these models into applications with real-time requirement is still infeasible. Recently, Hinton et al. have shown that the dark knowledge within a powerful teacher model can significantly help the training of a smaller and faster student network. These knowledge are vastly beneficial to improve the generalization ability of the student model. Inspired by their work, we introduce a new type of knowledge---cross sample similarities for model compression and acceleration. This knowledge can be naturally derived from deep metric learning model. To transfer them, we bring the &quot;learning to rank&quot; technique into deep metric learning formulation. We test our proposed DarkRank method on various metric learning tasks including pedestrian re-identification, image retrieval and image clustering. The results are quite encouraging. Our method can improve over the baseline method by a large margin. Moreover, it is fully compatible with other existing methods. When combined, the performance can be further boosted. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chen, Yuntao and Wang, Naiyan and Zhang, Zhaoxiang}, year={2018}, month={Apr.} }

@misc{qi2020simple,
      title={A Simple and Effective Framework for Pairwise Deep Metric Learning}, 
      author={Qi Qi and Yan Yan and Xiaoyu Wang and Tianbao Yang},
      year={2020},
      eprint={1912.11194},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{Yan_2021_CVPR,
    author    = {Yan, Jiexi and Luo, Lei and Deng, Cheng and Huang, Heng},
    title     = {Unsupervised Hyperbolic Metric Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {12465-12474}
}

@InProceedings{unsup_1,
author="Li, Yang
and Kan, Shichao
and He, Zhihai",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Unsupervised Deep Metric Learning with Transformed Attention Consistency and Contrastive Clustering Loss",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="141--157",
abstract="Existing approaches for unsupervised metric learning focus on exploring self-supervision information within the input image itself. We observe that, when analyzing images, human eyes often compare images against each other instead of examining images individually. In addition, they often pay attention to certain keypoints, image regions, or objects which are discriminative between image classes but highly consistent within classes. Even if the image is being transformed, the attention pattern will be consistent. Motivated by this observation, we develop a new approach to unsupervised deep metric learning where the network is learned based on self-supervision information across images instead of within one single image. To characterize the consistent pattern of human attention during image comparisons, we introduce the idea of transformed attention consistency. It assumes that visually similar images, even undergoing different image transforms, should share the same consistent visual attention map. This consistency leads to a pairwise self-supervision loss, allowing us to learn a Siamese deep neural network to encode and compare images against their transformed or matched pairs. To further enhance the inter-class discriminative power of the feature generated by this network, we adapt the concept of triplet loss from supervised metric learning to our unsupervised case and introduce the contrastive clustering loss. Our extensive experimental results on benchmark datasets demonstrate that our proposed method outperforms current state-of-the-art methods for unsupervised metric learning by a large margin.",
isbn="978-3-030-58621-8"
}

@InProceedings{unsup_2,
author = {Ye, Mang and Zhang, Xu and Yuen, Pong C. and Chang, Shih-Fu},
title = {Unsupervised Embedding Learning via Invariant and Spreading Instance Feature},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@InProceedings{unsup_3,
author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ondřej},
title = {Mining on Manifolds: Metric Learning Without Labels},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@article{unsup_4,
  author    = {Xuefei Cao and
               Bor{-}Chun Chen and
               Ser{-}Nam Lim},
  title     = {Unsupervised Deep Metric Learning via Auxiliary Rotation Loss},
  journal   = {CoRR},
  volume    = {abs/1911.07072},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.07072},
  archivePrefix = {arXiv},
  eprint    = {1911.07072},
  timestamp = {Mon, 02 Dec 2019 17:48:37 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-07072.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@InProceedings{Wang_2020_CVPR,
author = {Wang, Xun and Zhang, Haozhi and Huang, Weilin and Scott, Matthew R.},
title = {Cross-Batch Memory for Embedding Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@ARTICLE{dutta2020orthogonalunsupdml,
  author={Dutta, Ujjal Kr and Harandi, Mehrtash and Sekhar, Chellu Chandra},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Unsupervised Deep Metric Learning via Orthogonality Based Probabilistic Loss}, 
  year={2020},
  volume={1},
  number={1},
  pages={74-84},
  doi={10.1109/TAI.2020.3026982}}
  
@InProceedings{Deng_2021_CVPR,
    author    = {Deng, Jiankang and Guo, Jia and Yang, Jing and Lattas, Alexandros and Zafeiriou, Stefanos},
    title     = {Variational Prototype Learning for Deep Face Recognition},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {11906-11915}
}

@InProceedings{Suh_2019_CVPR,
author = {Suh, Yumin and Han, Bohyung and Kim, Wonsik and Lee, Kyoung Mu},
title = {Stochastic Class-Based Hard Example Mining for Deep Metric Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}


@inproceedings{Zhu2020graphdml,
 author = {Zhu, Yuehua and Yang, Muli and Deng, Cheng and Liu, Wei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {17792--17803},
 publisher = {Curran Associates, Inc.},
 title = {Fewer is More: A Deep Graph Metric Learning Perspective Using Fewer Proxies},
 url = {https://proceedings.neurips.cc/paper/2020/file/ce016f59ecc2366a43e1c96a4774d167-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{zhang2020sphericaldml,
 author = {Zhang, Dingyi and Li, Yingming and Zhang, Zhongfei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {18772--18783},
 publisher = {Curran Associates, Inc.},
 title = {Deep Metric Learning with Spherical Embedding},
 url = {https://proceedings.neurips.cc/paper/2020/file/d9812f756d0df06c7381945d2e2c7d4b-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{wang2020robustdml,
 author = {Wang, Lu and Liu, Xuanqing and Yi, Jinfeng and Jiang, Yuan and Hsieh, Cho-Jui},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {19302--19313},
 publisher = {Curran Associates, Inc.},
 title = {Provably Robust Metric Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/e038453073d221a4f32d0bab94ca7cee-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{seidenschwarz2021graphdml,
  title = 	 {Learning Intra-Batch Connections for Deep Metric Learning},
  author =       {Seidenschwarz, Jenny Denise and Elezi, Ismail and Leal-Taix{\'e}, Laura},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9410--9421},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/seidenschwarz21a/seidenschwarz21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/seidenschwarz21a.html},
  abstract = 	 {The goal of metric learning is to learn a function that maps samples to a lower-dimensional space where similar samples lie closer than dissimilar ones. Particularly, deep metric learning utilizes neural networks to learn such a mapping. Most approaches rely on losses that only take the relations between pairs or triplets of samples into account, which either belong to the same class or two different classes. However, these methods do not explore the embedding space in its entirety. To this end, we propose an approach based on message passing networks that takes all the relations in a mini-batch into account. We refine embedding vectors by exchanging messages among all samples in a given batch allowing the training process to be aware of its overall structure. Since not all samples are equally important to predict a decision boundary, we use an attention mechanism during message passing to allow samples to weigh the importance of each neighbor accordingly. We achieve state-of-the-art results on clustering and image retrieval on the CUB-200-2011, Cars196, Stanford Online Products, and In-Shop Clothes datasets. To facilitate further research, we make available the code and the models at https://github.com/dvl-tum/intra_batch_connections.}
}

@InProceedings{Mohan_2020_CVPR,
author = {Mohan, Deen Dayal and Sankaran, Nishant and Fedorishin, Dennis and Setlur, Srirangaraj and Govindaraju, Venu},
title = {Moving in the Right Direction: A Regularization for Deep Metric Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@misc{ko2021learning,
      title={Learning with Memory-based Virtual Classes for Deep Metric Learning}, 
      author={Byungsoo Ko and Geonmo Gu and Han-Gyu Kim},
      year={2021},
      eprint={2103.16940},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{omninet,
  author    = {Subhojeet Pramanik and
               Priyanka Agrawal and
               Aman Hussain},
  title     = {OmniNet: {A} unified architecture for multi-modal multi-task learning},
  journal   = {CoRR},
  volume    = {abs/1907.07804},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.07804},
  eprinttype = {arXiv},
  eprint    = {1907.07804},
  timestamp = {Tue, 23 Jul 2019 10:54:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-07804.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{virtex,
  author    = {Karan Desai and
               Justin Johnson},
  title     = {VirTex: Learning Visual Representations from Textual Annotations},
  journal   = {CoRR},
  volume    = {abs/2006.06666},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.06666},
  eprinttype = {arXiv},
  eprint    = {2006.06666},
  timestamp = {Sat, 13 Jun 2020 18:28:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-06666.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tian2020hynet,
 author = {Tian, Yurun and Barroso Laguna, Axel and Ng, Tony and Balntas, Vassileios and Mikolajczyk, Krystian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {7401--7412},
 publisher = {Curran Associates, Inc.},
 title = {HyNet: Learning Local Descriptor with Hybrid Similarity Measure and Triplet Loss},
 url = {https://proceedings.neurips.cc/paper/2020/file/52d2752b150f9c35ccb6869cbf074e48-Paper.pdf},
 volume = {33},
 year = {2020}
}

@InProceedings{Zheng_2021_CVPR_compositional,
    author    = {Zheng, Wenzhao and Wang, Chengkun and Lu, Jiwen and Zhou, Jie},
    title     = {Deep Compositional Metric Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {9320-9329}
}

@InProceedings{elezi2020grouploss,
author="Elezi, Ismail
and Vascon, Sebastiano
and Torcinovich, Alessandro
and Pelillo, Marcello
and Leal-Taix{\'e}, Laura",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="The Group Loss for Deep Metric Learning",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="277--294",
abstract="Deep metric learning has yielded impressive results in tasks such as clustering and image retrieval by leveraging neural networks to obtain highly discriminative feature embeddings, which can be used to group samples into different classes. Much research has been devoted to the design of smart loss functions or data mining strategies for training such networks. Most methods consider only pairs or triplets of samples within a mini-batch to compute the loss function, which is commonly based on the distance between embeddings. We propose Group Loss, a loss function based on a differentiable label-propagation method that enforces embedding similarity across all samples of a group while promoting, at the same time, low-density regions amongst data points belonging to different groups. Guided by the smoothness assumption that ``similar objects should belong to the same group'', the proposed loss trains the neural network for a classification task, enforcing a consistent labelling amongst samples within a class. We show state-of-the-art results on clustering and image retrieval on several datasets, and show the potential of our method when combined with other techniques such as ensembles. To facilitate further research, we make available the code and the models at https://github.com/dvl-tum/group{\_}loss.",
isbn="978-3-030-58571-6"
}


@InProceedings{Kim_2021_CVPR,
    author    = {Kim, Sungyeon and Kim, Dongwon and Cho, Minsu and Kwak, Suha},
    title     = {Embedding Transfer With Label Relaxation for Improved Metric Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {3967-3976}
}

@inproceedings{fu2020texturedml,
 author = {Fu, Huan and Li, Shunming and Jia, Rongfei and Gong, Mingming and Zhao, Binqiang and Tao, Dacheng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {14675--14687},
 publisher = {Curran Associates, Inc.},
 title = {Hard Example Generation by Texture Synthesis for Cross-domain Shape Similarity Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/a87d27f712df362cd22c7a8ef823e987-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{dml_ood_gen,
  author    = {Timo Milbich and
               Karsten Roth and
               Samarth Sinha and
               Ludwig Schmidt and
               Marzyeh Ghassemi and
               Bj{\"{o}}rn Ommer},
  title     = {Characterizing Generalization under Out-Of-Distribution Shifts in
               Deep Metric Learning},
  journal   = {CoRR},
  volume    = {abs/2107.09562},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.09562},
  archivePrefix = {arXiv},
  eprint    = {2107.09562},
  timestamp = {Thu, 29 Jul 2021 12:42:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-09562.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{s2sd,
  title = 	 {Simultaneous Similarity-based Self-Distillation for Deep Metric Learning},
  author =       {Roth, Karsten and Milbich, Timo and Ommer, Bjorn and Cohen, Joseph Paul and Ghassemi, Marzyeh},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9095--9106},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/roth21a/roth21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/roth21a.html},
  abstract = 	 {Deep Metric Learning (DML) provides a crucial tool for visual similarity and zero-shot retrieval applications by learning generalizing embedding spaces, although recent work in DML has shown strong performance saturation across training objectives. However, generalization capacity is known to scale with the embedding space dimensionality. Unfortunately, high dimensional embeddings also create higher retrieval cost for downstream applications. To remedy this, we propose S2SD - Simultaneous Similarity-based Self-distillation. S2SD extends DML with knowledge distillation from auxiliary, high-dimensional embedding and feature spaces to leverage complementary context during training while retaining test-time cost and with negligible changes to the training time. Experiments and ablations across different objectives and standard benchmarks show S2SD offering highly significant improvements of up to 7% in Recall@1, while also setting a new state-of-the-art.}
}


########### CROSS-MODAL (DML)
@InProceedings{Faraki_2021_CVPR,
    author    = {Faraki, Masoud and Yu, Xiang and Tsai, Yi-Hsuan and Suh, Yumin and Chandraker, Manmohan},
    title     = {Cross-Domain Similarity Learning for Face Recognition in Unseen Domains},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {15292-15301}
}

@InProceedings{Wray_2021_CVPR,
    author    = {Wray, Michael and Doughty, Hazel and Damen, Dima},
    title     = {On Semantic Similarity in Video Retrieval},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {3650-3660}
}

@InProceedings{Wang_2021_CVPR,
    author    = {Wang, Hao and Bai, Xiang and Yang, Mingkun and Zhu, Shenggao and Wang, Jing and Liu, Wenyu},
    title     = {Scene Text Retrieval via Joint Text Detection and Similarity Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {4558-4567}
}

@INPROCEEDINGS {Ren2021crossmodalmatching,
author = {L. Ren and K. Li and L. Wang and K. Hua},
booktitle = {2020 25th International Conference on Pattern Recognition (ICPR)},
title = {Beyond the Deep Metric Learning: Enhance the Cross-Modal Matching with Adversarial Discriminative Domain Regularization},
year = {2021},
volume = {},
issn = {1051-4651},
pages = {10165-10172},
keywords = {measurement;location awareness;visualization;semantics;benchmark testing;information retrieval;natural language processing},
doi = {10.1109/ICPR48806.2021.9412297},
url = {https://doi.ieeecomputersociety.org/10.1109/ICPR48806.2021.9412297},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jan}
}

@inproceedings{Huang2017crossmodaldml,
  author    = {Xin Huang and
               Yuxin Peng},
  title     = {Cross-modal deep metric learning with multi-task regularization},
  booktitle = {2017 {IEEE} International Conference on Multimedia and Expo, {ICME}
               2017, Hong Kong, China, July 10-14, 2017},
  pages     = {943--948},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/ICME.2017.8019340},
  doi       = {10.1109/ICME.2017.8019340},
  timestamp = {Wed, 16 Oct 2019 14:14:56 +0200},
  biburl    = {https://dblp.org/rec/conf/icmcs/HuangP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Wei_2020_CVPR,
author = {Wei, Jiwei and Xu, Xing and Yang, Yang and Ji, Yanli and Wang, Zheng and Shen, Heng Tao},
title = {Universal Weighting Metric Learning for Cross-Modal Matching},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@ARTICLE{Liong2017coupledcrossmodaldml,
  author={Liong, Venice Erin and Lu, Jiwen and Tan, Yap-Peng and Zhou, Jie},
  journal={IEEE Transactions on Multimedia}, 
  title={Deep Coupled Metric Learning for Cross-Modal Matching}, 
  year={2017},
  volume={19},
  number={6},
  pages={1234-1244},
  doi={10.1109/TMM.2016.2646180}}

@misc{tsimpoukelli2021multimodal,
      title={Multimodal Few-Shot Learning with Frozen Language Models}, 
      author={Maria Tsimpoukelli and Jacob Menick and Serkan Cabi and S. M. Ali Eslami and Oriol Vinyals and Felix Hill},
      year={2021},
      eprint={2106.13884},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{Dzabraev_2021_CVPR,
    author    = {Dzabraev, Maksim and Kalashnikov, Maksim and Komkov, Stepan and Petiushko, Aleksandr},
    title     = {MDMMT: Multidomain Multimodal Transformer for Video Retrieval},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2021},
    pages     = {3354-3363}
}

########### Domain-Adaptation
@inproceedings{
sohn2018unsupervised,
title={Unsupervised Domain Adaptation for Distance Metric Learning},
author={Kihyuk Sohn and Wenling Shang and Xiang Yu and Manmohan Chandraker},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BklhAj09K7},
}

@InProceedings{Zheng_2021_CVPR,
    author    = {Zheng, Kecheng and Liu, Wu and He, Lingxiao and Mei, Tao and Luo, Jiebo and Zha, Zheng-Jun},
    title     = {Group-aware Label Transfer for Domain Adaptive Person Re-identification},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {5310-5319}
}


@InProceedings{clip,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}


@article{gpt2,
abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
file = {:Users/shanest/Documents/Library/Radford et al/Unknown/Radford et al. - 2019 - Language Models are Unsupervised Multitask Learners.pdf:pdf},
keywords = {model},
title = {{Language Models are Unsupervised Multitask Learners}},
url = {https://openai.com/blog/better-language-models/},
year = {2019}
}

@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{transformers,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{huggingface,
      title={HuggingFace's Transformers: State-of-the-art Natural Language Processing}, 
      author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
      year={2020},
      eprint={1910.03771},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and M. Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}

@misc{distilbert,
      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
      year={2020},
      eprint={1910.01108},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{mpnet,
 author = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {16857--16867},
 publisher = {Curran Associates, Inc.},
 title = {MPNet: Masked and Permuted Pre-training for Language Understanding},
 url = {https://proceedings.neurips.cc/paper/2020/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@article{crossmodal_1,
author = {Xu, Xing and He, Li and Lu, Huimin and Gao, Lianli and Ji, Yanli},
title = {Deep Adversarial Metric Learning for Cross-Modal Retrieval},
year = {2019},
issue_date = {March 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-018-0541-x},
doi = {10.1007/s11280-018-0541-x},
abstract = {Cross-modal retrieval has become a highlighted research topic, to provide flexible
retrieval experience across multimedia data such as image, video, text and audio.
The core of existing cross-modal retrieval approaches is to narrow down the gap between
different modalities either by finding a maximally correlated embedding space. Recently,
researchers leverage Deep Neural Network (DNN) to learn nonlinear transformations
for each modality to obtain transformed features in a common subspace where cross-modal
matching can be performed. However, the statistical characteristics of the original
features for each modality are not explicitly preserved in the learned subspace. Inspired
by recent advances in adversarial learning, we propose a novel Deep Adversarial Metric
Learning approach, termed DAML for cross-modal retrieval. DAML nonlinearly maps labeled
data pairs of different modalities into a shared latent feature subspace, under which
the intra-class variation is minimized and the inter-class variation is maximized,
and the difference of each data pair captured from two modalities of the same class
is minimized, respectively. In addition to maximizing the correlations between modalities,
we add an additional regularization by introducing adversarial learning. In particular,
we introduce a modality classifier to predict the modality of a transformed feature,
which ensures that the transformed features are also statistically indistinguishable.
Experiments on three popular multimodal datasets show that DAML achieves superior
performance compared to several state of the art cross-modal retrieval methods.},
journal = {World Wide Web},
month = mar,
pages = {657–672},
numpages = {16},
keywords = {Metric learning, Adversarial learning, Cross-modal retrieval}
}

@INPROCEEDINGS{crossmodal_2,
  author={Huang, Xin and Peng, Yuxin},
  booktitle={2017 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={Cross-modal deep metric learning with multi-task regularization}, 
  year={2017},
  volume={},
  number={},
  pages={943-948},
  doi={10.1109/ICME.2017.8019340}}
  
@InProceedings{crossmodal_3,
author = {Zhen, Liangli and Hu, Peng and Wang, Xu and Peng, Dezhong},
title = {Deep Supervised Cross-Modal Retrieval},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{crossmodal_4,
title = {Hybrid representation learning for cross-modal retrieval},
journal = {Neurocomputing},
volume = {345},
pages = {45-57},
year = {2019},
note = {Deep Learning for Intelligent Sensing, Decision-Making and Control},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.10.082},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219301407},
author = {Wenming Cao and Qiubin Lin and Zhihai He and Zhiquan He},
keywords = {Cross-modal retrieval, Hybrid representation, DNNs},
abstract = {The rapid development of Deep Neural Networks (DNNs) in single-modal retrieval has promoted the wide application of DNNs in cross-modal retrieval tasks. Therefore, we propose a DNN-based method to learn the shared representation for each modality. Our method, hybrid representation learning (HRL), consists of three steps. In the first learning step, stacked restricted Boltzmann machines (SRBM) are utilized to extract the modality-friendly representation for each modality, with statistical properties that are more similar than those of the original input instances of both modalities, and a multimodal deep belief net (multimodal DBN) is utilized to extract the modality-mutual representation, which contains some missing information in the original input instances. In the second learning step, a two-level network containing a joint autoencoder and a three-layer feedforward neural net are used. From these steps, the hybrid representation is obtained, which combines the image representation constructed by the image-pathway SRBM and the modality-mutual representation, which involves the latent image representation and can be used to infer the missing values of the image via the multimodal DBN or vice-versa. In the third learning step, stacked bimodal autoencoders are used to obtain the final shared representation for each modality. The experimental results show that our proposed HRL method is superior to several advanced approaches according to three widely used cross-modal datasets.}
}

@ARTICLE{crossmodal_5,
  author={He, Yonghao and Xiang, Shiming and Kang, Cuicui and Wang, Jian and Pan, Chunhong},
  journal={IEEE Transactions on Multimedia}, 
  title={Cross-Modal Retrieval via Deep and Bidirectional Representation Learning}, 
  year={2016},
  volume={18},
  number={7},
  pages={1363-1377},
  doi={10.1109/TMM.2016.2558463}}
  
@article{crossmodal_6,
author = {Peng, Yuxin and Qi, Jinwei},
title = {CM-GANs: Cross-Modal Generative Adversarial Networks for Common Representation Learning},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3284750},
doi = {10.1145/3284750},
abstract = {It is known that the inconsistent distributions and representations of different modalities,
such as image and text, cause the heterogeneity gap, which makes it very challenging
to correlate heterogeneous data and measure their similarities. Recently, generative
adversarial networks (GANs) have been proposed and have shown their strong ability
to model data distribution and learn discriminative representation. It has also been
shown that adversarial learning can be fully exploited to learn discriminative common
representations for bridging the heterogeneity gap. Inspired by this, we aim to effectively
correlate large-scale heterogeneous data of different modalities with the power of
GANs to model cross-modal joint distribution. In this article, we propose Cross-modal
Generative Adversarial Networks (CM-GANs) with the following contributions. First,
a cross-modal GAN architecture is proposed to model joint distribution over the data
of different modalities. The inter-modality and intra-modality correlation can be
explored simultaneously in generative and discriminative models. Both compete with
each other to promote cross-modal correlation learning. Second, the cross-modal convolutional
autoencoders with weight-sharing constraint are proposed to form the generative model.
They not only exploit the cross-modal correlation for learning the common representations
but also preserve reconstruction information for capturing the semantic consistency
within each modality. Third, a cross-modal adversarial training mechanism is proposed,
which uses two kinds of discriminative models to simultaneously conduct intra-modality
and inter-modality discrimination. They can mutually boost to make the generated common
representations more discriminative by the adversarial training process. In summary,
our proposed CM-GAN approach can use GANs to perform cross-modal common representation
learning by which the heterogeneous data can be effectively correlated. Extensive
experiments are conducted to verify the performance of CM-GANs on cross-modal retrieval
compared with 13 state-of-the-art methods on 4 cross-modal datasets.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {22},
numpages = {24},
keywords = {cross-modal retrieval, Generative adversarial network, cross-modal adversarial mechanism, common representation learning}
}

@misc{crossmodal_7,
      title={Audio Retrieval with Natural Language Queries}, 
      author={Andreea-Maria Oncescu and A. Sophia Koepke and João F. Henriques and Zeynep Akata and Samuel Albanie},
      year={2021},
      eprint={2105.02192},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@InProceedings{crossmodal_8,
    author    = {Chen, Yanbei and Xian, Yongqin and Koepke, A. Sophia and Shan, Ying and Akata, Zeynep},
    title     = {Distilling Audio-Visual Knowledge by Compositional Contrastive Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {7016-7025}
}

@InProceedings{cm_1,
author="van de Weijer, Joost
and Khan, Fahad Shahbaz",
editor="Tr{\'e}meau, Alain
and Schettini, Raimondo
and Tominaga, Shoji",
title="An Overview of Color Name Applications in Computer Vision",
booktitle="Computational Color Imaging",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="16--22",
abstract="In this article we provide an overview of color name applications in computer vision. Color names are linguistic labels which humans use to communicate color. Computational color naming learns a mapping from pixels values to color names. In recent years color names have been applied to a wide variety of computer vision applications, including image classification, object recognition, texture classification, visual tracking and action recognition. Here we provide an overview of these results which show that in general color names outperform photometric invariants as a color representation.",
isbn="978-3-319-15979-9"
}


@inproceedings{cm_2,
  author    = {Raphael C. Prates and
               Cristianne R. S. Dutra and
               William Robson Schwartz},
  title     = {Predominant color name indexing structure for person re-identification},
  booktitle = {2016 {IEEE} International Conference on Image Processing, {ICIP} 2016,
               Phoenix, AZ, USA, September 25-28, 2016},
  pages     = {779--783},
  publisher = {{IEEE}},
  year      = {2016},
  url       = {https://doi.org/10.1109/ICIP.2016.7532463},
  doi       = {10.1109/ICIP.2016.7532463},
  timestamp = {Wed, 16 Oct 2019 14:14:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icip/PratesDS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{hier_cm_1,
  author       = "S. Ging and M. Zolfaghari and H. Pirsiavash and T. Brox",
  title        = "COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning",
  booktitle    = "Advances in Neural Information Processing Systems (NeurIPS)",
  volume       = "33",
  pages        = "22605--22618",
  month        = " ",
  year         = "2020",
  editor       = "H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin",
  publisher    = "Curran Associates, Inc.",
  url          = "http://lmb.informatik.uni-freiburg.de/Publications/2020/GZB20"
}


@InProceedings{perceiver,
  title = 	 {Perceiver: General Perception with Iterative Attention},
  author =       {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4651--4664},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/jaegle21a/jaegle21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/jaegle21a.html},
  abstract = 	 {Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver {–} a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.}
}




############# Detection
@InProceedings{He_2019_ICCV,
author = {He, Kaiming and Girshick, Ross and Dollar, Piotr},
title = {Rethinking ImageNet Pre-Training},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}


@InProceedings{detect_1,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2014}
}

@InProceedings{detect_2,
author="He, Kaiming
and Zhang, Xiangyu
and Ren, Shaoqing
and Sun, Jian",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="346--361",
abstract="Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g.224{\texttimes}224) input image. This requirement is ``artificial'' and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, ``spatial pyramid pooling'', to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101.",
isbn="978-3-319-10578-9"
}


@InProceedings{detect_3,
author = {Girshick, Ross},
title = {Fast R-CNN},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}

@inproceedings{detect_4,
 author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
 url = {https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf},
 volume = {28},
 year = {2015}
}


@InProceedings{detect_5,
author="Liu, Wei
and Anguelov, Dragomir
and Erhan, Dumitru
and Szegedy, Christian
and Reed, Scott
and Fu, Cheng-Yang
and Berg, Alexander C.",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="SSD: Single Shot MultiBox Detector",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="21--37",
abstract="We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For {\$}{\$}300 {\backslash}times 300{\$}{\$}300{\texttimes}300input, SSD achieves 74.3{\%} mAP on VOC2007 test at 59FPS on a Nvidia Titan X and for {\$}{\$}512 {\backslash}times 512{\$}{\$}512{\texttimes}512input, SSD achieves 76.9{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.",
isbn="978-3-319-46448-0"
}

@InProceedings{detect_6,
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
title = {You Only Look Once: Unified, Real-Time Object Detection},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@InProceedings{detect_7,
author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
title = {Feature Pyramid Networks for Object Detection},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}


@article{word_hierarchies,
author = {Cimiano, Ph and Hotho, Andreas and Staab, Steffen},
year = {2011},
month = {09},
pages = {},
title = {Learning Concept Hierarchies from Text Corpora using Formal Concept
Analysis},
volume = {24},
journal = {Journal of Artificial Intelligence Research},
doi = {10.1613/jair.1648}
}









############################# CONTINUAL LEARNING #####################
@ARTICLE{li_2018_lwf,
  author={Li, Zhizhong and Hoiem, Derek},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Learning without Forgetting}, 
  year={2018},
  volume={40},
  number={12},
  pages={2935-2947},
  doi={10.1109/TPAMI.2017.2773081}}

@InProceedings{yang_2021_ksm,
    author    = {Yang, Li and He, Zhezhi and Zhang, Junshan and Fan, Deliang},
    title     = {KSM: Fast Multiple Task Adaption via Kernel-Wise Soft Mask Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {13845-13853}
}

@inproceedings{riemer_2019_mer,
  author    = {Matthew Riemer and
               Ignacio Cases and
               Robert Ajemian and
               Miao Liu and
               Irina Rish and
               Yuhai Tu and
               Gerald Tesauro},
  title     = {Learning to Learn without Forgetting by Maximizing Transfer and Minimizing
               Interference},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=B1gTShAct7},
  timestamp = {Thu, 12 Sep 2019 14:49:23 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/RiemerCALRTT19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{li_2020_energy,
  title={Energy-Based Models for Continual Learning},
  author={Li, Shuang and Du, Yilun and van de Ven, Gido M and Torralba, Antonio and Mordatch, Igor},
  journal={arXiv preprint arXiv:2011.12216},
  year={2020}
}

@inproceedings{beaulieu_2020_anml,
  author    = {Shawn Beaulieu and
               Lapo Frati and
               Thomas Miconi and
               Joel Lehman and
               Kenneth O. Stanley and
               Jeff Clune and
               Nick Cheney},
  editor    = {Giuseppe De Giacomo and
               Alejandro Catal{\'{a}} and
               Bistra Dilkina and
               Michela Milano and
               Sen{\'{e}}n Barro and
               Alberto Bugar{\'{\i}}n and
               J{\'{e}}r{\^{o}}me Lang},
  title     = {Learning to Continually Learn},
  booktitle = {{ECAI} 2020 - 24th European Conference on Artificial Intelligence,
               29 August-8 September 2020, Santiago de Compostela, Spain, August
               29 - September 8, 2020 - Including 10th Conference on Prestigious
               Applications of Artificial Intelligence {(PAIS} 2020)},
  series    = {Frontiers in Artificial Intelligence and Applications},
  volume    = {325},
  pages     = {992--1001},
  publisher = {{IOS} Press},
  year      = {2020},
  url       = {https://doi.org/10.3233/FAIA200193},
  doi       = {10.3233/FAIA200193},
  timestamp = {Fri, 09 Apr 2021 18:50:05 +0200},
  biburl    = {https://dblp.org/rec/conf/ecai/BeaulieuFMLSCC20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{javed_2019_oml,
 author = {Javed, Khurram and White, Martha},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Meta-Learning Representations for Continual Learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/f4dd765c12f2ef67f98f3558c282a9cd-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{
pham_2021_dualnet,
title={DualNet: Continual Learning, Fast and Slow},
author={Quang Pham and Chenghao Liu and Steven HOI},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=eQ7Kh-QeWnO}
}

@misc{
wang_2022_prompt,
title={Learning to Prompt for Continual Learning},
author={Zifeng Wang and Zizhao Zhang and Chen-Yu Lee and Han Zhang and Ruoxi Sun and Xiaoqi Ren and Guolong Su and Vincent Perot and Jennifer Dy and Tomas Pfister},
year={2022},
url={https://openreview.net/forum?id=RzXb6a3H3rs}
}

@article{yoon_2021_onlinecoreset,
  author    = {Jaehong Yoon and
               Divyam Madaan and
               Eunho Yang and
               Sung Ju Hwang},
  title     = {Online Coreset Selection for Rehearsal-based Continual Learning},
  journal   = {CoRR},
  volume    = {abs/2106.01085},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.01085},
  eprinttype = {arXiv},
  eprint    = {2106.01085},
  timestamp = {Wed, 09 Jun 2021 18:45:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-01085.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{lee_2021_aim,
    author    = {Lee, Eugene and Huang, Cheng-Han and Lee, Chen-Yi},
    title     = {Few-Shot and Continual Learning With Attentive Independent Mechanisms},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {9455-9464}
}


@inproceedings{borsos_2020_bilevelcoreset,
 author = {Borsos, Zal\'{a}n and Mutny, Mojmir and Krause, Andreas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {14879--14890},
 publisher = {Curran Associates, Inc.},
 title = {Coresets via Bilevel Optimization for Continual Learning and Streaming},
 url = {https://proceedings.neurips.cc/paper/2020/file/aa2a77371374094fe9e0bc1de3f94ed9-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{ahn_2021_ssil,
    author    = {Ahn, Hongjoon and Kwak, Jihwan and Lim, Subin and Bang, Hyeonsu and Kim, Hyojun and Moon, Taesup},
    title     = {SS-IL: Separated Softmax for Incremental Learning},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {844-853}
}

@inproceedings{
ke_2021_achieving,
title={Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning},
author={Zixuan Ke and Bing Liu and Nianzu Ma and Hu Xu and Lei Shu},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=RJ7XFI15Q8f}
}


@inproceedings{mirzadeh_2020_understanding,
 author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Pascanu, Razvan and Ghasemzadeh, Hassan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {7308--7320},
 publisher = {Curran Associates, Inc.},
 title = {Understanding the Role of Training Regimes in Continual Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/518a38cc9a0173d0b2dc088166981cf8-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{
shi_2021_overcoming,
title={Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning by Finding Flat Minima},
author={Guangyuan SHI and Jiaxin Chen and Wenlong Zhang and Li-Ming Zhan and Xiao-Ming Wu},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=ALvt7nXa2q}
}

@InProceedings{Cha_2021_co2l,
    author    = {Cha, Hyuntak and Lee, Jaeho and Shin, Jinwoo},
    title     = {Co2L: Contrastive Continual Learning},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {9516-9525}
}
@inproceedings{prabhu_2020_gdumb,
  title={GDumb: A Simple Approach that Questions Our Progress in Continual Learning},
  author={Prabhu, Ameya and Torr, Philip and Dokania, Puneet},
  booktitle={The European Conference on Computer Vision (ECCV)},
  month={August},
  year={2020}
}

@InProceedings{bang_2021_rainbow,
    author    = {Bang, Jihwan and Kim, Heesu and Yoo, YoungJoon and Ha, Jung-Woo and Choi, Jonghyun},
    title     = {Rainbow Memory: Continual Learning With a Memory of Diverse Samples},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {8218-8227}
}

@InProceedings{mirzadeh_2020_dropout,
author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Ghasemzadeh, Hassan},
title = {Dropout as an Implicit Gating Mechanism for Continual Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
month = {June},
year = {2020}
}

@inproceedings{hung_2019_cpg,
 author = {Hung, Ching-Yi and Tu, Cheng-Hao and Wu, Cheng-En and Chen, Chien-Hung and Chan, Yi-Ming and Chen, Chu-Song},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Compacting, Picking and Growing for Unforgetting Continual Learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/3b220b436e5f3d917a1e649a0dc0281c-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{
oswald_2020_continualhypernetworks,
title={Continual learning with hypernetworks},
author={Johannes von Oswald and Christian Henning and João Sacramento and Benjamin F. Grewe},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJgwNerKvB}
}

@ARTICLE {rosenfeld_2018_dan,
author = {A. Rosenfeld and J. K. Tsotsos},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
title = {Incremental Learning Through Deep Adaptation},
year = {2020},
volume = {42},
number = {03},
issn = {1939-3539},
pages = {651-663},
keywords = {task analysis;switches;training;neural networks;computer architecture;convolutional codes},
doi = {10.1109/TPAMI.2018.2884462},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {mar}
}

@InProceedings{belouadah_2019_il2m,
author = {Belouadah, Eden and Popescu, Adrian},
title = {IL2M: Class Incremental Learning With Dual Memory},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@inproceedings{javed_2018_revisiting_icarl,
  title={Revisiting Distillation and Incremental Classifier Learning},
  author={Khurram Javed and Faisal Shafait},
  booktitle={ACCV},
  year={2018}
}

@InProceedings{rebuffi_2017_icarl,
author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H.},
title = {iCaRL: Incremental Classifier and Representation Learning},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}


@InProceedings{xiang_2019_adversarial_incremental,
author = {Xiang, Ye and Fu, Ying and Ji, Pan and Huang, Hua},
title = {Incremental Learning Using Conditional Adversarial Networks},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@article{chaudhry_2019_tinymemory,
  author    = {Arslan Chaudhry and
               Marcus Rohrbach and
               Mohamed Elhoseiny and
               Thalaiyasingam Ajanthan and
               Puneet Kumar Dokania and
               Philip H. S. Torr and
               Marc'Aurelio Ranzato},
  title     = {Continual Learning with Tiny Episodic Memories},
  journal   = {CoRR},
  volume    = {abs/1902.10486},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.10486},
  eprinttype = {arXiv},
  eprint    = {1902.10486},
  timestamp = {Tue, 21 May 2019 18:03:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-10486.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lopezpaz_2017_gradientepisodicmemory,
 author = {Lopez-Paz, David and Ranzato, Marc\textquotesingle Aurelio},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Gradient Episodic Memory for Continual Learning},
 url = {https://proceedings.neurips.cc/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf},
 volume = {30},
 year = {2017}
}


@InProceedings{zhao_2020_maintaining,
author = {Zhao, Bowen and Xiao, Xi and Gan, Guojun and Zhang, Bin and Xia, Shu-Tao},
title = {Maintaining Discrimination and Fairness in Class Incremental Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@InProceedings{wu_2019_largescaleincremental,
author = {Wu, Yue and Chen, Yinpeng and Wang, Lijuan and Ye, Yuancheng and Liu, Zicheng and Guo, Yandong and Fu, Yun},
title = {Large Scale Incremental Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@InProceedings{castro_2018_e2e,
author = {Castro, Francisco M. and Marin-Jimenez, Manuel J. and Guil, Nicolas and Schmid, Cordelia and Alahari, Karteek},
title = {End-to-End Incremental Learning},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@InProceedings{belouadah_2020_scail,
author = {Belouadah, Eden and Popescu, Adrian},
title = {ScaIL: Classifier Weights Scaling for Class Incremental Learning},
booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
month = {March},
year = {2020}
}

@InProceedings{schwarz_2018_progresscompress,
  title = 	 {Progress \& Compress: A scalable framework for continual learning},
  author =       {Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4528--4537},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/schwarz18a/schwarz18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/schwarz18a.html},
  abstract = 	 {We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously acquired skills. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. We demonstrate the progress &amp; compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.}
}

  
@InProceedings{mallya_2018_piggyback,
author = {Mallya, Arun and Davis, Dillon and Lazebnik, Svetlana},
title = {Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@inproceedings{mallya_2018_packnet,
title = "PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning",
abstract = "This paper presents a method for adding multiple tasks to a single deep neural network while avoiding catastrophic forgetting. Inspired by network pruning techniques, we exploit redundancies in large deep networks to free up parameters that can then be employed to learn new tasks. By performing iterative pruning and network re-training, we are able to sequentially 'pack' multiple tasks into a single network while ensuring minimal drop in performance and minimal storage overhead. Unlike prior work that uses proxy losses to maintain accuracy on older tasks, we always optimize for the task at hand. We perform extensive experiments on a variety of network architectures and large-scale datasets, and observe much better robustness against catastrophic forgetting than prior work. In particular, we are able to add three fine-grained classification tasks to a single ImageNet-trained VGG-16 network and achieve accuracies close to those of separately trained networks for each task.",
author = "Arun Mallya and Svetlana Lazebnik",
note = "Publisher Copyright: {\textcopyright} 2018 IEEE. Copyright: Copyright 2019 Elsevier B.V., All rights reserved.; 31st Meeting of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2018 ; Conference date: 18-06-2018 Through 22-06-2018",
year = "2018",
month = dec,
day = "14",
doi = "10.1109/CVPR.2018.00810",
language = "English (US)",
series = "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
publisher = "IEEE Computer Society",
pages = "7765--7773",
booktitle = "Proceedings - 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2018",
}

@article{rusu_2016_pnn,
  author    = {Andrei A. Rusu and
               Neil C. Rabinowitz and
               Guillaume Desjardins and
               Hubert Soyer and
               James Kirkpatrick and
               Koray Kavukcuoglu and
               Razvan Pascanu and
               Raia Hadsell},
  title     = {Progressive Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1606.04671},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.04671},
  eprinttype = {arXiv},
  eprint    = {1606.04671},
  timestamp = {Mon, 13 Aug 2018 16:46:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RusuRDSKKPH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article {kirkpatrick_2017_ewc,
	author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
	title = {Overcoming catastrophic forgetting in neural networks},
	volume = {114},
	number = {13},
	pages = {3521--3526},
	year = {2017},
	doi = {10.1073/pnas.1611835114},
	publisher = {National Academy of Sciences},
	abstract = {Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially.The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/114/13/3521},
	eprint = {https://www.pnas.org/content/114/13/3521.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@InProceedings{Singh_2021_CVPR,
    author    = {Singh, Pravendra and Mazumder, Pratik and Rai, Piyush and Namboodiri, Vinay P.},
    title     = {Rectification-Based Knowledge Retention for Continual Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {15282-15291}
}

@inproceedings{buzzega2020dark,
 author = {Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and Calderara, Simone},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {15920--15930},
 publisher = {Curran Associates, Inc.},
 title = {Dark Experience for General Continual Learning: a Strong, Simple Baseline},
 volume = {33},
 year = {2020}
}

@InProceedings{pmlr-v119-knoblauch20a,
  title = 	 {Optimal Continual Learning has Perfect Memory and is {NP}-hard},
  author =       {Knoblauch, Jeremias and Husain, Hisham and Diethe, Tom},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5327--5337},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/knoblauch20a/knoblauch20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/knoblauch20a.html},
  abstract = 	 {Continual Learning (CL) algorithms incrementally learn a predictor or representation across multiple sequentially observed tasks. Designing CL algorithms that perform reliably and avoid so-called catastrophic forgetting has proven a persistent challenge. The current paper develops a theoretical approach that explains why. In particular, we derive the computational properties which CL algorithms would have to possess in order to avoid catastrophic forgetting. Our main finding is that such optimal CL algorithms generally solve an NP-hard problem and will require perfect memory to do so. The findings are of theoretical interest, but also explain the excellent performance of CL algorithms using experience replay, episodic memory and core sets relative to regularization-based approaches.}
}

@article{pan2020continual,
  title = {Continual Deep Learning by Functional Regularisation of Memorable Past},
  author = {Pan, Pingbo and Swaroop, Siddharth and Immer, Alexander and Eschenhagen, Runa and Turner, Richard E and Khan, Mohammad Emtiyaz},
  journal = {Advances in neural information processing systems},
  year = {2020}
}



@inproceedings{AGEM,
  title={Efficient Lifelong Learning with A-GEM},
  author={Chaudhry, Arslan and Ranzato, Marc’Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
  booktitle={ICLR},
  year={2019}
}

@InProceedings{Kukleva_2021_ICCV,
    author    = {Kukleva, Anna and Kuehne, Hilde and Schiele, Bernt},
    title     = {Generalized and Incremental Few-Shot Learning by Explicit Learning and Calibration Without Forgetting},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {9020-9029}
}


@InProceedings{Smith_2021_ICCV,
    author    = {Smith, James and Hsu, Yen-Chang and Balloch, Jonathan and Shen, Yilin and Jin, Hongxia and Kira, Zsolt},
    title     = {Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {9374-9384}
}


@InProceedings{pmlr-v139-nekoei21a,
  title = 	 {Continuous Coordination As a Realistic Scenario for Lifelong Learning},
  author =       {Nekoei, Hadi and Badrinaaraayanan, Akilesh and Courville, Aaron and Chandar, Sarath},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8016--8024},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nekoei21a/nekoei21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nekoei21a.html},
  abstract = 	 {Current deep reinforcement learning (RL) algorithms are still highly task-specific and lack the ability to generalize to new environments. Lifelong learning (LLL), however, aims at solving multiple tasks sequentially by efficiently transferring and using knowledge between tasks. Despite a surge of interest in lifelong RL in recent years, the lack of a realistic testbed makes robust evaluation of LLL algorithms difficult. Multi-agent RL (MARL), on the other hand, can be seen as a natural scenario for lifelong RL due to its inherent non-stationarity, since the agents’ policies change over time. In this work, we introduce a multi-agent lifelong learning testbed that supports both zero-shot and few-shot settings. Our setup is based on Hanabi {—} a partially-observable, fully cooperative multi-agent game that has been shown to be challenging for zero-shot coordination. Its large strategy space makes it a desirable environment for lifelong RL tasks. We evaluate several recent MARL methods, and benchmark state-of-the-art LLL algorithms in limited memory and computation regimes to shed light on their strengths and weaknesses. This continual learning paradigm also provides us with a pragmatic way of going beyond centralized training which is the most commonly used training protocol in MARL. We empirically show that the agents trained in our setup are able to coordinate well with unseen agents, without any additional assumptions made by previous works. The code and all pre-trained models are available at https://github.com/chandar-lab/Lifelong-Hanabi.}
}

@article{yan2021dynamically,
  title={DER: Dynamically Expandable Representation for Class Incremental Learning},
  author={Yan, Shipeng and Xie, Jiangwei and He, Xuming},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021}
}

############################# MULTI-TASK LEARNING #####################
@InProceedings{mtl_pu,
author="Pu, Jian
and Jiang, Yu-Gang
and Wang, Jun
and Xue, Xiangyang",
title="Which Looks Like Which: Exploring Inter-class Relationships in Fine-Grained Visual Categorization",
booktitle="Proceedings of the IEEE European Conference on Computer Vision (ECCV)",
year="2014",
pages="425--440",
}

@INPROCEEDINGS{mtl_bhattarai, 
author={B. Bhattarai and G. Sharma and F. Jurie}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={CP-mtML: Coupled Projection Multi-Task Metric Learning for Large Scale Face Retrieval},
year={2016}, 
pages={4226-4235}
}

@Inbook{shared_properties,
author="Torralba, Antonio
and Murphy, Kevin P.
and Freeman, William T.",
title="Shared Features for Multiclass Object Detection",
year="2006",
publisher="Springer Berlin Heidelberg",
pages="345--361"
}

@inproceedings{torchvision,
author = {Marcel, S\'{e}bastien and Rodriguez, Yann},
title = {Torchvision the Machine-Vision Package of Torch},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1873951.1874254},
doi = {10.1145/1873951.1874254},
abstract = {This paper presents Torchvision an open source machine vision package for Torch. Torch
is a machine learning library providing a series of the state-of-the-art algorithms
such as Neural Networks, Support Vector Machines, Gaussian Mixture Models, Hidden
Markov Models and many others. Torchvision provides additional functionalities to
manipulate and process images with standard image processing algorithms. Hence, the
resulting images can be used directly with the Torch machine learning algorithms as
Torchvision is fully integrated with Torch. Both Torch and Torchvision are written
in C++ language and are publicly available under the Free-BSD License.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {1485–1488},
numpages = {4},
keywords = {vision, pattern recognition, open source, machine learning, face detection and recognition},
location = {Firenze, Italy},
series = {MM '10}
}

@inproceedings{eq_1,
author = {Wang, Feng and Xiang, Xiang and Cheng, Jian and Yuille, Alan Loddon},
title = {NormFace: L<sub>2</sub> Hypersphere Embedding for Face Verification},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123359},
doi = {10.1145/3123266.3123359},
abstract = {Thanks to the recent developments of Convolutional Neural Networks, the performance
of face verification methods has increased rapidly. In a typical face verification
method, feature normalization is a critical step for boosting performance. This motivates
us to introduce and study the effect of normalization during training. But we find
this is non-trivial, despite normalization being differentiable. We identify and study
four issues related to normalization through mathematical analysis, which yields understanding
and helps with parameter settings. Based on this analysis we propose two strategies
for training using normalized features. The first is a modification of softmax loss,
which optimizes cosine similarity instead of inner-product. The second is a reformulation
of metric learning by introducing an agent vector for each class. We show that both
strategies, and small variants, consistently improve performance by between 0.2% to
0.4% on the LFW dataset based on two models. This is significant because the performance
of the two models on LFW dataset is close to saturation at over 98%.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {1041–1049},
numpages = {9},
keywords = {feature normalization, face verification, metric learning},
location = {Mountain View, California, USA},
series = {MM '17}
}

@inproceedings{eq_2,
  author={Dingyi Zhang and Yingming Li and Zhongfei Zhang},
  title={Deep Metric Learning with Spherical Embedding},
  year={2020},
  cdate={1577836800000},
  url={https://proceedings.neurips.cc/paper/2020/hash/d9812f756d0df06c7381945d2e2c7d4b-Abstract.html},
  booktitle={NeurIPS}
}

@misc{timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}

@inproceedings{rbf,
author = {Bojanowski, Piotr and Joulin, Armand},
title = {Unsupervised Learning by Predicting Noise},
year = {2017},
publisher = {JMLR.org},
abstract = {Convolutional neural networks provide visual features that perform well in many computer
vision applications. However, training these networks requires large amounts of supervision;
this paper introduces a generic framework to train such networks, end-to-end, with
no supervision. We propose to fix a set of target representations, called Noise As
Targets (NAT), and to constrain the deep features to align to them. This domain agnostic
approach avoids the standard unsupervised learning issues of trivial solutions and
collapsing of features. Thanks to a stochastic batch reassignment strategy and a separable
square loss function, it scales to millions of images. The proposed approach produces
representations that perform on par with state-of-the-art unsupervised methods on
ImageNet and PASCAL VOC.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {517–526},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{realnvp,
  author    = {Laurent Dinh and
               Jascha Sohl{-}Dickstein and
               Samy Bengio},
  title     = {Density estimation using Real {NVP}},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=HkpbnH9lx},
  timestamp = {Thu, 25 Jul 2019 14:25:58 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/DinhSB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{
cinn,
title={Conditional Invertible Neural Networks for Guided Image Generation},
author={Lynton Ardizzone and Carsten L{\"u}th and Jakob Kruse and Carsten Rother and Ullrich K{\"o}the},
year={2020},
url={https://openreview.net/forum?id=SyxC9TEtPH}
}

@inproceedings{glow,
 author = {Kingma, Durk P and Dhariwal, Prafulla},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
 url = {https://proceedings.neurips.cc/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{normflows,
author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
title = {Variational Inference with Normalizing Flows},
year = {2015},
publisher = {JMLR.org},
abstract = {The choice of approximate posterior distribution is one of the core problems in variational
inference. Most applications of variational inference employ simple families of posterior
approximations in order to allow for efficient inference, focusing on mean-field or
other simple structured approximations. This restriction has a significant impact
on the quality of inferences made using variational methods. We introduce a new approach
for specifying flexible, arbitrarily complex and scalable approximate posterior distributions.
Our approximations are distributions constructed through a normalizing flow, whereby
a simple initial density is transformed into a more complex one by applying a sequence
of invertible transformations until a desired level of complexity is attained. We
use this view of normalizing flows to develop categories of finite and infinitesimal
flows and provide a unified view of approaches for constructing rich posterior approximations.
We demonstrate that the theoretical advantages of having posteriors that better match
the true posterior, combined with the scalability of amortized variational approaches,
provides a clear improvement in performance and applicability of variational inference.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1530–1538},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}

@inproceedings{nf_2,
 author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Masked Autoregressive Flow for Density Estimation},
 url = {https://proceedings.neurips.cc/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{vmf_mix_1,
  author    = {Md. Abul Hasnat and
               Julien Bohn{\'{e}} and
               Jonathan Milgram and
               St{\'{e}}phane Gentric and
               Liming Chen},
  title     = {von Mises-Fisher Mixture Model-based Deep learning: Application to
               Face Verification},
  journal   = {CoRR},
  volume    = {abs/1706.04264},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.04264},
  eprinttype = {arXiv},
  eprint    = {1706.04264},
  timestamp = {Mon, 13 Aug 2018 16:47:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HasnatBMGC17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{vmf_mix_2,
  title = 	 {Von Mises-Fisher Clustering Models},
  author = 	 {Gopal, Siddharth and Yang, Yiming},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {154--162},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/gopal14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/gopal14.html},
  abstract = 	 {This paper proposes a suite of models for clustering high-dimensional data on a unit sphere based on Von Mises-Fisher (vMF) distribution and for discovering more intuitive clusters than existing approaches. The proposed models include  a) A Bayesian formulation of vMF mixture that enables information sharing among clusters,  b) a Hierarchical vMF mixture that provides multi-scale shrinkage and tree structured view of the data and c) a Temporal vMF mixture that captures evolution of clusters in temporal data.  For posterior inference, we develop fast variational methods  as well as collapsed Gibbs sampling techniques for all three models. Our experiments on six datasets provide strong empirical support in favour of vMF based clustering models over other popular tools such as K-means, Multinomial Mixtures and Latent Dirichlet Allocation.}
}


@book{dlbook,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@inproceedings{nf_1,
  author    = {Laurent Dinh and
               David Krueger and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {{NICE:} Non-linear Independent Components Estimation},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1410.8516},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/DinhKB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{cinn_video,
    author    = {Dorkenwald, Michael and Milbich, Timo and Blattmann, Andreas and Rombach, Robin and Derpanis, Konstantinos G. and Ommer, Bjorn},
    title     = {Stochastic Image-to-Video Synthesis Using cINNs},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {3742-3753}
}

@InProceedings{samesame,
    author    = {Rudolph, Marco and Wandt, Bastian and Rosenhahn, Bodo},
    title     = {Same Same but DifferNet: Semi-Supervised Defect Detection With Normalizing Flows},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2021},
    pages     = {1907-1916}
}

@inproceedings{cinn_translate,
 author = {Rombach, Robin and Esser, Patrick and Ommer, Bjorn},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {2784--2797},
 publisher = {Curran Associates, Inc.},
 title = {Network-to-Network Translation with Conditional Invertible Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/1cfa81af29c6f2d8cacb44921722e753-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{cinn_imtoim,
  author    = {Lynton Ardizzone and
               Jakob Kruse and
               Carsten L{\"{u}}th and
               Niels Bracher and
               Carsten Rother and
               Ullrich K{\"{o}}the},
  title     = {Conditional Invertible Neural Networks for Diverse Image-to-Image
               Translation},
  journal   = {CoRR},
  volume    = {abs/2105.02104},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.02104},
  eprinttype = {arXiv},
  eprint    = {2105.02104},
  timestamp = {Wed, 12 May 2021 15:54:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-02104.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{dml_inversion,
  title = 	 {Contrastive Learning Inverts the Data Generating Process},
  author =       {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12979--12990},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zimmermann21a/zimmermann21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zimmermann21a.html},
  abstract = 	 {Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.}
}


@InProceedings{circle,
author = {Sun, Yifan and Cheng, Changmao and Zhang, Yuhan and Zhang, Chi and Zheng, Liang and Wang, Zhongdao and Wei, Yichen},
title = {Circle Loss: A Unified Perspective of Pair Similarity Optimization},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}


@InProceedings{rankmi,
author = {Kemertas, Mete and Pishdad, Leila and Derpanis, Konstantinos G. and Fazly, Afsaneh},
title = {RankMI: A Mutual Information Maximizing Ranking Loss},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@InProceedings{drml,
    author    = {Zheng, Wenzhao and Zhang, Borui and Lu, Jiwen and Zhou, Jie},
    title     = {Deep Relational Metric Learning},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {12065-12074}
}

@InProceedings{memvir,
    author    = {Ko, Byungsoo and Gu, Geonmo and Kim, Han-Gyu},
    title     = {Learning With Memory-Based Virtual Classes for Deep Metric Learning},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {11792-11801}
}

@article{fasttext,
  title={Enriching Word Vectors with Subword Information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.04606},
  year={2016}
}

@inproceedings{hier_lit_1,
title = {Integrating Domain Knowledge: Using Hierarchies to Improve Deep Classifiers},
booktitle = {Asian Conference on Pattern Recognition (ACPR)},
author = {Clemens-Alexander Brust and Joachim Denzler},
year = {2019},
}

@INPROCEEDINGS{hier_lit_2,
  author={Deselaers, Thomas and Ferrari, Vittorio},
  booktitle={CVPR 2011}, 
  title={Visual and semantic similarity in ImageNet}, 
  year={2011},
  volume={},
  number={},
  pages={1777-1784},
  doi={10.1109/CVPR.2011.5995474}}

@InProceedings{hier_lit_3,
author = {Bertinetto, Luca and Mueller, Romain and Tertikas, Konstantinos and Samangooei, Sina and Lord, Nicholas A.},
title = {Making Better Mistakes: Leveraging Class Hierarchies With Deep Networks},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@INPROCEEDINGS{hier_lit_4,
  author={Brust, Clemens-Alexander and Barz, Björn and Denzler, Joachim},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Making Every Label Count: Handling Semantic Imprecision by Integrating Domain Knowledge}, 
  year={2021},
  volume={},
  number={},
  pages={6866-6873},
  doi={10.1109/ICPR48806.2021.9413283}}

@article{hier_lit_5,
  title={YOLO9000: Better, Faster, Stronger},
  author={Joseph Redmon and Ali Farhadi},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={6517-6525}
}

@article{hiermatch,
  title={Hierarchy-Based Image Embeddings for Semantic Image Retrieval},
  author={Bj{\"o}rn Barz and Joachim Denzler},
  journal={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year={2019},
  pages={638-647}
}

@Book{wordnet,
    title = {WordNet: An Electronic Lexical Database},
    author = {Christiane Fellbaum},
    year = {1998},
    publisher = {Bradford Books},
  }
  
@inproceedings{glove,
  added-at = {2016-02-18T12:02:38.000+0100},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  biburl = {https://www.bibsonomy.org/bibtex/2a6e77a38c13e374ab250e13ae22993ec/thoni},
  booktitle = {EMNLP},
  interhash = {29813227df1eea94efa14c7df2b5553a},
  intrahash = {a6e77a38c13e374ab250e13ae22993ec},
  keywords = {deeplearning deepwiki glove semantic},
  pages = {1532--1543},
  timestamp = {2016-09-06T08:23:07.000+0200},
  title = {Glove: Global Vectors for Word Representation.},
  volume = 14,
  year = 2014
}



@inproceedings{
radam,
title={On the Variance of the Adaptive Learning Rate and Beyond},
author={Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgz2aEKDr}
}
@inproceedings{
Kitaev2020Reformer,
title={Reformer: The Efficient Transformer},
author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgNKkHtvB}
}

############################## METRIC LEARNING #######################
@article{musgrave2020metric,
  author    = {Kevin Musgrave and
               Serge J. Belongie and
               Ser{-}Nam Lim},
  title     = {A Metric Learning Reality Check},
  journal   = {CoRR},
  volume    = {abs/2003.08505},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.08505},
  archivePrefix = {arXiv},
  eprint    = {2003.08505},
  timestamp = {Tue, 24 Mar 2020 16:42:29 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-08505.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{fehervari2019unbiased,
    title={Unbiased Evaluation of Deep Metric Learning Algorithms},
    author={Istvan Fehervari and Avinash Ravichandran and Srikar Appalaraju},
    year={2019},
    eprint={1911.12528},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{rothgithub,
  author = {Karsten Roth and Biagio Brattoli},
  title = {Deep-Metric-Learning-Baselines},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Confusezius/Deep-Metric-Learning-Baselines}}
}

### ICML2019
@inproceedings{ala,
  title={Addressing the Loss-Metric Mismatch with Adaptive Loss Alignment},
  author={Chen Huang and Shuangfei Zhai and Walter Talbott and Miguel {\'A}ngel Bautista and Shih-Yu Sun and Carlos Guestrin and Josh Susskind},
  booktitle={ICML},
  year={2019}
}

@InProceedings{learn2rank,
author = {Cakir, Fatih and He, Kun and Xia, Xide and Kulis, Brian and Sclaroff, Stan},
title = {Deep Metric Learning to Rank},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@misc{cosface,
    title={CosFace: Large Margin Cosine Loss for Deep Face Recognition},
    author={Hao Wang and Yitong Wang and Zheng Zhou and Xing Ji and Dihong Gong and Jingchao Zhou and Zhifeng Li and Wei Liu},
    year={2018},
    eprint={1801.09414},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

### ICCV2019
@InProceedings{softriple,
author = {Qian, Qi and Shang, Lei and Sun, Baigui and Hu, Juhua and Li, Hao and Jin, Rong},
title = {SoftTriple Loss: Deep Metric Learning Without Triplet Sampling},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}


@inproceedings{mic,
  title={MIC: Mining Interclass Characteristics for Improved Metric Learning},
  author={Roth, Karsten and Brattoli, Biagio and Ommer, Bjorn},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={8000--8009},
  year={2019}
}


@article{alex2016deep,
  author    = {Alexander A. Alemi and
               Ian Fischer and
               Joshua V. Dillon and
               Kevin Murphy},
  title     = {Deep Variational Information Bottleneck},
  journal   = {CoRR},
  volume    = {abs/1612.00410},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.00410},
  archivePrefix = {arXiv},
  eprint    = {1612.00410},
  timestamp = {Mon, 13 Aug 2018 16:46:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AlemiFD016.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{milbich2020diva,
  author    = {Timo Milbich and
               Karsten Roth and
               Homanga Bharadhwaj and
               Samarth Sinha and
               Yoshua Bengio and
               Bj{\"{o}}rn Ommer and
               Joseph Paul Cohen},
  title     = {DiVA: Diverse Visual Feature Aggregation for Deep Metric Learning},
  journal   = {CoRR},
  volume    = {abs/2004.13458},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.13458},
  archivePrefix = {arXiv},
  eprint    = {2004.13458},
  timestamp = {Sat, 02 May 2020 19:17:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-13458.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{milbich2020sharing,
  author={T. {Milbich} and K. {Roth} and B. {Brattoli} and B. {Ommer}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Sharing Matters for Generalization in Deep Metric Learning}, 
  year={2020},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TPAMI.2020.3009620}}
  
@InProceedings{roth2020pads,
author = {Roth, Karsten and Milbich, Timo and Ommer, Bjorn},
title = {PADS: Policy-Adapted Sampling for Visual Similarity Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}


### CVPR2019
@InProceedings{horde,
author = {Jacob, Pierre and Picard, David and Histace, Aymeric and Edouard Klein},
title = {Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2019}
}

@InProceedings{Sanakoyeu_2019_CVPR,
author = {Sanakoyeu, Artsiom and Tschernezki, Vadim and Buchler, Uta and Ommer, Bjorn},
title = {Divide and Conquer the Embedding Space for Metric Learning},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2019}
}

@article{hardness-aware,
  title={Hardness-Aware Deep Metric Learning},
  author={Zheng, Wenzhao and Chen, Zhaodong and Lu, Jiwen and Zhou, Jie},
  journal={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}

@article{signal2noise,
  title={Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning},
  author={Yuan, Tongtong and Deng, Weihong and Tang, Jian and Tang, Yinan and Chen, Binghui},
  journal={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}

@article{rankedlist,
  title={Ranked List Loss for Deep Metric Learning},
  author={Wang, Xinshao and Hua, Yang and Kodirov, Elyor and Hu, Guosheng and Garnier, Romain and Robertson, Neil M.},
  journal={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}

### CLASSICAL
@inproceedings{quadtruplet,
  title={Beyond triplet loss: a deep quadruplet network for person re-identification},
  author={Chen, Weihua and Chen, Xiaotang and Zhang, Jianguo and Huang, Kaiqi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2017}
}

@inproceedings{abe,
    title = {Attention-Based Ensemble for Deep Metric Learning}, 
    author = {Kim, Wonsik and Goyal, Bhavya and Chawla, Kunal and Lee, Jungmin and Kwon, Keunjoo},
    booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
    year = {2018},
}

@inproceedings{yu2018correcting,
  title={Correcting the Triplet Selection Bias for Triplet Loss},
  author={Yu, Baosheng and Liu, Tongliang and Gong, Mingming and Ding, Changxing and Tao, Dacheng},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={71--87},
  year={2018}
}

@inproceedings{lifted,
  title={Deep metric learning via lifted structured feature embedding},
  author={Oh Song, Hyun and Xiang, Yu and Jegelka, Stefanie and Savarese, Silvio},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4004--4012},
  year={2016}
}

@inproceedings{facilitylocation,
  title={Deep metric learning via facility location},
  author={Oh Song, Hyun and Jegelka, Stefanie and Rathod, Vivek and Murphy, Kevin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5382--5390},
  year={2017}
}

@inproceedings{smartmining,
  title={Smart mining for deep metric learning},
  author={Harwood, Ben and Kumar, BG and Carneiro, Gustavo and Reid, Ian and Drummond, Tom and others},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2821--2829},
  year={2017}
}

@InProceedings{Parkhi15,
  author       = "Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew",
  title        = "Deep Face Recognition",
  booktitle    = "British Machine Vision Conference",
  year         = "2015",
}

@inproceedings{npairs,
  title={Improved deep metric learning with multi-class n-pair loss objective},
  author={Sohn, Kihyuk},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1857--1865},
  year={2016}
}

@inproceedings{stochastic,
  title={Stochastic variational deep kernel learning},
  author={Wilson, Andrew G and Hu, Zhiting and Salakhutdinov, Ruslan R and Xing, Eric P},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2586--2594},
  year={2016}
}

@InProceedings{dvml,
author = {Lin, Xudong and Duan, Yueqi and Dong, Qiyuan and Lu, Jiwen and Zhou, Jie},
title = {Deep Variational Metric Learning},
booktitle = {The European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@inproceedings{daml,
author = {Duan, Yueqi and Zheng, Wenzhao and Lin, Xudong and Lu, Jiwen and Zhou, Jie},
title = {Deep Adversarial Metric Learning},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@inproceedings{histogram,
  title={Learning deep embeddings with histogram loss},
  author={Ustinova, Evgeniya and Lempitsky, Victor},
  booktitle={Advances in Neural Information Processing Systems},
  year={2016}
}

@inproceedings{angular,
  title={Deep metric learning with angular loss},
  author={Wang, Jian and Zhou, Feng and Wen, Shilei and Liu, Xiao and Lin, Yuanqing},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2593--2601},
  year={2017}
}

@article{zhai2018classification,
  author    = {Andrew Zhai and
               Hao{-}Yu Wu},
  title     = {Making Classification Competitive for Deep Metric Learning},
  journal   = {CoRR},
  volume    = {abs/1811.12649},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.12649},
  archivePrefix = {arXiv},
  eprint    = {1811.12649},
  timestamp = {Mon, 03 Dec 2018 07:50:28 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-12649.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{diversifieddml,
      title={Diversified Mutual Learning for Deep Metric Learning}, 
      author={Wonpyo Park and Wonjae Kim and Kihyun You and Minsu Cho},
      year={2020},
      eprint={2009.04170},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{mutual_learning,
author = {Zhang, Ying and Xiang, Tao and Hospedales, Timothy M. and Lu, Huchuan},
title = {Deep Mutual Learning},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@article{selfreference,
  author    = {Xu Lan and
               Xiatian Zhu and
               Shaogang Gong},
  title     = {Self-Referenced Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1811.07598},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.07598},
  archivePrefix = {arXiv},
  eprint    = {1811.07598},
  timestamp = {Sun, 25 Nov 2018 18:57:12 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-07598.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{beyourownteacher,
author = {Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
title = {Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@article{MetaDistiller,
  author    = {Benlin Liu and
               Yongming Rao and
               Jiwen Lu and
               Jie Zhou and
               Cho{-}jui Hsieh},
  title     = {MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation},
  journal   = {CoRR},
  volume    = {abs/2008.12094},
  year      = {2020},
  url       = {https://arxiv.org/abs/2008.12094},
  archivePrefix = {arXiv},
  eprint    = {2008.12094},
  timestamp = {Tue, 15 Sep 2020 20:52:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2008-12094.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{htl,
  title={Deep metric learning with hierarchical triplet loss},
  author={Ge, Weifeng},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={269--285},
  year={2018}
}

@inproceedings{htg,
  title={An Adversarial Approach to Hard Triplet Generation},
  author={Zhao, Yiru and Jin, Zhongming and Qi, Guo-jun and Lu, Hongtao and Hua, Xian-sheng},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={501--517},
  year={2018}
}

@inproceedings{proxynca,
  title={No fuss distance metric learning using proxies},
  author={Movshovitz-Attias, Yair and Toshev, Alexander and Leung, Thomas K and Ioffe, Sergey and Singh, Saurabh},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={360--368},
  year={2017}
}

@inproceedings{proxyncapp,
  author    = {Eu Wern Teh and
               Terrance DeVries and
               Graham W. Taylor},
  editor    = {Andrea Vedaldi and
               Horst Bischof and
               Thomas Brox and
               Jan{-}Michael Frahm},
  title     = {ProxyNCA++: Revisiting and Revitalizing Proxy Neighborhood Component
               Analysis},
  booktitle = {Computer Vision - {ECCV} 2020 - 16th European Conference, Glasgow,
               UK, August 23-28, 2020, Proceedings, Part {XXIV}},
  series    = {Lecture Notes in Computer Science},
  volume    = {12369},
  pages     = {448--464},
  publisher = {Springer},
  year      = {2020},
  url       = {https://doi.org/10.1007/978-3-030-58586-0\_27},
  doi       = {10.1007/978-3-030-58586-0\_27},
  timestamp = {Mon, 30 Nov 2020 17:54:05 +0100},
  biburl    = {https://dblp.org/rec/conf/eccv/TehDT20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{margin,
  title={Sampling matters in deep embedding learning},
  author={Wu, Chao-Yuan and Manmatha, R and Smola, Alexander J and Krahenbuhl, Philipp},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2840--2848},
  year={2017}
}

@ARTICLE{ann_1,
  author={Malkov, Yu A. and Yashunin, D. A.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs}, 
  year={2020},
  volume={42},
  number={4},
  pages={824-836},
  doi={10.1109/TPAMI.2018.2889473}}

@InProceedings{ann_2,
author = {Baranchuk, Dmitry and Babenko, Artem and Malkov, Yury},
title = {Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@inproceedings{dreml,
  title={Deep Randomized Ensembles for Metric Learning},
  author={Xuan, Hong and Souvenir, Richard and Pless, Robert},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={723--734},
  year={2018}
}

@inproceedings{miningmanifold,
  title={Mining on manifolds: Metric learning without labels},
  author={Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\v{r}}ej},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7642--7651},
  year={2018}
}

@inproceedings{chopra2005learning,
  title={Learning a similarity metric discriminatively, with application to face verification},
  author={Chopra, Sumit and Hadsell, Raia and LeCun, Yann},
  pages={539--546},
  year={2005},
  organization={IEEE}
}

@inproceedings{shi2016embedding,
  title={Embedding deep metric for person re-identification: A study against large variations},
  author={Shi, Hailin and Yang, Yang and Zhu, Xiangyu and Liao, Shengcai and Lei, Zhen and Zheng, Weishi and Li, Stan Z},
  booktitle={European conference on computer vision},
  year={2016},
  organization={Springer}
}

@INPROCEEDINGS{face_verfication_inthewild, 
author={J. {Hu} and J. {Lu} and Y. {Tan}}, 
booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Discriminative Deep Metric Learning for Face Verification in the Wild}, 
year={2014}
}

@article{sphereface,
  title={SphereFace: Deep Hypersphere Embedding for Face Recognition},
  author={Weiyang Liu and Yandong Wen and Zhiding Yu and Ming Li and Bhiksha Raj and Le Song},
  journal={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017}
}

@inproceedings{contrastive,
 author = {Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
 title = {Dimensionality Reduction by Learning an Invariant Mapping},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 year = {2006}
}

############################# Unsupervised/Feature Learning ##########

@article{deepcluster,
  author    = {Mathilde Caron and
               Piotr Bojanowski and
               Armand Joulin and
               Matthijs Douze},
  title     = {Deep Clustering for Unsupervised Learning of Visual Features},
  journal   = {European Conference on Computer Vision},
  year      = {2018}
}

@inproceedings{cliquecnn,
  title={Cliquecnn: Deep unsupervised exemplar learning},
  author={Bautista, Miguel A and Sanakoyeu, Artsiom and Tikhoncheva, Ekaterina and Ommer, Bjorn},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3846--3854},
  year={2016}
}

@inproceedings{jigsaw,
  title={Unsupervised learning of visual representations by solving jigsaw puzzles},
  author={Noroozi, Mehdi and Favaro, Paolo},
  booktitle={European Conference on Computer Vision},
  pages={69--84},
  year={2016},
  organization={Springer}
}

@inproceedings{jigsaw++,
  title={Boosting self-supervised learning via knowledge transfer},
  author={Noroozi, Mehdi and Vinjimoor, Ananth and Favaro, Paolo and Pirsiavash, Hamed},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9359--9367},
  year={2018}
}

@inproceedings{Feng_2019_CVPR,
    title = {Self-Supervised Representation Learning by Rotation Feature Decoupling},
    author = {Feng, Zeyu and Xu, Chang and Tao, Dacheng},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2019}
}

@inproceedings{buechler_ECCV_2018,
	title = {Improving Spatiotemporal Self-Supervision by Deep Reinforcement Learning},
	author = {B{\"u}chler, U. and Brattoli, B. and Bj{\"o}rn Ommer},
	booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
	year = {2018}
}

@inproceedings{milbich2017unsupervised,
  title={Unsupervised video understanding by reconciliation of posture similarities},
  author={Milbich, Timo and Bautista, Miguel and Sutter, Ekaterina and Ommer, Bj{\"o}rn},
  booktitle={Proc. ICCV},
  year={2017}
}


############################## Classification ##################

@article{Zhe2018DirectionalSD,
  title={Directional statistics-based deep metric learning for image classification and retrieval},
  author={Xuefei Zhe and Shifeng Chen and Hong Yan},
  journal={Pattern Recognition},
  year={2018},
  volume={93}
}


############################## ENSEMBLES #######################
@inproceedings{hdc,
  title={Hard-aware deeply cascaded embedding},
  author={Yuan, Yuhui and Yang, Kuiyuan and Zhang, Chao},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={814--823},
  year={2017}
}


@inproceedings{bier,
  title={Bier-boosting independent embeddings robustly},
  author={Opitz, Michael and Waltner, Georg and Possegger, Horst and Bischof, Horst},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={5189--5198},
  year={2017}
}

@article{abier,
  title={Deep metric learning with BIER: Boosting independent embeddings robustly},
  author={Opitz, Michael and Waltner, Georg and Possegger, Horst and Bischof, Horst},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2018},
  publisher={IEEE}
}

@article{freund1997decision,
  title={A decision-theoretic generalization of on-line learning and an application to boosting},
  author={Freund, Yoav and Schapire, Robert E},
  journal={Journal of computer and system sciences},
  volume={55},
  number={1},
  pages={119--139},
  year={1997},
  publisher={Elsevier}
}

############################## DATASETS #######################
@inproceedings{cars196,
  title={3d object representations for fine-grained categorization},
  author={Krause, Jonathan and Stark, Michael and Deng, Jia and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={554--561},
  year={2013}
}


@techreport{cub200-2011,
	Title = {The Caltech-UCSD Birds-200-2011 Dataset},
	Author = {Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie, S.},
	Year = {2011},
	Institution = {California Institute of Technology},
	Number = {CNS-TR-2011-001}
}


@inproceedings{inshop,
 author = {Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang},
 title = {DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations},
 booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2016} 
 }
 
 
@inproceedings{pku,
 author = {H. Liu, Y. Tian, Y. Wang, L. Pang, and T. Huang},
 title = {Deep relative distance learning: Tell the difference between similar vehicles},
 booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2016},
 pages = {2167-2175}
 }
 
@article{nmi,
  title={Introduction to information retrieval},
  author={Manning, Christopher and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
  journal={Natural Language Engineering},
  volume={16},
  number={1},
  pages={100--103},
  year={2010},
  publisher={Cambridge university press}
}

@article{recall,
  title={Product quantization for nearest neighbor search},
  author={Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={1},
  pages={117--128},
  year={2011},
  publisher={IEEE}
}

@inproceedings{semihard,
  title={Facenet: A unified embedding for face recognition and clustering},
  author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={815--823},
  year={2015}
}

############################## OTHERS #######################
@article{googlenetv2,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={International Conference on Machine Learning},
  year={2015}
}

@inproceedings{googlenet,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{pytorch,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle={NIPS-W},
  year={2017}
}

@inproceedings{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{umap,
  title={UMAP: Uniform Manifold Approximation and Projection},
  author={McInnes, Leland and Healy, John and Saul, Nathaniel and Grossberger, Lukas},
  journal={The Journal of Open Source Software},
  volume={3},
  number={29},
  pages={861},
  year={2018}
}

@article{tsne,
  title={Visualizing data using t-SNE},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}

@article{kldiv2mle,
  title={Notes on Kullback-Leibler Divergence and Likelihood},
  author={Jonathon Shlens},
  journal={CoRR},
  year={2014},
  volume={abs/1404.2000}
}

@article{gradreverse,
 author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran\c{c}ois and Marchand, Mario and Lempitsky, Victor},
 title = {Domain-adversarial Training of Neural Networks},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2016},
 volume = {17},
 number = {1},
 month = jan,
 year = {2016},
 issn = {1532-4435},
 pages = {2096--2030},
 numpages = {-65},
 url = {http://dl.acm.org/citation.cfm?id=2946645.2946704},
 acmid = {2946704},
 publisher = {JMLR.org},
 keywords = {deep learning, domain adaptation, image classification, neural network, person re-identification, representation learning, sentiment analysis, synthetic data},
} 

@inproceedings{GlorotB10,
  author = {Glorot, Xavier and Bengio, Yoshua},
  series = {JMLR Proceedings},
  title = {Understanding the difficulty of training deep feedforward neural networks.},
  year = {2010}
}

@InProceedings{grouping,
author = {Bouchacourt, Diane and Tomioka, Ryota and Nowozin, Sebastian},
title = {Multi-Level Variational Autoencoder: Learning Disentangled Representations from Grouped Observations},
booktitle = {AAAI 2018},
year = {2018}
}

@inproceedings{vae,
  author = {Kingma, Diederik P and Welling, Max},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  title = {Auto-encoding variational bayes},
  year = 2013
}

@InProceedings{nat,
  title = 	 {Unsupervised Learning by Predicting Noise},
  author = 	 {Piotr Bojanowski and Armand Joulin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  year = 	 {2017}
}

@inproceedings{inceptionv1,
title	= {Going Deeper with Convolutions},
author	= {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
year	= {2015},
booktitle	= {Computer Vision and Pattern Recognition (CVPR)}
}

@inproceedings{batchnorm,
title	= {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
author	= {Sergey Ioffe and Christian Szegedy},
year	= {2015},
URL	= {http://jmlr.org/proceedings/papers/v37/ioffe15.pdf},
pages	= {448-456}
}


@misc{magnet,
    title={Metric Learning with Adaptive Density Discrimination},
    author={Oren Rippel and Manohar Paluri and Piotr Dollar and Lubomir Bourdev},
    year={2015},
    eprint={1511.05939},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}



###### Memory Bank #######
@inproceedings{MetaAug,
  title={Meta-Learning with Memory-Augmented Neural Networks},
  author={Adam Santoro and Sergey Bartunov and Matthew M Botvinick and Daan Wierstra and Timothy P. Lillicrap},
  booktitle={ICML},
  year={2016}
}


@misc{OneshotAug,
    title={One-shot Learning with Memory-Augmented Neural Networks},
    author={Adam Santoro and Sergey Bartunov and Matthew Botvinick and Daan Wierstra and Timothy Lillicrap},
    year={2016},
    eprint={1605.06065},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{assoBank,
    title={Unsupervised Learning using Pretrained CNN and Associative Memory Bank},
    author={Qun Liu and Supratik Mukhopadhyay},
    year={2018},
    eprint={1805.01033},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{parDiscr,
    title={Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination},
    author={Zhirong Wu and Yuanjun Xiong and Stella Yu and Dahua Lin},
    year={2018},
    eprint={1805.01978},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@InProceedings{moco,
author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
title = {Momentum Contrast for Unsupervised Visual Representation Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@misc{rainbow,
    title={Rainbow: Combining Improvements in Deep Reinforcement Learning},
    author={Matteo Hessel and Joseph Modayil and Hado van Hasselt and Tom Schaul and Georg Ostrovski and Will Dabney and Dan Horgan and Bilal Piot and Mohammad Azar and David Silver},
    year={2017},
    eprint={1710.02298},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@misc{knowledgedistill,
    title={Distilling the Knowledge in a Neural Network},
    author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
    year={2015},
    eprint={1503.02531},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}


@misc{fishergan,
    title={Fisher GAN},
    author={Youssef Mroueh and Tom Sercu},
    year={2017},
    eprint={1705.09675},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{savareseactive,
    title={Active Learning for Convolutional Neural Networks: A Core-Set Approach},
    author={Ozan Sener and Silvio Savarese},
    year={2017},
    eprint={1708.00489},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}


@inproceedings{wei_markov,
    title = "Using Document Summarization Techniques for Speech Data Subset Selection",
    author = "Wei, Kai  and
      Liu, Yuzong  and
      Kirchhoff, Katrin  and
      Bilmes, Jeff",
    booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N13-1086",
    pages = "721--726",
}

@inproceedings{pretextmisra,
  author    = {Ishan Misra and
               Laurens van der Maaten},
  title     = {Self-Supervised Learning of Pretext-Invariant Representations},
  booktitle = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2020, Seattle, WA, USA, June 13-19, 2020},
  pages     = {6706--6716},
  publisher = {{IEEE}},
  year      = {2020},
  url       = {https://doi.org/10.1109/CVPR42600.2020.00674},
  doi       = {10.1109/CVPR42600.2020.00674},
  timestamp = {Tue, 11 Aug 2020 16:59:49 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/MisraM20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{amazondml,
    title={Unbiased Evaluation of Deep Metric Learning Algorithms},
    author={Istvan Fehervari and Avinash Ravichandran and Srikar Appalaraju},
    year={2019},
    eprint={1911.12528},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{smith2017don,
  title={Don't decay the learning rate, increase the batch size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  journal={arXiv preprint arXiv:1711.00489},
  year={2017}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{sinha2019small,
  title={Small-GAN: Speeding Up GAN Training Using Core-sets},
  author={Sinha, Samarth and Zhang, Han and Goyal, Anirudh and Bengio, Yoshua and Larochelle, Hugo and Odena, Augustus},
  journal={arXiv preprint arXiv:1910.13540},
  year={2019}
}

@article{coresets,
  title={Geometric approximation via coresets},
  author={Agarwal, Pankaj K and Har-Peled, Sariel and Varadarajan, Kasturi R},
  journal={Combinatorial and computational geometry},
  volume={52},
  pages={1--30},
  year={2005},
  publisher={Cambridge University Press, New York}
}

@inproceedings{bouthillier2019unreproducible,
  title={Unreproducible Research is Reproducible},
  author={Bouthillier, Xavier and Laurent, C{\'e}sar and Vincent, Pascal},
  booktitle={International Conference on Machine Learning},
  pages={725--734},
  year={2019}
}

@article{kaya,
author = {Kaya, Mahmut and Bilge, H.s},
year = {2019},
month = {08},
pages = {1066},
title = {Deep Metric Learning: A Survey},
volume = {11},
journal = {Symmetry},
doi = {10.3390/sym11091066}
}

@misc{d2coreset,
    title={Practical Coreset Constructions for Machine Learning},
    author={Olivier Bachem and Mario Lucic and Andreas Krause},
    year={2017},
    eprint={1703.06476},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{emd,
author = {Rubner, Yossi and Tomasi, Carlo and Guibas, Leonidas J.},
title = {The Earth Mover’s Distance as a Metric for Image Retrieval},
year = {2000},
issue_date = {November 2000},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {40},
number = {2},
issn = {0920-5691},
url = {https://doi.org/10.1023/A:1026543900054},
doi = {10.1023/A:1026543900054},
journal = {Int. J. Comput. Vision},
month = nov,
pages = {99–121},
numpages = {23},
keywords = {color, texture, image retrieval, perceptual metrics, Earth Mover’s Distance}
}


@misc{fid,
    title={GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
    author={Martin Heusel and Hubert Ramsauer and Thomas Unterthiner and Bernhard Nessler and Sepp Hochreiter},
    year={2017},
    eprint={1706.08500},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@INPROCEEDINGS{arcface,
  author={J. {Deng} and J. {Guo} and N. {Xue} and S. {Zafeiriou}},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={ArcFace: Additive Angular Margin Loss for Deep Face Recognition}, 
  year={2019},
  volume={},
  number={},
  pages={4685-4694},
  doi={10.1109/CVPR.2019.00482}}
  

@misc{manifoldmixup,
    title={Manifold Mixup: Better Representations by Interpolating Hidden States},
    author={Vikas Verma and Alex Lamb and Christopher Beckham and Amir Najafi and Ioannis Mitliagkas and Aaron Courville and David Lopez-Paz and Yoshua Bengio},
    year={2018},
    eprint={1806.05236},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{ben-david,
author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
title = {A Theory of Learning from Different Domains},
year = {2010},
volume = {79},
journal = {Mach. Learn.}
}

@InProceedings{multisimilarity,
author = {Wang, Xun and Han, Xintong and Huang, Weilin and Dong, Dengke and Scott, Matthew R.},
title = {Multi-Similarity Loss With General Pair Weighting for Deep Metric Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{BigGAN,
  author    = {Andrew Brock and
               Jeff Donahue and
               Karen Simonyan},
  title     = {Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
  journal   = {CoRR},
  volume    = {abs/1809.11096},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.11096},
  archivePrefix = {arXiv},
  eprint    = {1809.11096},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1809-11096},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{
coresetoptim,
title={Coresets for Accelerating Incremental Gradient Methods},
author={Baharan Mirzasoleiman and Jeff Bilmes and Jure Leskovec},
year={2020},
url={https://openreview.net/forum?id=SygRikHtvS}
}


@inproceedings{imagenet,
        author = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        title = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
        year = {2009}
}

@incollection{weight_decay,
    title = {A Simple Weight Decay Can Improve Generalization},
    author = {Anders Krogh and John A. Hertz},
    booktitle = {Advances in Neural Information Processing Systems},
    year = {1992},
}



@misc{tishby2015deep,
    title={Deep Learning and the Information Bottleneck Principle},
    author={Naftali Tishby and Noga Zaslavsky},
    year={2015},
    eprint={1503.02406},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@misc{shwartzziv2017opening,
    title={Opening the Black Box of Deep Neural Networks via Information},
    author={Ravid Shwartz-Ziv and Naftali Tishby},
    year={2017},
    eprint={1703.00810},
    archivePrefix={arXiv},
    primaryClass={cs.LG}}


@misc{belghazi2018mutual,
    title={MINE: Mutual Information Neural Estimation},
    author={Mohamed Ishmael Belghazi and Aristide Baratin and Sai Rajeswar and Sherjil Ozair and Yoshua Bengio and Aaron Courville and R Devon Hjelm},
    year={2018},
    eprint={1801.04062},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{dai2019diagnosing,
  author    = {Bin Dai and
               David P. Wipf},
  title     = {Diagnosing and Enhancing {VAE} Models},
  journal   = {CoRR},
  volume    = {abs/1903.05789},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.05789},
  archivePrefix = {arXiv},
  eprint    = {1903.05789},
  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-05789.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{goyal2019infobot,
    title={InfoBot: Transfer and Exploration via the Information Bottleneck},
    author={Anirudh Goyal and Riashat Islam and Daniel Strouse and Zafarali Ahmed and Matthew Botvinick and Hugo Larochelle and Yoshua Bengio and Sergey Levine},
    year={2019},
    eprint={1901.10902},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}


@misc{alex2016information,
    title={Information Dropout: Learning Optimal Representations Through Noisy Computation},
    author={Alessandro Achille and Stefano Soatto},
    year={2016},
    eprint={1611.01353},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}



@article{Bellet_2015,
   title={Robustness and generalization for metric learning},
   volume={151},
   ISSN={0925-2312},
   url={http://dx.doi.org/10.1016/j.neucom.2014.09.044},
   DOI={10.1016/j.neucom.2014.09.044},
   journal={Neurocomputing},
   publisher={Elsevier BV},
   author={Bellet, Aurélien and Habrard, Amaury},
   year={2015},
   month={Mar},
   pages={259–267}
}


@inproceedings{fcn_gen,
  title     = {Deep Metric Learning: The Generalization Analysis and an Adaptive Algorithm},
  author    = {Huai, Mengdi and Xue, Hongfei and Miao, Chenglin and Yao, Liuyi and Su, Lu and Chen, Changyou and Zhang, Aidong},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {2535--2541},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/352},
  url       = {https://doi.org/10.24963/ijcai.2019/352},
}



@InProceedings{Zantedeschi_2016_CVPR,
author = {Zantedeschi, Valentina and Emonet, Remi and Sebban, Marc},
title = {Metric Learning as Convex Combinations of Local Models With Generalization Guarantees},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@misc{bellet2013supervised,
    title={Supervised Metric Learning with Generalization Guarantees},
    author={Aurélien Bellet},
    year={2013},
    eprint={1307.4514},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}



@inproceedings{
generalisation_measures,
title={Fantastic Generalization Measures and Where to Find Them},
author={Yiding Jiang* and Behnam Neyshabur* and Dilip Krishnan and Hossein Mobahi and Samy Bengio},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJgIPJBFvH}
}


@incollection{importance_sampling,
title = {Training Deep Models Faster with Robust, Approximate Importance Sampling},
author = {Johnson, Tyler B and Guestrin, Carlos},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {7265--7275},
year = {2018},
publisher = {Curran Associates, Inc.}
}


@InProceedings{roth2020revisiting, title = {Revisiting Training Strategies and Generalization Performance in Deep Metric Learning}, author = {Roth, Karsten and Milbich, Timo and Sinha, Samarth and Gupta, Prateek and Ommer, Bjorn and Cohen, Joseph Paul}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {8242--8252}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/roth20a/roth20a.pdf}, url = { http://proceedings.mlr.press/v119/roth20a.html }, abstract = {Deep Metric Learning (DML) is arguably one of the most influential lines of research for learning visual similarities with many proposed approaches every year. Although the field benefits from the rapid progress, the divergence in training protocols, architectures, and parameter choices make an unbiased comparison difficult. To provide a consistent reference point, we revisit the most widely used DML objective functions and conduct a study of the crucial parameter choices as well as the commonly neglected mini-batch sampling process. Under consistent comparison, DML objectives show much higher saturation than indicated by literature. Further based on our analysis, we uncover a correlation between the embedding space density and compression to the generalization performance of DML models. Exploiting these insights, we propose a simple, yet effective, training regularization to reliably boost the performance of ranking-based DML models on various standard benchmark datasets. Code and a publicly accessible WandB-repo are available at https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch.} }


@misc{genlifted,
    title={In Defense of the Triplet Loss for Person Re-Identification},
    author={Alexander Hermans and Lucas Beyer and Bastian Leibe},
    year={2017},
    eprint={1703.07737},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}


@InProceedings{kim2020proxy,
author = {Kim, Sungyeon and Kim, Dongwon and Cho, Minsu and Kwak, Suha},
title = {Proxy Anchor Loss for Deep Metric Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}


@article{faiss,
  title={Billion-scale similarity search with GPUs},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:1702.08734},
  year={2017}
}


@article{kmeans,
  title={Least squares quantization in PCM},
  author={Stuart P. Lloyd},
  journal={IEEE Trans. Information Theory},
  year={1982},
  volume={28},
  pages={129-136}
}


############ Few-Shot Learning #############

% Few-Shot
@misc{snell2017prototypical,
    title={Prototypical Networks for Few-shot Learning},
    author={Jake Snell and Kevin Swersky and Richard S. Zemel},
    year={2017},
    eprint={1703.05175},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{sung2017learning,
    title={Learning to Compare: Relation Network for Few-Shot Learning},
    author={Flood Sung and Yongxin Yang and Li Zhang and Tao Xiang and Philip H. S. Torr and Timothy M. Hospedales},
    year={2017},
    eprint={1711.06025},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{vinyals2016matching,
    title={Matching Networks for One Shot Learning},
    author={Oriol Vinyals and Charles Blundell and Timothy Lillicrap and Koray Kavukcuoglu and Daan Wierstra},
    year={2016},
    eprint={1606.04080},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{Ravi2017OptimizationAA,
  title={Optimization as a Model for Few-Shot Learning},
  author={Sachin Ravi and Hugo Larochelle},
  booktitle={ICLR},
  year={2017}
}


@misc{finn2017modelagnostic,
    title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
    author={Chelsea Finn and Pieter Abbeel and Sergey Levine},
    year={2017},
    eprint={1703.03400},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{raghu2019rapid,
  author    = {Aniruddh Raghu and
               Maithra Raghu and
               Samy Bengio and
               Oriol Vinyals},
  title     = {Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness
               of {MAML}},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=rkgMkCEtPB},
  timestamp = {Thu, 07 May 2020 17:11:47 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/RaghuRBV20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{gidaris2018dynamic,
    title={Dynamic Few-Shot Visual Learning without Forgetting},
    author={Spyros Gidaris and Nikos Komodakis},
    year={2018},
    eprint={1804.09458},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{hariharan2016lowshot,
    title={Low-shot Visual Recognition by Shrinking and Hallucinating Features},
    author={Bharath Hariharan and Ross Girshick},
    year={2016},
    eprint={1606.02819},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{wang2018lowshot,
    title={Low-Shot Learning from Imaginary Data},
    author={Yu-Xiong Wang and Ross Girshick and Martial Hebert and Bharath Hariharan},
    year={2018},
    eprint={1801.05401},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{tian2020rethinking,
  title={Rethinking few-shot image classification: a good embedding is all you need?},
  author={Tian, Yonglong and Wang, Yue and Krishnan, Dilip and Tenenbaum, Joshua B and Isola, Phillip},
  journal={arXiv preprint arXiv:2003.11539},
  year={2020}
}

############ Knowledge Distillation ############# 

@inproceedings{bucilu2006model,
  title={Model compression},
  author={Buciluǎ, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={535--541},
  year={2006}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{chen2020simple,
title	= {A Simple Framework for Contrastive Learning of Visual Representations},
author	= {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Everest Hinton},
year	= {2020},
URL	= {https://arxiv.org/abs/2002.05709}
}


@article{tian2019contrastive,
  author    = {Yonglong Tian and
               Dilip Krishnan and
               Phillip Isola},
  title     = {Contrastive Representation Distillation},
  journal   = {CoRR},
  volume    = {abs/1910.10699},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.10699},
  archivePrefix = {arXiv},
  eprint    = {1910.10699},
  timestamp = {Fri, 25 Oct 2019 14:59:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-10699.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{romero2014fitnets,
  author    = {Adriana Romero and
               Nicolas Ballas and
               Samira Ebrahimi Kahou and
               Antoine Chassang and
               Carlo Gatta and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {FitNets: Hints for Thin Deep Nets},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6550},
  timestamp = {Thu, 25 Jul 2019 14:25:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RomeroBKCGB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zagoruyko2016paying,
  author    = {Sergey Zagoruyko and
               Nikos Komodakis},
  title     = {Paying More Attention to Attention: Improving the Performance of Convolutional
               Neural Networks via Attention Transfer},
  journal   = {CoRR},
  volume    = {abs/1612.03928},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.03928},
  archivePrefix = {arXiv},
  eprint    = {1612.03928},
  timestamp = {Mon, 13 Aug 2018 16:47:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZagoruykoK16a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Park_2019,
   title={Relational Knowledge Distillation},
   ISBN={9781728132938},
   url={http://dx.doi.org/10.1109/CVPR.2019.00409},
   DOI={10.1109/cvpr.2019.00409},
   journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
   year={2019},
   month={Jun}
}

@misc{huang2017like,
    title={Like What You Like: Knowledge Distill via Neuron Selectivity Transfer},
    author={Zehao Huang and Naiyan Wang},
    year={2017},
    eprint={1707.01219},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{Heo_2019,
   title={Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons},
   volume={33},
   ISSN={2159-5399},
   url={http://dx.doi.org/10.1609/aaai.v33i01.33013779},
   DOI={10.1609/aaai.v33i01.33013779},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
   author={Heo, Byeongho and Lee, Minsik and Yun, Sangdoo and Choi, Jin Young},
   year={2019},
   month={Jul},
   pages={3779–3787}
}

@article{Ahn_2019,
   title={Variational Information Distillation for Knowledge Transfer},
   ISBN={9781728132938},
   url={http://dx.doi.org/10.1109/CVPR.2019.00938},
   DOI={10.1109/cvpr.2019.00938},
   journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Ahn, Sungsoo and Hu, Shell Xu and Damianou, Andreas and Lawrence, Neil D. and Dai, Zhenwen},
   year={2019},
   month={Jun}
}


@article{surez2018tutorial,
  author    = {Juan{-}Luis Su{\'{a}}rez and
               Salvador Garc{\'{\i}}a and
               Francisco Herrera},
  title     = {A Tutorial on Distance Metric Learning: Mathematical Foundations,
               Algorithms and Software},
  journal   = {CoRR},
  volume    = {abs/1812.05944},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.05944},
  archivePrefix = {arXiv},
  eprint    = {1812.05944},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-05944.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@misc{hypersphere,
    title={Hypersphere},
    author={Eric W. Weisstein},
    year={2002},
    journal={MathWorld--A Wolfram Web Resource},    
    url = https://mathworld.wolfram.com/Hypersphere.html,
}

@article{wang2020understanding,
  title={Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere},
  author={Wang, Tongzhou and Isola, Phillip},
  journal={arXiv preprint arXiv:2005.10242},
  year={2020}
}


@misc{khosla2020supervised,
    title={Supervised Contrastive Learning},
    author={Prannay Khosla and Piotr Teterwak and Chen Wang and Aaron Sarna and Yonglong Tian and Phillip Isola and Aaron Maschinot and Ce Liu and Dilip Krishnan},
    year={2020},
    eprint={2004.11362},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{rajasegaran2020selfsupervised,
  author    = {Jathushan Rajasegaran and
               Salman Khan and
               Munawar Hayat and
               Fahad Shahbaz Khan and
               Mubarak Shah},
  title     = {Self-supervised Knowledge Distillation for Few-shot Learning},
  journal   = {CoRR},
  volume    = {abs/2006.09785},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.09785},
  archivePrefix = {arXiv},
  eprint    = {2006.09785},
  timestamp = {Tue, 23 Jun 2020 17:57:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-09785.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{chen2020new,
    title={A New Meta-Baseline for Few-Shot Learning},
    author={Yinbo Chen and Xiaolong Wang and Zhuang Liu and Huijuan Xu and Trevor Darrell},
    year={2020},
    eprint={2003.04390},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{furlanello2018born,
  author    = {Tommaso Furlanello and
               Zachary Chase Lipton and
               Michael Tschannen and
               Laurent Itti and
               Anima Anandkumar},
  editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {Born-Again Neural Networks},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {1602--1611},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/furlanello18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/FurlanelloLTIA18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@incollection{metric_1,
title = {Metric Learning by Collapsing Classes},
author = {Globerson, Amir and Sam T. Roweis},
booktitle = {Advances in Neural Information Processing Systems 18},
editor = {Y. Weiss and B. Sch\"{o}lkopf and J. C. Platt},
pages = {451--458},
year = {2006},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2947-metric-learning-by-collapsing-classes.pdf}
}

@incollection{metric_2,
title = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},
author = {Weinberger, Kilian Q and John Blitzer and Lawrence K. Saul},
booktitle = {Advances in Neural Information Processing Systems 18},
editor = {Y. Weiss and B. Sch\"{o}lkopf and J. C. Platt},
pages = {1473--1480},
year = {2006},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf}
}

@article{metric_3,
title = "Supervised distance metric learning through maximization of the Jeffrey divergence",
journal = "Pattern Recognition",
volume = "64",
pages = "215 - 225",
year = "2017",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2016.11.010",
url = "http://www.sciencedirect.com/science/article/pii/S0031320316303600",
author = "Bac Nguyen and Carlos Morell and Bernard {De Baets}",
keywords = "Distance metric learning, Nearest neighbor, Linear transformation, Jeffrey divergence"
}

@article{metric_4,
  author       = {Nguyen Cong, Bac and Morell, Carlos and De Baets, Bernard},
  issn         = {0950-7051},
  journal      = {KNOWLEDGE-BASED SYSTEMS},
  keywords     = {Ordinal classification,Ordinal regression,Distance metric learning,Nearest neighbor,Semidefinite programming,REJECTIVE MULTIPLE TEST,NEAREST-NEIGHBOR,RELATIVE COMPARISONS,AGE ESTIMATION,REGRESSION,OPTIMIZATION,MINIMIZATION,DIVERGENCE,RANKING,TESTS},
  language     = {eng},
  pages        = {17--28},
  title        = {Distance metric learning for ordinal classification based on triplet constraints},
  url          = {http://dx.doi.org/10.1016/j.knosys.2017.11.022},
  volume       = {142},
  year         = {2018},
}


@incollection{chen2019curvilinear,
title = {Curvilinear Distance Metric Learning},
author = {Chen, Shuo and Luo, Lei and Yang, Jian and Gong, Chen and Li, Jun and Huang, Heng},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {4223--4232},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8675-curvilinear-distance-metric-learning.pdf}
}

@paper{chen2017darkrank,
	author = {Yuntao Chen and Naiyan Wang and Zhaoxiang Zhang},
	title = {DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2018},
	keywords = {Knowledge distillation; Model acceleration; Metric learning},
	abstract = {We have witnessed rapid evolution of deep neural network architecture design in the past years. These latest progresses greatly facilitate the developments in various areas such as computer vision and natural language processing. However, along with the extraordinary performance, these state-of-the-art models also bring in expensive computational cost. Directly deploying these models into applications with real-time requirement is still infeasible. Recently, Hinton et al. have shown that the dark knowledge within a powerful teacher model can significantly help the training of a smaller and faster student network. These knowledge are vastly beneficial to improve the generalization ability of the student model. Inspired by their work, we introduce a new type of knowledge---cross sample similarities for model compression and acceleration. This knowledge can be naturally derived from deep metric learning model. To transfer them, we bring the "learning to rank" technique into deep metric learning formulation. We test our proposed DarkRank method on various metric learning tasks including pedestrian re-identification, image retrieval and image clustering. The results are quite encouraging. Our method can improve over the baseline method by a large margin. Moreover, it is fully compatible with other existing methods. When combined, the performance can be further boosted.},

	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17147}
}

@InProceedings{Brattoli_2020_CVPR,
author = {Brattoli, Biagio and Tighe, Joseph and Zhdanov, Fedor and Perona, Pietro and Chalupka, Krzysztof},
title = {Rethinking Zero-Shot Video Classification: End-to-End Training for Realistic Applications},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@misc{yi2014bindev,
    title={Deep Metric Learning for Practical Person Re-Identification},
    author={Dong Yi and Zhen Lei and Stan Z. Li},
    year={2014},
    eprint={1407.4979},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@InProceedings{epshn,
author = {Xuan, Hong and Stylianou, Abby and Pless, Robert},
title = {Improved Embeddings with Easy Positive Triplet Mining},
booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
month = {March},
year = {2020}
}

@misc{alex2019revisiting,
    title={Revisiting Self-Supervised Visual Representation Learning},
    author={Alexander Kolesnikov and Xiaohua Zhai and Lucas Beyer},
    year={2019},
    eprint={1901.09005},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{budnik2020asymmetric,
    title={Asymmetric metric learning for knowledge transfer},
    author={Mateusz Budnik and Yannis Avrithis},
    year={2020},
    eprint={2006.16331},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{laskar2020dataefficient,
  author    = {Zakaria Laskar and
               Juho Kannala},
  title     = {Data-Efficient Ranking Distillation for Image Retrieval},
  journal   = {CoRR},
  volume    = {abs/2007.05299},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.05299},
  archivePrefix = {arXiv},
  eprint    = {2007.05299},
  timestamp = {Mon, 20 Jul 2020 14:20:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-05299.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Yu_2019,
   title={Learning Metrics From Teachers: Compact Networks for Image Embedding},
   ISBN={9781728132938},
   url={http://dx.doi.org/10.1109/CVPR.2019.00302},
   DOI={10.1109/cvpr.2019.00302},
   journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Yu, Lu and Yazici, Vacit Oguz and Liu, Xialei and van de Weijer, Joost and Cheng, Yongmei and Ramisa, Arnau},
   year={2019},
   month={Jun}
}

@article{Han2019DeepDM,
  title={Deep Distillation Metric Learning},
  author={Jiaxu Han and Tianyu Zhao and Changqing Zhang},
  journal={Proceedings of the ACM Multimedia Asia},
  year={2019}
}

@article{self_distill_1,
  author    = {Linfeng Zhang and
               Jiebo Song and
               Anni Gao and
               Jingwei Chen and
               Chenglong Bao and
               Kaisheng Ma},
  title     = {Be Your Own Teacher: Improve the Performance of Convolutional Neural
               Networks via Self Distillation},
  journal   = {CoRR},
  volume    = {abs/1905.08094},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.08094},
  archivePrefix = {arXiv},
  eprint    = {1905.08094},
  timestamp = {Tue, 28 May 2019 12:48:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-08094.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{self_distill_2,
  author    = {Samira Abnar and
               Mostafa Dehghani and
               Willem H. Zuidema},
  title     = {Transferring Inductive Biases through Knowledge Distillation},
  journal   = {CoRR},
  volume    = {abs/2006.00555},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.00555},
  archivePrefix = {arXiv},
  eprint    = {2006.00555},
  timestamp = {Mon, 08 Jun 2020 15:48:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-00555.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{self_distill_3,
author = {Yun, Sukmin and Park, Jongjin and Lee, Kimin and Shin, Jinwoo},
title = {Regularizing Class-Wise Predictions via Self-Knowledge Distillation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}




########### New Related Works
@misc{cohen2021singlenoun,
      title={The Single-Noun Prior for Image Clustering}, 
      author={Niv Cohen and Yedid Hoshen},
      year={2021},
      eprint={2104.03952},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{dogan2020labelsimilarity,
      title={Label-similarity Curriculum Learning}, 
      author={Urun Dogan and Aniket Anand Deshmukh and Marcin Machura and Christian Igel},
      year={2020},
      eprint={1911.06902},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

########### Explainable DML


########### New DML
@article{darkrank, title={DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11783}, abstractNote={ &lt;p&gt; We have witnessed rapid evolution of deep neural network architecture design in the past years. These latest progresses greatly facilitate the developments in various areas such as computer vision and natural language processing. However, along with the extraordinary performance, these state-of-the-art models also bring in expensive computational cost. Directly deploying these models into applications with real-time requirement is still infeasible. Recently, Hinton et al. have shown that the dark knowledge within a powerful teacher model can significantly help the training of a smaller and faster student network. These knowledge are vastly beneficial to improve the generalization ability of the student model. Inspired by their work, we introduce a new type of knowledge---cross sample similarities for model compression and acceleration. This knowledge can be naturally derived from deep metric learning model. To transfer them, we bring the &quot;learning to rank&quot; technique into deep metric learning formulation. We test our proposed DarkRank method on various metric learning tasks including pedestrian re-identification, image retrieval and image clustering. The results are quite encouraging. Our method can improve over the baseline method by a large margin. Moreover, it is fully compatible with other existing methods. When combined, the performance can be further boosted. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chen, Yuntao and Wang, Naiyan and Zhang, Zhaoxiang}, year={2018}, month={Apr.} }

@misc{qi2020simple,
      title={A Simple and Effective Framework for Pairwise Deep Metric Learning}, 
      author={Qi Qi and Yan Yan and Xiaoyu Wang and Tianbao Yang},
      year={2020},
      eprint={1912.11194},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{Yan_2021_CVPR,
    author    = {Yan, Jiexi and Luo, Lei and Deng, Cheng and Huang, Heng},
    title     = {Unsupervised Hyperbolic Metric Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {12465-12474}
}

@InProceedings{unsup_1,
author="Li, Yang
and Kan, Shichao
and He, Zhihai",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Unsupervised Deep Metric Learning with Transformed Attention Consistency and Contrastive Clustering Loss",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="141--157",
abstract="Existing approaches for unsupervised metric learning focus on exploring self-supervision information within the input image itself. We observe that, when analyzing images, human eyes often compare images against each other instead of examining images individually. In addition, they often pay attention to certain keypoints, image regions, or objects which are discriminative between image classes but highly consistent within classes. Even if the image is being transformed, the attention pattern will be consistent. Motivated by this observation, we develop a new approach to unsupervised deep metric learning where the network is learned based on self-supervision information across images instead of within one single image. To characterize the consistent pattern of human attention during image comparisons, we introduce the idea of transformed attention consistency. It assumes that visually similar images, even undergoing different image transforms, should share the same consistent visual attention map. This consistency leads to a pairwise self-supervision loss, allowing us to learn a Siamese deep neural network to encode and compare images against their transformed or matched pairs. To further enhance the inter-class discriminative power of the feature generated by this network, we adapt the concept of triplet loss from supervised metric learning to our unsupervised case and introduce the contrastive clustering loss. Our extensive experimental results on benchmark datasets demonstrate that our proposed method outperforms current state-of-the-art methods for unsupervised metric learning by a large margin.",
isbn="978-3-030-58621-8"
}

@InProceedings{unsup_2,
author = {Ye, Mang and Zhang, Xu and Yuen, Pong C. and Chang, Shih-Fu},
title = {Unsupervised Embedding Learning via Invariant and Spreading Instance Feature},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@InProceedings{unsup_3,
author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ondřej},
title = {Mining on Manifolds: Metric Learning Without Labels},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@article{unsup_4,
  author    = {Xuefei Cao and
               Bor{-}Chun Chen and
               Ser{-}Nam Lim},
  title     = {Unsupervised Deep Metric Learning via Auxiliary Rotation Loss},
  journal   = {CoRR},
  volume    = {abs/1911.07072},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.07072},
  archivePrefix = {arXiv},
  eprint    = {1911.07072},
  timestamp = {Mon, 02 Dec 2019 17:48:37 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-07072.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@InProceedings{Wang_2020_CVPR,
author = {Wang, Xun and Zhang, Haozhi and Huang, Weilin and Scott, Matthew R.},
title = {Cross-Batch Memory for Embedding Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@ARTICLE{dutta2020orthogonalunsupdml,
  author={Dutta, Ujjal Kr and Harandi, Mehrtash and Sekhar, Chellu Chandra},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Unsupervised Deep Metric Learning via Orthogonality Based Probabilistic Loss}, 
  year={2020},
  volume={1},
  number={1},
  pages={74-84},
  doi={10.1109/TAI.2020.3026982}}
  
@InProceedings{Deng_2021_CVPR,
    author    = {Deng, Jiankang and Guo, Jia and Yang, Jing and Lattas, Alexandros and Zafeiriou, Stefanos},
    title     = {Variational Prototype Learning for Deep Face Recognition},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {11906-11915}
}

@InProceedings{Suh_2019_CVPR,
author = {Suh, Yumin and Han, Bohyung and Kim, Wonsik and Lee, Kyoung Mu},
title = {Stochastic Class-Based Hard Example Mining for Deep Metric Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}


@inproceedings{Zhu2020graphdml,
 author = {Zhu, Yuehua and Yang, Muli and Deng, Cheng and Liu, Wei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {17792--17803},
 publisher = {Curran Associates, Inc.},
 title = {Fewer is More: A Deep Graph Metric Learning Perspective Using Fewer Proxies},
 url = {https://proceedings.neurips.cc/paper/2020/file/ce016f59ecc2366a43e1c96a4774d167-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{zhang2020sphericaldml,
 author = {Zhang, Dingyi and Li, Yingming and Zhang, Zhongfei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {18772--18783},
 publisher = {Curran Associates, Inc.},
 title = {Deep Metric Learning with Spherical Embedding},
 url = {https://proceedings.neurips.cc/paper/2020/file/d9812f756d0df06c7381945d2e2c7d4b-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{wang2020robustdml,
 author = {Wang, Lu and Liu, Xuanqing and Yi, Jinfeng and Jiang, Yuan and Hsieh, Cho-Jui},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {19302--19313},
 publisher = {Curran Associates, Inc.},
 title = {Provably Robust Metric Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/e038453073d221a4f32d0bab94ca7cee-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{seidenschwarz2021graphdml,
  title = 	 {Learning Intra-Batch Connections for Deep Metric Learning},
  author =       {Seidenschwarz, Jenny Denise and Elezi, Ismail and Leal-Taix{\'e}, Laura},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9410--9421},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/seidenschwarz21a/seidenschwarz21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/seidenschwarz21a.html},
  abstract = 	 {The goal of metric learning is to learn a function that maps samples to a lower-dimensional space where similar samples lie closer than dissimilar ones. Particularly, deep metric learning utilizes neural networks to learn such a mapping. Most approaches rely on losses that only take the relations between pairs or triplets of samples into account, which either belong to the same class or two different classes. However, these methods do not explore the embedding space in its entirety. To this end, we propose an approach based on message passing networks that takes all the relations in a mini-batch into account. We refine embedding vectors by exchanging messages among all samples in a given batch allowing the training process to be aware of its overall structure. Since not all samples are equally important to predict a decision boundary, we use an attention mechanism during message passing to allow samples to weigh the importance of each neighbor accordingly. We achieve state-of-the-art results on clustering and image retrieval on the CUB-200-2011, Cars196, Stanford Online Products, and In-Shop Clothes datasets. To facilitate further research, we make available the code and the models at https://github.com/dvl-tum/intra_batch_connections.}
}

@InProceedings{Mohan_2020_CVPR,
author = {Mohan, Deen Dayal and Sankaran, Nishant and Fedorishin, Dennis and Setlur, Srirangaraj and Govindaraju, Venu},
title = {Moving in the Right Direction: A Regularization for Deep Metric Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@misc{ko2021learning,
      title={Learning with Memory-based Virtual Classes for Deep Metric Learning}, 
      author={Byungsoo Ko and Geonmo Gu and Han-Gyu Kim},
      year={2021},
      eprint={2103.16940},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{omninet,
  author    = {Subhojeet Pramanik and
               Priyanka Agrawal and
               Aman Hussain},
  title     = {OmniNet: {A} unified architecture for multi-modal multi-task learning},
  journal   = {CoRR},
  volume    = {abs/1907.07804},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.07804},
  eprinttype = {arXiv},
  eprint    = {1907.07804},
  timestamp = {Tue, 23 Jul 2019 10:54:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-07804.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{virtex,
  author    = {Karan Desai and
               Justin Johnson},
  title     = {VirTex: Learning Visual Representations from Textual Annotations},
  journal   = {CoRR},
  volume    = {abs/2006.06666},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.06666},
  eprinttype = {arXiv},
  eprint    = {2006.06666},
  timestamp = {Sat, 13 Jun 2020 18:28:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-06666.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tian2020hynet,
 author = {Tian, Yurun and Barroso Laguna, Axel and Ng, Tony and Balntas, Vassileios and Mikolajczyk, Krystian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {7401--7412},
 publisher = {Curran Associates, Inc.},
 title = {HyNet: Learning Local Descriptor with Hybrid Similarity Measure and Triplet Loss},
 url = {https://proceedings.neurips.cc/paper/2020/file/52d2752b150f9c35ccb6869cbf074e48-Paper.pdf},
 volume = {33},
 year = {2020}
}

@InProceedings{Zheng_2021_CVPR_compositional,
    author    = {Zheng, Wenzhao and Wang, Chengkun and Lu, Jiwen and Zhou, Jie},
    title     = {Deep Compositional Metric Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {9320-9329}
}

@InProceedings{elezi2020grouploss,
author="Elezi, Ismail
and Vascon, Sebastiano
and Torcinovich, Alessandro
and Pelillo, Marcello
and Leal-Taix{\'e}, Laura",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="The Group Loss for Deep Metric Learning",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="277--294",
abstract="Deep metric learning has yielded impressive results in tasks such as clustering and image retrieval by leveraging neural networks to obtain highly discriminative feature embeddings, which can be used to group samples into different classes. Much research has been devoted to the design of smart loss functions or data mining strategies for training such networks. Most methods consider only pairs or triplets of samples within a mini-batch to compute the loss function, which is commonly based on the distance between embeddings. We propose Group Loss, a loss function based on a differentiable label-propagation method that enforces embedding similarity across all samples of a group while promoting, at the same time, low-density regions amongst data points belonging to different groups. Guided by the smoothness assumption that ``similar objects should belong to the same group'', the proposed loss trains the neural network for a classification task, enforcing a consistent labelling amongst samples within a class. We show state-of-the-art results on clustering and image retrieval on several datasets, and show the potential of our method when combined with other techniques such as ensembles. To facilitate further research, we make available the code and the models at https://github.com/dvl-tum/group{\_}loss.",
isbn="978-3-030-58571-6"
}


@InProceedings{Kim_2021_CVPR,
    author    = {Kim, Sungyeon and Kim, Dongwon and Cho, Minsu and Kwak, Suha},
    title     = {Embedding Transfer With Label Relaxation for Improved Metric Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {3967-3976}
}

@inproceedings{fu2020texturedml,
 author = {Fu, Huan and Li, Shunming and Jia, Rongfei and Gong, Mingming and Zhao, Binqiang and Tao, Dacheng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {14675--14687},
 publisher = {Curran Associates, Inc.},
 title = {Hard Example Generation by Texture Synthesis for Cross-domain Shape Similarity Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/a87d27f712df362cd22c7a8ef823e987-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{dml_ood_gen,
  author    = {Timo Milbich and
               Karsten Roth and
               Samarth Sinha and
               Ludwig Schmidt and
               Marzyeh Ghassemi and
               Bj{\"{o}}rn Ommer},
  title     = {Characterizing Generalization under Out-Of-Distribution Shifts in
               Deep Metric Learning},
  journal   = {CoRR},
  volume    = {abs/2107.09562},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.09562},
  archivePrefix = {arXiv},
  eprint    = {2107.09562},
  timestamp = {Thu, 29 Jul 2021 12:42:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-09562.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{s2sd,
  title = 	 {Simultaneous Similarity-based Self-Distillation for Deep Metric Learning},
  author =       {Roth, Karsten and Milbich, Timo and Ommer, Bjorn and Cohen, Joseph Paul and Ghassemi, Marzyeh},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9095--9106},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/roth21a/roth21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/roth21a.html},
  abstract = 	 {Deep Metric Learning (DML) provides a crucial tool for visual similarity and zero-shot retrieval applications by learning generalizing embedding spaces, although recent work in DML has shown strong performance saturation across training objectives. However, generalization capacity is known to scale with the embedding space dimensionality. Unfortunately, high dimensional embeddings also create higher retrieval cost for downstream applications. To remedy this, we propose S2SD - Simultaneous Similarity-based Self-distillation. S2SD extends DML with knowledge distillation from auxiliary, high-dimensional embedding and feature spaces to leverage complementary context during training while retaining test-time cost and with negligible changes to the training time. Experiments and ablations across different objectives and standard benchmarks show S2SD offering highly significant improvements of up to 7% in Recall@1, while also setting a new state-of-the-art.}
}


########### CROSS-MODAL (DML)
@InProceedings{Faraki_2021_CVPR,
    author    = {Faraki, Masoud and Yu, Xiang and Tsai, Yi-Hsuan and Suh, Yumin and Chandraker, Manmohan},
    title     = {Cross-Domain Similarity Learning for Face Recognition in Unseen Domains},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {15292-15301}
}

@InProceedings{Wray_2021_CVPR,
    author    = {Wray, Michael and Doughty, Hazel and Damen, Dima},
    title     = {On Semantic Similarity in Video Retrieval},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {3650-3660}
}

@InProceedings{Wang_2021_CVPR,
    author    = {Wang, Hao and Bai, Xiang and Yang, Mingkun and Zhu, Shenggao and Wang, Jing and Liu, Wenyu},
    title     = {Scene Text Retrieval via Joint Text Detection and Similarity Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {4558-4567}
}

@INPROCEEDINGS {Ren2021crossmodalmatching,
author = {L. Ren and K. Li and L. Wang and K. Hua},
booktitle = {2020 25th International Conference on Pattern Recognition (ICPR)},
title = {Beyond the Deep Metric Learning: Enhance the Cross-Modal Matching with Adversarial Discriminative Domain Regularization},
year = {2021},
volume = {},
issn = {1051-4651},
pages = {10165-10172},
keywords = {measurement;location awareness;visualization;semantics;benchmark testing;information retrieval;natural language processing},
doi = {10.1109/ICPR48806.2021.9412297},
url = {https://doi.ieeecomputersociety.org/10.1109/ICPR48806.2021.9412297},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jan}
}

@inproceedings{Huang2017crossmodaldml,
  author    = {Xin Huang and
               Yuxin Peng},
  title     = {Cross-modal deep metric learning with multi-task regularization},
  booktitle = {2017 {IEEE} International Conference on Multimedia and Expo, {ICME}
               2017, Hong Kong, China, July 10-14, 2017},
  pages     = {943--948},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/ICME.2017.8019340},
  doi       = {10.1109/ICME.2017.8019340},
  timestamp = {Wed, 16 Oct 2019 14:14:56 +0200},
  biburl    = {https://dblp.org/rec/conf/icmcs/HuangP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Wei_2020_CVPR,
author = {Wei, Jiwei and Xu, Xing and Yang, Yang and Ji, Yanli and Wang, Zheng and Shen, Heng Tao},
title = {Universal Weighting Metric Learning for Cross-Modal Matching},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@ARTICLE{Liong2017coupledcrossmodaldml,
  author={Liong, Venice Erin and Lu, Jiwen and Tan, Yap-Peng and Zhou, Jie},
  journal={IEEE Transactions on Multimedia}, 
  title={Deep Coupled Metric Learning for Cross-Modal Matching}, 
  year={2017},
  volume={19},
  number={6},
  pages={1234-1244},
  doi={10.1109/TMM.2016.2646180}}

@misc{tsimpoukelli2021multimodal,
      title={Multimodal Few-Shot Learning with Frozen Language Models}, 
      author={Maria Tsimpoukelli and Jacob Menick and Serkan Cabi and S. M. Ali Eslami and Oriol Vinyals and Felix Hill},
      year={2021},
      eprint={2106.13884},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{Dzabraev_2021_CVPR,
    author    = {Dzabraev, Maksim and Kalashnikov, Maksim and Komkov, Stepan and Petiushko, Aleksandr},
    title     = {MDMMT: Multidomain Multimodal Transformer for Video Retrieval},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2021},
    pages     = {3354-3363}
}

########### Domain-Adaptation
@inproceedings{
sohn2018unsupervised,
title={Unsupervised Domain Adaptation for Distance Metric Learning},
author={Kihyuk Sohn and Wenling Shang and Xiang Yu and Manmohan Chandraker},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BklhAj09K7},
}

@InProceedings{Zheng_2021_CVPR,
    author    = {Zheng, Kecheng and Liu, Wu and He, Lingxiao and Mei, Tao and Luo, Jiebo and Zha, Zheng-Jun},
    title     = {Group-aware Label Transfer for Domain Adaptive Person Re-identification},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {5310-5319}
}


@InProceedings{clip,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}


@article{gpt2,
abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
file = {:Users/shanest/Documents/Library/Radford et al/Unknown/Radford et al. - 2019 - Language Models are Unsupervised Multitask Learners.pdf:pdf},
keywords = {model},
title = {{Language Models are Unsupervised Multitask Learners}},
url = {https://openai.com/blog/better-language-models/},
year = {2019}
}

@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{transformers,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{huggingface,
      title={HuggingFace's Transformers: State-of-the-art Natural Language Processing}, 
      author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
      year={2020},
      eprint={1910.03771},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and M. Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}

@misc{distilbert,
      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
      year={2020},
      eprint={1910.01108},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{mpnet,
 author = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {16857--16867},
 publisher = {Curran Associates, Inc.},
 title = {MPNet: Masked and Permuted Pre-training for Language Understanding},
 url = {https://proceedings.neurips.cc/paper/2020/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@article{crossmodal_1,
author = {Xu, Xing and He, Li and Lu, Huimin and Gao, Lianli and Ji, Yanli},
title = {Deep Adversarial Metric Learning for Cross-Modal Retrieval},
year = {2019},
issue_date = {March 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-018-0541-x},
doi = {10.1007/s11280-018-0541-x},
abstract = {Cross-modal retrieval has become a highlighted research topic, to provide flexible
retrieval experience across multimedia data such as image, video, text and audio.
The core of existing cross-modal retrieval approaches is to narrow down the gap between
different modalities either by finding a maximally correlated embedding space. Recently,
researchers leverage Deep Neural Network (DNN) to learn nonlinear transformations
for each modality to obtain transformed features in a common subspace where cross-modal
matching can be performed. However, the statistical characteristics of the original
features for each modality are not explicitly preserved in the learned subspace. Inspired
by recent advances in adversarial learning, we propose a novel Deep Adversarial Metric
Learning approach, termed DAML for cross-modal retrieval. DAML nonlinearly maps labeled
data pairs of different modalities into a shared latent feature subspace, under which
the intra-class variation is minimized and the inter-class variation is maximized,
and the difference of each data pair captured from two modalities of the same class
is minimized, respectively. In addition to maximizing the correlations between modalities,
we add an additional regularization by introducing adversarial learning. In particular,
we introduce a modality classifier to predict the modality of a transformed feature,
which ensures that the transformed features are also statistically indistinguishable.
Experiments on three popular multimodal datasets show that DAML achieves superior
performance compared to several state of the art cross-modal retrieval methods.},
journal = {World Wide Web},
month = mar,
pages = {657–672},
numpages = {16},
keywords = {Metric learning, Adversarial learning, Cross-modal retrieval}
}

@INPROCEEDINGS{crossmodal_2,
  author={Huang, Xin and Peng, Yuxin},
  booktitle={2017 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={Cross-modal deep metric learning with multi-task regularization}, 
  year={2017},
  volume={},
  number={},
  pages={943-948},
  doi={10.1109/ICME.2017.8019340}}
  
@InProceedings{crossmodal_3,
author = {Zhen, Liangli and Hu, Peng and Wang, Xu and Peng, Dezhong},
title = {Deep Supervised Cross-Modal Retrieval},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{crossmodal_4,
title = {Hybrid representation learning for cross-modal retrieval},
journal = {Neurocomputing},
volume = {345},
pages = {45-57},
year = {2019},
note = {Deep Learning for Intelligent Sensing, Decision-Making and Control},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.10.082},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219301407},
author = {Wenming Cao and Qiubin Lin and Zhihai He and Zhiquan He},
keywords = {Cross-modal retrieval, Hybrid representation, DNNs},
abstract = {The rapid development of Deep Neural Networks (DNNs) in single-modal retrieval has promoted the wide application of DNNs in cross-modal retrieval tasks. Therefore, we propose a DNN-based method to learn the shared representation for each modality. Our method, hybrid representation learning (HRL), consists of three steps. In the first learning step, stacked restricted Boltzmann machines (SRBM) are utilized to extract the modality-friendly representation for each modality, with statistical properties that are more similar than those of the original input instances of both modalities, and a multimodal deep belief net (multimodal DBN) is utilized to extract the modality-mutual representation, which contains some missing information in the original input instances. In the second learning step, a two-level network containing a joint autoencoder and a three-layer feedforward neural net are used. From these steps, the hybrid representation is obtained, which combines the image representation constructed by the image-pathway SRBM and the modality-mutual representation, which involves the latent image representation and can be used to infer the missing values of the image via the multimodal DBN or vice-versa. In the third learning step, stacked bimodal autoencoders are used to obtain the final shared representation for each modality. The experimental results show that our proposed HRL method is superior to several advanced approaches according to three widely used cross-modal datasets.}
}

@ARTICLE{crossmodal_5,
  author={He, Yonghao and Xiang, Shiming and Kang, Cuicui and Wang, Jian and Pan, Chunhong},
  journal={IEEE Transactions on Multimedia}, 
  title={Cross-Modal Retrieval via Deep and Bidirectional Representation Learning}, 
  year={2016},
  volume={18},
  number={7},
  pages={1363-1377},
  doi={10.1109/TMM.2016.2558463}}
  
@article{crossmodal_6,
author = {Peng, Yuxin and Qi, Jinwei},
title = {CM-GANs: Cross-Modal Generative Adversarial Networks for Common Representation Learning},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3284750},
doi = {10.1145/3284750},
abstract = {It is known that the inconsistent distributions and representations of different modalities,
such as image and text, cause the heterogeneity gap, which makes it very challenging
to correlate heterogeneous data and measure their similarities. Recently, generative
adversarial networks (GANs) have been proposed and have shown their strong ability
to model data distribution and learn discriminative representation. It has also been
shown that adversarial learning can be fully exploited to learn discriminative common
representations for bridging the heterogeneity gap. Inspired by this, we aim to effectively
correlate large-scale heterogeneous data of different modalities with the power of
GANs to model cross-modal joint distribution. In this article, we propose Cross-modal
Generative Adversarial Networks (CM-GANs) with the following contributions. First,
a cross-modal GAN architecture is proposed to model joint distribution over the data
of different modalities. The inter-modality and intra-modality correlation can be
explored simultaneously in generative and discriminative models. Both compete with
each other to promote cross-modal correlation learning. Second, the cross-modal convolutional
autoencoders with weight-sharing constraint are proposed to form the generative model.
They not only exploit the cross-modal correlation for learning the common representations
but also preserve reconstruction information for capturing the semantic consistency
within each modality. Third, a cross-modal adversarial training mechanism is proposed,
which uses two kinds of discriminative models to simultaneously conduct intra-modality
and inter-modality discrimination. They can mutually boost to make the generated common
representations more discriminative by the adversarial training process. In summary,
our proposed CM-GAN approach can use GANs to perform cross-modal common representation
learning by which the heterogeneous data can be effectively correlated. Extensive
experiments are conducted to verify the performance of CM-GANs on cross-modal retrieval
compared with 13 state-of-the-art methods on 4 cross-modal datasets.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {22},
numpages = {24},
keywords = {cross-modal retrieval, Generative adversarial network, cross-modal adversarial mechanism, common representation learning}
}

@misc{crossmodal_7,
      title={Audio Retrieval with Natural Language Queries}, 
      author={Andreea-Maria Oncescu and A. Sophia Koepke and João F. Henriques and Zeynep Akata and Samuel Albanie},
      year={2021},
      eprint={2105.02192},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@InProceedings{crossmodal_8,
    author    = {Chen, Yanbei and Xian, Yongqin and Koepke, A. Sophia and Shan, Ying and Akata, Zeynep},
    title     = {Distilling Audio-Visual Knowledge by Compositional Contrastive Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {7016-7025}
}

@InProceedings{cm_1,
author="van de Weijer, Joost
and Khan, Fahad Shahbaz",
editor="Tr{\'e}meau, Alain
and Schettini, Raimondo
and Tominaga, Shoji",
title="An Overview of Color Name Applications in Computer Vision",
booktitle="Computational Color Imaging",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="16--22",
abstract="In this article we provide an overview of color name applications in computer vision. Color names are linguistic labels which humans use to communicate color. Computational color naming learns a mapping from pixels values to color names. In recent years color names have been applied to a wide variety of computer vision applications, including image classification, object recognition, texture classification, visual tracking and action recognition. Here we provide an overview of these results which show that in general color names outperform photometric invariants as a color representation.",
isbn="978-3-319-15979-9"
}


@inproceedings{cm_2,
  author    = {Raphael C. Prates and
               Cristianne R. S. Dutra and
               William Robson Schwartz},
  title     = {Predominant color name indexing structure for person re-identification},
  booktitle = {2016 {IEEE} International Conference on Image Processing, {ICIP} 2016,
               Phoenix, AZ, USA, September 25-28, 2016},
  pages     = {779--783},
  publisher = {{IEEE}},
  year      = {2016},
  url       = {https://doi.org/10.1109/ICIP.2016.7532463},
  doi       = {10.1109/ICIP.2016.7532463},
  timestamp = {Wed, 16 Oct 2019 14:14:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icip/PratesDS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{hier_cm_1,
  author       = "S. Ging and M. Zolfaghari and H. Pirsiavash and T. Brox",
  title        = "COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning",
  booktitle    = "Advances in Neural Information Processing Systems (NeurIPS)",
  volume       = "33",
  pages        = "22605--22618",
  month        = " ",
  year         = "2020",
  editor       = "H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin",
  publisher    = "Curran Associates, Inc.",
  url          = "http://lmb.informatik.uni-freiburg.de/Publications/2020/GZB20"
}


@InProceedings{perceiver,
  title = 	 {Perceiver: General Perception with Iterative Attention},
  author =       {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4651--4664},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/jaegle21a/jaegle21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/jaegle21a.html},
  abstract = 	 {Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver {–} a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.}
}




############# Detection
@InProceedings{He_2019_ICCV,
author = {He, Kaiming and Girshick, Ross and Dollar, Piotr},
title = {Rethinking ImageNet Pre-Training},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}


@InProceedings{detect_1,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2014}
}

@InProceedings{detect_2,
author="He, Kaiming
and Zhang, Xiangyu
and Ren, Shaoqing
and Sun, Jian",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="346--361",
abstract="Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g.224{\texttimes}224) input image. This requirement is ``artificial'' and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, ``spatial pyramid pooling'', to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101.",
isbn="978-3-319-10578-9"
}


@InProceedings{detect_3,
author = {Girshick, Ross},
title = {Fast R-CNN},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}

@inproceedings{detect_4,
 author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
 url = {https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf},
 volume = {28},
 year = {2015}
}


@InProceedings{detect_5,
author="Liu, Wei
and Anguelov, Dragomir
and Erhan, Dumitru
and Szegedy, Christian
and Reed, Scott
and Fu, Cheng-Yang
and Berg, Alexander C.",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="SSD: Single Shot MultiBox Detector",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="21--37",
abstract="We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For {\$}{\$}300 {\backslash}times 300{\$}{\$}300{\texttimes}300input, SSD achieves 74.3{\%} mAP on VOC2007 test at 59FPS on a Nvidia Titan X and for {\$}{\$}512 {\backslash}times 512{\$}{\$}512{\texttimes}512input, SSD achieves 76.9{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.",
isbn="978-3-319-46448-0"
}

@InProceedings{detect_6,
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
title = {You Only Look Once: Unified, Real-Time Object Detection},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@InProceedings{detect_7,
author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
title = {Feature Pyramid Networks for Object Detection},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{
goyal_2021_rim,
title={Recurrent Independent Mechanisms},
author={Anirudh Goyal and Alex Lamb and Jordan Hoffmann and Shagun Sodhani and Sergey Levine and Yoshua Bengio and Bernhard Sch{\"o}lkopf},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=mLcmdlEUxy-}
}

@inbook{zhang_2019_lookahead,
author = {Zhang, Michael R. and Lucas, James and Hinton, Geoffrey and Ba, Jimmy},
title = {Lookahead Optimizer: K Steps Forward, 1 Step Back},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of "fast weights" generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {861},
numpages = {12}
}

@InProceedings{zbontar_2021_barlow,
  title = 	 {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  author =       {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stephane},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12310--12320},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zbontar21a/zbontar21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zbontar21a.html},
  abstract = 	 {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow’s redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.}
}


@article{word_hierarchies,
author = {Cimiano, Ph and Hotho, Andreas and Staab, Steffen},
year = {2011},
month = {09},
pages = {},
title = {Learning Concept Hierarchies from Text Corpora using Formal Concept
Analysis},
volume = {24},
journal = {Journal of Artificial Intelligence Research},
doi = {10.1613/jair.1648}
}







@misc{lesort2022long,
  doi = {10.48550/ARXIV.2207.04543},
  
  url = {https://arxiv.org/abs/2207.04543},
  
  author = {Lesort, Timothée and Ostapenko, Oleksiy and Misra, Diganta and Arefin, Md Rifat and Rodríguez, Pau and Charlin, Laurent and Rish, Irina},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Scaling the Number of Tasks in Continual Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{bommasani2021foundation,
  doi = {10.48550/ARXIV.2108.07258},
  
  url = {https://arxiv.org/abs/2108.07258},
  
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the Opportunities and Risks of Foundation Models},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{lee2020clinical,
author = {Lee, Cecilia and Lee, Aaron},
year = {2020},
month = {06},
pages = {e279-e281},
title = {Clinical applications of continual learning machine learning},
volume = {2},
journal = {The Lancet Digital Health},
doi = {10.1016/S2589-7500(20)30102-3}
}


@misc{ostapenko2022foundation,
  doi = {10.48550/ARXIV.2205.00329},
  
  url = {https://arxiv.org/abs/2205.00329},
  
  author = {Ostapenko, Oleksiy and Lesort, Timothee and Rodríguez, Pau and Arefin, Md Rifat and Douillard, Arthur and Rish, Irina and Charlin, Laurent},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Continual Learning with Foundation Models: An Empirical Study of Latent Replay},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{
mehta2022an,
title={An Empirical Investigation of the Role of Pre-training in Lifelong Learning},
author={Sanket Vaibhav Mehta and Darshan Patil and Sarath Chandar and Emma Strubell},
year={2022},
url={https://openreview.net/forum?id=D9E8MKsfhw}
}

@inproceedings{
wu2022pretrained,
title={Pretrained Language Model in Continual Learning: A Comparative Study},
author={Tongtong Wu and Massimo Caccia and Zhuang Li and Yuan-Fang Li and Guilin Qi and Gholamreza Haffari},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=figzpGMrdD}
}

@inproceedings{
ramasesh2022effect,
title={Effect of scale on catastrophic forgetting in neural networks},
author={Vinay Venkatesh Ramasesh and Aitor Lewkowycz and Ethan Dyer},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=GhVS8_yPeEa}
}

@InProceedings{wortsman2022zeroshot,
    author    = {Wortsman, Mitchell and Ilharco, Gabriel and Kim, Jong Wook and Li, Mike and Kornblith, Simon and Roelofs, Rebecca and Lopes, Raphael Gontijo and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and Schmidt, Ludwig},
    title     = {Robust Fine-Tuning of Zero-Shot Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {7959-7971}
}