

\section{Results}
\subsection{Analyzing the State-of-the-Art Basecaller} \label{suppsec:overprovision}

We observe established  automatic-speech recognition (ASR) models  being directly
applied to basecalling without optimizing it for basecalling. Such an approach leads to large and
unoptimized basecaller architectures. We evaluate the effect of using  two popular model compression techniques on the \bon basecaller: (1) Pruning, and (2) Quantization.


\subsubsection{Effect of Pruning}
\label{suppsubsubsec:overprovision_prune}
We show the effect of pruning \bon on the validation accuracy and model size in Figure~\ref{supfig:over_prune}(a) and Figure~\ref{supfig:over_prune}(b), respectively. We use unstructured element pruning and structured channel pruning with different degrees of sparsity. %We use  unstructured element pruning to remove the model weights with varying amount of pruning ratio. Element pruning can reach the maximum model compression ratio, which helps to understand the limits of a neural network model. However, is unsuitable for hardware-acceleration as it leads high model sparsity. 

  \begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth,trim={0.2cm 0.85cm 0.4cm 0.3cm},clip]{images/res_pruning_def2.pdf}%   {images/prune_default_model.pdf}
%   \vspace{-15pt}
  \caption{Effect of pruning the elements and channels of \bon using unstructured and structured pruning, respectively, on: (a) validation accuracy and (b) model size.}
  \label{supfig:over_prune}
%   \vspace{-12pt}
\end{figure}
We make three major observations. First, \sr{pruning up to 85\% of the \bon model weights using unstructured pruning reduces the model size by 6.67$\times$ while maintaining the same accuracy as the baseline, unpruned \bon model.} While  pruning 30-40\% of the \bon model filters, using structured pruning reduces the model size by 1.46-1.66$\times$ while maintaining the same accuracy of the baseline, unpruned \bon model. Such \sr{a} high pruning ratio shows that most of the weights are redundant and \sr{do} not contribute to the actual accuracy. Second, after pruning 97$\%$ (60$\%$) of the model weights, \bon provides 81.20$\%$ (72.66$\%$) basecalling accuracy while using 33.33$\times$ (2.62$\times$) smaller model using unstructured pruning (structured pruning). Third, the \emph{knee point}\footnote{We define knee point as the point beyond which a basecaller is unable to basecall at an acceptable level of accuracy.} for unstructured pruning and structured pruning is at 98$\%$ and 60$\%$ where \bon provides 65.14$\%$ and 72.66$\%$ of basecalling accuracy, respectively. Beyond the knee-point, \bon losses its complete prediction power. We conclude that \bon is over-parameterized and \sr{contains} redundant logic and features.



\subsubsection{Effect of Quantization}
\label{suppsubsubsec:overprovision_quant}
Figure~\ref{supfig:over_quant} shows the effect \sr{of} using a quantized model to basecall \gsss{on the basecalling accuracy.} %\srr{on what?}. \srr{you don't need to mention the rest of the sentence now} for four different species. 
In Figure~\ref{supfig:quant_def_model_size}, we show the effect of quantization on the model size. We quantize both the weight and activation using six different bit-width configurations ($<$\texttt{3,2}$>$, $<$\texttt{4,2}$>$, $<$\texttt{4,4}$>$, $<$\texttt{4,8}$>$, $<$\texttt{8,4}$>$, and $<$\texttt{16,16}$>$). 
We also show the results with the default floating-point precision ($<$\texttt{fp32,fp32}$>$). We use static quantization that uses the same precision for each neural network layer. 


  \begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth,trim={0.2cm 0.25cm 0.4cm 0.3cm},clip]{images/over_quant_2.pdf}
  \vspace{-5pt}
  \caption{Basecalling using quantized model.}
  \label{supfig:over_quant}
%   \vspace{-12pt}
\end{figure}


  \vspace{5pt}
\begin{SCfigure}[][h]
  \centering
%   \hspace{0.2cm}
 \includegraphics[width=0.45\linewidth,trim={0cm 0cm 0.2cm 0.35cm},clip]{images/quant_def_model_size2.pdf}
 \vspace{-0.2cm}
 \hspace{-0.6cm} \caption{Effect of quantizing weight and activation of \bon on model size. We quantize both the weight and activation with static precision. Since weights are the trainable parameters in a neural network,  only weights contribute to the final model size.}
\label{supfig:quant_def_model_size}
% \vspace{-0.2cm}
\end{SCfigure}


We make four main observations. First, \sr{using a precision of $<$\texttt{8,8}$>$ for weight and activation for all the layers \sr{of} \bon causes a negligible accuracy} loss (0.18\%-0.67\%), while \sr{reducing the model size by} 4.03$\times$. Second, \bon is more sensitive to weight precision than activation precision.
\sr{For example, we} observe a loss of 1.82\%-9.48\% accuracy \sr{when using a precision of} $<$\texttt{4,8}$>$ instead of $<$\texttt{16,16}$>$ bits compared to an accuracy loss of only 0.51\%-3.02\% \sr{when using a precision of} $<$\texttt{8,4}$>$ instead of $<$\texttt{16,16}$>$ bits. 
Third, we observe a significant drop in accuracy (\sr{by} 9.17\%-15.07\%), while using \sr{less} than 4 bits for weights (e.g., using $<$\texttt{3,2}$>$ configuration). Fourth, \sr{using bit-width precision of $<$16,16$>$ bits provides $\sim$2$\times$ reductions in model size and with no \sr{accuracy} loss compared to using full precision ($<$\texttt{fp32,fp32}$>$) floating\sr{-point} implementation.} 
We conclude that the current state-of-the-art basecaller, \bon, can \sr{still} efficiently \sr{perform basecalling even when using} lower precision for both the weight and activation. 


\subsection{\mech: Overall Trend}
\label{subsection:results_overall}
\sr{We compare} the \sr{overall} basecalling \sr{performance} \sr{of} \mech with \sr{that of the} baseline basecaller\sr{s} in terms of  basecalling accuracy, model parameters, and model size in Figure~\ref{fig:model_compare}(a), \ref{fig:model_compare}(b), and \ref{fig:model_compare}(c), respectively.
%with trainable neural network parameters in a basecaller and the total model size, respectively. We draw four observations. 

 \begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{images/model_compare9_AMD.pdf}
%   \vspace{-15pt}
  \caption{Comparison of basecalling throughput for \mechmp with baseline basecaller in terms of: (a)  basecalling accuracy, (b) model parameters, and (b) model size.}
  \label{fig:model_compare}
%   \vspace{-12pt}
\end{figure}

\sr{We make six key observations.}
\gs{First, compared to \bon, which is the most accurate basecaller, \mechmp provides 16.56$\times$ higher basecalling throughput without any loss in accuracy. This is because \mechmp has a mixed precision neural architecture that can exploit low-precision compute units present on our evaluated spatial vector computing systems, i.e., the AMD-Xilinx Versal AI Engine (AIE). Compared to \gpf, which is the fastest basecaller, \mechmp provides, on average, 2.97$\%$ higher accuracy with 7.06$\times$ higher basecalling throughput. Therefore, \mechmp provides both accuracy and high basecalling throughput compared to the baseline basecallers.}
Second, \bon has the highest number of neural network model parameters, which are 2.71$\times$, 13.33$\times$, and 2.94$\times$ more than \cc, \gpf, and \mechmp, respectively.
Third, \cc (\bon) has 2.71$\times$ fewer (2.93$\times$ greater) neural network model parameters but is  72.55$\times$ (16.56$\times$) slower in performance when compared to \mechmp. This is because skip connections in \cc and \bon create \sr{a} performance bottleneck by increasing the data lifetime and adding additional computation. Therefore, we observe a performance degradation instead of higher performance due to the linear relation usually present between the number of model  parameters and the speed  of a model. Fourth, \gpf has the lowest number of trainable \sr{model} parameters that are 4.92$\times$, 13.33$\times$, and 4.54$\times$ lower than \cc, \bon, and \mechmp. 
Fifth, \mechmp provides 2.55$\times$ and \sr{6.93$\times$} smaller \sr{model} size compared to \cc and \bon, respectively. 
The decrease in model size is due to: (1) \sr{a} lower number of neural network layers; and (2) optimum bit-width precision for each neural network layer. Sixth, all the baseline basecallers use floating-point arithmetic precision for all neural network layers. This leads to very high memory bandwidth and processing demands. 
We conclude that \mechmp provides the ability to basecall accurately, quickly, and efficiently scale basecalling by providing a reduction in both model size and  neural network model parameters.
 
 

\subsection{Performance Comparison}
\label{subsection:results_perf_compare}
\sr{We compare} the speed of \mechmp against baseline basecallers in Figure~\ref{fig:perf}.
We make three major observations. First, \mechmp consistently outperforms all the other basecallers for all the evaluated species. 
\mechmp improves average performance by 72.55$\times$,		7.06$\times$, and 16.56$\times$ over \cc,  \gpf, and \bon, respectively. Second, by leveraging low precision capabilities, \mechmp provides 7.45$\times$ higher performance  when compared to its floating-point implementation (\mechfp).
%Second, \mech-MP \sr{provides 7.45$\times$ higher} performance over \mech-FP  by leveraging low precision capabilities \sr{provided by} spatial vector computing systems. 
Third, \mechfp, by using floating-point precision, provides 9.74$\times$, 0.95$\times$, \sr{and} 2.23$\times$ performance compared to \cc, \gpf, and \bon, respectively.    We conclude \sr{that} using mixed-precision computation, \mechmp consistently performs better than the baseline basecallers.
% $\times$

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{images/basecalling_speed_aie_onlyCNN4_AMD.pdf}
  \vspace{-13pt}
  \caption{Performance comparison of \mech (using floating-point precision (\mechfp) and mixed-precision (\mechmp))~\sr{and three state-of-the-art basecallers.}\sr{The y-axis is on a logarithmic scale.} }
  \label{fig:perf}
%   \vspace{-12pt}
\end{figure}




%  \begin{figure*}[h]
% \centering
% \begin{subfigure}[h]{0.52\linewidth}
%   \centering
%   \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0cm 0.2cm},clip]{images/model_parameters.pdf}
% %   \caption{High-end and middle-end (\textsf{H\&M}).
%   \vspace{-0.6cm}
% \caption{\label{fig:model_parameters}}
% \end{subfigure}%
% \begin{subfigure}[h]{0.48\linewidth}
%   \centering
%   \vspace{-0.04cm}
%   \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0.2cm 0.2cm}, clip]{images/model_size.pdf}
% %   \caption{High-end and low-end (\textsf{H\&L}).
%   \vspace{-0.6cm}
%   \caption{\label{fig:model_size}}
% \end{subfigure}
% \caption{Comparison of \mech with baseline basecaller in terms of: (a) model parameters, and (b) model size.
% \label{fig:model}}
% % \vspace{-0.5cm}
% \end{figure*}
 
 
 
% Figure~\ref{fig:perf} compares the speed of \mech against baseline policies. We make three major observations. First, \mech-AIE consistently outperfoms all the other basecallers for all the species. \mech improves average performance by $68.24\times$,	$13.22\times$,	$4.61\times$, and $23.02\times$ over \cc, \gp, \gpf, and \bon, respectively. Second, \mech-GPU outperforms baseline basecallers by $15.38\times$,	$2.98\times$,	$1.037\times$,	$5.19\times$ compared to \cc, \gp, \gpf, and \bon, respectively. Third, all the baseline basecallers use floating-point arithmetic precision for all neural network layers. This leads to very high memory bandwidth and processing demands. Fourth, \mech-AIE improves $4.43\times$ performance over \mech-GPU  by leveraging low precision capabilities available on spatial vector computing systems.  
% % $\times$

% Figure compares the basecalling speed with trainable neural network parameters in a basecallers and the total model size, respectively. We draw four observations. First, \bon has the highest number of parameters with
%  $2.71\times$, $1.52\times$, $13.33\times$, and $2.94\times$ more parameters than \cc, \gp, \gpf, and \mech, respecitvely.  Second,  Usually their is linear relationship between the number of parameters and the speed (accuracy) of a model. However,  skip connections in CNN-based basecaller (\cc and \bon) create performance bottleneck by increasing the data lifetime and adds additional computation. Therefore, though \cc has $1.79\times$ and	$2.71$ lower parameters than \gp and \bon, has $5.16\times$	and $2.96\times$ lower performance. Third, \gpf has the lowest number of trainable parameters that are $4.92\times$, $8.79\times$, $13.33\times$, and $4.54\times$ lower than \cc, \gp, \bon, and \mech.  Fourth, \mech provides $2.55\times$,	$4.57\times$, 	and 	$6.93$  smaller size compared to \cc, \gp,  and \bon, respectively. The decrease in model size is due to lower number layers and using lower precision than other baseline basecallers.
 


% \begin{SCfigure}[][h]
%   \centering
% %  - \hspace{0.5cm}
%  \includegraphics[width=0.5\linewidth,trim={0cm 0.02cm 0.2cm 0cm},clip]{images/basecalling_speed_aie_onlyCNN.pdf}
%  \caption{Performance comparison of \mech}
% \label{fig:perf}
% % \vspace{0.0cm}
% \end{SCfigure}


\subsection{Basecalling Accuracy}
\label{subsection:results_base_acc}
\sr{We compare} the basecalling accuracy of \mech against baseline basecallers in Figure \ref{fig:identity}. 
% \gs{Basecalling accuracy} measures the sequence identity of individual basecalled reads relative to a trusted reference.  
We make three major observations. 
First, \mech provides 6.54$\%$ and 0.57$\%$ higher accuracy than CNN-based basecaller \cc and \bon, respectively. Compared to a state-of-the-art RNN-based basecaller, \gpf, \mech achieves 2.97\% higher accuracy. Second, \bon has 2.93$\times$ higher parameters (Figure~\ref{fig:model_compare}(a)) but is 0.57$\%$ less accurate than \mech. %The skip connections in  \bon, similar to \cc, create a performance bottleneck by increasing the data lifetime and adding additional computation. 
% This observation is in line with our discussion about performance comparison in Section~\ref{subsection:results_perf_compare}. Therefore, we observe an accuracy drop instead of a linear relation usually present between the number of parameters and the accuracy of a model.} 
Third, \cc is unable to align half of \emph{Haemophilus haemolyticus M1C132\_1} reads to its reference. Therefore, it is deemed unaligned and cannot be used to determine its read accuracy. %Third,  \gp provides a slightly higher accuracy at the expense of  $4.57\times$ and $1.94\times$ more model size and model parameters, respectively. This increase in model size and parameters leads to $13.22\times$ ($2.98\times$) reductions in basecalling speed when compared to \mech-AIE (\mech-GPU). 
% Therefore, \mech provides both high basecalling speed (Figure~\ref{fig:perf}) and accuracy compared to \gpf. 
We conclude that \gs{\mech provides the highest accuracy compared to other basecallers.}

% Figure \ref{fig:identity} compares the read identity of \mech against baseline policies. We make three major observations. First, compared to CNN-based basecaller  \cc (\bon), \mech provides $6.54\%$ ($0.57\%$) higher	accuracy. Second,  compared to RNN-based basecaller \gpf, \mech provides $2.97\%$ higher accuracy. Therefore, \mech provides both high basecalling speed (Figure~\ref{fig:perf}) and accuracy compared to \gpf. Third,  \gp provides a slightly higher accuracy at the expense of  $4.57\times$ and
% $1.94\times$ more model size and model parameters, respectively. This increase in model size and parameters leads to 
% $13.22\times$ ($2.98\times$) reductions in basecalling speed when compared to \mech-AIE (\mech-GPU). We conclude that \mech is optimized for both basecalling speed and accuracy compared to other basecallers.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{images/read_identity_aie_only_CNN4.pdf}
  \vspace{-15pt}
  \caption{\gs{Basecalling accuracy} comparison of \mech.}
  \label{fig:identity}
%   \vspace{-12pt}
\end{figure}


% \begin{SCfigure*}[][h]
%   \centering
% %  - \hspace{0.5cm}
%  \includegraphics[width=1.5\linewidth,trim={0cm 0.02cm 0.2cm 0cm},clip]{images/read_identity_aie_only_CNN.pdf}
% %  
%  \caption{Read Identity comparison of \mech}
% \label{fig:identity}
% % \vspace{0.0cm}
% \end{SCfigure*}



% \clearpage

\subsection{Downstream Analysis}
\subsubsection{De Novo Assembly}
\sr{We provide} the statistics related to the accuracy, completeness, and contiguity of assemblies we generate using the basecalled reads from \cc, \gpf, \bon, and \mech in Table~\ref{tab:downstream}. 
\input{tables/downstream_new}
We make four key observations. First, assemblies constructed \sr{using reads basecalled} by \mech provide the best reference genome coverage for \emph{all} datasets (``Genome Fraction'' in Table~\ref{tab:downstream}). 
%\srr{We should say both \mech and \bon provide the best reference genome coverage}\ccanf{I would also want to emphasize the fact that \mech is the \textbf{only} tool that is \textbf{consistently} the best, although \bon is not the best for 2 datasets. Do you think the current version is better now? If not, I will just change it to your suggestion.} \srr{looks perfect now}
This means that \sr{assemblies built using \mech-basecalled reads} \sr{are more complete than assemblies built using reads from other basecallers} \sr{since a larger portion of the corresponding reference genomes align to their assemblies using \mech-basecalled reads compared to that of using reads from other basecallers}. 
Second, \sr{assemblies constructed using the \mech reads}  usually have a higher average identity than that of \cc, \gpf, and \bon. %\srr{you can move the average identity column after the genome fraction column to respect the order of explaining them..}\ccanf{Gagan, please see my comments in the downstream.tex file. Could you please update the table accordingly?}. 
These average identity results are tightly in line with the basecalling accuracy results we show in Figure~\ref{fig:identity}. 
\sr{Although} \gpf provides \sr{a} higher average identity for the Haemophilus haemolyticus M1C132\_1 dataset (i.e., 91.51\%), \sr{the} genome coverage \sr{provided by} \sr{\gpf} is 2.2$\%$ lower than that \sr{provided by} \mech for the same dataset. \sr{This means a large} portion of the assembly \sr{provided by} \gpf ~\sr{has low-quality} regions as 
%\srr{is this reason correct? how is the reference genome aligned to regions?}\ccanf{please see the definition of Genome Fraction above. dnadiff aligns ref to assembly (and vice-versa but we only report the ``percentage of ref aligned to assembly'' numbers). Is this still unclear?}\srr{looks good}
the reference genome cannot align to these regions due to high dissimilarity.
Third, \sr{assemblies constructed using the \mech reads} provide better completeness and contiguity as they have 1)~assembly lengths closer to their corresponding reference genomes and 2)~higher NG50 results in most cases than \sr{those constructed using the \gpf and \bon reads}. 
Fourth, although \cc usually provides the best results in terms of the assembly lengths and NG50 results, we suspect that these high NG50 and assembly length results are caused due to highly repetitive and inaccurate regions in these assemblies due to their poor genome fraction and average GC content results. The average GC content of the assemblies constructed using the \cc reads is significantly distant from the GC content of their corresponding reference genomes in most cases. This poor genome fraction and average GC content results \sr{suggest} that such large NG50 and assembly length values from \cc may also be caused \sr{by} poorly basecalled reads that lead to unresolved repetitive regions (i.e., bubbles in genome assembly graphs) or \sr{a} strong bias toward certain error types (i.e., homopolymer insertions of a certain base) in the assembly~\cite{ferrarini_evaluation_2013, chen_effects_2013}.
% \input{tables/downstream_new}
% sr{to} the reference genome do not show \sr{a} strong bias \srr{this reasoning is not clear} towards certain sequencing error types (i.e., homopolymer insertions of a certain base) or unresolved repetitive regions (i.e., bubbles in genome assembly graphs), as average GC content of these assemblies are still close to the GC content of the reference genome . 

% The average GC content of the assemblies constructed using the \cc reads is significantly distant from the GC content of their corresponding reference genomes in most cases. This \sr{suggests} that such large NG50 values may also be caused \sr{by} unresolved repeats and sequencing errors appended in these assemblies as we discussed earlier \srr{where? and provide citations here}. 

We conclude that, in most cases, the reads \sr{basecalled by} \mech lead to high\sr{er} quality, \sr{more} contiguous, and \sr{more} complete assemblies than that \sr{provided by other state-of-the-art basecallers,} \cc, \gpf, and \bon.
% \input{tables/downstream}



\subsubsection{Read Mapping}
\sr{We provide} the comparison of \mech with \cc, \gpf, and \bon in terms of \sr{the total number of base} mismatches, \sr{the total number of  mapped} bases, \sr{the total number of mapped} reads, and \sr{the total number of unmapped reads} in Figure~\ref{fig:samtool}(a), \ref{fig:samtool}(b), \ref{fig:samtool}(c), and \ref{fig:samtool}(d), respectively.
% \srr{I don't have te raw data so i cannot infer by how many x \mech is better than others, please add this numbers to this section to make it more scientific}


\begin{figure*}[h]
\centering
\begin{subfigure}[h]{0.5\textwidth}
  \centering
  \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0cm 0.2cm},clip]{images/samtools_mismatch3.pdf}
%   \caption{High-end and middle-end (\textsf{H\&M}).
  \vspace{-0.6cm}
% \caption{\label{fig:sam_mis}}
\end{subfigure}%
\begin{subfigure}[h]{0.5\textwidth}
  \centering
  \vspace{-0.04cm}
  \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0.2cm 0.2cm}, clip]{images/samtools_mapped_bases3.pdf}
%   \caption{High-end and low-end (\textsf{H\&L}).
  \vspace{-0.6cm}
\end{subfigure}

\begin{subfigure}[h]{0.49\textwidth}
  \centering
  \vspace{-0.04cm}
  \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0.2cm 0.2cm}, clip]{images/samtools_read_map3.pdf}
%   \caption{High-end and low-end (\textsf{H\&L}).
  \vspace{-0.6cm}
\end{subfigure}
\begin{subfigure}[h]{0.5\textwidth}
  \centering
  \vspace{-0.04cm}
  \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0.2cm 0.2cm}, clip]{images/samtools_read_unmap3.pdf}
%   \caption{High-end and low-end (\textsf{H\&L}).
  \vspace{-0.6cm}
\end{subfigure}
\caption{Comparison of \mech for (a) mismatches, (b) bases mapped, (c) reads mapped, and (d) reads unmapped.
\label{fig:samtool}}
% \vspace{-0.5cm}
\end{figure*}

\sr{We make four key observations.
First, \mech provides the lowest number of base mismatches, \gs{which are 26.49$\%$, 21.35$\%$, and 1.82$\%$ lower} compared to \cc, \gpf, and \bon, respectively. 
This indicates that \mech provides more accurate basecalled reads that share large similarity with the reference genome.
This is in line with the fact that \mech provides the highest basecalling accuracy, as we evaluate in Section~\ref{subsection:results_base_acc}.
Second, \mech provides, on average, the highest number of mapped bases, \gs{which are 0.56$\%$, 0.83$\%$, and 0.77$\%$ higher} compared to \cc, \gpf, and \bon.
Mapping more bases to the target reference genome confirms that the careful design and optimizations we perform when building \mech have no trade-offs with the basecalling accuracy.
Third, unlike \cc, \mech, \gpf, and \bon all provide a high number of mapped reads.
However, \mech is the only basecaller that provides high-quality reads that have the highest number of base matches and the lowest number of base mismatches.
Fourth, \mech provides the lowest number of unmapped reads compared to \cc, \gpf, and \bon. \gs{\mech achieves 78.04$\%$, 19.98$\%$, and 14.23$\%$ lower unmapped reads compared to \cc, \gpf, and \bon, respectively.}
This indicates that using \cc, \gpf, and \bon wastes a valuable expensive resource, i.e., sequencing data, by not mapping reads to the reference genome due to basecalling inaccuracies during basecalling.
If a read is flagged as unmapped read during read mapping, then this read is excluded from all the following analysis steps affecting the overall downstream analysis results.

We conclude that \mech reads provides the highest-quality read mapping results with the largest number of mapped bases and mapped reads. 
}


\subsection{\strim Analysis}
Figure~\ref{fig:strim_analysis} shows the effect of \strim on validation accuracy using three different strides at which we remove a skip connection from a block. Our \nas-designed model has five blocks with skip connections. We make three observations. First, \texttt{Stride 1} converges faster to the baseline accuracy compared to \texttt{Stride 2} and \texttt{Stride 3}. By using \texttt{Stride 1}, we quickly remove all the skip connections (in five epochs) giving enough fine-tuning iterations for the model to recover its loss in accuracy. Second, all the strides show the maximum drop in accuracy (1.27\%-2.88\%) when removing skip connections from block 1 and block 4. We observe these blocks consist of the highest number of neural network model parameters due to the skip connections (30.73\% and 25.62\% of the total model parameters are present in skip connections in block 1 and block 4, respectively). Therefore, the model requires more training epochs to recover its accuracy after the removal of skip connections from these blocks. Third, a lower stride can get rid of skip connections faster than using a higher stride. However, all strides eventually converge to the baseline accuracy at the expense of more training iterations. We conclude that \strim provides an efficient mechanism to remove hardware-unfriendly skip connections without any loss in basecalling accuracy.
  \begin{figure}[h]
  \centering
\includegraphics[width=\linewidth,trim={0.2cm 0.85cm 0.4cm 0.3cm},clip]{images/strim.pdf}
  \vspace{-35pt}
  \caption{Effect of different strides while removing skip connections.}
  \label{fig:strim_analysis}
%   \vspace{-12pt}
\end{figure}


\subsection{Effect of Pruning \mech}
Figure~\ref{fig:res_prune_rubicon} shows the effect of pruning \mech using two different pruning methods: unstructured element pruning  and structured channel pruning.

We make four major observations. First, we can remove up to 15$\%$ and 5$\%$ of model parameters providing 1.18$\%$ and 1.05$\%$ reductions in model size without any loss in accuracy by using unstructured pruning and structured pruning, respectively. However,  unstructured pruning is unsuitable for hardware acceleration due to high sparsity, and structured pruning provides minimal savings in model size (or parameters).  Therefore, we do not apply these pruning techniques to optimize \mech further. Second, we observe a drop in accuracy for pruning levels greater than 15\% and 5\% for unstructured and structured pruning, respectively. This shows that \nas found an optimal architecture as there is little room for pruning \mech further without loss in accuracy.

 Third, we observe that the \emph{knee point} for unstructured pruning  and structured pruning lies at 90$\%$ and 50$\%$, where we achieve 80.65$\%$ and 70.10$\%$ of accuracy with 9.99$\times$ and 1.99$\times$ savings model size, respectively. After the knee point, we observe a sharp decline in accuracy. Fourth, below the knee point, we can trade accuracy for speed to further accelerate \mech for hardware computation and resources by removing unimportant network weights. We  conclude that pruning provides a tradeoff between accuracy and model size that can lead to further reductions in processing and memory demands for \mech, depending on the type of device on which genomic analyses would be performed. 
  \begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth,trim={0.2cm 0.85cm 0.4cm 0.3cm},clip]{images/res_pruning_rubicon1.pdf}
%   \vspace{-15pt}
  \caption{Effect of pruning \mech on: (a) validation accuracy and (2) model size.}
  \label{fig:res_prune_rubicon}
%   \vspace{-12pt}
\end{figure}

% We make four major observations. First, we can remove up to $20\%$, $20\%$, and $10\%$ of model parameters without loss in accuracy by using unstructured pruning, \gp, and structured pruning, respectively. We achieve $19.53\%$,$19.93\%$, and $9.94\%$ reductions in model size with unstructured pruning, \gp, and structured pruning, respectively. Since unstructured pruning is unsuitable for hardware acceleration due to high sparsity and structured pruning provides the lowest savings inmodel size (or parameters), we use \gp with $20\%$ to achieve our final \mech model.  Second, for pruning levels higher than \> $20\%$, $20\%$, and $10\%$ for unstructured pruning, \gp, and structured pruning, respectively, we observe a drop in accuracy. This shows that \nas found an optimal architecture as there is not much room for pruning the basecaller further without loss in accuracy. Third, we observe the \emph{break-even point}\footnote{We define break-even point as the point where the basecaller is not able to to basecall at an acceptable level of accuracy.} for unstructured pruning, \gp, and structured pruning lies at $\%$, $\%$, and $50\%$, respectively. After the break-even point, we observe a sharp decline in accuracy. Fourth, below the break-even point, we can trade accuracy for speed to further optimize our basecaller for hardware computation and resources by removing unimportant network weights. We  conclude that pruning provides a tradeoff between accuracy and model size that can lead to further reductions in processing and memory demands for a basecaller.
% By further using our SkipClip approach and hardware-aware pruning,
% RUBICON achieves a 9.1x and 2.9× reduction in neural network model size and parameters, respectively.


\subsection{Explainability Into \nas Results}
We perform an explainability analysis to understand our results
further and explain \texttt{QABAS}’s decisions. 
The search performed by \nas provides insight into whether \nas has learned meaningful representations in basecalling. In Figure~\ref{fig:explainability}(a) and \ref{fig:explainability}(b), we extract the number of model parameters and precision of each parameter in a neural network layer to calculate the total size for each layer for \bon and \mech, respectively. We make three observations. First, \nas uses higher bits in the initial layers than the final layers in \mech. \nas learns that the input to \mech uses an analog squiggle that requires higher precision, while the output is only the nucleotide bases (A, C, G, T), which can be represented using lower precision. 

  \begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth,trim={0.2cm 0.85cm 0.4cm 0.3cm},clip]{images/layer_size_1.pdf}
%   \vspace{-15pt}
  \caption{Layer size comparison for basecallers: (a) \bon, and (b) \mech.}
  \label{fig:explainability}
%   \vspace{-12pt}
\end{figure}


Second, \mech uses 1.97$\times$ less number of neural network layers than \bon while providing similar or higher basecalling accuracy on the evaluated species (Section~\ref{subsection:results_base_acc}). Thus, the superior performance of a basecaller architecture is not explicitly linked to its model complexity, and  \nas-designed models are parameter efficient.  Third, \bon uses the same single-precision floating-point representation (FP32) for all neural network  layers, which leads to very high memory bandwidth and processing demands. Whereas \mech has every layer quantized to a different quantization domain. We conclude that \nas provides an efficient automated method for designing more accurate, efficient, and hardware-friendly genomic basecallers compared to expert-designed basecallers.

 

 
 
 
% \begin{figure*}[h]
% \centering
% \begin{subfigure}[h]{\textwidth}
%   \centering
%   \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0cm 0.2cm},clip]{images/bonito_layer_size.pdf}
% %   \caption{High-end and middle-end (\textsf{H\&M}).
%   \vspace{-0.6cm}
% \caption{\label{fig:exp_bonito}}
% \end{subfigure}%

% \begin{subfigure}[h]{\textwidth}
%   \centering
%   \vspace{-0.04cm}
%   \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0.2cm 0.2cm}, clip]{images/rubicon_layer_size.pdf}
% %   \caption{High-end and low-end (\textsf{H\&L}).
%   \vspace{-0.6cm}
%   \caption{\label{fig:exp_rubi}}
% \end{subfigure}
% \caption{Layer size comparison
% \label{fig:explainability}}
% % \vspace{-0.5cm}
% \end{figure*}




% We replace the mechanism in minimap2 that generates hash values for seeds with \proposal to find fuzzy seed matches when performing end-to-end read overlapping and read mapping. We also incorporate the \texttt{\proposal-I} and \texttt{\proposal-S} mechanisms in the implementation and provide the user to choose either of these mechanisms when using \proposal. We provide a set of default parameters we optimize based on sequencing technology and the application to perform (e.g., read overlapping). We explain the details regarding the parameters in Supplementary Tables~\ref{supptab:ovpars} and~\ref{supptab:mappars}. We determine these default parameters empirically by testing the performance and accuracy of \proposal with different values for some parameters (i.e., k-mer length, number of k-mers to include in a seed, and the window length) as shown in Supplementary Table~\ref{supptab:parameter_exploration}. We show the trade-offs between the seeding mechanisms \texttt{\proposal-I} and \texttt{\proposal-S} in Supplementary Figures~\ref{suppfig:overlap_perf-blend} and ~\ref{suppfig:read_mapping_perf-blend} and Supplementary Tables~\ref{supptab:overlap_assembly-blend} -~\ref{supptab:mapping_accuracy-blend} regarding their performance and accuracy.
% % We provide the design of the SIMD implementation for calculating the hash values for seeds in \incomplete{Supplementary Section X}.

% For our evaluation, we use real and simulated read datasets as well as their corresponding reference genomes. We list the details of these datasets in Table~\ref{tab:dataset}. To evaluate \proposal in several common scenarios in read overlapping and read mapping, we classify our datasets into three categories: 1)~highly accurate long reads (i.e., PacBio HiFi), 2)~erroneous long reads (i.e., PacBio CLR and Oxford Nanopore Technologies), and 3)~short reads (i.e., Illumina). We use PBSIM2~\cite{ono_pbsim2_2021} to simulate erroneous reads from both PacBio CLR and Oxford Nanopore Technologies (ONT). HiFi and Illumina reads are from real datasets. To use realistic depth of coverage, we use SeqKit~\cite{shen_seqkit_2016} to down-sample the original \emph{E. coli}, and \emph{D. ananassae} reads to $100\times$ and $50\times$ sequencing depth of coverage, respectively. For \emph{D. ananassae} and \emph{E. coli} genomes, the reference genomes are the high-quality assemblies generated using the same read sets we use for these genomes~\cite{tvedte_comparison_2021}.

% % \vspace{-10pt}
% \begin{table}[tbh]
% \centering
% \caption{Details of datasets used in evaluation.}\label{tab:dataset}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{@{}llrrll@{}}\toprule
% \textbf{Organism} & \textbf{Library} & \textbf{Reads (\#)} & \textbf{Seq.} & \textbf{SRA} & \textbf{Reference}\\
% \textbf{} & \textbf{} & \textbf{} & \textbf{Depth} & \textbf{Accession} & \textbf{Genome}\\\midrule
% \emph{Human CHM13} & PacBio HiFi & 3,167,477 & 16 & SRR1129212{2-3} & GCA\_009914755.3 \\\cmidrule{1-6}
% \emph{D. ananassae} & PacBio HiFi & 1,195,370 & 50 & SRR11442117 & \cite{tvedte_comparison_2021} \\\cmidrule{1-6}
% \emph{E. coli} & PacBio HiFi & 38,703 & 100 & SRR11434954 & \cite{tvedte_comparison_2021}\\\cmidrule{1-6}
% \emph{Yeast} & PacBio CLR$^{*}$ & 270,849 & 200 & Simulated P6-C4 & GCA\_000146045.2\\
% & Oxford Nanopore Tech.$^{*}$ & 135,296 & 100 & Simulated R9.5 & GCA\_000146045.2\\
% & Illumina MiSeq & 3,318,467 & 80 & ERR1938683 & \\\bottomrule
% \multicolumn{6}{l}{\footnotesize $^{*}$ We use PBSIM2 to generate the simulated PacBio and ONT reads from the Yeast genome.}\\
% \multicolumn{6}{l}{We include the simulated chemistry under SRA Accession.}\\
% \end{tabular}}
% % \vspace{-12pt}
% \end{table}

% We evaluate \proposal based on two use cases: 1)~read overlapping and 2)~read mapping to a reference genome. For read overlapping, we perform \emph{all-vs-all overlapping} to find all pairs of overlapping reads within the same dataset (i.e., the target and query sequences are the same set of sequences). To evaluate the efficiency of overlaps, we calculate the average length of overlaps and the number of seed matches per overlap to calculate the overlap statistics. To evaluate the quality of overlapping reads based on the accuracy of the assemblies we generate from overlaps, we use miniasm~\cite{li_minimap_2016}. We use miniasm because it does not perform error correction when generating \emph{de novo} assemblies, which allows us to directly assess the quality of overlaps without using additional approaches for improving the accuracy of assemblies. We use \texttt{mhap2paf.pl} package as provided by miniasm to convert the output of MHAP to the format miniasm requires (i.e., PAF). We use QUAST~\cite{gurevich_quast_2013} to measure statistics related to the contiguity, length, and the accuracy of \emph{de novo} assemblies, such as the overall assembly length, largest contig, NG50 and NGA50 statistics (i.e., statistics related to the length of the shortest contig at the half of the overall reference genome length), k-mer completeness (i.e., amount of shared k-mers between the reference genome and an assembly), the ratio of the misassembly length to the actual assembly length, and GC content (i.e., the ratio of G and C bases to A and T bases in an assembly). We use dnadiff~\cite{marcais_mummer4_2018} to measure the accuracy of \emph{de novo} assemblies based on 1)~the average identity of an assembly when compared to its reference genome and 2)~the fraction of overall bases in a reference genome that align to a given assembly (i.e., genome fraction). We compare \proposal with minimap2~\cite{li_minimap2_2018} and MHAP~\cite{berlin_assembling_2015} for read overlapping. For the Human CHM13 genome, MHAP generates a large output such that we cannot generate the assembly as miniasm exceeds the memory space we have in our system (i.e., 1TB).

% For read mapping, we map all reads in a dataset (i.e., query sequences) to their corresponding reference genome (i.e., target sequence). To evaluate the coverage of read mapping, we use BEDTools~\cite{quinlan_bedtools_2010} and Mosdepth~\cite{pedersen_mosdepth_2018} to calculate the breadth of coverage (i.e., percentage of bases in a reference genome covered by at least one read) and mean depth of coverage (i.e., the average number of read alignments per base in a reference genome), respectively. To evaluate the quality of read mapping, we use BAMUtil~\cite{jun_efficient_2015} to calculate the mapping rate (i.e., number of aligned reads) and rate of properly paired reads for paired-end mapping. To evaluate the accuracy of read mapping, we measure 1)~the overall number of true mappings with very high mapping quality (i.e., 60 mapping quality score) and 2)~the overall error rate in read mapping. To generate these results, we use the \texttt{paftools mapeval} tool provided by minimap2 that takes the reads from PBSIM2 such that the read IDs are annotated with their true mapping information and finds incorrect mappings in the SAM files provided by read mapping tools.
% % We map reads to their corresponding reference genomes as specified in Table~\ref{tab:dataset}.
% We compare \proposal with minimap2, LRA~\cite{ren_lra_2021}, Winnowmap2~\cite{jain_long-read_2022, jain_weighted_2020}, and S-conLSH~\cite{chakraborty_conlsh_2020, chakraborty_s-conlsh_2021} when performing read mapping. We do not evaluate 1)~LRA, Winnowmap2, and S-conLSH when mapping paired-end short reads as these tools do not support mapping paired-end short reads, and 2)~S-conLSH for the \emph{D. ananassae} genome as S-conLSH crashes due to a segmentation fault when mapping reads to the \emph{D. ananassae} reference genome.

% For both use cases, we use the \texttt{time} command in Linux to evaluate the performance and peak memory footprints.
% % We assign 32 threads for each tool, including \proposal, to run the tools multi-threaded.
% When applicable, we use the default parameters of all the tools suggested for certain use cases and sequencing technologies (e.g., mapping HiFi reads in minimap2). Since minimap2 and MHAP do not provide default parameters for read overlapping using HiFi reads, we use the parameters that HiCanu~\cite{nurk_hicanu_2020} uses for overlapping HiFi reads with minimap2 and MHAP. We provide the details regarding the parameters and versions we use for each tool in Supplementary Tables~\ref{supptab:ovpars}, \ref{supptab:mappars}, and \ref{supptab:version}. To show that \proposal can provide better accuracy with the same set of parameters, we use the same parameters that \proposal uses in minimap2 and show the performance and accuracy results in Supplementary Figures~\ref{suppfig:overlap_perf-eq} and~\ref{suppfig:read_mapping_perf-eq} and Supplementary Tables~\ref{supptab:overlap_assembly-eq} -~\ref{supptab:mapping_accuracy-eq}.
% %We choose MHAP and S-conLSH for comparison because these tools are state-of-the-art methods that find fuzzy seed matches for read overlapping and read mapping, respectively. Minimap2, BLASR, and Winnowmap2 are widely used tools for read overlapping or read mapping of long or short reads.

% % \subsubsection{Evaluating the K-mer Selection Mechanisms}
% % Figure~\ref{fig:kmer_selection} shows the comparisons between each k-mer selection mechanism that \proposal provides: 1) All K-mers, 2) Shifted, and 3) Minimizer+ on various metrics. We make four key observations. We find that the Shifted k-mer selection mechanism provides the best performance and memory efficiency compared to other selection mechanism. Second, Minimizer+ selection mechanism is faster and more memory efficient than the All K-mers selection mechanism. Third, when we use the overlaps generated using each of the selection mechanism to construct \emph{de novo} assemblies, almost all the assemblies have the same similarity (i.e., assembly identity) compared to their corresponding reference genome. Fourth, the assemblies generated using the overlaps from the Shifted k-mer selection mechanism covers the smallest area of the reference genome (i.e., genome fraction) compared to other selection mechanisms. We conclude that the Shifted k-mer selection mechanism is the best selection method when the accuracy does not have the utmost importance while Minimizer+ provides the best trade-off between performance, memory and accuracy. Thus, we use the Minimizer+ selection mechanism when evaluating \proposal for overlap finding and read mapping use cases as these use cases require high accuracy.

% % \begin{figure}[tbh]
% % \includegraphics[width=\linewidth]{figures/kmerselection_parameters.pdf}
% % \vspace{-20pt}
% % \caption{Effects of the k-mer selection mechanisms on various metrics}
% % \label{fig:kmer_selection}
% % \vspace{-12pt}
% % \end{figure}

% % \vspace{-12pt}
% \subsection{Use Case 1: Read Overlapping}\label{subsec:overlap}
% \subsubsection{Performance}\label{subsubsec:overlap-performance}

% Figure~\ref{fig:overlap_perf} shows the CPU time and peak memory footprint comparisons for read overlapping. We make the following five observations.
% First, \proposal provides speedup by \movpM-\ovpM (on average \avgovpM) and \movpMH-\ovpMH (on average \avgovpMH) while reducing the memory footprint by \movmM-\ovmM (on average \avgovmM) and \movmMH-\ovmMH (on average \avgovmMH) compared to minimap2 and MHAP, respectively. \proposal is significantly more performant and provides less memory overheads than MHAP because MHAP generates many hash values for seeds regardless of the length of the sequences, while \proposal provides windowing guarantees when generating seeds, which allows sampling the number of seeds based on the sequence length.
% Second, when considering only HiFi reads, \proposal provides even higher speedup, on average, by \avgovpHM and \avgovpHMH while reducing the memory footprint by \avgovmHM and \avgovmHMH compared to minimap2 and MHAP, respectively.
% HiFi reads allow \proposal to increase the window length (i.e., $w=200$) when finding the minimizer k-mer of a seed, which improves the performance and reduces the memory overhead without reducing the accuracy. This is possible mainly because \proposal can find \emph{both} fuzzy and exact seed matches, which enables \proposal to find \emph{unique} fuzzy seed matches that minimap2 \emph{cannot} find due to its exact-matching seed requirement.
% Third, we find that \proposal requires less than 16GB of memory space for \emph{all} the datasets, making it largely possible to find overlapping reads even with a personal computer with relatively small memory space.
% % Fourth, we show that \proposal significantly reduces the memory footprint compared to minimap2, MHAP, and BLASR, on average, by \avgovmM (up to \ovmM), \avgovmMH (up to \ovmMH), and \avgovmB (up to \ovmB), respectively.
% \proposal has a lower memory footprint because 1)~\proposal uses as many seeds as the number of minimizer k-mers per sequence to benefit from the reduced storage requirements that minimizer k-mers provide, and 2)~the window length is larger than minimap2 as \proposal can tolerate increasing this window length with the fuzzy seed matches without reducing the accuracy.
% Fourth, when using erroneous reads (i.e., PacBio CLR and ONT),  \proposal provides better performance but similar memory overhead to minimap2. The set of parameters we use for erroneous reads prevents \proposal from using large windows (i.e., $w=10$ instead of $w=200$) without reducing the accuracy of read overlapping. This causes \proposal to use many seeds, which requires higher memory than when using erroneous reads.
% Fifth, we use the same set of parameters (i.e., the seed length and the window length) with minimap2 that \proposal uses to observe the benefits that \proposal provides with PacBio CLR and ONT datasets. We cannot perform the same experiment for the HiFi datasets because \proposal uses seeds of length $31$, which minimap2 cannot support due to the maximum seed length limitation in its implementation (i.e., max. $28$). We call this version of minimap2, minimap2-Eq. We show in Supplementary Figure~\ref{suppfig:overlap_perf-eq} that minimap2-Eq performs, on average, $\sim10\%$ better than \proposal when using the same set of parameters while providing worse accuracy than \proposal when generating the assemblies, as shown in Supplementary Table~\ref{supptab:overlap_assembly-eq}. Performing worse than minimap2-Eq is expected because we apply the same sampling rate and k-mer size and use a much simpler hashing mechanism than \proposal, which leads to using a similar number of seeds while processing them faster with cheaper hashing. The main benefit of \proposal is to provide overall higher accuracy than both the baseline minimap2 and minimap-Eq, which we can achieve by finding unique fuzzy seed matches that minimap2 cannot find.
% We conclude that \proposal is significantly more memory-efficient and faster than other tools to find overlaps, especially when using HiFi reads with its ability to sample many seeds using high values of $w$ without reducing the accuracy.

% % \vspace{-8pt}
% \begin{figure}[tbh!]
% \centering
% \includegraphics[width=0.95\linewidth]{figures/overlap_finding-perf-all_perf.pdf}
% % \vspace{-5pt}
% \caption{CPU time and peak memory footprint comparisons of read overlapping.}
% \label{fig:overlap_perf}
% % \vspace{-12pt}
% \end{figure}

% % \vspace{-12pt}
% \subsubsection{Overlap Statistics}\label{subsubsec:overlap-statistics}

% Figure~\ref{fig:overlap_stats} shows the average length of overlaps and the average number of seed matches that each tool finds to identify the overlaps between reads. We make the following two key observations.
% First, we observe that \proposal finds overlaps longer than minimap2 and MHAP can find in most cases. \proposal can 1)~uniquely find the fuzzy seed matches that the exact-matching-based tools cannot find and 2)~perform chaining on these fuzzy seed matches to increase the length of overlap using many fuzzy seed matches that are relatively close to each other. Finding more distinct seeds and chaining these seeds enable \proposal to find longer overlaps than other tools.
% Second, \proposal uses significantly fewer seed matches than other tools, up to \ovmaxseed, to find these longer overlaps. This is mainly because \proposal needs much fewer seeds as it uses 1)~higher window lengths than minimap2 and 2)~provides windowing guarantees, unlike MHAP.
% We conclude that the performance and memory-efficiency improvements in read overlapping are proportional to the reduction in the seed matches that \proposal uses to find overlapping reads. Thus, reducing the number of seed matches helps improve the performance and memory space efficiency of \proposal.

% % \vspace{-8pt}
% \begin{figure}[tbh]
% \centering
%   \includegraphics[width=\linewidth]{figures/overlap_stats-all_combined_metrics.pdf}
% %   \vspace{-15pt}
%     \caption{Average length of overlaps and average number of seeds used to find a single overlap.}
%     \label{fig:overlap_stats}
%     % \vspace{-12pt}
% \end{figure}

% % \vspace{-12pt}
% \subsubsection{Assembly Quality Assessment}\label{subsubsec:overlap-assembly}

% Our goal is to assess the quality of assemblies generated using the overlapping reads found by \proposal, minimap2, and MHAP. Table~\ref{tab:overlap_assembly} shows the statistics related to the accuracy of assemblies (i.e., the 7 statistics on the left-most part of the table) and the statistics related to assembly length and contiguity (i.e., the 3 statistics on the right-most part of the table) when compared to their respective reference genomes. We make the following five key observations based on the accuracy results of assemblies.
% First, we observe that we can construct more accurate assemblies in terms of average identity and k-mer completeness when we use the overlapping reads that \proposal finds than those minimap2 and MHAP find. These results show that the assemblies we generate using the \proposal overlaps are more similar to their corresponding reference genome. \proposal can find unique and accurate overlaps using fuzzy seed matches that lead to more accurate \emph{de novo} assemblies than the minimap2 and MHAP overlaps due to their lack of support for fuzzy seed matching.
% Second, we observe that assemblies generated using \proposal overlaps usually cover a higher fraction of the reference genome than minimap2 and MHAP overlaps.
% Third, although the average identity and genome fraction results seem mixed for the PacBio CLR and ONT reads such that \proposal is best in terms of either average identity or genome fraction, we believe these two statistics should be considered together (e.g., by multiplying both results). This is because a highly accurate but much smaller fraction of the assembly can align to a reference genome, giving the best results for the average identity. We observe that this is the case for the \emph{D. ananassae} and \emph{Yeast} (PacBio CLR) genomes such that MHAP provides a very high average identity only for the much smaller fraction of the assemblies than the assemblies generated using \proposal and minimap2 overlaps. Thus, when we combine average identity and genome fraction results, we observe that \proposal consistently provides the best results for all the datasets.
% Fourth, \proposal and minimap2 usually provide the best results when the assemblies are aligned to the reference genome using QUAST in terms of aligned length, misassembly ratio, and NGA50 statistics. In most cases, QUAST cannot generate these statistics for the MHAP results as usually a small portion of the assemblies align the reference genome when the MHAP overlaps are used.
% Fifth, we find that assemblies generated from \proposal overlaps are less biased than minimap2 and MHAP overlaps. Our observation is based on the average GC content in Table~\ref{tab:overlap_assembly} and the GC distributions in Supplementary Figure~\ref{suppfig:overlap_gc-hist} that are mostly closer to the reference genomes. We conclude that \proposal overlaps yield assemblies with higher accuracy and less bias than the assemblies that the minimap2 and MHAP overlaps generate in most cases.

% \begin{table*}[tbh]
% % \vspace{-10pt}
% \centering
% \caption{Assembly quality comparisons.}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{@{}llrrrrrrr|rrr@{}}\toprule
% \textbf{Dataset} 	  & \textbf{Tool} & \textbf{Average}	   & \textbf{Genome}		& \textbf{K-mer}	   & \textbf{Aligned}	   & \textbf{Misassembly} & \textbf{NGA50} & \textbf{Average} & \textbf{Assembly}	  & \textbf{Largest}	  & \textbf{NG50}  \\
% 					  & 			  & \textbf{Identity (\%)} & \textbf{Fraction (\%)} & \textbf{Compl. (\%)} & \textbf{Length (Mbp)} & \textbf{Ratio (\%)}  & \textbf{(Kbp)} & \textbf{GC (\%)} & \textbf{Length (Mbp)} & \textbf{Contig (Mbp)} & \textbf{(Kbp)} \\\midrule
% \emph{Human CHM13} 	  & \proposal 	  & \textbf{99.8526}	   & \textbf{98.4847}		& \textbf{90.15}	   & 3,092.59 			   & \textbf{0.0108}	  & 5,442 		   & \textbf{40.78}	  & \textbf{3,095.210} 	  & 22.840 				  & 5,442 		   \\
% 					  & minimap2 	  & 99.7421				   & 97.1493 				& 83.05 			   & \textbf{3,095.49} 	   & 0.0503 			  & \textbf{7,133} & 40.71 			  & 3,100.974 			  & \textbf{47.139}		  & \textbf{7,134} \\
% 					  & MHAP 		  & N/A					   & N/A 					& N/A 				   & N/A 				   & N/A 				  & N/A 		   & N/A 			  & N/A 				  & N/A 				  & N/A 		   \\
% 					  & Reference 	  & 100 				   & 100 					& 100 				   & 3,054.83 			   & 0 					  & 154,260 	   & 40.85 			  & 3,054.832 			  & 248.387 			  & 154,260 	   \\\cmidrule{1-12}
% \emph{D. ananassae}   & \proposal 	  & \textbf{99.7856}	   & \textbf{97.2308}		& \textbf{86.43}	   & 240.39 			   & 0.1230 			  & \textbf{792}   & \textbf{41.75}	  & \textbf{247.153} 	  & \textbf{6.233}		  & \textbf{799}   \\
% 					  & minimap2 	  & 99.7044 			   & 96.3190 				& 72.33 			   & \textbf{289.45} 	   & \textbf{0.1013}	  & 273 		   & 41.68 			  & 298.280 			  & 4.434 				  & 273 		   \\
% 					  & MHAP 		  & 99.5551 			   & 0.7276 				& 0.21 				   & 2.29 				   & 0.2197 			  & N/A 		   & 42.07 			  & 2.350 				  & 0.286 				  & N/A 		   \\
% 					  & Reference 	  & 100 				   & 100 					& 100 				   & 213.81 			   & 0 					  & 26,427 		   & 41.81 			  & 213.818 			  & 30.673 				  & 26,427 		   \\\cmidrule{1-12}
% \emph{E. coli} 		  & \proposal 	  & \textbf{99.8320} 	   & \textbf{99.8801} 		& \textbf{87.91}	   & \textbf{5.12} 		   & \textbf{0.0344}	  & \textbf{3,417} & \textbf{50.53}	  & 5.122 				  & \textbf{3.417}		  & \textbf{3,417} \\
% 					  & minimap2 	  & 99.7064 			   & 99.8748 				& 79.27 			   & 5.09 				   & 0.3068 			  & 3,087 		   & 50.47 			  & \textbf{5.042} 		  & 3.089 				  & 3,089 		   \\
% 					  & MHAP 		  & N/A 				   & N/A 					& N/A 				   & N/A 				   & N/A 				  & N/A 		   & N/A 			  & 5.094 				  & N/A 				  & N/A 		   \\
% 					  & Reference 	  & 100 				   & 100 					& 100 				   & 5.05 				   & 0 					  & 4,945 		   & 50.52 			  & 5.046 				  & 4.946 				  & 4,945 		   \\\cmidrule{1-12}
% \emph{Yeast (PacBio)} & \proposal 	  & 89.1582 	   		   & \textbf{97.6297} 		& \textbf{11.13}	   & \textbf{0.227} 	   & N/A 				  & N/A 		   & \textbf{38.80}	  & 13.679 				  & 1.105 				  & 551 		   \\
% 					  & minimap2 	  & 88.9002 			   & 96.9709 				& 9.74 				   & 0.195 				   & N/A 				  & N/A 		   & 38.85 			  & \textbf{12.333} 	  & \textbf{1.561}		  & \textbf{828}   \\
% 					  & MHAP 		  & \textbf{89.2182} 	   & 88.5928 				& 9.5 				   & 0.186 				   & N/A 				  & N/A 		   & 38.81 			  & 10.990 				  & 1.024 				  & 436 		   \\
% 					  & Reference 	  & 100 				   & 100 					& 100 				   & 12.16 				   & 0 					  & 924 		   & 38.15 			  & 12.157 				  & 1.532 				  & 924 		   \\\cmidrule{1-12}
% \emph{Yeast (ONT)} 	  & \proposal 	  & \textbf{89.7622} 	   & 99.2982 				& \textbf{13.68}	   & \textbf{0.377} 	   & N/A 				  & N/A 		   & \textbf{38.66}	  & \textbf{12.164} 	  & 1.554 				  & 825 		   \\
% 					  & minimap2 	  & 88.9393 			   & \textbf{99.6878}		& 12.06 			   & 0.328 				   & N/A 				  & N/A 		   & 38.74 			  & 12.373 				  & \textbf{1.560}		  & \textbf{942}   \\
% 					  & MHAP 		  & 89.1970 			   & 89.2785 				& 11.35 			   & 0.297 				   & N/A 				  & N/A 		   & 38.84 			  & 10.920 				  & 1.443 				  & 619 		   \\
% 					  & Reference 	  & 100 				   & 100 					& 100 				   & 12.16 				   & 0 					  & 924 		   & 38.15 			  & 12.157 				  & 1.532 				  & 924 		   \\\cmidrule{1-12}
% \multicolumn{12}{l}{\footnotesize Best results are highlighted with \textbf{bold} text. For most metrics, the best results are the ones closest to the corresponding value of the reference genome.}\\
% \multicolumn{12}{l}{\footnotesize The best results for \emph{Aligned Length} are determined by the highest number within each dataset. We do not highlight the reference results as the best results.}\\
% \multicolumn{12}{l}{\footnotesize N/A indicates that we could not generate the corresponding result because either the tool failed or QUAST failed to generate the statistic.} \\
% \end{tabular}}
% \label{tab:overlap_assembly}
% % \vspace{-12pt}
% \end{table*}

% Table~\ref{tab:overlap_assembly} shows the results related to assembly length and contiguity on its right-most part, from which we make the following two observations.
% First, we show that \proposal yields assemblies with better contiguity when using HiFi reads due to mostly 1)~higher NG50 and 2)~the largest contigs that \proposal generates are longer than that of minimap2 with the exception of the human genome. \proposal finds unique overlaps that minimap2 cannot find because of \proposal's ability to detect fuzzy seed matches. We believe such unique overlaps \emph{fill the gap} that minimap2 cannot, which improves the contiguity in most cases.
% Second, we observe that \proposal and minimap2 result in assemblies where the overall length is mostly closer to the reference genome assembly.
% % We suspect the human genome result is an exception potentially because of the high amount of contigs that minimap2 generates, which usually results in a higher amount of bases due to uncollapsed bubbles in the graph that miniasm generates. Such bubbles are known to provide a trade-off between the contiguity and the information loss in the assembly.
% We conclude that \proposal enables generating assemblies with contiguity to minimap2. \proposal achieves both high accuracy and high contiguity by using 1)~fuzzy seed matches and 2)~fewer seeds to find overlapping reads. Fewer overlaps with fuzzy seed matches are likely to provide the assembly graph with fewer edges to remove in transitive reduction and bubbles to collapse in the genome assembly step, which results in more effective overlap construction when using miniasm to generate \emph{de novo} assemblies~\cite{li_minimap_2016}.

% % \vspace{-12pt}
% \subsection{Use Case 2: Read Mapping}\label{subsec:mapping}
% \subsubsection{Performance}\label{subsubsec:mapping-perf}

% Figure~\ref{fig:mapping_perf} shows the CPU time and the peak memory footprint comparisons when performing read mapping to the corresponding reference genomes. We make the following two key observations.
% First, we observe that \proposal provides speedup by \mrmpM-\rmpM (on average \avgrmpM), \mrmpL-\rmpL (on average \avgrmpL), \mrmpW-\rmpW (on average \avgrmpW), and \mrmpS-\rmpS (on average \avgrmpS) compared to minimap2, LRA, Winnowmap2, and S-conLSH, respectively. Although \proposal performs better than most of these tools, the speedup ratio is usually lower than what we observe in read overlapping. Read mapping includes an additional computationally costly step that read overlapping skips, which is the read alignment. The extra overhead of read alignment slightly hinders the benefits that \proposal provides that we observe in read overlapping.
% Second, we find that \proposal provides slightly more memory overhead than minimap2 and LRA (on average \avgrmmM and \avgrmmL) and lower than Winnowmap2 and S-conLSH by \mrmmW-\rmmW (on average \avgrmmW), \mrmmS-\rmmS (on average \avgrmmS), respectively. \proposal cannot provide the similar reductions in the memory overhead that we observe in read overlapping due to the more narrow window length ($w=50$ instead of $w=200$) it uses to find the minimizer k-mers for HiFi reads with high accuracy. Using a narrow window length generates more seeds to store in a hash table than using a high window length, which proportionally increases the peak memory space requirements.
% Third, we use the same parameters that \proposal uses with minimap2 to assess the accuracy benefits of finding fuzzy seed matches rather than just using exact-matching seeds in read mapping. We call the version of minimap2 that uses the same set of parameters minimap2-Eq. We observe that \proposal performs, on average, $1.3\times$ better than minimap2-Eq, as shown in Supplementary Figure~\ref{suppfig:read_mapping_perf-eq}, while mostly providing higher read mapping quality and accuracy, as shown in Supplementary Tables~\ref{supptab:mapping_quality-eq} and \ref{supptab:mapping_accuracy-eq}.
% We conclude that \proposal, on average, performs better than all tools without reducing the accuracy and provides a better memory footprint than Winnowmap2 and S-conLSH.

% % \vspace{-8pt}
% \begin{figure}[tbh]
% \centering
%   \includegraphics[width=\columnwidth]{figures/read_mapping-perf-all_perf.pdf}
% %   \vspace{-15pt}
%     \caption{CPU time and peak memory footprint comparisons of read mapping.}
%     \label{fig:mapping_perf}
%     % \vspace{-12pt}
% \end{figure}

% % \vspace{-12pt}
% \subsubsection{Read Mapping Accuracy}\label{subsubsec:mapping-accuracy}

% To assess the accuracy of read mapping, we map two simulated erroneous read sets (i.e., PacBio CLR and ONT) to their reference genomes and compare the read mapping results with their true mapping results as generated by PBSIM2. To this end, we use paftools to assess the read mapping accuracy and show these results in Supplementary Table~\ref{supptab:mapping_accuracy}. We make two observations. First, we observe that \proposal provides the most high-quality true mappings for the PacBio CLR reads, and the lowest overall error rate for the ONT reads. For the cases where \proposal does not provide the best results, minimap2 provides the most high-quality true mappings for the ONT reads, and Winnowmap2 provides the best error rate for the PacBio CLR reads. Second, LRA and S-conLSH map most of the reads to incorrect locations, making these tools inaccurate for mapping erroneous reads compared to \proposal, minimap2, and Winnowmap2. We conclude that although the results are mixed, \proposal is the only tool that always performs best in either of these two metrics, providing the overall best accuracy results as generated by the paftools.

% % Supplementary Figure S2 shows the precision and the distance to true mapping results of \proposal, minimap2, LRA, Winnowmap2, and S-conLSH. We make the following two key observations.}
% % First, based on the precision results, \proposal mostly maps the \emph{highest} portion of the reads to their correct chromosomes. Winnowmap2 can map only a third of the reads to their true chromosomes, while S-conLSH maps almost no reads to their corresponding true chromosomes. This shows us that \proposal is the \emph{only} tool that can find fuzzy seed matches with high precision, while other tools find fuzzy seed matches in incorrect chromosomes.
% % Second, when the reads are mapped to their correct chromosomes, S-conLSH reports the mapping locations further from the true locations than \proposal, minimap2, and Winnowmap2 reports. We believe this is because S-conLSH is mainly designed as an alignment-free tool that does not aim for high accuracy in read alignment, although it still provides the mechanism to perform alignment.
% % We conclude that 1)~\proposal is the \emph{only} tool that can use fuzzy seed matches with high precision, and 2)~\proposal and minimap2 are significantly more accurate read mappers for mapping erroneous reads to their correct locations than Winnowmap2 and S-conLSH.

% % \vspace{-5pt}
% \subsubsection{Read Mapping Quality}\label{subsubsec:mapping-quality}

% Our goal is to assess the quality of read mappings in terms of four metrics: mean depth of coverage, breadth of coverage, number of aligned reads, and the ratio of the paired-end reads that are properly paired in mapping. Table~\ref{tab:mapping_quality} shows the quality of read mappings based on these metrics when using \proposal, minimap2, LRA, and Winnowmap2. We exclude S-conLSH from the read mapping quality comparisons as we cannot convert its SAM output to BAM format to properly index the BAM file due to the issues with its SAM output format. We make three observations.
% First, \proposal, minimap2, and Winnowmap2 cover most portion of the reference genomes after read mapping than LRA does. This result shows that these tools are less biased in mapping reads to particular regions (e.g., repetitive regions) than LRA.
% Second, both \proposal and minimap2 map an almost complete set of reads to the reference genome for all the datasets, while Winnowmap2 suffers from a slightly lower number of aligned reads when mapping erroneous PacBio CLR and ONT reads. This result shows that although Winnowmap2 generates lower error rates for the PacBio CLR reads, the higher portion of the unmapped reads is not counted towards the error rate. This suggests us Winnowmap2 provides a better precision while \proposal and minimap2 provide a better recall in terms of the read mapping accuracy.
% Third, we find that all the tools generate read mappings with a depth of coverage significantly close to their sequencing depth of coverage. This shows that almost all reads map to the reference genome evenly.
% We conclude that read mapping qualities of \proposal, minimap2, and Winnowmap2 are highly similar, while LRA provides slightly worse results. It is worth noting that \proposal can achieve comparable accuracy while using larger window lengths when finding the minimizer k-mers, which usually has an effect on the accuracy. \proposal does this by finding unique fuzzy seed matches that the other tools cannot find due to their exact-matching seed requirements.

% % \vspace{-7pt}
% \begin{table}[tbhp]
% \centering
% \caption{Read mapping quality comparisons.}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{@{}llrrrr@{}}\toprule
% \textbf{Dataset} 		& \textbf{Tool} & \textbf{Mean} 			 & \textbf{Breadth of} & \textbf{Aligned} 	& \textbf{Properly}\\
% 				 		& 				& \textbf{Depth of} 		 & \textbf{Coverage}   & \textbf{Reads} 	& \textbf{Paired}\\
% 				 		& 				& \textbf{Cov. (${\times}$)} & \textbf{(\%)} 	   & \textbf{(\#)} 		& \textbf{(\%)}\\\midrule
% \emph{Human CHM13} 		& \proposal 	& \textbf{16.58} 			 & \textbf{99.99} 	   & 3,171,916          & NA \\
% 						& minimap2 		& \textbf{16.58} 			 & \textbf{99.99} 	   & \textbf{3,172,261} & NA \\
% 						& LRA 			& 16.37 					 & 99.06 			   & 3,137,631 			& NA \\
% 						& Winnowmap2 	& \textbf{16.58} 			 & \textbf{99.99} 	   & 3,171,313 			& NA \\\cmidrule{1-6}
% \emph{D. ananassae} 	& \proposal 	& 57.37 					 & 99.66 			   & 1,223,388 			& NA \\
% 						& minimap2 		& \textbf{57.57} 			 & \textbf{99.67} 	   & 1,245,931 			& NA \\
% 						& LRA 			& 57.06 					 & 99.60 			   & 1,235,098 			& NA \\
% 						& Winnowmap2 	& 57.40 			 		 & 99.66 	   		   & \textbf{1,249,575} & NA \\\cmidrule{1-6}
% \emph{E. coli} 			& \proposal 	& \textbf{99.14} 			 & 99.90 			   & 39,048 			& NA\\
% 						& minimap2 		& \textbf{99.14} 			 & 99.90 			   & \textbf{39,065} 	& NA\\
% 						& LRA 			& 99.10 			 		 & 99.90 			   & 39,063 			& NA\\
% 						& Winnowmap2 	& \textbf{99.14} 			 & 99.90 			   & 39,036 			& NA\\\cmidrule{1-6}
% \emph{Yeast} (PacBio) 	& \proposal 	& 195.84 			 		 & \textbf{99.98} 	   & 269,804 			& NA\\
% 						& minimap2 		& \textbf{195.86} 			 & \textbf{99.98} 	   & \textbf{269,935} 	& NA\\
% 						& LRA 			& 194.65 					 & 99.97 			   & 267,399 			& NA\\
% 						& Winnowmap2 	& 192.35 			 		 & \textbf{99.98}	   & 259,073 			& NA\\\cmidrule{1-6}
% \emph{Yeast} (ONT) 		& \proposal 	& 97.86 			 		 & \textbf{99.97} 	   & 134,721 			& NA\\
% 						& minimap2 		& \textbf{97.88} 			 & 99.96 			   & \textbf{134,885} 	& NA\\
% 						& LRA 			& 97.25 					 & 99.95 			   & 132,862 			& NA\\
% 						& Winnowmap2 	& 97.04 			 		 & 99.96 	   		   & 130,978 			& NA\\\cmidrule{1-6}
% \emph{Yeast} (Illumina) & \proposal 	& \textbf{79.93} 			 & 99.97 			   & \textbf{6,494,489} & 95.89 \\
% 						& minimap2 		& 79.91 					 & 99.97 			   & 6,492,994 			& 95.89 \\\bottomrule
% \multicolumn{6}{l}{\footnotesize Best results are highlighted with \textbf{bold} text.} \\
% \multicolumn{6}{l}{\footnotesize Properly paired rate is only available for paired-end Illumina reads.} \\
% \end{tabular}}
% \label{tab:mapping_quality}
% % \vspace{-12pt}
% \end{table}
