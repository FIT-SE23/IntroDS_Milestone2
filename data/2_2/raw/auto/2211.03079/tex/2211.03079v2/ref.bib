@article{urnet_zhang2020nanopore,
  title={{Nanopore Basecalling from a Perspective of Instance Segmentation}},
  author={Zhang, Yao-zhong and Akdemir, Arda and Tremmel, Georg and Imoto, Seiya and Miyano, Satoru and Shibuya, Tetsuo and Yamaguchi, Rui},
  journal={BMC bioinformatics},
  year={2020}
}

@article{alser2022molecules,
  title={{From Molecules to Genomic Variations: Accelerating Genome Analysis via Intelligent Algorithms and Architectures}},
  author={Alser, Mohammed and Lindegger, Joel and Firtina, Can and Almadhoun, Nour and Mao, Haiyu and Singh, Gagandeep and Gomez-Luna, Juan and Mutlu, Onur},
  journal={Computational and Structural Biotechnology Journal},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{catcaller_lv2020end,
  title={{An End-to-End Oxford Nanopore Basecaller Using Convolution-Augmented Transformer}},
  author={Lv, Xuan and Chen, Zhiguang and Lu, Yutong and Yang, Yuedong},
  booktitle={2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
  pages={337--342},
  year={2020},
  organization={IEEE}
}

@article{zeng2020causalcall,
  title={{Causalcall: Nanopore Basecalling Using a Temporal Convolutional Network}},
  author={Zeng, Jingwen and Cai, Hongmin and Peng, Hong and Wang, Haiyan and Zhang, Yue and Akutsu, Tatsuya},
  journal={Frontiers in Genetics},
  pages={1332},
  year={2020},
  publisher={Frontiers}
}
@article{perevsini2021nanopore,
  title={{Nanopore Base Calling on the Edge}},
  author={Pere{\v{s}}{\'\i}ni, Peter and Bo{\v{z}}a, Vladim{\'\i}r and Brejov{\'a}, Bro{\v{n}}a and Vina{\v{r}}, Tom{\'a}{\v{s}}},
  journal={Bioinformatics},
  volume={37},
  number={24},
  pages={4661--4667},
  year={2021},
  publisher={Oxford University Press}
}
@inproceedings{lou2020helix,
  title={{Helix: Algorithm/Architecture Co-design for Accelerating Nanopore Genome Base-calling}},
  author={Lou, Qian and Janga, Sarath Chandra and Jiang, Lei},
  booktitle={Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques},
  pages={293--304},
  year={2020}
}
@article{xu2021fast,
  title={{Fast-bonito: A Faster Deep Learning Based Basecaller for Nanopore Sequencing}},
  author={Xu, Zhimeng and Mai, Yuting and Liu, Denghui and He, Wenjun and Lin, Xinyuan and Xu, Chi and Zhang, Lei and Meng, Xin and Mafofo, Joseph and Zaher, Walid Abbas and others},
  journal={Artificial Intelligence in the Life Sciences},
  volume={1},
  pages={100011},
  year={2021},
  publisher={Elsevier}
}

@article{konishi2021halcyon,
  title={{Halcyon: An Accurate Basecaller Exploiting an Encoder--Decoder Model with Monotonic Attention}},
  author={Konishi, Hiroki and Yamaguchi, Rui and Yamaguchi, Kiyoshi and Furukawa, Yoichi and Imoto, Seiya},
  journal={Bioinformatics},
  volume={37},
  number={9},
  pages={1211--1217},
  year={2021},
  publisher={Oxford University Press}
}

@article{huang2020sacall,
  title={{SACall: A Neural Network Basecaller for Oxford Nanopore Sequencing Data Based on Self-Attention Mechanism}},
  author={Huang, Neng and Nie, Fan and Ni, Peng and Luo, Feng and Wang, Jianxin},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  year={2020},
  publisher={IEEE}
}
@article{neumann2022rodan,
  title={{RODAN: A Fully Convolutional Architecture for Basecalling Nanopore RNA Sequencing Data}},
  author={Neumann, Don and Reddy, Anireddy SN and Ben-Hur, Asa},
  journal={BMC bioinformatics},
  volume={23},
  number={1},
  pages={1--9},
  year={2022},
  publisher={BioMed Central}
}

@article{wick2019performance,
  title={{Performance of Neural Network Basecalling Tools for Oxford Nanopore Sequencing}},
  author={Wick, Ryan R and Judd, Louise M and Holt, Kathryn E},
  journal={Genome biology},
  volume={20},
  number={1},
  pages={1--10},
  year={2019},
  publisher={Springer}
}
@article{amarasinghe2020opportunities,
  title={{Opportunities and Challenges in Long-Read Sequencing Data Analysis}},
  author={Amarasinghe, Shanika L and Su, Shian and Dong, Xueyi and Zappia, Luke and Ritchie, Matthew E and Gouil, Quentin},
  journal={Genome biology},
  volume={21},
  number={1},
  pages={1--16},
  year={2020},
  publisher={BioMed Central}
}
@article{logsdon2020long,
  title={{Long-Read Human Genome Sequencing and its Applications}},
  author={Logsdon, Glennis A and Vollger, Mitchell R and Eichler, Evan E},
  journal={Nature Reviews Genetics},
  volume={21},
  number={10},
  pages={597--614},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{dias2019artificial,
  title={{Artificial Intelligence in Clinical and Genomic Diagnostics}},
  author={Dias, Raquel and Torkamani, Ali},
  journal={Genome medicine},
  volume={11},
  number={1},
  pages={1--12},
  year={2019},
  publisher={BioMed Central}
}
@article{wang2021nanopore,
  title={{Nanopore Sequencing Technology, Bioinformatics and Applications}},
  author={Wang, Yunhao and Zhao, Yue and Bollas, Audrey and Wang, Yuru and Au, Kin Fai},
  journal={Nature biotechnology},
  volume={39},
  number={11},
  pages={1348--1365},
  year={2021},
  publisher={Nature Publishing Group}
}
@article{pages2022comprehensive,
  title={{Comprehensive and Standardized Benchmarking of Deep Learning Architectures for Basecalling Nanopore Sequencing Data}},
  author={Pages-Gallego, Marc and de Ridder, Jeroen},
  journal={bioRxiv},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{ren2021comprehensive,
  title={{A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions}},
  author={Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Chen, Xiaojiang and Wang, Xin},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={4},
  pages={1--34},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{ambernas_zhang2021automated,
  title={{An Automated Framework for Efficiently Designing Deep Convolutional Neural Networks in Genomics}},
  author={Zhang, Zijun and Park, Christopher Y and Theesfeld, Chandra L and Troyanskaya, Olga G},
  journal={Nature Machine Intelligence},
  volume={3},
  number={5},
  pages={392--400},
  year={2021},
  publisher={Nature Publishing Group}
}

















@inproceedings{schleimer2003winnowing,
  title={{Winnowing: Local Algorithms for Document Fingerprinting}},
  author={Schleimer, Saul and Wilkerson, Daniel S and Aiken, Alex},
  booktitle={Proceedings of the 2003 ACM SIGMOD international conference on Management of data},
  pages={76--85},
  year={2003}
}
@article{lapierre2020metalign,
  title={{Metalign: Efficient Alignment-Based Metagenomic Profiling via Containment Min Hash}},
  author={LaPierre, Nathan and Alser, Mohammed and Eskin, Eleazar and Koslicki, David and Mangul, Serghei},
  journal={Genome biology},
  volume={21},
  number={1},
  pages={1--15},
  year={2020},
  publisher={BioMed Central}
}
@article{meyer2021critical,
  title={{Critical Assessment of Metagenome Interpretation-The Second Round of Challenges}},
  author={Meyer, F and Fritz, A and Deng, Z-L and Koslicki, D and Gurevich, A and Robertson, G and Alser, M and Antipov, D and Beghini, F and Bertrand, D and others},
  journal={bioRxiv},
  year={2021},
  publisher={Cold Spring Harbor Laboratory}
}

@article{shendure_dna_2017,
	title = {{DNA Sequencing at 40: Past, Present and Future}},
	volume = {550},
	issn = {1476-4687},
	abstract = {This review commemorates the 40th anniversary of DNA sequencing, a period in which we have already witnessed multiple technological revolutions and a growth in scale from a few kilobases to the first human genome, and now to millions of human and a myriad of other genomes. DNA sequencing has been extensively and creatively repurposed, including as a ‘counter’ for a vast range of molecular phenomena. We predict that in the long view of history, the impact of DNA sequencing will be on a par with that of the microscope.},
	number = {7676},
	journal = {Nature},
	author = {Shendure, Jay and Balasubramanian, Shankar and Church, George M. and Gilbert, Walter and Rogers, Jane and Schloss, Jeffery A. and Waterston, Robert H.},
	month = oct,
	year = {2017},
	pages = {345--353},
}

@article{j_wang_survey_2018,
  author={Wang, Jingdong and Zhang, Ting and Song, Jingkuan and Sebe, Nicu and Shen, Heng Tao},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={{A Survey on Learning to Hash}}, 
  year={2018},
  volume={40},
  number={4},
  pages={769-790},
  doi={10.1109/TPAMI.2017.2699960}
}


@article{baichoo_computational_2017,
	title = {{Computational Complexity of Algorithms for Sequence Comparison, Short-Read Assembly and Genome Alignment}},
	volume = {156-157},
	issn = {0303-2647},
	abstract = {A multitude of algorithms for sequence comparison, short-read assembly and whole-genome alignment have been developed in the general context of molecular biology, to support technology development for high-throughput sequencing, numerous applications in genome biology and fundamental research on comparative genomics. The computational complexity of these algorithms has been previously reported in original research papers, yet this often neglected property has not been reviewed previously in a systematic manner and for a wider audience. We provide a review of space and time complexity of key sequence analysis algorithms and highlight their properties in a comprehensive manner, in order to identify potential opportunities for further research in algorithm or data structure optimization. The complexity aspect is poised to become pivotal as we will be facing challenges related to the continuous increase of genomic data on unprecedented scales and complexity in the foreseeable future, when robust biological simulation at the cell level and above becomes a reality.},
	journal = {Biosystems},
	author = {Baichoo, Shakuntala and Ouzounis, Christos A.},
	month = jun,
	year = {2017},
	keywords = {Bioinformatics algorithms, Computational complexity, Genome alignment, Sequence comparison, Short-read assembly},
	pages = {72--85},
}

@inproceedings{l_guo_hardware_2019,
  author={Guo, Licheng and Lau, Jason and Ruan, Zhenyuan and Wei, Peng and Cong, Jason},
  booktitle={2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)}, 
  title={{Hardware Acceleration of Long Read Pairwise Overlapping in Genome Sequencing: A Race Between FPGA and GPU}}, 
  year={2019},
  volume={},
  number={},
  pages={127-135},
  doi={10.1109/FCCM.2019.00027}
}

@article{alser_technology_2021,
	title = {{Technology Dictates Algorithms: Recent Developments in Read Alignment}},
	volume = {22},
	issn = {1474-760X},
	abstract = {Aligning sequencing reads onto a reference is an essential step of the majority of genomic analysis pipelines. Computational algorithms for read alignment have evolved in accordance with technological advances, leading to today’s diverse array of alignment methods. We provide a systematic survey of algorithmic foundations and methodologies across 107 alignment methods, for both short and long reads. We provide a rigorous experimental evaluation of 11 read aligners to demonstrate the effect of these underlying algorithms on speed and efficiency of read alignment. We discuss how general alignment algorithms have been tailored to the specific needs of various domains in biology.},
	number = {1},
	journal = {Genome Biology},
	author = {Alser, Mohammed and Rotman, Jeremy and Deshpande, Dhrithi and Taraszka, Kodi and Shi, Huwenbo and Baykal, Pelin Icer and Yang, Harry Taegyun and Xue, Victor and Knyazev, Sergey and Singer, Benjamin D. and Balliu, Brunilda and Koslicki, David and Skums, Pavel and Zelikovsky, Alex and Alkan, Can and Mutlu, Onur and Mangul, Serghei},
	month = aug,
	year = {2021},
	pages = {249},
}

@article{zheng_improved_2020,
	title = {{Improved Design and Analysis of Practical Minimizers}},
	volume = {36},
	issn = {1367-4803},
	abstract = {Minimizers are methods to sample k-mers from a string, with the guarantee that similar set of k-mers will be chosen on similar strings. It is parameterized by the k-mer length k, a window length w and an order on the k-mers. Minimizers are used in a large number of softwares and pipelines to improve computation efficiency and decrease memory usage. Despite the method’s popularity, many theoretical questions regarding its performance remain open. The core metric for measuring performance of a minimizer is the density, which measures the sparsity of sampled k-mers. The theoretical optimal density for a minimizer is 1/w, provably not achievable in general. For given k and w, little is known about asymptotically optimal minimizers, that is minimizers with density O(1/w).We derive a necessary and sufficient condition for existence of asymptotically optimal minimizers. We also provide a randomized algorithm, called the Miniception, to design minimizers with the best theoretical guarantee to date on density in practical scenarios. Constructing and using the Miniception is as easy as constructing and using a random minimizer, which allows the design of efficient minimizers that scale to the values of k and w used in current bioinformatics software programs.Reference implementation of the Miniception and the codes for analysis can be found at https://github.com/kingsford-group/miniception.Supplementary data are available at Bioinformatics online.},
	number = {Supplement\_1},
	urldate = {2021-10-20},
	journal = {Bioinformatics},
	author = {Zheng, Hongyu and Kingsford, Carl and Marçais, Guillaume},
	month = jul,
	year = {2020},
	pages = {i119--i127},
}

@article{tvedte_comparison_2021,
	title = {{Comparison of Long-Read Sequencing Technologies in Interrogating Bacteria and Fly Genomes}},
	volume = {11},
	issn = {2160-1836},
	abstract = {The newest generation of DNA sequencing technology is highlighted by the ability to generate sequence reads hundreds of kilobases in length. Pacific Biosciences (PacBio) and Oxford Nanopore Technologies (ONT) have pioneered competitive long read platforms, with more recent work focused on improving sequencing throughput and per-base accuracy. We used whole-genome sequencing data produced by three PacBio protocols (Sequel II CLR, Sequel II HiFi, RS II) and two ONT protocols (Rapid Sequencing and Ligation Sequencing) to compare assemblies of the bacteria Escherichia coli and the fruit fly Drosophila ananassae. In both organisms tested, Sequel II assemblies had the highest consensus accuracy, even after accounting for differences in sequencing throughput. ONT and PacBio CLR had the longest reads sequenced compared to PacBio RS II and HiFi, and genome contiguity was highest when assembling these datasets. ONT Rapid Sequencing libraries had the fewest chimeric reads in addition to superior quantification of E. coli plasmids versus ligation-based libraries. The quality of assemblies can be enhanced by adopting hybrid approaches using Illumina libraries for bacterial genome assembly or polishing eukaryotic genome assemblies, and an ONT-Illumina hybrid approach would be more cost-effective for many users. Genome-wide DNA methylation could be detected using both technologies, however ONT libraries enabled the identification of a broader range of known E. coli methyltransferase recognition motifs in addition to undocumented D. ananassae motifs. The ideal choice of long read technology may depend on several factors including the question or hypothesis under examination. No single technology outperformed others in all metrics examined.},
	number = {6},
	urldate = {2021-10-18},
	journal = {G3 Genes{\textbar}Genomes{\textbar}Genetics},
	author = {Tvedte, Eric S and Gasser, Mark and Sparklin, Benjamin C and Michalski, Jane and Hjelmen, Carl E and Johnston, J Spencer and Zhao, Xuechu and Bromley, Robin and Tallon, Luke J and Sadzewicz, Lisa and Rasko, David A and Dunning Hotopp, Julie C},
	month = jun,
	year = {2021},
}

@article{chakraborty_s-conlsh_2021,
	title = {{S-{conLSH}: Alignment-Free Gapped Mapping of Noisy Long Reads}},
	volume = {22},
	issn = {1471-2105},
	abstract = {The advancement of SMRT technology has unfolded new opportunities of genome analysis with its longer read length and low GC bias. Alignment of the reads to their appropriate positions in the respective reference genome is the first but costliest step of any analysis pipeline based on SMRT sequencing. However, the state-of-the-art aligners often fail to identify distant homologies due to lack of conserved regions, caused by frequent genetic duplication and recombination. Therefore, we developed a novel alignment-free method of sequence mapping that is fast and accurate.},
	number = {1},
	journal = {BMC Bioinformatics},
	author = {Chakraborty, Angana and Morgenstern, Burkhard and Bandyopadhyay, Sanghamitra},
	month = feb,
	year = {2021},
	pages = {64},
}

@article{bloom_massively_2021,
	title = {{Massively Scaled-Up Testing for {SARS}-{CoV}-2 {RNA} via Next-Generation Sequencing of Pooled and Barcoded Nasal and Saliva Samples}},
	volume = {5},
	issn = {2157-846X},
	abstract = {Frequent and widespread testing of members of the population who are asymptomatic for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is essential for the mitigation of the transmission of the virus. Despite the recent increases in testing capacity, tests based on quantitative polymerase chain reaction (qPCR) assays cannot be easily deployed at the scale required for population-wide screening. Here, we show that next-generation sequencing of pooled samples tagged with sample-specific molecular barcodes enables the testing of thousands of nasal or saliva samples for SARS-CoV-2 RNA in a single run without the need for RNA extraction. The assay, which we named SwabSeq, incorporates a synthetic RNA standard that facilitates end-point quantification and the calling of true negatives, and that reduces the requirements for automation, purification and sample-to-sample normalization. We used SwabSeq to perform 80,000 tests, with an analytical sensitivity and specificity comparable to or better than traditional qPCR tests, in less than two months with turnaround times of less than 24 h. SwabSeq could be rapidly adapted for the detection of other pathogens.},
	number = {7},
	journal = {Nature Biomedical Engineering},
	author = {Bloom, Joshua S. and Sathe, Laila and Munugala, Chetan and Jones, Eric M. and Gasperini, Molly and Lubock, Nathan B. and Yarza, Fauna and Thompson, Erin M. and Kovary, Kyle M. and Park, Jimin and Marquette, Dawn and Kay, Stephania and Lucas, Mark and Love, TreQuan and Sina Booeshaghi, A. and Brandenberg, Oliver F. and Guo, Longhua and Boocock, James and Hochman, Myles and Simpkins, Scott W. and Lin, Isabella and LaPierre, Nathan and Hong, Duke and Zhang, Yi and Oland, Gabriel and Choe, Bianca Judy and Chandrasekaran, Sukantha and Hilt, Evann E. and Butte, Manish J. and Damoiseaux, Robert and Kravit, Clifford and Cooper, Aaron R. and Yin, Yi and Pachter, Lior and Garner, Omai B. and Flint, Jonathan and Eskin, Eleazar and Luo, Chongyuan and Kosuri, Sriram and Kruglyak, Leonid and Arboleda, Valerie A.},
	month = jul,
	year = {2021},
	pages = {657--665},
}

@article{aynaud_multiplexed_2021,
	title = {{A Multiplexed, Next Generation Sequencing Platform for High-Throughput Detection of {SARS}-{CoV}-2}},
	volume = {12},
	issn = {2041-1723},
	abstract = {Population scale sweeps of viral pathogens, such as SARS-CoV-2, require high intensity testing for effective management. Here, we describe “Systematic Parallel Analysis of RNA coupled to Sequencing for Covid-19 screening” (C19-SPAR-Seq), a multiplexed, scalable, readily automated platform for SARS-CoV-2 detection that is capable of analyzing tens of thousands of patient samples in a single run. To address strict requirements for control of assay parameters and output demanded by clinical diagnostics, we employ a control-based Precision-Recall and Receiver Operator Characteristics (coPR) analysis to assign run-specific quality control metrics. C19-SPAR-Seq coupled to coPR on a trial cohort of several hundred patients performs with a specificity of 100\% and sensitivity of 91\% on samples with low viral loads, and a sensitivity of {\textgreater}95\% on high viral loads associated with disease onset and peak transmissibility. This study establishes the feasibility of employing C19-SPAR-Seq for the large-scale monitoring of SARS-CoV-2 and other pathogens.},
	number = {1},
	journal = {Nature Communications},
	author = {Aynaud, Marie-Ming and Hernandez, J. Javier and Barutcu, Seda and Braunschweig, Ulrich and Chan, Kin and Pearson, Joel D. and Trcka, Daniel and Prosser, Suzanna L. and Kim, Jaeyoun and Barrios-Rodiles, Miriam and Jen, Mark and Song, Siyuan and Shen, Jess and Bruce, Christine and Hazlett, Bryn and Poutanen, Susan and Attisano, Liliana and Bremner, Rod and Blencowe, Benjamin J. and Mazzulli, Tony and Han, Hong and Pelletier, Laurence and Wrana, Jeffrey L.},
	month = mar,
	year = {2021},
	pages = {1405},
}

@article{friedman_genome-wide_2019,
	title = {{Genome-Wide Sequencing in Acutely Ill Infants: Genomic Medicine’s Critical Application?}},
	volume = {21},
	issn = {1530-0366},
	abstract = {Diagnostic genome-wide sequencing (exome or genome sequencing and data analysis for high-penetrance disease-causing variants) in acutely ill infants appears to be clinically useful, but the value of this diagnostic test should be rigorously demonstrated before it is accepted as a standard of care. This white paper was developed by the Paediatric Task Team of the Global Alliance for Genomics and Health’s Regulatory and Ethics Work Stream to address the question of how we can determine the clinical value of genome-wide sequencing in infants in an intensive care setting. After reviewing available clinical and ethics literature on this question, we conclude that evaluating diagnostic genome-wide sequencing as a comprehensive scan for major genetic disease (rather than as a large panel of single-gene tests) provides a practical approach to assessing its clinical value in acutely ill infants. Comparing the clinical value of diagnostic genome-wide sequencing to chromosomal microarray analysis, the current evidence-based standard of care, per case of serious genetic disease diagnosed provides a practical means of assessing clinical value. Scientifically rigorous studies of this kind are needed to determine if clinical genome-wide sequencing should be established as a standard of care supported by healthcare systems and insurers for diagnosis of genetic disease in seriously ill newborn infants.},
	number = {2},
	journal = {Genetics in Medicine},
	author = {Friedman, Jan M. and Bombard, Yvonne and Cornel, Martina C. and Fernandez, Conrad V. and Junker, Anne K. and Plon, Sharon E. and Stark, Zornitza and Knoppers, Bartha Maria and {for the Paediatric Task Team of the Global Alliance for Genomics and Health Regulatory and Ethics Work Stream}},
	month = feb,
	year = {2019},
	pages = {498--504},
}

@article{merker_long-read_2018,
	title = {{Long-Read Genome Sequencing Identifies Causal Structural Variation in a {Mendelian} Disease}},
	volume = {20},
	issn = {1530-0366},
	abstract = {Current clinical genomics assays primarily utilize short-read sequencing (SRS), but SRS has limited ability to evaluate repetitive regions and structural variants. Long-read sequencing (LRS) has complementary strengths, and we aimed to determine whether LRS could offer a means to identify overlooked genetic variation in patients undiagnosed by SRS.},
	number = {1},
	journal = {Genetics in Medicine},
	author = {Merker, Jason D and Wenger, Aaron M and Sneddon, Tam and Grove, Megan and Zappala, Zachary and Fresard, Laure and Waggott, Daryl and Utiramerur, Sowmi and Hou, Yanli and Smith, Kevin S and Montgomery, Stephen B and Wheeler, Matthew and Buchan, Jillian G and Lambert, Christine C and Eng, Kevin S and Hickey, Luke and Korlach, Jonas and Ford, James and Ashley, Euan A},
	month = jan,
	year = {2018},
	pages = {159--163},
}

@article{logsdon_long-read_2020,
	title = {{Long-Read Human Genome Sequencing and its Applications}},
	volume = {21},
	issn = {1471-0064},
	abstract = {Over the past decade, long-read, single-molecule DNA sequencing technologies have emerged as powerful players in genomics. With the ability to generate reads tens to thousands of kilobases in length with an accuracy approaching that of short-read sequencing technologies, these platforms have proven their ability to resolve some of the most challenging regions of the human genome, detect previously inaccessible structural variants and generate some of the first telomere-to-telomere assemblies of whole chromosomes. Long-read sequencing technologies will soon permit the routine assembly of diploid genomes, which will revolutionize genomics by revealing the full spectrum of human genetic variation, resolving some of the missing heritability and leading to the discovery of novel mechanisms of disease.},
	number = {10},
	journal = {Nature Reviews Genetics},
	author = {Logsdon, Glennis A. and Vollger, Mitchell R. and Eichler, Evan E.},
	month = oct,
	year = {2020},
	pages = {597--614},
}

@article{mantere_long-read_2019,
	title = {Long-{Read} {Sequencing} {Emerging} in {Medical} {Genetics}},
	volume = {10},
	issn = {1664-8021},
	abstract = {The wide implementation of next-generation sequencing (NGS) technologies has revolutionized the field of medical genetics. However, the short read lengths of currently used sequencing approaches pose a limitation for the identification of structural variants, sequencing repetitive regions, phasing of alleles and distinguishing highly homologous genomic regions. These limitations may significantly contribute to the diagnostic gap in patients with genetic disorders who have undergone standard NGS, like whole exome or even genome sequencing. Now, the emerging long-read sequencing (LRS) technologies may offer improvements in the characterization of genetic variation and regions that are difficult to assess with the prevailing NGS approaches. LRS has so far mainly been used to investigate genetic disorders with previously known or strongly suspected disease loci. While these targeted approaches already show the potential of LRS, it remains to be seen whether LRS technologies can soon enable true whole genome sequencing routinely. Ultimately, this could allow the de novo assembly of individual whole genomes used as a generic test for genetic disorders. In this article, we summarize the current LRS-based research on human genetic disorders and discuss the potential of these technologies to facilitate the next major advancements in medical genetics.},
	journal = {Frontiers in Genetics},
	author = {Mantere, Tuomo and Kersten, Simone and Hoischen, Alexander},
	year = {2019},
	pages = {426},
}

@article{glenn_field_2011,
	title = {{Field Guide to Next-Generation {DNA} Sequencers}},
	volume = {11},
	issn = {1755-098X},
	abstract = {Abstract The diversity of available 2nd and 3rd generation DNA sequencing platforms is increasing rapidly. Costs for these systems range from {\textless}\$100?000 to more than \$1?000?000, with instrument run times ranging from minutes to weeks. Extensive trade-offs exist among these platforms. I summarize the major characteristics of each commercially available platform to enable direct comparisons. In terms of cost per megabase (Mb) of sequence, the Illumina and SOLiD platforms are clearly superior (≤\$0.10/Mb vs. {\textgreater}\$10/Mb for 454 and some Ion Torrent chips). In terms of cost per nonmultiplexed sample and instrument run time, the Pacific Biosciences and Ion Torrent platforms excel, with the 454 GS Junior and Illumina MiSeq also notable in this regard. All platforms allow multiplexing of samples, but details of library preparation, experimental design and data analysis can constrain the options. The wide range of characteristics among available platforms provides opportunities both to conduct groundbreaking studies and to waste money on scales that were previously infeasible. Thus, careful thought about the desired characteristics of these systems is warranted before purchasing or using any of them. Updated information from this guide will be maintained at: http://dna.uga.edu/ and http://tomato.biol.trinity.edu/blog/.},
	number = {5},
	urldate = {2021-09-19},
	journal = {Molecular Ecology Resources},
	author = {Glenn, Travis C.},
	month = sep,
	year = {2011},
	keywords = {2nd and 3rd generation sequencing, 454, Helicos, Illumina, Ion Torrent, Life Technologies, massively parallel sequencing, Pacific Biosystems, Roche, SOLiD},
	pages = {759--769},
}

@article{alser_accelerating_2020,
	title = {Accelerating {Genome} {Analysis}: {A} {Primer} on an {Ongoing} {Journey}},
	volume = {40},
	issn = {1937-4143},
	number = {5},
	journal = {IEEE Micro},
	author = {Alser, Mohammed and Bingöl, Zulal and Cali, Damla Senol and Kim, Jeremie S. and Ghose, Saugata and Alkan, Can and Mutlu, Onur},
	month = oct,
	year = {2020},
	pages = {65--75},
}

@article{huddleston_reconstructing_2014,
	title = {{Reconstructing Complex Regions of Genomes using Long-Read Sequencing Technology}},
	volume = {24},
	abstract = {Obtaining high-quality sequence continuity of complex regions of recent segmental duplication remains one of the major challenges of finishing genome assemblies. In the human and mouse genomes, this was achieved by targeting large-insert clones using costly and laborious capillary-based sequencing approaches. Sanger shotgun sequencing of clone inserts, however, has now been largely abandoned, leaving most of these regions unresolved in newer genome assemblies generated primarily by next-generation sequencing hybrid approaches. Here we show that it is possible to resolve regions that are complex in a genome-wide context but simple in isolation for a fraction of the time and cost of traditional methods using long-read single molecule, real-time (SMRT) sequencing and assembly technology from Pacific Biosciences (PacBio). We sequenced and assembled BAC clones corresponding to a 1.3-Mbp complex region of chromosome 17q21.31, demonstrating 99.994\% identity to Sanger assemblies of the same clones. We targeted 44 differences using Illumina sequencing and find that PacBio and Sanger assemblies share a comparable number of validated variants, albeit with different sequence context biases. Finally, we targeted a poorly assembled 766-kbp duplicated region of the chimpanzee genome and resolved the structure and organization for a fraction of the cost and time of traditional finishing approaches. Our data suggest a straightforward path for upgrading genomes to a higher quality finished state.},
	number = {4},
	journal = {Genome Research},
	author = {Huddleston, John and Ranade, Swati and Malig, Maika and Antonacci, Francesca and Chaisson, Mark and Hon, Lawrence and Sudmant, Peter H. and Graves, Tina A. and Alkan, Can and Dennis, Megan Y. and Wilson, Richard K. and Turner, Stephen W. and Korlach, Jonas and Eichler, Evan E.},
	month = apr,
	year = {2014},
	pages = {688--696},
}

@article{jain_nanopore_2018,
	title = {{Nanopore Sequencing and Assembly of a Human Genome with Ultra-Long Reads}},
	volume = {36},
	issn = {1546-1696},
	abstract = {A human genome is sequenced and assembled de novo using a pocket-sized nanopore device.},
	number = {4},
	journal = {Nature Biotechnology},
	author = {Jain, Miten and Koren, Sergey and Miga, Karen H and Quick, Josh and Rand, Arthur C and Sasani, Thomas A and Tyson, John R and Beggs, Andrew D and Dilthey, Alexander T and Fiddes, Ian T and Malla, Sunir and Marriott, Hannah and Nieto, Tom and O'Grady, Justin and Olsen, Hugh E and Pedersen, Brent S and Rhie, Arang and Richardson, Hollian and Quinlan, Aaron R and Snutch, Terrance P and Tee, Louise and Paten, Benedict and Phillippy, Adam M and Simpson, Jared T and Loman, Nicholas J and Loose, Matthew},
	month = apr,
	year = {2018},
	pages = {338--345},
}

@article{payne_bulkvis_2019,
	title = {{BulkVis}: {A Graphical Viewer for {Oxford} Nanopore Bulk {FAST5} Files}},
	volume = {35},
	issn = {1367-4803},
	abstract = {The Oxford Nanopore Technologies (ONT) MinION is used for sequencing a wide variety of sample types with diverse methods of sample extraction. Nanopore sequencers output FAST5 files containing signal data subsequently base called to FASTQ format. Optionally, ONT devices can collect data from all sequencing channels simultaneously in a bulk FAST5 file enabling inspection of signal in any channel at any point. We sought to visualize this signal to inspect challenging or difficult to sequence samples.The BulkVis tool can load a bulk FAST5 file and overlays MinKNOW (the software that controls ONT sequencers) classifications on the signal trace and can show mappings to a reference. Users can navigate to a channel and time or, given a FASTQ header from a read, jump to its specific position. BulkVis can export regions as Nanopore base caller compatible reads. Using BulkVis, we find long reads can be incorrectly divided by MinKNOW resulting in single DNA molecules being split into two or more reads. The longest seen to date is 2 272 580 bases in length and reported in eleven consecutive reads. We provide helper scripts that identify and reconstruct split reads given a sequencing summary file and alignment to a reference. We note that incorrect read splitting appears to vary according to input sample type and is more common in ’ultra-long’ read preparations.The software is available freely under an MIT license at https://github.com/LooseLab/bulkvis.Supplementary data are available at Bioinformatics online.},
	number = {13},
	urldate = {2021-09-19},
	journal = {Bioinformatics},
	author = {Payne, Alexander and Holmes, Nadine and Rakyan, Vardhman and Loose, Matthew},
	month = jul,
	year = {2019},
	pages = {2193--2198},
}

@article{senol_cali_nanopore_2019,
	title = {{Nanopore Sequencing Technology and Tools for Genome Assembly: Computational Analysis of the Current State, Bottlenecks and Future Directions}},
	volume = {20},
	issn = {1477-4054},
	abstract = {Nanopore sequencing technology has the potential to render other sequencing technologies obsolete with its ability to generate long reads and provide portability. However, high error rates of the technology pose a challenge while generating accurate genome assemblies. The tools used for nanopore sequence analysis are of critical importance, as they should overcome the high error rates of the technology. Our goal in this work is to comprehensively analyze current publicly available tools for nanopore sequence analysis to understand their advantages, disadvantages and performance bottlenecks. It is important to understand where the current tools do not perform well to develop better tools. To this end, we (1) analyze the multiple steps and the associated tools in the genome assembly pipeline using nanopore sequence data, and (2) provide guidelines for determining the appropriate tools for each step. Based on our analyses, we make four key observations: (1) the choice of the tool for basecalling plays a critical role in overcoming the high error rates of nanopore sequencing technology. (2) Read-to-read overlap finding tools, GraphMap and Minimap, perform similarly in terms of accuracy. However, Minimap has a lower memory usage, and it is faster than GraphMap. (3) There is a trade-off between accuracy and performance when deciding on the appropriate tool for the assembly step. The fast but less accurate assembler Miniasm can be used for quick initial assembly, and further polishing can be applied on top of it to increase the accuracy, which leads to faster overall assembly. (4) The state-of-the-art polishing tool, Racon, generates high-quality consensus sequences while providing a significant speedup over another polishing tool, Nanopolish. We analyze various combinations of different tools and expose the trade-offs between accuracy, performance, memory usage and scalability. We conclude that our observations can guide researchers and practitioners in making conscious and effective choices for each step of the genome assembly pipeline using nanopore sequence data. Also, with the help of bottlenecks we have found, developers can improve the current tools or build new ones that are both accurate and fast, to overcome the high error rates of the nanopore sequencing technology.},
	number = {4},
	urldate = {2021-09-19},
	journal = {Briefings in Bioinformatics},
	author = {Senol Cali, Damla and Kim, Jeremie S and Ghose, Saugata and Alkan, Can and Mutlu, Onur},
	month = jul,
	year = {2019},
	pages = {1542--1559},
}

@article{pollard_long_2018,
	title = {{Long Reads: Their Purpose and Place}},
	volume = {27},
	issn = {0964-6906},
	abstract = {In recent years long-read technologies have moved from being a niche and specialist field to a point of relative maturity likely to feature frequently in the genomic landscape. Analogous to next generation sequencing, the cost of sequencing using long-read technologies has materially dropped whilst the instrument throughput continues to increase. Together these changes present the prospect of sequencing large numbers of individuals with the aim of fully characterizing genomes at high resolution. In this article, we will endeavour to present an introduction to long-read technologies showing: what long reads are; how they are distinct from short reads; why long reads are useful and how they are being used. We will highlight the recent developments in this field, and the applications and potential of these technologies in medical research, and clinical diagnostics and therapeutics.},
	number = {R2},
	urldate = {2021-09-19},
	journal = {Human Molecular Genetics},
	author = {Pollard, Martin O and Gurdasani, Deepti and Mentzer, Alexander J and Porter, Tarryn and Sandhu, Manjinder S},
	month = aug,
	year = {2018},
	pages = {R234--R241},
}

@article{stoler_sequencing_2021,
	title = {{Sequencing Error Profiles of {Illumina} Sequencing Instruments}},
	volume = {3},
	issn = {2631-9268},
	abstract = {Sequencing technology has achieved great advances in the past decade. Studies have previously shown the quality of specific instruments in controlled conditions. Here, we developed a method able to retroactively determine the error rate of most public sequencing datasets. To do this, we utilized the overlaps between reads that are a feature of many sequencing libraries. With this method, we surveyed 1943 different datasets from seven different sequencing instruments produced by Illumina. We show that among public datasets, the more expensive platforms like HiSeq and NovaSeq have a lower error rate and less variation. But we also discovered that there is great variation within each platform, with the accuracy of a sequencing experiment depending greatly on the experimenter. We show the importance of sequence context, especially the phenomenon where preceding bases bias the following bases toward the same identity. We also show the difference in patterns of sequence bias between instruments. Contrary to expectations based on the underlying chemistry, HiSeq X Ten and NovaSeq 6000 share notable exceptions to the preceding-base bias. Our results demonstrate the importance of the specific circumstances of every sequencing experiment, and the importance of evaluating the quality of each one.},
	number = {1},
	urldate = {2021-09-19},
	journal = {NAR Genomics and Bioinformatics},
	author = {Stoler, Nicholas and Nekrutenko, Anton},
	month = mar,
	year = {2021},
}

@article{zhang_comprehensive_2020,
	title = {{A Comprehensive Evaluation of Long Read Error Correction Methods}},
	volume = {21},
	issn = {1471-2164},
	abstract = {Third-generation single molecule sequencing technologies can sequence long reads, which is advancing the frontiers of genomics research. However, their high error rates prohibit accurate and efficient downstream analysis. This difficulty has motivated the development of many long read error correction tools, which tackle this problem through sampling redundancy and/or leveraging accurate short reads of the same biological samples. Existing studies to asses these tools use simulated data sets, and are not sufficiently comprehensive in the range of software covered or diversity of evaluation measures used.},
	number = {6},
	journal = {BMC Genomics},
	author = {Zhang, Haowen and Jain, Chirag and Aluru, Srinivas},
	month = dec,
	year = {2020},
	pages = {889},
}

@article{wei_npbss_2018,
	title = {{NPBSS: A New {PacBio} Sequencing Simulator for Generating the Continuous Long Reads with an Empirical Model}},
	volume = {19},
	issn = {1471-2105},
	abstract = {PacBio sequencing platform offers longer read lengths than the second-generation sequencing technologies. It has revolutionized de novo genome assembly and enabled the automated reconstruction of reference-quality genomes. Due to its extremely wide range of application areas, fast sequencing simulation systems with high fidelity are in great demand to facilitate the development and comparison of subsequent analysis tools. Although there are several available simulators (e.g., PBSIM, SimLoRD and FASTQSim) that target the specific generation of PacBio libraries, the error rate of simulated sequences is not well matched to the quality value of raw PacBio datasets, especially for PacBio’s continuous long reads (CLR).},
	number = {1},
	journal = {BMC Bioinformatics},
	author = {Wei, Ze-Gang and Zhang, Shao-Wu},
	month = may,
	year = {2018},
	pages = {177},
}

@article{ma_analysis_2019,
	title = {{Analysis of Error Profiles in Deep Next-Generation Sequencing Data}},
	volume = {20},
	issn = {1474-760X},
	abstract = {Sequencing errors are key confounding factors for detecting low-frequency genetic variants that are important for cancer molecular diagnosis, treatment, and surveillance using deep next-generation sequencing (NGS). However, there is a lack of comprehensive understanding of errors introduced at various steps of a conventional NGS workflow, such as sample handling, library preparation, PCR enrichment, and sequencing. In this study, we use current NGS technology to systematically investigate these questions.},
	number = {1},
	journal = {Genome Biology},
	author = {Ma, Xiaotu and Shao, Ying and Tian, Liqing and Flasch, Diane A. and Mulder, Heather L. and Edmonson, Michael N. and Liu, Yu and Chen, Xiang and Newman, Scott and Nakitandwe, Joy and Li, Yongjin and Li, Benshang and Shen, Shuhong and Wang, Zhaoming and Shurtleff, Sheila and Robison, Leslie L. and Levy, Shawn and Easton, John and Zhang, Jinghui},
	month = mar,
	year = {2019},
	pages = {50},
}

@article{hon_highly_2020,
	title = {{Highly Accurate Long-Read {HiFi} Sequencing Data for Five Complex Genomes}},
	volume = {7},
	issn = {2052-4463},
	abstract = {The PacBio® HiFi sequencing method yields highly accurate long-read sequencing datasets with read lengths averaging 10–25 kb and accuracies greater than 99.5\%. These accurate long reads can be used to improve results for complex applications such as single nucleotide and structural variant detection, genome assembly, assembly of difficult polyploid or highly repetitive genomes, and assembly of metagenomes. Currently, there is a need for sample data sets to both evaluate the benefits of these long accurate reads as well as for development of bioinformatic tools including genome assemblers, variant callers, and haplotyping algorithms. We present deep coverage HiFi datasets for five complex samples including the two inbred model genomes Mus musculus and Zea mays, as well as two complex genomes, octoploid Fragaria × ananassa and the diploid anuran Rana muscosa. Additionally, we release sequence data from a mock metagenome community. The datasets reported here can be used without restriction to develop new algorithms and explore complex genome structure and evolution. Data were generated on the PacBio Sequel II System.},
	number = {1},
	journal = {Scientific Data},
	author = {Hon, Ting and Mars, Kristin and Young, Greg and Tsai, Yu-Chih and Karalius, Joseph W. and Landolin, Jane M. and Maurer, Nicholas and Kudrna, David and Hardigan, Michael A. and Steiner, Cynthia C. and Knapp, Steven J. and Ware, Doreen and Shapiro, Beth and Peluso, Paul and Rank, David R.},
	month = nov,
	year = {2020},
	pages = {399},
}

@article{canzar_short_2017,
	title = {Short {Read} {Mapping}: {An} {Algorithmic} {Tour}},
	volume = {105},
	issn = {1558-2256},
	number = {3},
	journal = {Proceedings of the IEEE},
	author = {Canzar, Stefan and Salzberg, Steven L.},
	month = mar,
	year = {2017},
	pages = {436--458},
}

@article{robertson_novo_2010,
	title = {{De Novo Assembly and Analysis of {RNA}-seq Data}},
	volume = {7},
	issn = {1548-7105},
	abstract = {The Trans-ABySS pipeline is an integrated approach for transcript assembly and analysis to identify new mRNA isoforms and structures.},
	number = {11},
	journal = {Nature Methods},
	author = {Robertson, Gordon and Schein, Jacqueline and Chiu, Readman and Corbett, Richard and Field, Matthew and Jackman, Shaun D and Mungall, Karen and Lee, Sam and Okada, Hisanaga Mark and Qian, Jenny Q and Griffith, Malachi and Raymond, Anthony and Thiessen, Nina and Cezard, Timothee and Butterfield, Yaron S and Newsome, Richard and Chan, Simon K and She, Rong and Varhol, Richard and Kamoh, Baljit and Prabhu, Anna-Liisa and Tam, Angela and Zhao, YongJun and Moore, Richard A and Hirst, Martin and Marra, Marco A and Jones, Steven J M and Hoodless, Pamela A and Birol, Inanc},
	month = nov,
	year = {2010},
	pages = {909--912},
}

@article{cheng_haplotype-resolved_2021,
	title = {{Haplotype-Resolved De Novo Assembly Using Phased Assembly Graphs with Hifiasm}},
	volume = {18},
	issn = {1548-7105},
	abstract = {Haplotype-resolved de novo assembly is the ultimate solution to the study of sequence variations in a genome. However, existing algorithms either collapse heterozygous alleles into one consensus copy or fail to cleanly separate the haplotypes to produce high-quality phased assemblies. Here we describe hifiasm, a de novo assembler that takes advantage of long high-fidelity sequence reads to faithfully represent the haplotype information in a phased assembly graph. Unlike other graph-based assemblers that only aim to maintain the contiguity of one haplotype, hifiasm strives to preserve the contiguity of all haplotypes. This feature enables the development of a graph trio binning algorithm that greatly advances over standard trio binning. On three human and five nonhuman datasets, including California redwood with a {\textasciitilde}30-Gb hexaploid genome, we show that hifiasm frequently delivers better assemblies than existing tools and consistently outperforms others on haplotype-resolved assembly.},
	number = {2},
	journal = {Nature Methods},
	author = {Cheng, Haoyu and Concepcion, Gregory T. and Feng, Xiaowen and Zhang, Haowen and Li, Heng},
	month = feb,
	year = {2021},
	pages = {170--175},
}

@article{ekim_minimizer-space_2021,
	title = {{Minimizer-Space de {Bruijn} Graphs: {Whole}-Genome Assembly of Long Reads in Minutes on a Personal Computer}},
	volume = {12},
	issn = {2405-4712},
	abstract = {Summary
DNA sequencing data continue to progress toward longer reads with increasingly lower sequencing error rates. Here, we define an algorithmic approach, mdBG, that makes use of minimizer-space de Bruijn graphs to enable long-read genome assembly. mdBG achieves orders-of-magnitude improvement in both speed and memory usage over existing methods without compromising accuracy. A human genome is assembled in under 10 min using 8 cores and 10 GB RAM, and 60 Gbp of metagenome reads are assembled in 4 min using 1 GB RAM. In addition, we constructed a minimizer-space de Bruijn graph-based representation of 661,405 bacterial genomes, comprising 16 million nodes and 45 million edges, and successfully search it for anti-microbial resistance (AMR) genes in 12 min. We expect our advances to be essential to sequence analysis, given the rise of long-read sequencing in genomics, metagenomics, and pangenomics. Code for constructing mdBGs is freely available for download at https://github.com/ekimb/rust-mdbg/.},
	number = {10},
	journal = {Cell Systems},
	author = {Ekim, Barış and Berger, Bonnie and Chikhi, Rayan},
	month = oct,
	year = {2021},
	keywords = {metagenomics, bacterial genomes, data structures, de Bruijn graphs, genome assembly, genome graphs, long-read sequencing, minimizers, pangenomics, partial order alignment},
	pages = {958--968.e6},
}

@article{wood_improved_2019,
	title = {{Improved Metagenomic Analysis with {Kraken} 2}},
	volume = {20},
	issn = {1474-760X},
	abstract = {Although Kraken’s k-mer-based approach provides a fast taxonomic classification of metagenomic sequence data, its large memory requirements can be limiting for some applications. Kraken 2 improves upon Kraken 1 by reducing memory usage by 85\%, allowing greater amounts of reference genomic data to be used, while maintaining high accuracy and increasing speed fivefold. Kraken 2 also introduces a translated search mode, providing increased sensitivity in viral metagenomics analysis.},
	number = {1},
	journal = {Genome Biology},
	author = {Wood, Derrick E. and Lu, Jennifer and Langmead, Ben},
	month = nov,
	year = {2019},
	pages = {257},
}

@article{loman_complete_2015,
	title = {{A Complete Bacterial Genome Assembled De Novo Using Only Nanopore Sequencing Data}},
	volume = {12},
	issn = {1548-7105},
	abstract = {By error-correcting long nanopore reads and calling a consensus sequence using nanopore signal data, an entire bacterial genome is assembled de novo.},
	number = {8},
	journal = {Nature Methods},
	author = {Loman, Nicholas J and Quick, Joshua and Simpson, Jared T},
	month = aug,
	year = {2015},
	pages = {733--735},
}

@article{vaser_fast_2017,
	title = {{Fast and Accurate De Novo Genome Assembly from Long Uncorrected Reads}},
	volume = {27},
	abstract = {The assembly of long reads from Pacific Biosciences and Oxford Nanopore Technologies typically requires resource-intensive error-correction and consensus-generation steps to obtain high-quality assemblies. We show that the error-correction step can be omitted and that high-quality consensus sequences can be generated efficiently with a SIMD-accelerated, partial-order alignment–based, stand-alone consensus module called Racon. Based on tests with PacBio and Oxford Nanopore data sets, we show that Racon coupled with miniasm enables consensus genomes with similar or better quality than state-of-the-art methods while being an order of magnitude faster.},
	number = {5},
	journal = {Genome Research},
	author = {Vaser, Robert and Sović, Ivan and Nagarajan, Niranjan and Šikić, Mile},
	month = may,
	year = {2017},
	pages = {737--746},
}

@article{walker_pilon_2014,
	title = {Pilon: {An} {Integrated} {Tool} for {Comprehensive} {Microbial} {Variant} {Detection} and {Genome} {Assembly} {Improvement}},
	volume = {9},
	abstract = {Advances in modern sequencing technologies allow us to generate sufficient data to analyze hundreds of bacterial genomes from a single machine in a single day. This potential for sequencing massive numbers of genomes calls for fully automated methods to produce high-quality assemblies and variant calls. We introduce Pilon, a fully automated, all-in-one tool for correcting draft assemblies and calling sequence variants of multiple sizes, including very large insertions and deletions. Pilon works with many types of sequence data, but is particularly strong when supplied with paired end data from two Illumina libraries with small e.g., 180 bp and large e.g., 3–5 Kb inserts. Pilon significantly improves draft genome assemblies by correcting bases, fixing mis-assemblies and filling gaps. For both haploid and diploid genomes, Pilon produces more contiguous genomes with fewer errors, enabling identification of more biologically relevant genes. Furthermore, Pilon identifies small variants with high accuracy as compared to state-of-the-art tools and is unique in its ability to accurately identify large sequence variants including duplications and resolve large insertions. Pilon is being used to improve the assemblies of thousands of new genomes and to identify variants from thousands of clinically relevant bacterial strains. Pilon is freely available as open source software.},
	number = {11},
	journal = {PLOS ONE},
	author = {Walker, Bruce J. and Abeel, Thomas and Shea, Terrance and Priest, Margaret and Abouelliel, Amr and Sakthikumar, Sharadha and Cuomo, Christina A. and Zeng, Qiandong and Wortman, Jennifer and Young, Sarah K. and Earl, Ashlee M.},
	month = nov,
	year = {2014},
	pages = {e112963},
}

@article{firtina_apollo_2020,
	title = {{Apollo: A Sequencing-Technology-Independent, Scalable and Accurate Assembly Polishing Algorithm}},
	volume = {36},
	issn = {1367-4803},
	abstract = {Third-generation sequencing technologies can sequence long reads that contain as many as 2 million base pairs. These long reads are used to construct an assembly (i.e. the subject’s genome), which is further used in downstream genome analysis. Unfortunately, third-generation sequencing technologies have high sequencing error rates and a large proportion of base pairs in these long reads is incorrectly identified. These errors propagate to the assembly and affect the accuracy of genome analysis. Assembly polishing algorithms minimize such error propagation by polishing or fixing errors in the assembly by using information from alignments between reads and the assembly (i.e. read-to-assembly alignment information). However, current assembly polishing algorithms can only polish an assembly using reads from either a certain sequencing technology or a small assembly. Such technology-dependency and assembly-size dependency require researchers to (i) run multiple polishing algorithms and (ii) use small chunks of a large genome to use all available readsets and polish large genomes, respectively.We introduce Apollo, a universal assembly polishing algorithm that scales well to polish an assembly of any size (i.e. both large and small genomes) using reads from all sequencing technologies (i.e. second- and third-generation). Our goal is to provide a single algorithm that uses read sets from all available sequencing technologies to improve the accuracy of assembly polishing and that can polish large genomes. Apollo (i) models an assembly as a profile hidden Markov model (pHMM), (ii) uses read-to-assembly alignment to train the pHMM with the Forward–Backward algorithm and (iii) decodes the trained model with the Viterbi algorithm to produce a polished assembly. Our experiments with real readsets demonstrate that Apollo is the only algorithm that (i) uses reads from any sequencing technology within a single run and (ii) scales well to polish large assemblies without splitting the assembly into multiple parts.Source code is available at https://github.com/CMU-SAFARI/Apollo.Supplementary data are available at Bioinformatics online.},
	number = {12},
	urldate = {2021-09-19},
	journal = {Bioinformatics},
	author = {Firtina, Can and Kim, Jeremie S and Alser, Mohammed and Senol Cali, Damla and Cicek, A Ercument and Alkan, Can and Mutlu, Onur},
	month = jun,
	year = {2020},
	pages = {3669--3679},
}

@article{the_computational_pan-genomics_consortium_computational_2018,
	title = {{Computational Pan-Genomics: Status, Promises and Challenges}},
	volume = {19},
	issn = {1477-4054},
	abstract = {Many disciplines, from human genetics and oncology to plant breeding, microbiology and virology, commonly face the challenge of analyzing rapidly increasing numbers of genomes. In case of Homo sapiens, the number of sequenced genomes will approach hundreds of thousands in the next few years. Simply scaling up established bioinformatics pipelines will not be sufficient for leveraging the full potential of such rich genomic data sets. Instead, novel, qualitatively different computational methods and paradigms are needed. We will witness the rapid extension of computational pan-genomics, a new sub-area of research in computational biology. In this article, we generalize existing definitions and understand a pan-genome as any collection of genomic sequences to be analyzed jointly or to be used as a reference. We examine already available approaches to construct and use pan-genomes, discuss the potential benefits of future technologies and methodologies and review open challenges from the vantage point of the above-mentioned biological disciplines. As a prominent example for a computational paradigm shift, we particularly highlight the transition from the representation of reference genomes as strings to representations as graphs. We outline how this and other challenges from different application domains translate into common computational problems, point out relevant bioinformatics techniques and identify open problems in computer science. With this review, we aim to increase awareness that a joint approach to computational pan-genomics can help address many of the problems currently faced in various domains.},
	number = {1},
	urldate = {2021-09-19},
	journal = {Briefings in Bioinformatics},
	author = {{The Computational Pan-Genomics Consortium}},
	month = jan,
	year = {2018},
	pages = {118--135},
}

@article{alkan_genome_2011,
	title = {{Genome Structural Variation Discovery and Genotyping}},
	volume = {12},
	issn = {1471-0064},
	abstract = {Structural variation was originally defined as insertions, deletions and inversions greater than 1 kb in size, but with the sequencing of human genomes now becoming routine, the operational spectrum of structural variants has widened to include events {\textgreater}50 bp in length.The main focus of structural variant (SV) studies should be accurate characterization of the copy, content and structure of genomic variants.Methods to discover and genotype structural variation can be divided into two main types: experimental and computational.Experimental methods for discovering SVs include hybridization-based approaches (SNP microarrays and array comparative genomic hybridization) and single-molecule analysis (optical mapping). In addition, PCR-based techniques can be used to genotype SVs.Computational methods use genome sequencing data to discover and genotype SVs. There are four main computational approaches: read-pair, read-depth, split-read and sequence-assembly methods.All existing platforms and methods have different biases and limitations. Accurate characterization of the full spectrum of structural variation remains a challenge.},
	number = {5},
	journal = {Nature Reviews Genetics},
	author = {Alkan, Can and Coe, Bradley P. and Eichler, Evan E.},
	month = may,
	year = {2011},
	pages = {363--376},
}

@article{alkan_limitations_2011,
	title = {{Limitations of Next-Generation Genome Sequence Assembly}},
	volume = {8},
	issn = {1548-7105},
	abstract = {High-throughput sequencing technologies promise to transform the fields of genetics and comparative biology by delivering tens of thousands of genomes in the near future. Although it is feasible to construct de novo genome assemblies in a few months, there has been relatively little attention to what is lost by sole application of short sequence reads. We compared the recent de novo assemblies using the short oligonucleotide analysis package (SOAP), generated from the genomes of a Han Chinese individual and a Yoruban individual, to experimentally validated genomic features. We found that de novo assemblies were 16.2\% shorter than the reference genome and that 420.2 megabase pairs of common repeats and 99.1\% of validated duplicated sequences were missing from the genome. Consequently, over 2,377 coding exons were completely missing. We conclude that high-quality sequencing approaches must be considered in conjunction with high-throughput sequencing for comparative genomics analyses and studies of genome evolution.},
	number = {1},
	journal = {Nature Methods},
	author = {Alkan, Can and Sajjadian, Saba and Eichler, Evan E},
	month = jan,
	year = {2011},
	pages = {61--65},
}

@article{xin_context-aware_2020,
	title = {{Context-Aware Seeds for Read Mapping}},
	volume = {15},
	issn = {1748-7188},
	abstract = {Most modern seed-and-extend NGS read mappers employ a seeding scheme that requires extracting t non-overlapping seeds in each read in order to find all valid mappings under an edit distance threshold of t. As t grows, this seeding scheme forces mappers to use more and shorter seeds, which increases the seed hits (seed frequencies) and therefore reduces the efficiency of mappers.},
	number = {1},
	journal = {Algorithms for Molecular Biology},
	author = {Xin, Hongyi and Shao, Mingfu and Kingsford, Carl},
	month = may,
	year = {2020},
	pages = {10},
}

@inproceedings{cali_genasm_2020,
	title = {{GenASM}: {A} {High}-{Performance}, {Low}-{Power} {Approximate} {String} {Matching} {Acceleration} {Framework} for {Genome} {Sequence} {Analysis}},
	booktitle = {2020 53rd {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Senol Cali, Damla and Kalsi, Gurpreet S. and Bingöl, Zülal and Firtina, Can and Subramanian, Lavanya and Kim, Jeremie S. and Ausavarungnirun, Rachata and Alser, Mohammed and Gomez-Luna, Juan and Boroumand, Amirali and Norion, Anant and Scibisz, Allison and Subramoneyon, Sreenivas and Alkan, Can and Ghose, Saugata and Mutlu, Onur},
	year = {2020},
	pages = {951--966},
}


@article{marcais_mummer4_2018,
	title = {{MUMmer4}: {A Fast and Versatile Genome Alignment System}},
	volume = {14},
	abstract = {The MUMmer system and the genome sequence aligner nucmer included within it are among the most widely used alignment packages in genomics. Since the last major release of MUMmer version 3 in 2004, it has been applied to many types of problems including aligning whole genome sequences, aligning reads to a reference genome, and comparing different assemblies of the same genome. Despite its broad utility, MUMmer3 has limitations that can make it difficult to use for large genomes and for the very large sequence data sets that are common today. In this paper we describe MUMmer4, a substantially improved version of MUMmer that addresses genome size constraints by changing the 32-bit suffix tree data structure at the core of MUMmer to a 48-bit suffix array, and that offers improved speed through parallel processing of input query sequences. With a theoretical limit on the input size of 141Tbp, MUMmer4 can now work with input sequences of any biologically realistic length. We show that as a result of these enhancements, the nucmer program in MUMmer4 is easily able to handle alignments of large genomes; we illustrate this with an alignment of the human and chimpanzee genomes, which allows us to compute that the two species are 98\% identical across 96\% of their length. With the enhancements described here, MUMmer4 can also be used to efficiently align reads to reference genomes, although it is less sensitive and accurate than the dedicated read aligners. The nucmer aligner in MUMmer4 can now be called from scripting languages such as Perl, Python and Ruby. These improvements make MUMer4 one the most versatile genome alignment packages available.},
	number = {1},
	journal = {PLOS Computational Biology},
	author = {Marçais, Guillaume and Delcher, Arthur L. and Phillippy, Adam M. and Coston, Rachel and Salzberg, Steven L. and Zimin, Aleksey},
	month = jan,
	year = {2018},
	pages = {e1005944},
}

@article{li_minimap_2016,
	title = {{Minimap and Miniasm: Fast Mapping and De Novo Assembly for Noisy Long Sequences}},
	volume = {32},
	issn = {1367-4803},
	abstract = {Motivation: Single Molecule Real-Time (SMRT) sequencing technology and Oxford Nanopore technologies (ONT) produce reads over 10 kb in length, which have enabled high-quality genome assembly at an affordable cost. However, at present, long reads have an error rate as high as 10–15\%. Complex and computationally intensive pipelines are required to assemble such reads.Results: We present a new mapper, minimap and a de novo assembler, miniasm, for efficiently mapping and assembling SMRT and ONT reads without an error correction stage. They can often assemble a sequencing run of bacterial data into a single contig in a few minutes, and assemble 45-fold Caenorhabditis elegans data in 9 min, orders of magnitude faster than the existing pipelines, though the consensus sequence error rate is as high as raw reads. We also introduce a pairwise read mapping format and a graphical fragment assembly format, and demonstrate the interoperability between ours and current tools.Availability and implementation:https://github.com/lh3/minimap and https://github.com/lh3/miniasmContact:hengli@broadinstitute.orgSupplementary information:Supplementary data are available at Bioinformatics online.},
	number = {14},
	urldate = {2021-10-14},
	journal = {Bioinformatics},
	author = {Li, Heng},
	month = jul,
	year = {2016},
	pages = {2103--2110},
}

@article{shen_seqkit_2016,
	title = {{SeqKit}: {A} {Cross}-{Platform} and {Ultrafast} {Toolkit} for {FASTA}/{Q} {File} {Manipulation}},
	volume = {11},
	abstract = {FASTA and FASTQ are basic and ubiquitous formats for storing nucleotide and protein sequences. Common manipulations of FASTA/Q file include converting, searching, filtering, deduplication, splitting, shuffling, and sampling. Existing tools only implement some of these manipulations, and not particularly efficiently, and some are only available for certain operating systems. Furthermore, the complicated installation process of required packages and running environments can render these programs less user friendly. This paper describes a cross-platform ultrafast comprehensive toolkit for FASTA/Q processing. SeqKit provides executable binary files for all major operating systems, including Windows, Linux, and Mac OSX, and can be directly used without any dependencies or pre-configurations. SeqKit demonstrates competitive performance in execution time and memory usage compared to similar tools. The efficiency and usability of SeqKit enable researchers to rapidly accomplish common FASTA/Q file manipulations. SeqKit is open source and available on Github at https://github.com/shenwei356/seqkit.},
	number = {10},
	journal = {PLOS ONE},
	author = {Shen, Wei and Le, Shuai and Li, Yan and Hu, Fuquan},
	month = oct,
	year = {2016},
	pages = {e0163962},
}


@article{quinlan_bedtools_2010,
	title = {{BEDTools: A Flexible Suite of Utilities for Comparing Genomic Features}},
	volume = {26},
	issn = {1367-4803},
	abstract = {Motivation: Testing for correlations between different sets of genomic features is a fundamental task in genomics research. However, searching for overlaps between features with existing web-based methods is complicated by the massive datasets that are routinely produced with current sequencing technologies. Fast and flexible tools are therefore required to ask complex questions of these data in an efficient manner.Results: This article introduces a new software suite for the comparison, manipulation and annotation of genomic features in Browser Extensible Data (BED) and General Feature Format (GFF) format. BEDTools also supports the comparison of sequence alignments in BAM format to both BED and GFF features. The tools are extremely efficient and allow the user to compare large datasets (e.g. next-generation sequencing data) with both public and custom genome annotation tracks. BEDTools can be combined with one another as well as with standard UNIX commands, thus facilitating routine genomics tasks as well as pipelines that can quickly answer intricate questions of large genomic datasets.Availability and implementation: BEDTools was written in C++. Source code and a comprehensive user manual are freely available at http://code.google.com/p/bedtoolsContact:aaronquinlan@gmail.com; imh4y@virginia.eduSupplementary information:Supplementary data are available at Bioinformatics online.},
	number = {6},
	urldate = {2021-10-14},
	journal = {Bioinformatics},
	author = {Quinlan, Aaron R. and Hall, Ira M.},
	month = mar,
	year = {2010},
	pages = {841--842},
}

@article{pedersen_mosdepth_2018,
	title = {{Mosdepth: Quick Coverage Calculation for Genomes and Exomes}},
	volume = {34},
	issn = {1367-4803},
	abstract = {Mosdepth is a new command-line tool for rapidly calculating genome-wide sequencing coverage. It measures depth from BAM or CRAM files at either each nucleotide position in a genome or for sets of genomic regions. Genomic regions may be specified as either a BED file to evaluate coverage across capture regions, or as a fixed-size window as required for copy-number calling. Mosdepth uses a simple algorithm that is computationally efficient and enables it to quickly produce coverage summaries. We demonstrate that mosdepth is faster than existing tools and provides flexibility in the types of coverage profiles produced.mosdepth is available from https://github.com/brentp/mosdepth under the MIT license.Supplementary data are available at Bioinformatics online.},
	number = {5},
	urldate = {2021-10-14},
	journal = {Bioinformatics},
	author = {Pedersen, Brent S and Quinlan, Aaron R},
	month = mar,
	year = {2018},
	pages = {867--868},
}

@article{jun_efficient_2015,
	title = {{An Efficient and Scalable Analysis Framework for Variant Extraction and Refinement from Population Scale {DNA} Sequence Data}},
	abstract = {The analysis of next-generation sequencing data is computationally and statistically challenging because of massive data volumes and imperfect data quality. We present GotCloud, a pipeline for efficiently detecting and genotyping high-quality variants from large-scale sequencing data. GotCloud automates sequence alignment, sample-level quality control, variant calling, filtering of likely artifacts using machine learning techniques, and genotype refinement using haplotype information. The pipeline can process thousands of samples in parallel and requires less computational resources than current alternatives. Experiments with whole genome and exome targeted sequence data generated by the 1000 Genomes Project show that the pipeline provides effective filtering against false positive variants and high power to detect true variants. Our pipeline has already contributed to variant detection and genotyping in several large-scale sequencing projects, including the 1000 Genomes Project and the NHLBI Exome Sequencing Project.  We hope it will now prove useful to many medical sequencing studies.},
	journal = {Genome Research},
	author = {Jun, Goo and Wing, Mary Kate and Abecasis, Gonçalo R and Kang, Hyun Min},
	month = apr,
	year = {2015},
}


@Article{Goodwin2016,
  author          = {Goodwin, Sara and McPherson, John D and McCombie, W Richard},
  title           = {{Coming of Age: Ten Years of Next-Generation Sequencing Technologies}},
  journaltitle    = {Nature Reviews Genetics},
  year            = {2016},
  volume          = {17},
  issue           = {6},
  month           = {may},
  pages           = {333--351},
  issn            = {1471-0064},
  doi             = {10.1038/nrg.2016.49},
  abstract        = {Since the completion of the human genome project in 2003, extraordinary progress has been made in genome sequencing technologies, which has led to a decreased cost per megabase and an increase in the number and diversity of sequenced genomes. An astonishing complexity of genome architecture has been revealed, bringing these sequencing technologies to even greater advancements. Some approaches maximize the number of bases sequenced in the least amount of time, generating a wealth of data that can be used to understand increasingly complex phenotypes. Alternatively, other approaches now aim to sequence longer contiguous pieces of DNA, which are essential for resolving structurally complex regions. These and other strategies are providing researchers and clinicians a variety of tools to probe genomes in greater depth, leading to an enhanced understanding of how genome sequence variants underlie phenotype and disease.},
  citation-subset = {IM},
  country         = {England},
  created         = {2016-05-17},
  issn-linking    = {1471-0056},
  journal         = {Nature Reviews Genetics},
  nlm-id          = {100962779},
  owner           = {calkan},
  pii             = {nrg.2016.49},
  pmid            = {27184599},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2016-05-17},
  timestamp       = {2016.12.11},
}


@article{gurevich_quast_2013,
	title = {{QUAST}: {Quality Assessment Tool for Genome Assemblies}},
	volume = {29},
	issn = {1367-4803},
	abstract = {Summary: Limitations of genome sequencing techniques have led to dozens of assembly algorithms, none of which is perfect. A number of methods for comparing assemblers have been developed, but none is yet a recognized benchmark. Further, most existing methods for comparing assemblies are only applicable to new assemblies of finished genomes; the problem of evaluating assemblies of previously unsequenced species has not been adequately considered. Here, we present QUAST—a quality assessment tool for evaluating and comparing genome assemblies. This tool improves on leading assembly comparison software with new ideas and quality metrics. QUAST can evaluate assemblies both with a reference genome, as well as without a reference. QUAST produces many reports, summary tables and plots to help scientists in their research and in their publications. In this study, we used QUAST to compare several genome assemblers on three datasets. QUAST tables and plots for all of them are available in the Supplementary Material, and interactive versions of these reports are on the QUAST website.Availability:http://bioinf.spbau.ru/quastContact:gurevich@bioinf.spbau.ruSupplementary information:Supplementary data are available at Bioinformatics online.},
	number = {8},
	urldate = {2021-10-14},
	journal = {Bioinformatics},
	author = {Gurevich, Alexey and Saveliev, Vladislav and Vyahhi, Nikolay and Tesler, Glenn},
	month = apr,
	year = {2013},
	pages = {1072--1075},
}

@article{ono_pbsim_2013,
	title = {{PBSIM}: {PacBio Reads Simulator—Toward Accurate Genome Assembly}},
	volume = {29},
	issn = {1367-4803},
	abstract = {Motivation: PacBio sequencers produce two types of characteristic reads (continuous long reads: long and high error rate and circular consensus sequencing: short and low error rate), both of which could be useful for de novo assembly of genomes. Currently, there is no available simulator that targets the specific generation of PacBio libraries.Results: Our analysis of 13 PacBio datasets showed characteristic features of PacBio reads (e.g. the read length of PacBio reads follows a log-normal distribution). We have developed a read simulator, PBSIM, that captures these features using either a model-based or sampling-based method. Using PBSIM, we conducted several hybrid error correction and assembly tests for PacBio reads, suggesting that a continuous long reads coverage depth of at least 15 in combination with a circular consensus sequencing coverage depth of at least 30 achieved extensive assembly results.Availability: PBSIM is freely available from the web under the GNU GPL v2 license (http://code.google.com/p/pbsim/).Contact:mhamada@k.u-tokyo.ac.jpSupplementary information:Supplementary data are available at Bioinformatics online.},
	number = {1},
	urldate = {2021-10-14},
	journal = {Bioinformatics},
	author = {Ono, Yukiteru and Asai, Kiyoshi and Hamada, Michiaki},
	month = jan,
	year = {2013},
	pages = {119--121},
}

@article{ballouz_is_2019,
	title = {{Is it Time to Change the Reference Genome?}},
	volume = {20},
	issn = {1474-760X},
	abstract = {The use of the human reference genome has shaped methods and data across modern genomics. This has offered many benefits while creating a few constraints. In the following opinion, we outline the history, properties, and pitfalls of the current human reference genome. In a few illustrative analyses, we focus on its use for variant-calling, highlighting its nearness to a ‘type specimen’. We suggest that switching to a consensus reference would offer important advantages over the continued use of the current reference with few disadvantages.},
	number = {1},
	journal = {Genome Biology},
	author = {Ballouz, Sara and Dobin, Alexander and Gillis, Jesse A.},
	month = aug,
	year = {2019},
	pages = {159},
}

@inproceedings{ahmed_comparison_2016,
	title = {{A Comparison of Seed-and-Extend Techniques in Modern {DNA} Read Alignment Algorithms}},
	booktitle = {2016 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Ahmed, Nauman and Bertels, Koen and Al-Ars, Zaid},
	year = {2016},
	pages = {1421--1428},
}

@article{xin_accelerating_2013,
	title = {{Accelerating Read Mapping with FastHASH}},
	volume = {14},
	issn = {1471-2164},
	abstract = {With the introduction of next-generation sequencing (NGS) technologies, we are facing an exponential increase in the amount of genomic sequence data. The success of all medical and genetic applications of next-generation sequencing critically depends on the existence of computational techniques that can process and analyze the enormous amount of sequence data quickly and accurately. Unfortunately, the current read mapping algorithms have difficulties in coping with the massive amounts of data generated by NGS.},
	number = {1},
	journal = {BMC Genomics},
	author = {Xin, Hongyi and Lee, Donghyuk and Hormozdiari, Farhad and Yedkar, Samihan and Mutlu, Onur and Alkan, Can},
	month = jan,
	year = {2013},
	pages = {S13},
}

@article{alser_gatekeeper_2017,
	title = {{GateKeeper}: {A New Hardware Architecture for Accelerating Pre-Alignment in {DNA} Short Read Mapping}},
	volume = {33},
	issn = {1367-4803},
	abstract = {High throughput DNA sequencing (HTS) technologies generate an excessive number of small DNA segments -called short reads- that cause significant computational burden. To analyze the entire genome, each of the billions of short reads must be mapped to a reference genome based on the similarity between a read and ‘candidate’ locations in that reference genome. The similarity measurement, called alignment, formulated as an approximate string matching problem, is the computational bottleneck because: (i) it is implemented using quadratic-time dynamic programming algorithms and (ii) the majority of candidate locations in the reference genome do not align with a given read due to high dissimilarity. Calculating the alignment of such incorrect candidate locations consumes an overwhelming majority of a modern read mapper’s execution time. Therefore, it is crucial to develop a fast and effective filter that can detect incorrect candidate locations and eliminate them before invoking computationally costly alignment algorithms.We propose GateKeeper, a new hardware accelerator that functions as a pre-alignment step that quickly filters out most incorrect candidate locations. GateKeeper is the first design to accelerate pre-alignment using Field-Programmable Gate Arrays (FPGAs), which can perform pre-alignment much faster than software. When implemented on a single FPGA chip, GateKeeper maintains high accuracy (on average \&gt;96\%) while providing, on average, 90-fold and 130-fold speedup over the state-of-the-art software pre-alignment techniques, Adjacency Filter and Shifted Hamming Distance (SHD), respectively. The addition of GateKeeper as a pre-alignment step can reduce the verification time of the mrFAST mapper by a factor of 10.https://github.com/BilkentCompGen/GateKeeperSupplementary data are available at Bioinformatics online.},
	number = {21},
	urldate = {2021-09-20},
	journal = {Bioinformatics},
	author = {Alser, Mohammed and Hassan, Hasan and Xin, Hongyi and Ergin, Oğuz and Mutlu, Onur and Alkan, Can},
	month = nov,
	year = {2017},
	pages = {3355--3363},
}

@article{xin_shifted_2015,
	title = {{Shifted {Hamming} Distance: A Fast and Accurate {SIMD}-Friendly Filter to Accelerate Alignment Verification in Read Mapping}},
	volume = {31},
	issn = {1367-4803},
	abstract = {Motivation: Calculating the edit-distance (i.e. minimum number of insertions, deletions and substitutions) between short DNA sequences is the primary task performed by seed-and-extend based mappers, which compare billions of sequences. In practice, only sequence pairs with a small edit-distance provide useful scientific data. However, the majority of sequence pairs analyzed by seed-and-extend based mappers differ by significantly more errors than what is typically allowed. Such error-abundant sequence pairs needlessly waste resources and severely hinder the performance of read mappers. Therefore, it is crucial to develop a fast and accurate filter that can rapidly and efficiently detect error-abundant string pairs and remove them from consideration before more computationally expensive methods are used.Results: We present a simple and efficient algorithm, Shifted Hamming Distance (SHD), which accelerates the alignment verification procedure in read mapping, by quickly filtering out error-abundant sequence pairs using bit-parallel and SIMD-parallel operations. SHD only filters string pairs that contain more errors than a user-defined threshold, making it fully comprehensive. It also maintains high accuracy with moderate error threshold (up to 5\% of the string length) while achieving a 3-fold speedup over the best previous algorithm (Gene Myers’s bit-vector algorithm). SHD is compatible with all mappers that perform sequence alignment for verification.Availability and implementation: We provide an implementation of SHD in C with Intel SSE instructions at: https://github.com/CMU-SAFARI/SHD.Contact: hxin@cmu.edu, calkan@cs.bilkent.edu.tr or onur@cmu.eduSupplementary information: Supplementary data are available at Bioinformatics online.},
	number = {10},
	urldate = {2021-09-20},
	journal = {Bioinformatics},
	author = {Xin, Hongyi and Greth, John and Emmons, John and Pekhimenko, Gennady and Kingsford, Carl and Alkan, Can and Mutlu, Onur},
	month = may,
	year = {2015},
	pages = {1553--1560},
}

@article{li_minimap2_2018,
	title = {{Minimap2: Pairwise Alignment for Nucleotide Sequences}},
	volume = {34},
	issn = {1367-4803},
	abstract = {Recent advances in sequencing technologies promise ultra-long reads of ∼100 kb in average, full-length mRNA or cDNA reads in high throughput and genomic contigs over 100 Mb in length. Existing alignment programs are unable or inefficient to process such data at scale, which presses for the development of new alignment algorithms.Minimap2 is a general-purpose alignment program to map DNA or long mRNA sequences against a large reference database. It works with accurate short reads of ≥100 bp in length, ≥1 kb genomic reads at error rate ∼15\%, full-length noisy Direct RNA or cDNA reads and assembly contigs or closely related full chromosomes of hundreds of megabases in length. Minimap2 does split-read alignment, employs concave gap cost for long insertions and deletions and introduces new heuristics to reduce spurious alignments. It is 3–4 times as fast as mainstream short-read mappers at comparable accuracy, and is ≥30 times faster than long-read genomic or cDNA mappers at higher accuracy, surpassing most aligners specialized in one type of alignment.https://github.com/lh3/minimap2Supplementary data are available at Bioinformatics online.},
	number = {18},
	urldate = {2021-09-20},
	journal = {Bioinformatics},
	author = {Li, Heng},
	month = sep,
	year = {2018},
	pages = {3094--3100},
}

@article{jain_weighted_2020,
	title = {{Weighted Minimizer Sampling Improves Long Read Mapping}},
	volume = {36},
	issn = {1367-4803},
	abstract = {In this era of exponential data growth, minimizer sampling has become a standard algorithmic technique for rapid genome sequence comparison. This technique yields a sub-linear representation of sequences, enabling their comparison in reduced space and time. A key property of the minimizer technique is that if two sequences share a substring of a specified length, then they can be guaranteed to have a matching minimizer. However, because the k-mer distribution in eukaryotic genomes is highly uneven, minimizer-based tools (e.g. Minimap2, Mashmap) opt to discard the most frequently occurring minimizers from the genome to avoid excessive false positives. By doing so, the underlying guarantee is lost and accuracy is reduced in repetitive genomic regions.We introduce a novel weighted-minimizer sampling algorithm. A unique feature of the proposed algorithm is that it performs minimizer sampling while considering a weight for each k-mer; i.e. the higher the weight of a k-mer, the more likely it is to be selected. By down-weighting frequently occurring k-mers, we are able to meet both objectives: (i) avoid excessive false-positive matches and (ii) maintain the minimizer match guarantee. We tested our algorithm, Winnowmap, using both simulated and real long-read data and compared it to a state-of-the-art long read mapper, Minimap2. Our results demonstrate a reduction in the mapping error-rate from 0.14\% to 0.06\% in the recently finished human X chromosome (154.3 Mbp), and from 3.6\% to 0\% within the highly repetitive X centromere (3.1 Mbp). Winnowmap improves mapping accuracy within repeats and achieves these results with sparser sampling, leading to better index compression and competitive runtimes. Winnowmap is built on top of the Minimap2 codebase and is available at https://github.com/marbl/winnowmap.},
	number = {Supplement\_1},
	urldate = {2021-09-20},
	journal = {Bioinformatics},
	author = {Jain, Chirag and Rhie, Arang and Zhang, Haowen and Chu, Claudia and Walenz, Brian P and Koren, Sergey and Phillippy, Adam M},
	month = jul,
	year = {2020},
	pages = {i111--i118},
}

@inproceedings{deblasio_practical_2019,
	address = {New York, NY, USA},
	series = {{BCB} '19},
	title = {Practical {Universal} {K}-{Mer} {Sets} for {Minimizer} {Schemes}},
	isbn = {978-1-4503-6666-3},
	abstract = {Minimizer schemes have found widespread use in genomic applications as a way to quickly predict the matching probability of large sequences. Most methods for minimizer schemes use randomized (or close to randomized) ordering of k-mers when finding minimizers, but recent work has shown that not all non-lexicographic orderings perform the same. One way to find k-mer orderings for minimizer schemes is through the use of universal k-mer sets, which are subsets of k-mers that are guaranteed to cover all windows. The smaller this set the fewer false positives (where two poorly aligned sequences are labeled as possible matches) are identified. Current methods for creating universal k-mer sets are limited in the length of the k-mer that can be considered, and cannot compute sets in the range of lengths currently used in practice. We take some of the first steps in creating universal k-mer sets that can be used to construct minimizer orders for large values of k that are practical. We do this using iterative extension of the k-mers in a set, and guided contraction of the set itself. We also show that this process will be guaranteed to never increase the number of distinct minimizers chosen in a sequence, and thus can only decrease the number of false positives over using the current sets on small k-mers.},
	booktitle = {Proceedings of the 10th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology} and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {DeBlasio, Dan and Gbosibo, Fiyinfoluwa and Kingsford, Carl and Marçais, Guillaume},
	year = {2019},
	keywords = {genomics, k-mer, minimizer schemes, universal sets},
	pages = {167--176},
}

@incollection{guidi_bella_2021,
	series = {Proceedings},
	title = {{BELLA}: {Berkeley} {Efficient} {Long}-{Read} to {Long}-{Read} {Aligner} and {Overlapper}},
	booktitle = {Proceedings of the 2021 {SIAM} {Conference} on {Applied} and {Computational} {Discrete} {Algorithms} ({ACDA21})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Guidi, Giulia and Ellis, Marquita and Rokhsar, Daniel and Yelick, Katherine and Buluç, Ayd?n},
	month = jan,
	year = {2021},
	pages = {123--134},
}


@article{chaisson_mapping_2012,
	title = {{Mapping Single Molecule Sequencing Reads Using Basic Local Alignment with Successive Refinement ({BLASR}): Application and Theory}},
	volume = {13},
	issn = {1471-2105},
	abstract = {Recent methods have been developed to perform high-throughput sequencing of DNA by Single Molecule Sequencing (SMS). While Next-Generation sequencing methods may produce reads up to several hundred bases long, SMS sequencing produces reads up to tens of kilobases long. Existing alignment methods are either too inefficient for high-throughput datasets, or not sensitive enough to align SMS reads, which have a higher error rate than Next-Generation sequencing.},
	number = {1},
	journal = {BMC Bioinformatics},
	author = {Chaisson, Mark J. and Tesler, Glenn},
	month = sep,
	year = {2012},
	pages = {238},
}


@article{ondov_mash_2016,
	title = {{Mash: Fast Genome and Metagenome Distance Estimation Using} {MinHash}},
	volume = {17},
	issn = {1474-760X},
	abstract = {Mash extends the MinHash dimensionality-reduction technique to include a pairwise mutation distance and P value significance test, enabling the efficient clustering and search of massive sequence collections. Mash reduces large sequences and sequence sets to small, representative sketches, from which global mutation distances can be rapidly estimated. We demonstrate several use cases, including the clustering of all 54,118 NCBI RefSeq genomes in 33 CPU h; real-time database search using assembled or unassembled Illumina, Pacific Biosciences, and Oxford Nanopore data; and the scalable clustering of hundreds of metagenomic samples by composition. Mash is freely released under a BSD license (https://github.com/marbl/mash).},
	number = {1},
	journal = {Genome Biology},
	author = {Ondov, Brian D. and Treangen, Todd J. and Melsted, Páll and Mallonee, Adam B. and Bergman, Nicholas H. and Koren, Sergey and Phillippy, Adam M.},
	month = jun,
	year = {2016},
	pages = {132},
}

@article{roberts_reducing_2004,
	title = {{Reducing Storage Requirements for Biological Sequence Comparison}},
	volume = {20},
	issn = {1367-4803},
	abstract = {Motivation: Comparison of nucleic acid and protein sequences is a fundamental tool of modern bioinformatics. A dominant method of such string matching is the ‘seed-and-extend’ approach, in which occurrences of short subsequences called ‘seeds’ are used to search for potentially longer matches in a large database of sequences. Each such potential match is then checked to see if it extends beyond the seed. To be effective, the seed-and-extend approach needs to catalogue seeds from virtually every substring in the database of search strings. Projects such as mammalian genome assemblies and large-scale protein matching, however, have such large sequence databases that the resulting list of seeds cannot be stored in RAM on a single computer. This significantly slows the matching process.Results: We present a simple and elegant method in which only a small fraction of seeds, called ‘minimizers’, needs to be stored. Using minimizers can speed up string-matching computations by a large factor while missing only a small fraction of the matches found using all seeds.},
	number = {18},
	urldate = {2021-09-20},
	journal = {Bioinformatics},
	author = {Roberts, Michael and Hayes, Wayne and Hunt, Brian R. and Mount, Stephen M. and Yorke, James A.},
	month = dec,
	year = {2004},
	pages = {3363--3369},
}

@article{kim_grim-filter_2018,
	title = {{GRIM}-{Filter}: {Fast Seed Location Filtering in {DNA} Read Mapping Using Processing-in-Memory Technologies}},
	volume = {19},
	issn = {1471-2164},
	abstract = {Seed location filtering is critical in DNA read mapping, a process where billions of DNA fragments (reads) sampled from a donor are mapped onto a reference genome to identify genomic variants of the donor. State-of-the-art read mappers 1) quickly generate possible mapping locations for seeds (i.e., smaller segments) within each read, 2) extract reference sequences at each of the mapping locations, and 3) check similarity between each read and its associated reference sequences with a computationally-expensive algorithm (i.e., sequence alignment) to determine the origin of the read. A seed location filter comes into play before alignment, discarding seed locations that alignment would deem a poor match. The ideal seed location filter would discard all poor match locations prior to alignment such that there is no wasted computation on unnecessary alignments.},
	number = {2},
	journal = {BMC Genomics},
	author = {Kim, Jeremie S. and Senol Cali, Damla and Xin, Hongyi and Lee, Donghyuk and Ghose, Saugata and Alser, Mohammed and Hassan, Hasan and Ergin, Oguz and Alkan, Can and Mutlu, Onur},
	month = may,
	year = {2018},
	pages = {89},
}

@article{alser_shouji_2019,
	title = {{Shouji: A Fast and Efficient Pre-Alignment Filter for Sequence Alignment}},
	volume = {35},
	issn = {1367-4803},
	abstract = {The ability to generate massive amounts of sequencing data continues to overwhelm the processing capability of existing algorithms and compute infrastructures. In this work, we explore the use of hardware/software co-design and hardware acceleration to significantly reduce the execution time of short sequence alignment, a crucial step in analyzing sequenced genomes. We introduce Shouji, a highly parallel and accurate pre-alignment filter that remarkably reduces the need for computationally-costly dynamic programming algorithms. The first key idea of our proposed pre-alignment filter is to provide high filtering accuracy by correctly detecting all common subsequences shared between two given sequences. The second key idea is to design a hardware accelerator that adopts modern field-programmable gate array (FPGA) architectures to further boost the performance of our algorithm.Shouji significantly improves the accuracy of pre-alignment filtering by up to two orders of magnitude compared to the state-of-the-art pre-alignment filters, GateKeeper and SHD. Our FPGA-based accelerator is up to three orders of magnitude faster than the equivalent CPU implementation of Shouji. Using a single FPGA chip, we benchmark the benefits of integrating Shouji with five state-of-the-art sequence aligners, designed for different computing platforms. The addition of Shouji as a pre-alignment step reduces the execution time of the five state-of-the-art sequence aligners by up to 18.8×. Shouji can be adapted for any bioinformatics pipeline that performs sequence alignment for verification. Unlike most existing methods that aim to accelerate sequence alignment, Shouji does not sacrifice any of the aligner capabilities, as it does not modify or replace the alignment step.https://github.com/CMU-SAFARI/Shouji.Supplementary data are available at Bioinformatics online.},
	number = {21},
	urldate = {2021-09-20},
	journal = {Bioinformatics},
	author = {Alser, Mohammed and Hassan, Hasan and Kumar, Akash and Mutlu, Onur and Alkan, Can},
	month = nov,
	year = {2019},
	pages = {4255--4263},
}

@article{alser_sneakysnake_2020,
	title = {{SneakySnake}: {A Fast and Accurate Universal Genome Pre-Alignment Filter} for {CPUs}, {GPUs} and {FPGAs}},
	volume = {36},
	issn = {1367-4803},
	abstract = {We introduce SneakySnake, a highly parallel and highly accurate pre-alignment filter that remarkably reduces the need for computationally costly sequence alignment. The key idea of SneakySnake is to reduce the approximate string matching (ASM) problem to the single net routing (SNR) problem in VLSI chip layout. In the SNR problem, we are interested in finding the optimal path that connects two terminals with the least routing cost on a special grid layout that contains obstacles. The SneakySnake algorithm quickly solves the SNR problem and uses the found optimal path to decide whether or not performing sequence alignment is necessary. Reducing the ASM problem into SNR also makes SneakySnake efficient to implement on CPUs, GPUs and FPGAs.SneakySnake significantly improves the accuracy of pre-alignment filtering by up to four orders of magnitude compared to the state-of-the-art pre-alignment filters, Shouji, GateKeeper and SHD. For short sequences, SneakySnake accelerates Edlib (state-of-the-art implementation of Myers’s bit-vector algorithm) and Parasail (state-of-the-art sequence aligner with a configurable scoring function), by up to 37.7× and 43.9× (\&gt;12× on average), respectively, with its CPU implementation, and by up to 413× and 689× (\&gt;400× on average), respectively, with FPGA and GPU acceleration. For long sequences, the CPU implementation of SneakySnake accelerates Parasail and KSW2 (sequence aligner of minimap2) by up to 979× (276.9× on average) and 91.7× (31.7× on average), respectively. As SneakySnake does not replace sequence alignment, users can still obtain all capabilities (e.g. configurable scoring functions) of the aligner of their choice, unlike existing acceleration efforts that sacrifice some aligner capabilities.https://github.com/CMU-SAFARI/SneakySnake.Supplementary data are available at Bioinformatics online.},
	number = {22-23},
	urldate = {2021-09-20},
	journal = {Bioinformatics},
	author = {Alser, Mohammed and Shahroodi, Taha and Gómez-Luna, Juan and Alkan, Can and Mutlu, Onur},
	month = dec,
	year = {2020},
	pages = {5282--5290},
}

@inproceedings{nag_gencache_2019,
	address = {New York, NY, USA},
	series = {{MICRO} '52},
	title = {{GenCache}: {Leveraging} {In}-{Cache} {Operators} for {Efficient} {Sequence} {Alignment}},
	isbn = {978-1-4503-6938-1},
	abstract = {Precision Medicine will rely on frequent genomic analysis, especially for patients undergoing cancer treatments or suffering from rare diseases. Sequence alignment is invoked in multiple stages of the genomic analysis pipeline. Recent projects have introduced accelerators, GenAx and Darwin, for 2nd and 3rd generation sequencers respectively. In this work, we improve upon the GenAx design by increasing its parallelism and reducing its memory bandwidth demands. This is achieved with a combination of hardware and software innovations. We first integrate in-cache operators from prior work into the GenAx memory hierarchy; we then augment the in-cache peripheral circuit to support additional new operators. We then re-structure the sequence alignment algorithm to (i) leverage the many in-cache operators, (ii) exploit the common case in genomic datasets, (iii) use Bloom Filters to reduce futile accesses, and (iv) maximize data reuse within a re-organized memory hierarchy. While the baseline GenAx accelerator processes a batch of reads in 194 seconds while nearly saturating the 153.6 GB/s memory bandwidth, the proposed GenCache architecture processes the same batch of reads in 37 seconds at an improved energy efficiency of 8.6×, while demanding 20 GB/s average memory bandwidth. Our hardware and software techniques thus interact synergistically to target both memory and compute bottlenecks, while not affecting the outputs of the application. We show that the basic principles in GenCache can also be exploited by 3rd generation sequence aligners.},
	booktitle = {Proceedings of the 52nd {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {Association for Computing Machinery},
	author = {Nag, Anirban and Ramachandra, C. N. and Balasubramonian, Rajeev and Stutsman, Ryan and Giacomin, Edouard and Kambalasubramanyam, Hari and Gaillardon, Pierre-Emmanuel},
	year = {2019},
	keywords = {Cache Partitioning, CISC Instructions, Genomics, Hardware Acceleration, In-Cache Operators, Sequence Alignment},
	pages = {334--346},
}

@article{berlin_assembling_2015,
	title = {{Assembling Large Genomes with Single-Molecule Sequencing and Locality-Sensitive Hashing}},
	volume = {33},
	issn = {1546-1696},
	abstract = {An assembly algorithm that overlaps noisy long reads enables accurate and fast assembly of large genomes from single-molecule real-time sequences.},
	number = {6},
	journal = {Nature Biotechnology},
	author = {Berlin, Konstantin and Koren, Sergey and Chin, Chen-Shan and Drake, James P and Landolin, Jane M and Phillippy, Adam M},
	month = jun,
	year = {2015},
	pages = {623--630},
}

@article{chakraborty_conlsh_2020,
	title = {{conLSH}: {Context based {Locality} {Sensitive} {Hashing} for Mapping of Noisy {SMRT} Reads}},
	volume = {85},
	issn = {1476-9271},
	abstract = {Single Molecule Real-Time (SMRT) sequencing is a recent advancement of Next Gen technology developed by Pacific Bio (PacBio). It comes with an explosion of long and noisy reads demanding cutting edge research to get most out of it. To deal with the high error probability of SMRT data, a novel contextual Locality Sensitive Hashing (conLSH) based algorithm is proposed in this article, which can effectively align the noisy SMRT reads to the reference genome. Here, sequences are hashed together based not only on their closeness, but also on similarity of context. The algorithm has O(nρ+1) space requirement, where n is the number of sequences in the corpus and ρ is a constant. The indexing time and querying time are bounded by Onρ+1·lnnln1P2 and O(nρ) respectively, where P2 {\textgreater} 0, is a probability value. This algorithm is particularly useful for retrieving similar sequences, a widely used task in biology. The proposed conLSH based aligner is compared with rHAT, popularly used for aligning SMRT reads, and is found to comprehensively beat it in speed as well as in memory requirements. In particular, it takes approximately 24.2\% less processing time, while saving about 70.3\% in peak memory requirement for H.sapiens PacBio dataset.},
	journal = {Computational Biology and Chemistry},
	author = {Chakraborty, Angana and Bandyopadhyay, Sanghamitra},
	month = apr,
	year = {2020},
	keywords = {Algorithm, Locality Sensitive Hashing, PacBio dataset, Sequence alignment, Sequence analysis, Single Molecule Real-Time (SMRT) sequencing},
	pages = {107206},
}

@article{baker_dashing_2019,
	title = {Dashing: {Fast and Accurate Genomic Distances with HyperLogLog}},
	volume = {20},
	issn = {1474-760X},
	abstract = {Dashing is a fast and accurate software tool for estimating similarities of genomes or sequencing datasets. It uses the HyperLogLog sketch together with cardinality estimation methods that are specialized for set unions and intersections. Dashing summarizes genomes more rapidly than previous MinHash-based methods while providing greater accuracy across a wide range of input sizes and sketch sizes. It can sketch and calculate pairwise distances for over 87K genomes in 6 minutes. Dashing is open source and available at https://github.com/dnbaker/dashing.},
	number = {1},
	journal = {Genome Biology},
	author = {Baker, Daniel N. and Langmead, Ben},
	month = dec,
	year = {2019},
	pages = {265},
}

@inproceedings{flajolet_hyperloglog_2007,
	address = {Juan les Pins, France},
	series = {{DMTCS} {Proceedings}},
	title = {{HyperLogLog}: the analysis of a near-optimal cardinality estimation algorithm},
	volume = {DMTCS Proceedings vol. AH, 2007 Conference on Analysis of Algorithms (AofA 07)},
	booktitle = {{AofA}: {Analysis} of {Algorithms}},
	publisher = {Discrete Mathematics and Theoretical Computer Science},
	author = {Flajolet, Philippe and Fusy, Eric and Gandouet, Olivier and Meunier, Frédéric},
	editor = {Jacquet, Philippe},
	month = jun,
	year = {2007},
	keywords = {cardinality estimation, Probabilistic algorithm},
	pages = {137--156},
}

@article{jaccard_nouvelles_1908,
	title = {{Nouvelles Recherches sur la Distribution Florale}},
	volume = {44},
	journal = {Bull. Soc. Vaud. Sci. Nat.},
	author = {Jaccard, P.},
	year = {1908},
	pages = {223--270},
}

@article{yi_kssd_2021,
	title = {{Kssd: Sequence Dimensionality Reduction by k-mer Substring Space Sampling Enables Real-Time Large-Scale Datasets Analysis}},
	volume = {22},
	issn = {1474-760X},
	abstract = {Here, we develop k -mer substring space decomposition (Kssd), a sketching technique which is significantly faster and more accurate than current sketching methods. We show that it is the only method that can be used for large-scale dataset comparisons at population resolution on simulated and real data. Using Kssd, we prioritize references for all 1,019,179 bacteria whole genome sequencing (WGS) runs from NCBI Sequence Read Archive and find misidentification or contamination in 6164 of these. Additionally, we analyze WGS and exome runs of samples from the 1000 Genomes Project.},
	number = {1},
	journal = {Genome Biology},
	author = {Yi, Huiguang and Lin, Yanling and Lin, Chengqi and Jin, Wenfei},
	month = mar,
	year = {2021},
	pages = {84},
}

@inproceedings{heule_hyperloglog_2013,
	address = {New York, NY, USA},
	series = {{EDBT} '13},
	title = {{HyperLogLog} in {Practice}: {Algorithmic} {Engineering} of a {State} of the {Art} {Cardinality} {Estimation} {Algorithm}},
	isbn = {978-1-4503-1597-5},
	abstract = {Cardinality estimation has a wide range of applications and is of particular importance in database systems. Various algorithms have been proposed in the past, and the HyperLogLog algorithm is one of them. In this paper, we present a series of improvements to this algorithm that reduce its memory requirements and significantly increase its accuracy for an important range of cardinalities. We have implemented our proposed algorithm for a system at Google and evaluated it empirically, comparing it to the original HyperLogLog algorithm. Like HyperLogLog, our improved algorithm parallelizes perfectly and computes the cardinality estimate in a single pass.},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Extending} {Database} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Heule, Stefan and Nunkesser, Marc and Hall, Alexander},
	year = {2013},
	pages = {683--692},
}

@inproceedings{charikar_similarity_2002,
	address = {New York, NY, USA},
	series = {{STOC} '02},
	title = {Similarity {Estimation} {Techniques} from {Rounding} {Algorithms}},
	isbn = {1-58113-495-9},
	abstract = {(MATH) A locality sensitive hashing scheme is a distribution on a family \$F\$ of hash functions operating on a collection of objects, such that for two objects x,y, PrhεF[h(x) = h(y)] = sim(x,y), where sim(x,y) ε [0,1] is some similarity function defined on the collection of objects. Such a scheme leads to a compact representation of objects so that similarity of objects can be estimated from their compact sketches, and also leads to efficient algorithms for approximate nearest neighbor search and clustering. Min-wise independent permutations provide an elegant construction of such a locality sensitive hashing scheme for a collection of subsets with the set similarity measure sim(A,B) = frac{\textbar}A ∩ B{\textbar}{\textbar}A ∪ B{\textbar}.(MATH) We show that rounding algorithms for LPs and SDPs used in the context of approximation algorithms can be viewed as locality sensitive hashing schemes for several interesting collections of objects. Based on this insight, we construct new locality sensitive hashing schemes for:A collection of vectors with the distance between → over u and → over v measured by Ø(→ over u, → over v)/π, where Ø(→ over u, → over v) is the angle between → over u) and → over v). This yields a sketching scheme for estimating the cosine similarity measure between two vectors, as well as a simple alternative to minwise independent permutations for estimating set similarity.A collection of distributions on n points in a metric space, with distance between distributions measured by the Earth Mover Distance (EMD), (a popular distance measure in graphics and vision). Our hash functions map distributions to points in the metric space such that, for distributions P and Q, EMD(P,Q) ≤ EhεF [d(h(P),h(Q))] ≤ O(log n log log n). EMD(P, Q).},
	booktitle = {Proceedings of the {Thiry}-{Fourth} {Annual} {ACM} {Symposium} on {Theory} of {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Charikar, Moses S.},
	year = {2002},
	pages = {380--388},
}

@inproceedings{manku_detecting_2007,
	address = {New York, NY, USA},
	series = {{WWW} '07},
	title = {Detecting {Near}-{Duplicates} for {Web} {Crawling}},
	isbn = {978-1-59593-654-7},
	abstract = {Near-duplicate web documents are abundant. Two such documents differ from each other in a very small portion that displays advertisements, for example. Such differences are irrelevant for web search. So the quality of a web crawler increases if it can assess whether a newly crawled web page is a near-duplicate of a previously crawled web page or not. In the course of developing a near-duplicate detection system for a multi-billion page repository, we make two research contributions. First, we demonstrate that Charikar's fingerprinting technique is appropriate for this goal. Second, we present an algorithmic technique for identifying existing f-bit fingerprints that differ from a given fingerprint in at most k bit-positions, for small k. Our technique is useful for both online queries (single fingerprints) and all batch queries (multiple fingerprints). Experimental evaluation over real data confirms the practicality of our design.},
	booktitle = {Proceedings of the 16th {International} {Conference} on {World} {Wide} {Web}},
	publisher = {Association for Computing Machinery},
	author = {Manku, Gurmeet Singh and Jain, Arvind and Das Sarma, Anish},
	year = {2007},
	keywords = {fingerprint, hamming distance, near-duplicate, search, similarity, sketch, web crawl, web document},
	pages = {141--150},
}


@article{petrucci_iterative_2020,
	title = {Iterative {Spaced} {Seed} {Hashing}: {Closing} the {Gap} {Between} {Spaced} {Seed} {Hashing} and k-mer {Hashing}},
	volume = {27},
	abstract = {Alignment-free classification of sequences has enabled high-throughput processing of sequencing data in many bioinformatics pipelines. Much work has been done to speed up the indexing of k-mers through hash-table and other data structures. These efforts have led to very fast indexes, but because they are k-mer based, they often lack sensitivity due to sequencing errors or polymorphisms. Spaced seeds are a special type of pattern that accounts for errors or mutations. They allow to improve the sensitivity and they are now routinely used instead of k-mers in many applications. The major drawback of spaced seeds is that they cannot be efficiently hashed and thus their usage increases substantially the computational time. In this article we address the problem of efficient spaced seed hashing. We propose an iterative algorithm that combines multiple spaced seed hashes by exploiting the similarity of adjacent hash values to efficiently compute the next hash. We report a series of experiments on HTS reads hashing, with several spaced seeds. Our algorithm can compute the hashing values of spaced seeds with a speedup in range of [3.5????7???], outperforming previous methods. Software and data sets are available at Iterative Spaced Seed Hashing.},
	number = {2},
	urldate = {2021-12-03},
	journal = {Journal of Computational Biology},
	author = {Petrucci, Enrico and Noé, Laurent and Pizzi, Cinzia and Comin, Matteo},
	month = feb,
	year = {2020},
	pages = {223--233},
}

@article{mallik_ales_2021,
	title = {{ALeS}: {Adaptive-Length Spaced-Seed Design}},
	volume = {37},
	issn = {1367-4803},
	abstract = {Sequence similarity is the most frequently used procedure in biological research, as proved by the widely used BLAST program. The consecutive seed used by BLAST can be dramatically improved by considering multiple spaced seeds. Finding the best seeds is a hard problem and much effort went into developing heuristic algorithms and software for designing highly sensitive spaced seeds.We introduce a new algorithm and software, ALeS, that produces more sensitive seeds than the current state-of-the-art programs, as shown by extensive testing. We also accurately estimate the sensitivity of a seed, enabling its computation for arbitrary seeds.The source code is freely available at github.com/lucian-ilie/ALeS.Supplementary data are available at Bioinformatics online.},
	number = {9},
	urldate = {2021-12-03},
	journal = {Bioinformatics},
	author = {Mallik, Arnab and Ilie, Lucian},
	month = may,
	year = {2021},
	pages = {1206--1210},
}

@article{lin_zoom_2008,
	title = {{ZOOM}! {Zillions of Oligos Mapped}},
	volume = {24},
	issn = {1367-4803},
	abstract = {Motivation: The next generation sequencing technologies are generating billions of short reads daily. Resequencing and personalized medicine need much faster software to map these deep sequencing reads to a reference genome, to identify SNPs or rare transcripts.Results: We present a framework for how full sensitivity mapping can be done in the most efficient way, via spaced seeds. Using the framework, we have developed software called ZOOM, which is able to map the Illumina/Solexa reads of 15× coverage of a human genome to the reference human genome in one CPU-day, allowing two mismatches, at full sensitivity.Availability: ZOOM is freely available to non-commercial users at http://www.bioinfor.com/zoomContact:bma@csd.uwo.ca, mli@uwaterloo.ca},
	number = {21},
	urldate = {2021-12-03},
	journal = {Bioinformatics},
	author = {Lin, Hao and Zhang, Zefeng and Zhang, Michael Q. and Ma, Bin and Li, Ming},
	month = nov,
	year = {2008},
	pages = {2431--2437},
}

@article{david_shrimp2_2011,
	title = {{SHRiMP2}: {Sensitive} yet {Practical} {Short} {Read} {Mapping}},
	volume = {27},
	issn = {1367-4803},
	abstract = {Summary: We report on a major update (version 2) of the original SHort Read Mapping Program (SHRiMP). SHRiMP2 primarily targets mapping sensitivity, and is able to achieve high accuracy at a very reasonable speed. SHRiMP2 supports both letter space and color space (AB/SOLiD) reads, enables for direct alignment of paired reads and uses parallel computation to fully utilize multi-core architectures.Availability: SHRiMP2 executables and source code are freely available at: http://compbio.cs.toronto.edu/shrimp/.Contact:shrimp@cs.toronto.eduSupplementary information:Supplementary data are available at Bioinformatics online.},
	number = {7},
	urldate = {2021-12-03},
	journal = {Bioinformatics},
	author = {David, Matei and Dzamba, Misko and Lister, Dan and Ilie, Lucian and Brudno, Michael},
	month = apr,
	year = {2011},
	pages = {1011--1012},
}

@article{ma_patternhunter_2002,
	title = {{PatternHunter}: {Faster and More Sensitive Homology Search}},
	volume = {18},
	issn = {1367-4803},
	abstract = {Motivation: Genomics and proteomics studies routinely depend on homology searches based on the strategy of finding short seed
matches which are then extended. The exploding genomic data growth presents a dilemma for DNA homology search techniques: increasing seed size decreases sensitivity whereas decreasing seed size slows down computation.Results: We present a new homology search algorithm ‘PatternHunter’ that uses a novel seed model for
increased sensitivity and new hit-processing techniques for
significantly increased speed. At Blast levels of sensitivity,
PatternHunter is able to find homologies between sequences as large
as human chromosomes, in mere hours on a desktop.Availability: PatternHunter is available at http://www.bioinformaticssolutions.com,
as a commercial package. It runs on all platforms that support Java.
PatternHunter technology is being patented; commercial use requires
a license from BSI, while non-commercial use will be free.Contact: mli@cs.ucsb.edu},
	number = {3},
	urldate = {2021-12-03},
	journal = {Bioinformatics},
	author = {Ma, Bin and Tromp, John and Li, Ming},
	month = mar,
	year = {2002},
	pages = {440--445},
}


@article{pop_comparative_2004,
	title = {{Comparative Genome Assembly}},
	volume = {5},
	issn = {1467-5463},
	abstract = {One of the most complex and computationally intensive tasks of genome sequence analysis is genome assembly. Even today, few centres have the resources, in both software and hardware, to assemble a genome from the thousands or millions of individual sequences generated in a whole-genome shotgun sequencing project. With the rapid growth in the number of sequenced genomes has come an increase in the number of organisms for which two or more closely related species have been sequenced. This has created the possibility of building a comparative genome assembly algorithm, which can assemble a newly sequenced genome by mapping it onto a reference genome.We describe here a novel algorithm for comparative genome assembly that can accurately assemble a typical bacterial genome in less than four minutes on a standard desktop computer. The software is available as part of the open-source AMOS project.},
	number = {3},
	urldate = {2021-12-05},
	journal = {Briefings in Bioinformatics},
	author = {Pop, Mihai and Phillippy, Adam and Delcher, Arthur L. and Salzberg, Steven L.},
	month = sep,
	year = {2004},
	pages = {237--248},
}


@article{mckenna_genome_2010,
	title = {The {Genome} {Analysis} {Toolkit}: {A} {MapReduce Framework for Analyzing Next-Generation {DNA} Sequencing Data}},
	volume = {20},
	abstract = {Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes Project, are already revolutionizing our understanding of genetic variation among individuals. However, the massive data sets generated by NGS—the 1000 Genome pilot alone includes nearly five terabases—make writing feature-rich, efficient, and robust analysis tools difficult for even computationally sophisticated individuals. Indeed, many professionals are limited in the scope and the ease with which they can answer scientific questions by the complexity of accessing and manipulating the data produced by these machines. Here, we discuss our Genome Analysis Toolkit (GATK), a structured programming framework designed to ease the development of efficient and robust analysis tools for next-generation DNA sequencers using the functional programming philosophy of MapReduce. The GATK provides a small but rich set of data access patterns that encompass the majority of analysis tool needs. Separating specific analysis calculations from common data management infrastructure enables us to optimize the GATK framework for correctness, stability, and CPU and memory efficiency and to enable distributed and shared memory parallelization. We highlight the capabilities of the GATK by describing the implementation and application of robust, scale-tolerant tools like coverage calculators and single nucleotide polymorphism (SNP) calling. We conclude that the GATK programming framework enables developers and analysts to quickly and easily write efficient and robust NGS tools, many of which have already been incorporated into large-scale sequencing projects like the 1000 Genomes Project and The Cancer Genome Atlas.},
	number = {9},
	journal = {Genome Research},
	author = {McKenna, Aaron and Hanna, Matthew and Banks, Eric and Sivachenko, Andrey and Cibulskis, Kristian and Kernytsky, Andrew and Garimella, Kiran and Altshuler, David and Gabriel, Stacey and Daly, Mark and DePristo, Mark A.},
	month = sep,
	year = {2010},
	pages = {1297--1303},
}

@article{sahlin_faster_2021,
	title = {{Faster Short-Read Mapping with Strobemer Seeds Constructed from Syncmers}},
	abstract = {Alignment of short reads to genomes is a fundamental computational step used in many bioinformatic analyses. It is there-fore desirable to perform such computation as fast as possible. Most alignment algorithms consider a seed-and-extend approach. Several popular programs perform the seeding step based on the Burrows-Wheeler Transform with a low memory footprint, but they are relatively slow compared to more recent approaches that use minimizer-based seeding-and-chaining strategies. Recently, syncmers and strobemers were proposed for sequence comparison. Both methods were designed for improved conservation of matches between sequences under mutations. Syncmers is a k-mer subsampling method proposed as an alternative to minimizers, while strobemers is a method for linking gapped sequences and was proposed as an alternative to k-mers.The main contribution in this work is a new seeding approach that combines syncmers and strobemers. We use a strobemer method (randstrobes) to link together syncmers, instead of considering k-mers over the entire sequence as strobemers were first described. Our method allows us to create longer seeds that preserve mapping accuracy and reduce the number of hits, allowing faster mapping. Our second contribution is an implementation of the seeding approach in a shortread aligner, strobealign. Strobealign supports both single-end and paired-end read alignment modes, and aligns single-end reads about 2-3 times faster than minimap2 and 12-15 times faster than BWA and Bowtie2 with similar accuracy. In paired-end mode, strobealign aligns reads of lengths 200nt and above more than 3 times as fast as minimap2 with nearidentical accuracy and about 10 times faster than BWA and Bowtie2 with a 0.1-0.2\% loss in accuracy. Our contribution is algorithmic and requires no hardware architecture or system-specific instructions. We believe that our seeding approach can be applied to other mapping applications such as long-read mapping and clustering. Strobealign is available at https://github.com/ksahlin/strobealign.Competing Interest StatementThe authors have declared no competing interest.},
	journal = {bioRxiv},
	author = {Sahlin, Kristoffer},
	month = jan,
	year = {2021},
	pages = {2021.06.18.449070},
}


@article{sahlin_effective_2021,
	title = {{Effective Sequence Similarity Detection with Strobemers}},
	volume = {31},
	abstract = {k-mer-based methods are widely used in bioinformatics for various types of sequence comparisons. However, a single mutation will mutate k consecutive k-mers and make most k-mer-based applications for sequence comparison sensitive to variable mutation rates. Many techniques have been studied to overcome this sensitivity, for example, spaced k-mers and k-mer permutation techniques, but these techniques do not handle indels well. For indels, pairs or groups of small k-mers are commonly used, but these methods first produce k-mer matches, and only in a second step, a pairing or grouping of k-mers is performed. Such techniques produce many redundant k-mer matches owing to the size of k. Here, we propose strobemers as an alternative to k-mers for sequence comparison. Intuitively, strobemers consist of two or more linked shorter k-mers, where the combination of linked k-mers is decided by a hash function. We use simulated data to show that strobemers provide more evenly distributed sequence matches and are less sensitive to different mutation rates than k-mers and spaced k-mers. Strobemers also produce higher match coverage across sequences. We further implement a proof-of-concept sequence-matching tool StrobeMap and use synthetic and biological Oxford Nanopore sequencing data to show the utility of using strobemers for sequence comparison in different contexts such as sequence clustering and alignment scenarios.},
	number = {11},
	journal = {Genome Research},
	author = {Sahlin, Kristoffer},
	month = nov,
	year = {2021},
	pages = {2080--2094},
}


@article{edgar_syncmers_2021,
	title = {{Syncmers are More Sensitive than Minimizers for Selecting Conserved k‑mers in Biological Sequences}},
	volume = {9},
	issn = {2167-8359},
	abstract = {Minimizers are widely used to select subsets of fixed-length substrings (k-mers) from biological sequences in applications ranging from read mapping to taxonomy prediction and indexing of large datasets. The minimizer of a string of w consecutive k-mers is the k-mer with smallest value according to an ordering of all k-mers. Syncmers are defined here as a family of alternative methods which select k-mers by inspecting the position of the smallest-valued substring of length s {\textless} k within the k-mer. For example, a closed syncmer is selected if its smallest s-mer is at the start or end of the k-mer. At least one closed syncmer must be found in every window of length (k - s) k-mers. Unlike a minimizer, a syncmer is identified by its sequence alone, and is therefore synchronized in the following sense: if a given k-mer is selected from one sequence, it will also be selected from any other sequence. Also, minimizers can be deleted by mutations in flanking sequence, which cannot happen with syncmers. Experiments on minimizers with parameters used in the minimap2 read mapper and Kraken taxonomy prediction algorithm respectively show that syncmers can simultaneously achieve both lower density and higher conservation compared to minimizers.},
	language = {eng},
	journal = {PeerJ},
	author = {Edgar, Robert},
	month = feb,
	year = {2021},
	keywords = {Sequence analysis, Minimizers, Alignment-free methods, k-mers, String index},
	pages = {e10805--e10805},
}


@article{chin_human_2019,
	title = {Human {Genome} {Assembly} in 100 {Minutes}},
	journal = {bioRxiv},
	author = {Chin, Chen-Shan and Khalak, Asif},
	month = jan,
	year = {2019},
	pages = {705616},
}


@article{ren_lra_2021,
	title = {{lra: {A} Long Read Aligner for Sequences and Contigs}},
	volume = {17},
	abstract = {Author summary Any two human genomes will have sequence differences across multiple scales: from single-nucleotide variants to large gains, losses, or rearrangements of DNA called structural variants. Long-read single-molecule sequencing has been shown to help discover structural variation because the reads span across the entire variant. The computational problem for discovering a structural variant is to find the optimal alignment of the read to the genome with gaps that accurately reflect the variant. Here we demonstrate a method, lra, that uses an efficient implementation of concave-cost alignment for structural variant discovery using long reads. On standardized benchmark data, we show that structural variant discovery is improved for multiple combinations of variant detection algorithms and long-read sequence using alignments generated by lra compared to existing methods. Finally, we show that it is possible to use lra to accurately discover a complete spectrum of structural variants using de novo assemblies constructed from long-read sequence data. This implies a future model of comparative genomics where variants are discovered only by comparing de novo assemblies and not a comparison of reads against a reference.},
	number = {6},
	journal = {PLOS Computational Biology},
	author = {Ren, Jingwen and Chaisson, Mark J. P.},
	month = jun,
	year = {2021},
	pages = {e1009078},
}


@article{ono_pbsim2_2021,
	title = {{PBSIM2}: {A Simulator for Long-Read Sequencers with a Novel Generative Model of Quality Scores}},
	volume = {37},
	issn = {1367-4803},
	abstract = {Recent advances in high-throughput long-read sequencers, such as PacBio and Oxford Nanopore sequencers, produce longer reads with more errors than short-read sequencers. In addition to the high error rates of reads, non-uniformity of errors leads to difficulties in various downstream analyses using long reads. Many useful simulators, which characterize long-read error patterns and simulate them, have been developed. However, there is still room for improvement in the simulation of the non-uniformity of errors.To capture characteristics of errors in reads for long-read sequencers, here, we introduce a generative model for quality scores, in which a hidden Markov Model with a latest model selection method, called factorized information criteria, is utilized. We evaluated our developed simulator from various points, indicating that our simulator successfully simulates reads that are consistent with real reads.The source codes of PBSIM2 are freely available from https://github.com/yukiteruono/pbsim2.Supplementary data are available at Bioinformatics online.},
	number = {5},
	urldate = {2022-03-15},
	journal = {Bioinformatics},
	author = {Ono, Yukiteru and Asai, Kiyoshi and Hamada, Michiaki},
	month = mar,
	year = {2021},
	pages = {589--595},
}


@article{nurk_hicanu_2020,
	title = {{{HiCanu}: Accurate Assembly of Segmental Duplications, Satellites, and Allelic Variants from High-Fidelity Long Reads}},
	abstract = {Complete and accurate genome assemblies form the basis of most downstream genomic analyses and are of critical importance. Recent genome assembly projects have relied on a combination of noisy long-read sequencing and accurate short-read sequencing, with the former offering greater assembly continuity and the latter providing higher consensus accuracy. The recently introduced PacBio HiFi sequencing technology bridges this divide by delivering long reads (\&gt;10 kbp) with high per-base accuracy (\&gt;99.9\%). Here we present HiCanu, a significant modification of the Canu assembler designed to leverage the full potential of HiFi reads via homopolymer compression, overlap-based error correction, and aggressive false overlap filtering. We benchmark HiCanu with a focus on the recovery of haplotype diversity, major histocompatibility complex (MHC) variants, satellite DNAs, and segmental duplications. For diploid human genomes sequenced to 30× HiFi coverage, HiCanu achieved superior accuracy and allele recovery compared to the current state of the art. On the effectively haploid CHM13 human cell line, HiCanu achieved an NG50 contig size of 77 Mbp with a per-base consensus accuracy of 99.999\% (QV50), surpassing recent assemblies of high-coverage, ultra-long Oxford Nanopore reads in terms of both accuracy and continuity. This HiCanu assembly correctly resolves 337 out of 341 validation BACs sampled from known segmental duplications and provides the first preliminary assemblies of 9 complete human centromeric regions. Although gaps and errors still remain within the most challenging regions of the genome, these results represent a significant advance towards the complete assembly of human genomes.Availability HiCanu is implemented within the Canu assembly framework and is available from https://github.com/marbl/canu.},
	journal = {bioRxiv},
	author = {Nurk, Sergey and Walenz, Brian P. and Rhie, Arang and Vollger, Mitchell R. and Logsdon, Glennis A. and Grothe, Robert and Miga, Karen H. and Eichler, Evan E. and Phillippy, Adam M. and Koren, Sergey},
	month = jan,
	year = {2020},
	pages = {2020.03.14.992248},
}


@inproceedings{mansouri_ghiasi_genstore_2022,
	address = {New York, NY, USA},
	series = {{ASPLOS} 2022},
	title = {{GenStore}: {A} {High}-{Performance} in-{Storage} {Processing} {System} for {Genome} {Sequence} {Analysis}},
	isbn = {978-1-4503-9205-1},
	abstract = {Read mapping is a fundamental step in many genomics applications. It is used to identify potential matches and differences between fragments (called reads) of a sequenced genome and an already known genome (called a reference genome). Read mapping is costly because it needs to perform approximate string matching (ASM) on large amounts of data. To address the computational challenges in genome analysis, many prior works propose various approaches such as accurate filters that select the reads within a dataset of genomic reads (called a read set) that must undergo expensive computation, efficient heuristics, and hardware acceleration. While effective at reducing the amount of expensive computation, all such approaches still require the costly movement of a large amount of data from storage to the rest of the system, which can significantly lower the end-to-end performance of read mapping in conventional and emerging genomics systems. We propose GenStore, the first in-storage processing system designed for genome sequence analysis that greatly reduces both data movement and computational overheads of genome sequence analysis by exploiting low-cost and accurate in-storage filters. GenStore leverages hardware/software co-design to address the challenges of in-storage processing, supporting reads with 1)\&nbsp;different properties such as read lengths and error rates, which highly depend on the sequencing technology, and 2)\&nbsp;different degrees of genetic variation compared to the reference genome, which highly depends on the genomes that are being compared. Through rigorous analysis of read mapping processes of reads with different properties and degrees of genetic variation, we meticulously design low-cost hardware accelerators and data/computation flows inside a NAND flash-based solid-state drive (SSD). Our evaluation using a wide range of real genomic datasets shows that GenStore, when implemented in three modern NAND flash-based SSDs, significantly improves the read mapping performance of state-of-the-art software (hardware) baselines by 2.07-6.05× (1.52-3.32×) for read sets with high similarity to the reference genome and 1.45-33.63× (2.70-19.2×) for read sets with low similarity to the reference genome.},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Mansouri Ghiasi, Nika and Park, Jisung and Mustafa, Harun and Kim, Jeremie and Olgun, Ataberk and Gollwitzer, Arvid and Senol Cali, Damla and Firtina, Can and Mao, Haiyu and Almadhoun Alserr, Nour and Ausavarungnirun, Rachata and Vijaykumar, Nandita and Alser, Mohammed and Mutlu, Onur},
	year = {2022},
	keywords = {Genomics, Filtering, Near-Data Processing, Read Mapping, Storage},
	pages = {635--654},
}

@inproceedings{a_zeni_logan_2020,
	title = {{LOGAN}: {High}-{Performance} {GPU}-{Based} {X}-{Drop} {Long}-{Read} {Alignment}},
	isbn = {1530-2075},
	booktitle = {2020 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {{A. Zeni} and {G. Guidi} and {M. Ellis} and {N. Ding} and {M. D. Santambrogio} and {S. Hofmeyr} and {A. Buluç} and {L. Oliker} and {K. Yelick}},
	month = may,
	year = {2020},
	pages = {462--471},
}

@article{jain_long-read_2022,
	title = {{Long-Read Mapping to Repetitive Reference Sequences Using {Winnowmap2}}},
	issn = {1548-7105},
	doi = {10.1038/s41592-022-01457-8},
	abstract = {Approximately 5–10\% of the human genome remains inaccessible due to the presence of repetitive sequences such as segmental duplications and tandem repeat arrays. We show that existing long-read mappers often yield incorrect alignments and variant calls within long, near-identical repeats, as they remain vulnerable to allelic bias. In the presence of a nonreference allele within a repeat, a read sampled from that region could be mapped to an incorrect repeat copy. To address this limitation, we developed a new long-read mapping method, Winnowmap2, by using minimal confidently alignable substrings. Winnowmap2 computes each read mapping through a collection of confident subalignments. This approach is more tolerant of structural variation and more sensitive to paralog-specific variants within repeats. Our experiments highlight that Winnowmap2 successfully addresses the issue of allelic bias, enabling more accurate downstream variant calls in repetitive sequences.},
	journal = {Nature Methods},
	author = {Jain, Chirag and Rhie, Arang and Hansen, Nancy F. and Koren, Sergey and Phillippy, Adam M.},
	month = apr,
	year = {2022},
}

@article{singh_fpga-based_2021,
	title = {{FPGA}-{Based} {Near}-{Memory} {Acceleration} of {Modern} {Data}-{Intensive} {Applications}},
	volume = {41},
	issn = {1937-4143},
	doi = {10.1109/MM.2021.3088396},
	number = {4},
	journal = {IEEE Micro},
	author = {Singh, Gagandeep and Alser, Mohammed and Senol Cali, Damla and Diamantopoulos, Diamantopoulos and Gómez-Luna, Juan and Corporaal, Henk and Mutlu, Onur},
	month = aug,
	year = {2021},
	pages = {39--48},
}

@inproceedings{angizi_pim-aligner_2020,
	title = {{PIM}-{Aligner}: {A} {Processing}-in-{MRAM} {Platform} for {Biological} {Sequence} {Alignment}},
	doi = {10.23919/DATE48585.2020.9116303},
	booktitle = {2020 {Design}, {Automation} {Test} in {Europe} {Conference} {Exhibition} ({DATE})},
	author = {Angizi, Shaahin and Sun, Jiao and Zhang, Wei and Fan, Deliang},
	year = {2020},
	pages = {1265--1270},
}

@inproceedings{goenka_segalign_2020,
	title = {{SegAlign}: {A} {Scalable} {GPU}-{Based} {Whole} {Genome} {Aligner}},
	doi = {10.1109/SC41405.2020.00043},
	booktitle = {{SC20}: {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Goenka, Sneha D. and Turakhia, Yatish and Paten, Benedict and Horowitz, Mark},
	year = {2020},
	pages = {1--13},
}

@article{kim_fastremap_2022,
  title = {{FastRemap}: {A} {Tool} for {Quickly} {Remapping} {Reads} between {Genome} {Assemblies}},
  journal = {arXiv},
  author = {Kim, Jeremie S. and Firtina, Can and Cavlak, Meryem Banu and Senol Cali, Damla and Alkan, Can and Mutlu, Onur},
  year = {2022},
  month = jan,
}

@article{shahroodi_demeter_2022,
  title = {Demeter: A Fast and Energy-Efficient Food Profiler using Hyperdimensional Computing in Memory},
  journal = {arXiv},
  author = {Shahroodi, Taha and Zahedi, Mahdi and Firtina, Can and Alser, Mohammed and Wong, Stephan and Mutlu, Onur and Hamdioui, Said},
  year = {2022},
  month = jun,
}

@article{kim_airlift_2021,
	title = {{AirLift}: {A} {Fast} and {Comprehensive} {Technique} for {Remapping} {Alignments} between {Reference} {Genomes}},
	doi = {10.1101/2021.02.16.431517},
	journal = {bioRxiv},
	author = {Kim, Jeremie S. and Firtina, Can and Cavlak, Meryem Banu and Senol Cali, Damla and Hajinazar, Nastaran and Alser, Mohammed and Alkan, Can and Mutlu, Onur},
	month = jan,
	year = {2021},
	pages = {2021.02.16.431517},
}

@article{turakhia_darwin_2018,
	title = {Darwin: {A} {Genomics} {Co}-{Processor} {Provides} up to 15,{000X} {Acceleration} on {Long} {Read} {Assembly}},
	volume = {53},
	issn = {0362-1340},
	doi = {10.1145/3296957.3173193},
	abstract = {Genomics is transforming medicine and our understanding of life in fundamental ways. Genomics data, however, is far outpacing Moore»s Law. Third-generation sequencing technologies produce 100X longer reads than second generation technologies and reveal a much broader mutation spectrum of disease and evolution. However, these technologies incur prohibitively high computational costs. Over 1,300 CPU hours are required for reference-guided assembly of the human genome, and over 15,600 CPU hours are required for de novo assembly. This paper describes "Darwin" — a co-processor for genomic sequence alignment that, without sacrificing sensitivity, provides up to \$15,000X speedup over the state-of-the-art software for reference-guided assembly of third-generation reads. Darwin achieves this speedup through hardware/algorithm co-design, trading more easily accelerated alignment for less memory-intensive filtering, and by optimizing the memory system for filtering. Darwin combines a hardware-accelerated version of D-SOFT, a novel filtering algorithm, alignment at high speed, and with a hardware-accelerated version of GACT, a novel alignment algorithm. GACT generates near-optimal alignments of arbitrarily long genomic sequences using constant memory for the compute-intensive step. Darwin is adaptable, with tunable speed and sensitivity to match emerging sequencing technologies and to meet the requirements of genomic applications beyond read assembly.},
	number = {2},
	journal = {SIGPLAN Not.},
	author = {Turakhia, Yatish and Bejerano, Gill and Dally, William J.},
	month = mar,
	year = {2018},
	keywords = {genome assembly, co-processor, hardware-acceleration, long reads},
	pages = {199--213},
}

@inproceedings{cali_segram_2022,
	address = {New York, NY, USA},
	series = {{ISCA} '22},
	title = {{SeGraM}: {A} {Universal} {Hardware} {Accelerator} for {Genomic} {Sequence}-to-{Graph} and {Sequence}-to-{Sequence} {Mapping}},
	isbn = {978-1-4503-8610-4},
	doi = {10.1145/3470496.3527436},
	booktitle = {Proceedings of the 49th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {Association for Computing Machinery},
	author = {Senol Cali, Damla and Kanellopoulos, Konstantinos and Lindegger, Joël and Bingöl, Zülal and Kalsi, Gurpreet S. and Zuo, Ziyi and Firtina, Can and Cavlak, Meryem Banu and Kim, Jeremie and Ghiasi, Nika Mansouri and Singh, Gagandeep and Gómez-Luna, Juan and Alserr, Nour Almadhoun and Alser, Mohammed and Subramoney, Sreenivas and Alkan, Can and Ghose, Saugata and Mutlu, Onur},
	year = {2022},
	keywords = {genomics, genome graphs, algorithm/hardware co-design, bitvector, genome analysis, hardware accelerator, minimizer, read alignment, read mapping, seeding},
	pages = {638--655},
}

@inproceedings{senol_cali_genasm_2020,
	title = {{GenASM}: {A} {High}-{Performance}, {Low}-{Power} {Approximate} {String} {Matching} {Acceleration} {Framework} for {Genome} {Sequence} {Analysis}},
	doi = {10.1109/MICRO50266.2020.00081},
	booktitle = {2020 53rd {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Senol Cali, Damla and Kalsi, Gurpreet S. and Bingöl, Zülal and Firtina, Can and Subramanian, Lavanya and Kim, Jeremie S. and Ausavarungnirun, Rachata and Alser, Mohammed and Gomez-Luna, Juan and Boroumand, Amirali and Norion, Anant and Scibisz, Allison and Subramoneyon, Sreenivas and Alkan, Can and Ghose, Saugata and Mutlu, Onur},
	year = {2020},
	pages = {951--966},
}

@article{alser_going_2022,
  author = {Alser, Mohammed and Lindegger, Joel and Firtina, Can and Almadhoun, Nour and Mao, Haiyu and Singh, Gagandeep and Gomez-Luna, Juan and Mutlu, Onur},
  title = {{Going From Molecules to Genomic Variations to Scientific Discovery: Intelligent Algorithms and Architectures for Intelligent Genome Analysis}},
  journal = {arXiv},
  year = {2022},
  month = may,
}

@article{alser_packaging_2022,
  author = {Alser, Mohammed and Waymost, Sharon and Ayyala, Ram and Lawlor, Brendan and Abdill, Richard J. and Rajkumar, Neha and LaPierre, Nathan and Brito, Jaqueline and Ribeiro-dos-Santos, Andre M. and Firtina, Can and Almadhoun, Nour and Sarwal, Varuni and Eskin, Eleazar and Hu, Qiyang and Strong, Derek and Byoung-Do,  and {Kim} and Abedalthagafi, Malak S. and Mutlu, Onur and Mangul, Serghei},
  title = {{Packaging, Containerization, and Virtualization of Computational Omics Methods: Advances, Challenges, and Opportunities}},
  journal = {arXiv},
  year = {2022},
  month = mar,
}

@article{firtina_hercules_2018,
  title = {{Hercules: A Profile {HMM}-based Hybrid Error Correction Algorithm for Long Reads}},
  volume = {46},
  issn = {0305-1048},
  doi = {10.1093/nar/gky724},
  abstract = {Choosing whether to use second or third generation sequencing platforms can lead to trade-offs between accuracy and read length. Several types of studies require long and accurate reads. In such cases researchers often combine both technologies and the erroneous long reads are corrected using the short reads. Current approaches rely on various graph or alignment based techniques and do not take the error profile of the underlying technology into account. Efficient machine learning algorithms that address these shortcomings have the potential to achieve more accurate integration of these two technologies. We propose Hercules, the first machine learning-based long read error correction algorithm. Hercules models every long read as a profile Hidden Markov Model with respect to the underlying platform’s error profile. The algorithm learns a posterior transition/emission probability distribution for each long read to correct errors in these reads. We show on two DNA-seq BAC clones (CH17-157L1 and CH17-227A2) that Hercules-corrected reads have the highest mapping rate among all competing algorithms and have the highest accuracy when the breadth of coverage is high. On a large human CHM1 cell line WGS data set, Hercules is one of the few scalable algorithms; and among those, it achieves the highest accuracy.},
  number = {21},
  journal = {Nucleic Acids Research},
  author = {Firtina, Can and Bar-Joseph, Ziv and Alkan, Can and Cicek, A. Ercument},
  month= nov,
  year = {2018},
  pages = {e125--e125},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%SUPPLEMENTARY REFERENCES%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{supp_li_minimap2_2018,
	title = {{Minimap2: Pairwise Alignment for Nucleotide Sequences}},
	volume = {34},
	issn = {1367-4803},
	abstract = {Recent advances in sequencing technologies promise ultra-long reads of ∼100 kb in average, full-length mRNA or cDNA reads in high throughput and genomic contigs over 100 Mb in length. Existing alignment programs are unable or inefficient to process such data at scale, which presses for the development of new alignment algorithms.Minimap2 is a general-purpose alignment program to map DNA or long mRNA sequences against a large reference database. It works with accurate short reads of ≥100 bp in length, ≥1 kb genomic reads at error rate ∼15\%, full-length noisy Direct RNA or cDNA reads and assembly contigs or closely related full chromosomes of hundreds of megabases in length. Minimap2 does split-read alignment, employs concave gap cost for long insertions and deletions and introduces new heuristics to reduce spurious alignments. It is 3–4 times as fast as mainstream short-read mappers at comparable accuracy, and is ≥30 times faster than long-read genomic or cDNA mappers at higher accuracy, surpassing most aligners specialized in one type of alignment.https://github.com/lh3/minimap2Supplementary data are available at Bioinformatics online.},
	number = {18},
	urldate = {2021-09-20},
	journal = {Bioinformatics},
	author = {Li, Heng},
	month = sep,
	year = {2018},
	pages = {3094--3100},
}

@article{supp_berlin_assembling_2015,
	title = {{Assembling Large Genomes with Single-Molecule Sequencing and Locality-Sensitive Hashing}},
	volume = {33},
	issn = {1546-1696},
	abstract = {An assembly algorithm that overlaps noisy long reads enables accurate and fast assembly of large genomes from single-molecule real-time sequences.},
	number = {6},
	journal = {Nature Biotechnology},
	author = {Berlin, Konstantin and Koren, Sergey and Chin, Chen-Shan and Drake, James P and Landolin, Jane M and Phillippy, Adam M},
	month = jun,
	year = {2015},
	pages = {623--630},
}

@article{supp_ono_pbsim2_2021,
	title = {{{PBSIM2}: A Simulator for Long-Read Sequencers with a Novel Generative Model of Quality Scores}},
	volume = {37},
	issn = {1367-4803},
	abstract = {Recent advances in high-throughput long-read sequencers, such as PacBio and Oxford Nanopore sequencers, produce longer reads with more errors than short-read sequencers. In addition to the high error rates of reads, non-uniformity of errors leads to difficulties in various downstream analyses using long reads. Many useful simulators, which characterize long-read error patterns and simulate them, have been developed. However, there is still room for improvement in the simulation of the non-uniformity of errors.To capture characteristics of errors in reads for long-read sequencers, here, we introduce a generative model for quality scores, in which a hidden Markov Model with a latest model selection method, called factorized information criteria, is utilized. We evaluated our developed simulator from various points, indicating that our simulator successfully simulates reads that are consistent with real reads.The source codes of PBSIM2 are freely available from https://github.com/yukiteruono/pbsim2.Supplementary data are available at Bioinformatics online.},
	number = {5},
	urldate = {2022-03-15},
	journal = {Bioinformatics},
	author = {Ono, Yukiteru and Asai, Kiyoshi and Hamada, Michiaki},
	month = mar,
	year = {2021},
	pages = {589--595},
}

@article{supp_nurk_hicanu_2020,
	title = {{HiCanu}: {Accurate Assembly of Segmental Duplications, Satellites, and Allelic Variants from High-Fidelity Long Reads}},
	abstract = {Complete and accurate genome assemblies form the basis of most downstream genomic analyses and are of critical importance. Recent genome assembly projects have relied on a combination of noisy long-read sequencing and accurate short-read sequencing, with the former offering greater assembly continuity and the latter providing higher consensus accuracy. The recently introduced PacBio HiFi sequencing technology bridges this divide by delivering long reads (\&gt;10 kbp) with high per-base accuracy (\&gt;99.9\%). Here we present HiCanu, a significant modification of the Canu assembler designed to leverage the full potential of HiFi reads via homopolymer compression, overlap-based error correction, and aggressive false overlap filtering. We benchmark HiCanu with a focus on the recovery of haplotype diversity, major histocompatibility complex (MHC) variants, satellite DNAs, and segmental duplications. For diploid human genomes sequenced to 30× HiFi coverage, HiCanu achieved superior accuracy and allele recovery compared to the current state of the art. On the effectively haploid CHM13 human cell line, HiCanu achieved an NG50 contig size of 77 Mbp with a per-base consensus accuracy of 99.999\% (QV50), surpassing recent assemblies of high-coverage, ultra-long Oxford Nanopore reads in terms of both accuracy and continuity. This HiCanu assembly correctly resolves 337 out of 341 validation BACs sampled from known segmental duplications and provides the first preliminary assemblies of 9 complete human centromeric regions. Although gaps and errors still remain within the most challenging regions of the genome, these results represent a significant advance towards the complete assembly of human genomes.Availability HiCanu is implemented within the Canu assembly framework and is available from https://github.com/marbl/canu.},
	journal = {bioRxiv},
	author = {Nurk, Sergey and Walenz, Brian P. and Rhie, Arang and Vollger, Mitchell R. and Logsdon, Glennis A. and Grothe, Robert and Miga, Karen H. and Eichler, Evan E. and Phillippy, Adam M. and Koren, Sergey},
	month = jan,
	year = {2020},
	pages = {2020.03.14.992248},
}

@article{supp_ren_lra_2021,
	title = {lra: {A} {Long Read Aligner for Sequences and Contigs}},
	volume = {17},
	abstract = {Author summary Any two human genomes will have sequence differences across multiple scales: from single-nucleotide variants to large gains, losses, or rearrangements of DNA called structural variants. Long-read single-molecule sequencing has been shown to help discover structural variation because the reads span across the entire variant. The computational problem for discovering a structural variant is to find the optimal alignment of the read to the genome with gaps that accurately reflect the variant. Here we demonstrate a method, lra, that uses an efficient implementation of concave-cost alignment for structural variant discovery using long reads. On standardized benchmark data, we show that structural variant discovery is improved for multiple combinations of variant detection algorithms and long-read sequence using alignments generated by lra compared to existing methods. Finally, we show that it is possible to use lra to accurately discover a complete spectrum of structural variants using de novo assemblies constructed from long-read sequence data. This implies a future model of comparative genomics where variants are discovered only by comparing de novo assemblies and not a comparison of reads against a reference.},
	number = {6},
	journal = {PLOS Computational Biology},
	author = {Ren, Jingwen and Chaisson, Mark J. P.},
	month = jun,
	year = {2021},
	pages = {e1009078},
}

@article{supp_jain_weighted_2020,
	title = {{Weighted Minimizer Sampling Improves Long Read Mapping}},
	volume = {36},
	issn = {1367-4803},
	abstract = {In this era of exponential data growth, minimizer sampling has become a standard algorithmic technique for rapid genome sequence comparison. This technique yields a sub-linear representation of sequences, enabling their comparison in reduced space and time. A key property of the minimizer technique is that if two sequences share a substring of a specified length, then they can be guaranteed to have a matching minimizer. However, because the k-mer distribution in eukaryotic genomes is highly uneven, minimizer-based tools (e.g. Minimap2, Mashmap) opt to discard the most frequently occurring minimizers from the genome to avoid excessive false positives. By doing so, the underlying guarantee is lost and accuracy is reduced in repetitive genomic regions.We introduce a novel weighted-minimizer sampling algorithm. A unique feature of the proposed algorithm is that it performs minimizer sampling while considering a weight for each k-mer; i.e. the higher the weight of a k-mer, the more likely it is to be selected. By down-weighting frequently occurring k-mers, we are able to meet both objectives: (i) avoid excessive false-positive matches and (ii) maintain the minimizer match guarantee. We tested our algorithm, Winnowmap, using both simulated and real long-read data and compared it to a state-of-the-art long read mapper, Minimap2. Our results demonstrate a reduction in the mapping error-rate from 0.14\% to 0.06\% in the recently finished human X chromosome (154.3 Mbp), and from 3.6\% to 0\% within the highly repetitive X centromere (3.1 Mbp). Winnowmap improves mapping accuracy within repeats and achieves these results with sparser sampling, leading to better index compression and competitive runtimes. Winnowmap is built on top of the Minimap2 codebase and is available at https://github.com/marbl/winnowmap.},
	number = {Supplement\_1},
	urldate = {2021-09-20},
	journal = {Bioinformatics},
	author = {Jain, Chirag and Rhie, Arang and Zhang, Haowen and Chu, Claudia and Walenz, Brian P and Koren, Sergey and Phillippy, Adam M},
	month = jul,
	year = {2020},
	pages = {i111--i118},
}

@article{supp_jain_long-read_2022,
	title = {{Long-Read Mapping to Repetitive Reference Sequences Using {Winnowmap2}}},
	issn = {1548-7105},
	doi = {10.1038/s41592-022-01457-8},
	abstract = {Approximately 5–10\% of the human genome remains inaccessible due to the presence of repetitive sequences such as segmental duplications and tandem repeat arrays. We show that existing long-read mappers often yield incorrect alignments and variant calls within long, near-identical repeats, as they remain vulnerable to allelic bias. In the presence of a nonreference allele within a repeat, a read sampled from that region could be mapped to an incorrect repeat copy. To address this limitation, we developed a new long-read mapping method, Winnowmap2, by using minimal confidently alignable substrings. Winnowmap2 computes each read mapping through a collection of confident subalignments. This approach is more tolerant of structural variation and more sensitive to paralog-specific variants within repeats. Our experiments highlight that Winnowmap2 successfully addresses the issue of allelic bias, enabling more accurate downstream variant calls in repetitive sequences.},
	journal = {Nature Methods},
	author = {Jain, Chirag and Rhie, Arang and Hansen, Nancy F. and Koren, Sergey and Phillippy, Adam M.},
	month = apr,
	year = {2022},
}

@article{supp_chakraborty_conlsh_2020,
	title = {{conLSH}: {Context} based {Locality} {Sensitive} {Hashing} for {Mapping of Noisy {SMRT} Reads}},
	volume = {85},
	issn = {1476-9271},
	abstract = {Single Molecule Real-Time (SMRT) sequencing is a recent advancement of Next Gen technology developed by Pacific Bio (PacBio). It comes with an explosion of long and noisy reads demanding cutting edge research to get most out of it. To deal with the high error probability of SMRT data, a novel contextual Locality Sensitive Hashing (conLSH) based algorithm is proposed in this article, which can effectively align the noisy SMRT reads to the reference genome. Here, sequences are hashed together based not only on their closeness, but also on similarity of context. The algorithm has O(nρ+1) space requirement, where n is the number of sequences in the corpus and ρ is a constant. The indexing time and querying time are bounded by Onρ+1·lnnln1P2 and O(nρ) respectively, where P2 {\textgreater} 0, is a probability value. This algorithm is particularly useful for retrieving similar sequences, a widely used task in biology. The proposed conLSH based aligner is compared with rHAT, popularly used for aligning SMRT reads, and is found to comprehensively beat it in speed as well as in memory requirements. In particular, it takes approximately 24.2\% less processing time, while saving about 70.3\% in peak memory requirement for H.sapiens PacBio dataset.},
	journal = {Computational Biology and Chemistry},
	author = {Chakraborty, Angana and Bandyopadhyay, Sanghamitra},
	month = apr,
	year = {2020},
	keywords = {Algorithm, Locality Sensitive Hashing, PacBio dataset, Sequence alignment, Sequence analysis, Single Molecule Real-Time (SMRT) sequencing},
	pages = {107206},
}

@article{supp_chakraborty_s-conlsh_2021,
	title = {S-{conLSH}: {Alignment-Free Gapped Mapping of Noisy Long Reads}},
	volume = {22},
	issn = {1471-2105},
	abstract = {The advancement of SMRT technology has unfolded new opportunities of genome analysis with its longer read length and low GC bias. Alignment of the reads to their appropriate positions in the respective reference genome is the first but costliest step of any analysis pipeline based on SMRT sequencing. However, the state-of-the-art aligners often fail to identify distant homologies due to lack of conserved regions, caused by frequent genetic duplication and recombination. Therefore, we developed a novel alignment-free method of sequence mapping that is fast and accurate.},
	number = {1},
	journal = {BMC Bioinformatics},
	author = {Chakraborty, Angana and Morgenstern, Burkhard and Bandyopadhyay, Sanghamitra},
	month = feb,
	year = {2021},
	pages = {64},
}

@inproceedings{supp_charikar_similarity_2002,
	address = {New York, NY, USA},
	series = {{STOC} '02},
	title = {Similarity {Estimation} {Techniques} from {Rounding} {Algorithms}},
	isbn = {1-58113-495-9},
	abstract = {(MATH) A locality sensitive hashing scheme is a distribution on a family \$F\$ of hash functions operating on a collection of objects, such that for two objects x,y, PrhεF[h(x) = h(y)] = sim(x,y), where sim(x,y) ε [0,1] is some similarity function defined on the collection of objects. Such a scheme leads to a compact representation of objects so that similarity of objects can be estimated from their compact sketches, and also leads to efficient algorithms for approximate nearest neighbor search and clustering. Min-wise independent permutations provide an elegant construction of such a locality sensitive hashing scheme for a collection of subsets with the set similarity measure sim(A,B) = frac{\textbar}A ∩ B{\textbar}{\textbar}A ∪ B{\textbar}.(MATH) We show that rounding algorithms for LPs and SDPs used in the context of approximation algorithms can be viewed as locality sensitive hashing schemes for several interesting collections of objects. Based on this insight, we construct new locality sensitive hashing schemes for:A collection of vectors with the distance between → over u and → over v measured by Ø(→ over u, → over v)/π, where Ø(→ over u, → over v) is the angle between → over u) and → over v). This yields a sketching scheme for estimating the cosine similarity measure between two vectors, as well as a simple alternative to minwise independent permutations for estimating set similarity.A collection of distributions on n points in a metric space, with distance between distributions measured by the Earth Mover Distance (EMD), (a popular distance measure in graphics and vision). Our hash functions map distributions to points in the metric space such that, for distributions P and Q, EMD(P,Q) ≤ EhεF [d(h(P),h(Q))] ≤ O(log n log log n). EMD(P, Q).},
	booktitle = {Proceedings of the {Thiry}-{Fourth} {Annual} {ACM} {Symposium} on {Theory} of {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Charikar, Moses S.},
	year = {2002},
	pages = {380--388},
}

@inproceedings{supp_manku_detecting_2007,
	address = {New York, NY, USA},
	series = {{WWW} '07},
	title = {Detecting {Near}-{Duplicates} for {Web} {Crawling}},
	isbn = {978-1-59593-654-7},
	abstract = {Near-duplicate web documents are abundant. Two such documents differ from each other in a very small portion that displays advertisements, for example. Such differences are irrelevant for web search. So the quality of a web crawler increases if it can assess whether a newly crawled web page is a near-duplicate of a previously crawled web page or not. In the course of developing a near-duplicate detection system for a multi-billion page repository, we make two research contributions. First, we demonstrate that Charikar's fingerprinting technique is appropriate for this goal. Second, we present an algorithmic technique for identifying existing f-bit fingerprints that differ from a given fingerprint in at most k bit-positions, for small k. Our technique is useful for both online queries (single fingerprints) and all batch queries (multiple fingerprints). Experimental evaluation over real data confirms the practicality of our design.},
	booktitle = {Proceedings of the 16th {International} {Conference} on {World} {Wide} {Web}},
	publisher = {Association for Computing Machinery},
	author = {Manku, Gurmeet Singh and Jain, Arvind and Das Sarma, Anish},
	year = {2007},
	keywords = {fingerprint, hamming distance, near-duplicate, search, similarity, sketch, web crawl, web document},
	pages = {141--150},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%END OF SUPPLEMENTARY REFERENCES%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




@article{liu2018darts,
  title={{Darts: Differentiable Architecture Search}},
  author={Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  journal={arXiv},
  year={2018}
}


@article{cai2018proxylessnas,
  title={{Proxylessnas: Direct Neural Architecture Search on Target Task and Hardware}},
  author={Cai, Han and Zhu, Ligeng and Han, Song},
  journal={arXiv},
  year={2018}
}

@misc{amdEPYC,
  title = {{Introducing 3rd Gen AMD EPYC™ Processors}, \url{https://www.amd.com/en/events/epyc}
}}
@misc{mi50,
  title = {{AMD Radeon Instinct™ MI50 Accelerator (32GB)}, \url{https://www.amd.com/system/files/documents/radeon-instinct-mi50-datasheet.pdf}
}}

@misc{rocm511,
  title = {{ROCm}, \url{https://github.com/RadeonOpenCompute/ROCm}
}}
@misc{a100,
  title = {{NVIDIA A100 Tensor Core GPU}, \url{https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/a100-80gb-datasheet-update-nvidia-us-1521051-r2-web.pdf}
}}
@inproceedings{SMT,
  title={{Simultaneous Multithreading: Maximizing On-Chip Parallelism}},
  author={Tullsen, Dean M and Eggers, Susan J and Levy, Henry M},
  booktitle={ISCA},
  year={1995}
}
@misc{rdimm,
  title = {{RDIMM}, \url{https://www.micron.com/products/dram-modules/rdimm}
}}
@misc{ubuntu,
  title = {{Ubuntu 20.04.3 LTS (Focal Fossa)}, \url{https://releases.ubuntu.com/20.04/}
}}

@misc{nvidia-smi,
  title = {{NVIDIA System Management Interface}, \url{https://developer.nvidia.com/nvidia-system-management-interface}
}}

@misc{nvcc,
  title = {{NVIDIA CUDA Compiler Driver NVCC}, \url{https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html}
}}


@misc{aiemlvc2802,
  title = {{Versal ACAP AI Core Series Product Selection Guide}, \url{https://www.xilinx.com/content/dam/xilinx/support/documents/selection-guides/versal-ai-core-product-selection-guide.pdf}
}}
@misc{hpcfund,
  title = {{AMD HPC Fund}, \url{https://www.amd.com/en/corporate/hpc-fund}
}}
@misc{onnx,
  title = {{Open Neural Network Exchange (ONNX)}, \url{https://github.com/onnx/onnx}
}}
@misc{aiexcvc1902,
  title = {{Versal Architecture and Product Data Sheet: Overview}, \url{https://www.xilinx.com/support/documentation/data_sheets/ds950-versal-overview.pdf}
}}
@misc{bonito,
  title = {{Bonito}, \url{https://github.com/nanoporetech/bonito}
}}
@misc{guppy,
  title = {{How Basecalling Works}, \url{https://nanoporetech.com/how-it-works/basecalling}
}}
@misc{xilinxdimm,
  title = {{Memory Interfaces Design Hub - UltraScale DDR3/DDR4 Memory}, \url{https://www.xilinx.com/support/documentation-navigation/design-hubs/dh0061-ultrascale-memory-interface-ddr4-ddr3-hub.html
}
}}

@misc{armOnAIE,
  title = {{ARM Cortex-A72 MPCore Processor Technical Reference Manual r0p3}, \url{https://developer.arm.com/documentation/100095/0003}
}}
@misc{vck190,
  title = {{Versal AI Core Series VCK190 Evaluation Kit}, \url{https://www.xilinx.com/products/boards-and-kits/vck190.html}
}}
@article{illiac_simd_1968,
  title={{The ILLIAC IV Computer}},
  author={Barnes, George H and Brown, Richard M and Kato, Maso and Kuck, David J and Slotnick, Daniel L and Stokes, Richard A},
  journal={IEEE Transactions on Computers},
  year={1968}
}

@article{lee2015method,
  title={{A Method to Predict the Impact of Regulatory Variants from DNA Sequence}},
  author={Lee, Dongwon and Gorkin, David U and Baker, Maggie and Strober, Benjamin J and Asoni, Alessandro L and McCallion, Andrew S and Beer, Michael A},
  journal={Nature genetics},
  volume={47},
  number={8},
  pages={955--961},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{alipanahi2015predicting,
  title={{Predicting the Sequence Specificities of DNA-and RNA-Binding Proteins by Deep Learning}},
  author={Alipanahi, Babak and Delong, Andrew and Weirauch, Matthew T and Frey, Brendan J},
  journal={Nature biotechnology},
  volume={33},
  number={8},
  pages={831--838},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{boemo2021dnascent,
  title={{DNAscent v2: Detecting Replication Forks in Nanopore Sequencing Data with Deep Learning}},
  author={Boemo, Michael A},
  journal={BMC genomics},
  volume={22},
  number={1},
  pages={1--8},
  year={2021},
  publisher={BioMed Central}
}

@inproceedings{sabba2021residual,
  title={{Residual Neural Network for Predicting Super-Enhancers on Genome Scale}},
  author={Sabba, Sara and Smara, Meroua and Benhacine, Mehdi and Hameurlaine, Amina},
  booktitle={International Conference on Artificial Intelligence and its Applications},
  pages={32--42},
  year={2021},
  organization={Springer}
}

@article{robertson2010novo,
  title={{De Novo Assembly and Analysis of RNA-seq Data}},
  author={Robertson, Gordon and Schein, Jacqueline and Chiu, Readman and Corbett, Richard and Field, Matthew and Jackman, Shaun D and Mungall, Karen and Lee, Sam and Okada, Hisanaga Mark and Qian, Jenny Q and others},
  journal={Nature methods},
  volume={7},
  number={11},
  pages={909--912},
  year={2010},
  publisher={Nature Publishing Group}
}
@article{li2010rna,
  title={{RNA-Seq Gene Expression Estimation with Read Mapping Uncertainty}},
  author={Li, Bo and Ruotti, Victor and Stewart, Ron M and Thomson, James A and Dewey, Colin N},
  journal={Bioinformatics},
  volume={26},
  number={4},
  pages={493--500},
  year={2010},
  publisher={Oxford University Press}
}

@article{li2009sequence,
  title={{The Sequence Alignment/Map (SAM) Format and SAMtools}},
  author={Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard and 1000 Genome Project Data Processing Subgroup and others},
  journal={Bioinformatics},
  volume={25},
  number={16},
  pages={2078--2079},
  year={2009},
  publisher={Citeseer}
}

@online{gnu,
  title = {{GCC, the GNU Compiler Collection}}, 
  url = {https://gcc.gnu.org/},
}

@article{ferrarini_evaluation_2013,
	title = {{An Evaluation of the {PacBio} {RS} Platform for Sequencing and De Novo Assembly of a Chloroplast Genome}},
	volume = {14},
	issn = {1471-2164},
	doi = {10.1186/1471-2164-14-670},
	number = {1},
	journal = {BMC Genomics},
	author = {Ferrarini, Marco and Moretto, Marco and Ward, Judson A. and Šurbanovski, Nada and Stevanović, Vladimir and Giongo, Lara and Viola, Roberto and Cavalieri, Duccio and Velasco, Riccardo and Cestaro, Alessandro and Sargent, Daniel J.},
	month = oct,
	year = {2013},
	pages = {670},
}

@article{chen_effects_2013,
	title = {Effects of {GC} {Bias} in {Next}-{Generation}-{Sequencing} {Data} on {De} {Novo} {Genome} {Assembly}},
	volume = {8},
	doi = {10.1371/journal.pone.0062856},
	number = {4},
	journal = {PLOS ONE},
	author = {Chen, Yen-Chun and Liu, Tsunglin and Yu, Chun-Hui and Chiang, Tzen-Yuh and Hwang, Chi-Chuan},
	month = apr,
	year = {2013},
	pages = {e62856},
}


@article{kingma2014adam,
  title={{Adam: A Method for Stochastic Optimization}},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv},
  year={2014}
}
@article{amari1993backpropagation,
  title={{Backpropagation and Stochastic Gradient Descent Method}},
  author={Amari, Shun-ichi},
  journal={Neurocomputing},
  volume={5},
  number={4-5},
  pages={185--196},
  year={1993},
  publisher={Elsevier}
}

@article{loshchilov2016sgdr,
  title={{Sgdr: Stochastic Gradient Descent with Warm Restarts}},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}

@misc{dorado,
  title = {{Dorado}, \url{https://github.com/nanoporetech/dorado.git}
}}

@article{kang2019accelerator,
  title={{Accelerator-Aware Pruning for Convolutional Neural Networks}},
  author={Kang, Hyeong-Ju},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={30},
  number={7},
  pages={2093--2103},
  year={2019},
  publisher={IEEE}
}

@article{lecun1989optimal,
  title={{Optimal Brain Damage}},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}
@article{han2015learning,
  title={{Learning both Weights and Connections for Efficient Neural Network}},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{han2015deep,
  title={{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv},
  year={2015}
}

@article{frankle2018lottery,
  title={{The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks}},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{gale2019state,
  title={{The State of Sparsity in Deep Neural Networks}},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}

@article{kruschke1991benefits,
  title={{Benefits of Gain: Speeded Learning and Minimal Hidden Layers in Back-Propagation Networks}},
  author={Kruschke, John K and Movellan, Javier R},
  journal={IEEE Transactions on systems, Man, and Cybernetics},
  volume={21},
  number={1},
  pages={273--280},
  year={1991},
  publisher={IEEE}
}
@article{liu2018rethinking,
  title={{Rethinking the Value of Network Pruning}},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  journal={arXiv preprint arXiv:1810.05270},
  year={2018}
}


@article{ginsburg2018precision,
author = {Ginsburg, Geoffrey and Phillips, Kathryn},
year = {2018},
month = {05},
pages = {694-701},
title = {{Precision Medicine: From Science To Value}},
volume = {37},
journal = {Health Affairs},
doi = {10.1377/hlthaff.2017.1624}
}

@article {aryan2020moving,
	Title = {{Moving Genomics to Routine Care: An Initial Pilot in Acute Cardiovascular Disease}},
	Author = {Aryan, Zahra and Szanto, Attila and Pantazi, Angeliki and Reddi, Tejaswini and Rheinstein, Carolyn and Powers, Winslow and Wilson, Evan and Deo, Rahul C and Chowdhury, Shimul and Salz, Lisa and Dimmock, David and Nahas, Shareef and Benson, Wendy and Kingsmore, Stephen F and MacRae, Calum A and Vuzman, Dana},
	DOI = {10.1161/circgen.120.002961},
	Number = {5},
	Volume = {13},
	Month = {October},
	Year = {2020},
	Journal = {Circulation. Genomic and precision medicine},
	ISSN = {2574-8300},
	Pages = {406—416},
	URL = {https://doi.org/10.1161/CIRCGEN.120.002961},
}

@article {clark2019diagnosis,
	Title = {{Diagnosis of Genetic Diseases in Seriously Ill Children by Rapid Whole-Genome Sequencing and Automated Phenotyping and Interpretation}},
	Author = {Clark, Michelle M and Hildreth, Amber and Batalov, Sergey and Ding, Yan and Chowdhury, Shimul and Watkins, Kelly and Ellsworth, Katarzyna and Camp, Brandon and Kint, Cyrielle I and Yacoubian, Calum and Farnaes, Lauge and Bainbridge, Matthew N and Beebe, Curtis and Braun, Joshua J A and Bray, Margaret and Carroll, Jeanne and Cakici, Julie A and Caylor, Sara A and Clarke, Christina and Creed, Mitchell P and Friedman, Jennifer and Frith, Alison and Gain, Richard and Gaughran, Mary and George, Shauna and Gilmer, Sheldon and Gleeson, Joseph and Gore, Jeremy and Grunenwald, Haiying and Hovey, Raymond L and Janes, Marie L and Lin, Kejia and McDonagh, Paul D and McBride, Kyle and Mulrooney, Patrick and Nahas, Shareef and Oh, Daeheon and Oriol, Albert and Puckett, Laura and Rady, Zia and Reese, Martin G and Ryu, Julie and Salz, Lisa and Sanford, Erica and Stewart, Lawrence and Sweeney, Nathaly and Tokita, Mari and Van Der Kraan, Luca and White, Sarah and Wigby, Kristen and Williams, Brett and Williams, Brett and Wong, Terence and Wright, Meredith S and Yamada, Catherine and Schols, Peter and Reynders, John and Hall, Kevin and Dimmock, David and Veeraraghavan, Narayanan and Defay, Thomas and Kingsmore, Stephen F and Kingsmore, Stephen F},
	DOI = {10.1126/scitranslmed.aat6177},
	Number = {489},
	Volume = {11},
	Month = {April},
	Year = {2019},
	Journal = {Science translational medicine},
	ISSN = {1946-6234},
	Pages = {eaat6177},
	URL = {https://doi.org/10.1126/scitranslmed.aat6177},
}

@article {kingsmore2022genome,
	Title = {{A Genome Sequencing System for Universal Newborn Screening, Diagnosis, and Precision Medicine for Severe Genetic Diseases}},
	Author = {Kingsmore, Stephen F and Smith, Laurie D and Kunard, Chris M and Bainbridge, Matthew and Batalov, Sergey and Benson, Wendy and Blincow, Eric and Caylor, Sara and Chambers, Christina and Del Angel, Guillermo and Dimmock, David P and Ding, Yan and Ellsworth, Katarzyna and Feigenbaum, Annette and Frise, Erwin and Green, Robert C and Guidugli, Lucia and Hall, Kevin P and Hansen, Christian and Hobbs, Charlotte A and Kahn, Scott D and Kiel, Mark and Van Der Kraan, Lucita and Krilow, Chad and Kwon, Yong H and Madhavrao, Lakshminarasimha and Le, Jennie and Lefebvre, Sebastien and Mardach, Rebecca and Mowrey, William R and Oh, Danny and Owen, Mallory J and Powley, George and Scharer, Gunter and Shelnutt, Seth and Tokita, Mari and Mehtalia, Shyamal S and Oriol, Albert and Papadopoulos, Stavros and Perry, James and Rosales, Edwin and Sanford, Erica and Schwartz, Steve and Tran, Duke and Reese, Martin G and Wright, Meredith and Veeraraghavan, Narayanan and Wigby, Kristen and Willis, Mary J and Wolen, Aaron R and Defay, Thomas},
	DOI = {10.1016/j.ajhg.2022.08.003},
	Number = {9},
	Volume = {109},
	Month = {September},
	Year = {2022},
	Journal = {American journal of human genetics},
	ISSN = {0002-9297},
	Pages = {1605—1619},
	URL = {https://doi.org/10.1016/j.ajhg.2022.08.003},
}

@article{ginsburg2009genomic,
title = {{Genomic and Personalized Medicine: Foundations and Applications}},
journal = {Translational Research},
volume = {154},
number = {6},
pages = {277-287},
year = {2009},
note = {Special Issue on Personalized Medicine},
issn = {1931-5244},
doi = {https://doi.org/10.1016/j.trsl.2009.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1931524409002746},
author = {Geoffrey S. Ginsburg and Huntington F. Willard},
abstract = {The last decade has witnessed a steady embrace of genomic and personalized medicine by senior government officials, industry leadership, health care providers, and the public. Genomic medicine, which is the use of information from genomes and their derivatives (RNA, proteins, and metabolites) to guide medical decision making—is a key component of personalized medicine, which is a rapidly advancing field of health care that is informed by each person's unique clinical, genetic, genomic, and environmental information. As medicine begins to embrace genomic tools that enable more precise prediction and treatment disease, which include “whole genome” interrogation of sequence variation, transcription, proteins, and metabolites, the fundamentals of genomic and personalized medicine will require the development, standardization, and integration of several important tools into health systems and clinical workflows. These tools include health risk assessment, family health history, and clinical decision support for complex risk and predictive information. Together with genomic information, these tools will enable a paradigm shift to a comprehensive approach that will identify individual risks and guide clinical management and decision making, all of which form the basis for a more informed and effective approach to patient care. DNA-based risk assessment for common complex disease, molecular signatures for cancer diagnosis and prognosis, and genome-guided therapy and dose selection are just among the few important examples for which genome information has already enabled personalized health care along the continuum from health to disease. In addition, information from individual genomes, which is a fast-moving area of technological development, is spawning a social and information revolution among consumers that will undoubtedly affect health care decision making. Although these and other scientific findings are making their way from the genome to the clinic, the full application of genomic and personalized medicine in health care will require dramatic changes in regulatory and reimbursement policies as well as legislative protections for privacy for system-wide adoption. Thus, there are challenges from both a scientific and a policy perspective to personalized health care; however, they will be confronted and solved with the certainty that the science behind genomic medicine is sound and the practice of medicine that it informs is evidence based.}
}

@Article{bloom2021massively,
author={Bloom, Joshua S.
and Sathe, Laila
and Munugala, Chetan
and Jones, Eric M.
and Gasperini, Molly
and Lubock, Nathan B.
and Yarza, Fauna
and Thompson, Erin M.
and Kovary, Kyle M.
and Park, Jimin
and Marquette, Dawn
and Kay, Stephania
and Lucas, Mark
and Love, TreQuan
and Sina Booeshaghi, A.
and Brandenberg, Oliver F.
and Guo, Longhua
and Boocock, James
and Hochman, Myles
and Simpkins, Scott W.
and Lin, Isabella
and LaPierre, Nathan
and Hong, Duke
and Zhang, Yi
and Oland, Gabriel
and Choe, Bianca Judy
and Chandrasekaran, Sukantha
and Hilt, Evann E.
and Butte, Manish J.
and Damoiseaux, Robert
and Kravit, Clifford
and Cooper, Aaron R.
and Yin, Yi
and Pachter, Lior
and Garner, Omai B.
and Flint, Jonathan
and Eskin, Eleazar
and Luo, Chongyuan
and Kosuri, Sriram
and Kruglyak, Leonid
and Arboleda, Valerie A.},
title={{Massively Scaled-Up Testing for SARS-CoV-2 RNA via Next-Generation Sequencing of Pooled and Barcoded Nasal and Saliva Samples}},
journal={Nature Biomedical Engineering},
year={2021},
month={Jul},
day={01},
volume={5},
number={7},
pages={657-665},
abstract={Frequent and widespread testing of members of the population who are asymptomatic for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is essential for the mitigation of the transmission of the virus. Despite the recent increases in testing capacity, tests based on quantitative polymerase chain reaction (qPCR) assays cannot be easily deployed at the scale required for population-wide screening. Here, we show that next-generation sequencing of pooled samples tagged with sample-specific molecular barcodes enables the testing of thousands of nasal or saliva samples for SARS-CoV-2 RNA in a single run without the need for RNA extraction. The assay, which we named SwabSeq, incorporates a synthetic RNA standard that facilitates end-point quantification and the calling of true negatives, and that reduces the requirements for automation, purification and sample-to-sample normalization. We used SwabSeq to perform 80,000 tests, with an analytical sensitivity and specificity comparable to or better than traditional qPCR tests, in less than two months with turnaround times of less than 24{\thinspace}h. SwabSeq could be rapidly adapted for the detection of other pathogens.},
issn={2157-846X},
doi={10.1038/s41551-021-00754-5},
url={https://doi.org/10.1038/s41551-021-00754-5}
}

@misc{quick2016realtime,
publisher = {Nature Research},
title = {{Real-Time, Portable Genome Sequencing for Ebola Surveillance}},
volume = {530},
year = {2016-02-11},
abstract = {The Ebola virus disease epidemic in West Africa is the largest on record, responsible for over 28,599 cases and more than 11,299 deaths. Genome sequencing in viral outbreaks is desirable to characterize the infectious agent and determine its evolutionary rate. Genome sequencing also allows the identification of signatures of host adaptation, identification and monitoring of diagnostic targets, and characterization of responses to vaccines and treatments. The Ebola virus (EBOV) genome substitution rate in the Makona strain has been estimated at between 0.87 × 10(-3) and 1.42 × 10(-3) mutations per site per year. This is equivalent to 16-27 mutations in each genome, meaning that sequences diverge rapidly enough to identify distinct sub-lineages during a prolonged epidemic. Genome sequencing provides a high-resolution view of pathogen evolution and is increasingly sought after for outbreak surveillance. Sequence data may be used to guide control measures, but only if the results are generated quickly enough to inform interventions. Genomic surveillance during the epidemic has been sporadic owing to a lack of local sequencing capacity coupled with practical difficulties transporting samples to remote sequencing facilities. To address this problem, here we devise a genomic surveillance system that utilizes a novel nanopore DNA sequencing instrument. In April 2015 this system was transported in standard airline luggage to Guinea and used for real-time genomic surveillance of the ongoing epidemic. We present sequence data and analysis of 142 EBOV samples collected during the period March to October 2015. We were able to generate results less than 24 h after receiving an Ebola-positive sample, with the sequencing process taking as little as 15-60 min. We show that real-time genomic surveillance is possible in resource-limited settings and can be established rapidly to monitor outbreaks. },
author = {Quick, Joshua and Loman, Nicholas J and Duraffour, Sophie and Simpson, Jared T and Severi, Ettore and Cowley, Lauren and Bore, Joseph Akoi and Koundouno, Raymond and Dudas, Gytis and Mikhail, Amy and Ouedraogo, Nobila and Afrough, Babak and Bah, Amadou and Baum, Jonathan HJ and Becker-Ziaja, Beate and Boettcher, Jan Peter and Cabeza-Cabrerizo, Mar and Camino-Sanchez, Alvaro and Carter, Lisa L and Doerrbecker, Juliane and Enkirch, Theresa and Garcia-Dorival, Isabel and Hetzelt, Nicole and Hinzmann, Julia and Holm, Tobias and Kafetzopoulou, Liana Eleni and Koropogui, Michel and Kosgey, Abigael and Kuisma, Eeva and Logue, Christopher H and Mazzarelli, Antonio and Meisel, Sarah and Mertens, Marc and Michel, Janine and Ngabo, Didier and Nitzsche, Katja and Pallasch, Elisa and Patrono, Livia Victoria and Portmann, Jasmine and Repits, Johanna Gabriella and Rickett, Natasha Y and Sachse, Andreas and Singethan, Katrin and Vitoriano, Ines and Emanaberhan, Rahel LY and Zekeng, Elsa G and Racine, Trina and Bello, Alexander and Sall, Amadou Alpha and Faye, Ousmane and Faye, Oumar and Magassouba, N'Faly and Williams, Cecelia V and Amburgey, Victoria and Winona, Linda and Davis, Emily and Gerlach, Jon and Washington, Frank and Monteil, Vanessa and Jourdain, Marine and Bererd, Marion and Camara, Alimou and Somlare, Hermann and Camara, Abdoulaye and Gerard, Marianne and Bado, Guillaume and Baillet, Bernard and Delaune, Deborah and Nebie, Koumpingnin Yacouba and Diarra, Abdoulaye and Savane, Yacouba and Pallawo, Raymond Bernard and Gutierrez, Giovanna Jaramillo and Milhano, Natacha and Roger, Isabelle and Williams, Christopher J and Yattara, Facinet and Lewandowski, Kuiama and Taylor, James and Rachwal, Phillip and Turner, Daniel J and Pollakis, Georgios and Hiscox, Julian A and Matthews, David A and O'Shea, Matthew K and Johnston, Andrew McD and Wilson, Duncan and Hutley, Emma and Smit, Erasmus and DiCaro, Antonino and Woelfel, Roman and Stoecker, Kilian and Fleischmann, Erna and Gabriel, Martin and Weller, Simon A and Koivogui, Lamine and Diallo, Boubacar and Keita, Sakoba and Rambaut, Andrew and Formenty, Pierre and Guenther, Stephan and Carroll, Miles W},
issn = {1476-4687},
journal = {Nature},
keywords = {Science & Technology},
language = {eng},
number = {7589},
}

@article {yelagandula2021multiplexed,
	Title = {{Multiplexed Detection of SARS-CoV-2 and Other Respiratory Infections in High Throughput by SARSeq}},
	Author = {Yelagandula, Ramesh and Bykov, Aleksandr and Vogt, Alexander and Heinen, Robert and Özkan, Ezgi and Strobl, Marcus Martin and Baar, Juliane Christina and Uzunova, Kristina and Hajdusits, Bence and Kordic, Darja and Suljic, Erna and Kurtovic-Kozaric, Amina and Izetbegovic, Sebija and Schaeffer, Justine and Hufnagl, Peter and Zoufaly, Alexander and Seitz, Tamara and {VCDI} and Födinger, Manuela and Allerberger, Franz and Stark, Alexander and Cochella, Luisa and Elling, Ulrich},
	DOI = {10.1038/s41467-021-22664-5},
	Number = {1},
	Volume = {12},
	Month = {May},
	Year = {2021},
	Journal = {Nature communications},
	ISSN = {2041-1723},
	Pages = {3132},
	Abstract = {The COVID-19 pandemic has demonstrated the need for massively-parallel, cost-effective tests monitoring viral spread. Here we present SARSeq, saliva analysis by RNA sequencing, a method to detect SARS-CoV-2 and other respiratory viruses on tens of thousands of samples in parallel. SARSeq relies on next generation sequencing of multiple amplicons generated in a multiplexed RT-PCR reaction. Two-dimensional, unique dual indexing, using four indices per sample, enables unambiguous and scalable assignment of reads to individual samples. We calibrate SARSeq on SARS-CoV-2 synthetic RNA, virions, and hundreds of human samples of various types. Robustness and sensitivity were virtually identical to quantitative RT-PCR. Double-blinded benchmarking to gold standard quantitative-RT-PCR performed by human diagnostics laboratories confirms this high sensitivity. SARSeq can be used to detect Influenza A and B viruses and human rhinovirus in parallel, and can be expanded for detection of other pathogens. Thus, SARSeq is ideally suited for differential diagnostic of infections during a pandemic.},
	URL = {https://europepmc.org/articles/PMC8149640},
}

@article{le2013selected,
  title={{Selected Insights from Application of Whole-Genome Sequencing for Outbreak Investigations}},
  author={Vien Thi Minh Le and Binh An Diep},
  journal={Current Opinion in Critical Care},
  year={2013},
  volume={19},
  pages={432–439}
}

@article{vladyslav2016whole,
author = {Nikolayevskyy, Vladyslav and Kranzer, Katharina and Niemann, Stefan and Drobniewski, Francis},
year = {2016},
month = {03},
pages = {},
title = {{Whole Genome Sequencing of M.tuberculosis for Detection of Recent Transmission and Tracing Outbreaks: A Systematic Review}},
volume = {98},
journal = {Tuberculosis},
doi = {10.1016/j.tube.2016.02.009}
}


@article{lapierre2019micop,
author = {LaPierre, Nathan and Mangul, Serghei and Alser, Mohammed and Mandric, Igor and Wu, Nicholas and Koslicki, David and Eskin, Eleazar},
year = {2019},
month = {06},
pages = {423},
title = {{MiCoP: Microbial Community Profiling Method for Detecting Viral and Fungal Organisms in Metagenomic Samples}},
volume = {20},
journal = {BMC Genomics},
doi = {10.1186/s12864-019-5699-9}
}

@Article{meyer2022critical,
author={Meyer, Fernando
and Fritz, Adrian
and Deng, Zhi-Luo
and Koslicki, David
and Lesker, Till Robin
and Gurevich, Alexey
and Robertson, Gary
and Alser, Mohammed
and Antipov, Dmitry
and Beghini, Francesco
and Bertrand, Denis
and Brito, Jaqueline J.
and Brown, C. Titus
and Buchmann, Jan
and Bulu{\c{c}}, Aydin
and Chen, Bo
and Chikhi, Rayan
and Clausen, Philip T. L. C.
and Cristian, Alexandru
and Dabrowski, Piotr Wojciech
and Darling, Aaron E.
and Egan, Rob
and Eskin, Eleazar
and Georganas, Evangelos
and Goltsman, Eugene
and Gray, Melissa A.
and Hansen, Lars Hestbjerg
and Hofmeyr, Steven
and Huang, Pingqin
and Irber, Luiz
and Jia, Huijue
and J{\o}rgensen, Tue Sparholt
and Kieser, Silas D.
and Klemetsen, Terje
and Kola, Axel
and Kolmogorov, Mikhail
and Korobeynikov, Anton
and Kwan, Jason
and LaPierre, Nathan
and Lemaitre, Claire
and Li, Chenhao
and Limasset, Antoine
and Malcher-Miranda, Fabio
and Mangul, Serghei
and Marcelino, Vanessa R.
and Marchet, Camille
and Marijon, Pierre
and Meleshko, Dmitry
and Mende, Daniel R.
and Milanese, Alessio
and Nagarajan, Niranjan
and Nissen, Jakob
and Nurk, Sergey
and Oliker, Leonid
and Paoli, Lucas
and Peterlongo, Pierre
and Piro, Vitor C.
and Porter, Jacob S.
and Rasmussen, Simon
and Rees, Evan R.
and Reinert, Knut
and Renard, Bernhard
and Robertsen, Espen Mikal
and Rosen, Gail L.
and Ruscheweyh, Hans-Joachim
and Sarwal, Varuni
and Segata, Nicola
and Seiler, Enrico
and Shi, Lizhen
and Sun, Fengzhu
and Sunagawa, Shinichi
and S{\o}rensen, S{\o}ren Johannes
and Thomas, Ashleigh
and Tong, Chengxuan
and Trajkovski, Mirko
and Tremblay, Julien
and Uritskiy, Gherman
and Vicedomini, Riccardo
and Wang, Zhengyang
and Wang, Ziye
and Wang, Zhong
and Warren, Andrew
and Willassen, Nils Peder
and Yelick, Katherine
and You, Ronghui
and Zeller, Georg
and Zhao, Zhengqiao
and Zhu, Shanfeng
and Zhu, Jie
and Garrido-Oter, Ruben
and Gastmeier, Petra
and Hacquard, Stephane
and H{\"a}u{\ss}ler, Susanne
and Khaledi, Ariane
and Maechler, Friederike
and Mesny, Fantin
and Radutoiu, Simona
and Schulze-Lefert, Paul
and Smit, Nathiana
and Strowig, Till
and Bremges, Andreas
and Sczyrba, Alexander
and McHardy, Alice Carolyn},
title={{Critical Assessment of Metagenome Interpretation: The Second Round of Challenges}},
journal={Nature Methods},
year={2022},
month={Apr},
day={01},
volume={19},
number={4},
pages={429-440},
abstract={Evaluating metagenomic software is key for optimizing metagenome interpretation and focus of the Initiative for the Critical Assessment of Metagenome Interpretation (CAMI). The CAMI II challenge engaged the community to assess methods on realistic and complex datasets with long- and short-read sequences, created computationally from around 1,700 new and known genomes, as well as 600 new plasmids and viruses. Here we analyze 5,002 results by 76 program versions. Substantial improvements were seen in assembly, some due to long-read data. Related strains still were challenging for assembly and genome recovery through binning, as was assembly quality for the latter. Profilers markedly matured, with taxon profilers and binners excelling at higher bacterial ranks, but underperforming for viruses and Archaea. Clinical pathogen detection results revealed a need to improve reproducibility. Runtime and memory usage analyses identified efficient programs, including top performers with other metrics. The results identify challenges and guide researchers in selecting methods for analyses.},
issn={1548-7105},
doi={10.1038/s41592-022-01431-4},
url={https://doi.org/10.1038/s41592-022-01431-4}
}


@Article{jain2018nanopore,
author={Jain, Miten
and Koren, Sergey
and Miga, Karen H.
and Quick, Josh
and Rand, Arthur C.
and Sasani, Thomas A.
and Tyson, John R.
and Beggs, Andrew D.
and Dilthey, Alexander T.
and Fiddes, Ian T.
and Malla, Sunir
and Marriott, Hannah
and Nieto, Tom
and O'Grady, Justin
and Olsen, Hugh E.
and Pedersen, Brent S.
and Rhie, Arang
and Richardson, Hollian
and Quinlan, Aaron R.
and Snutch, Terrance P.
and Tee, Louise
and Paten, Benedict
and Phillippy, Adam M.
and Simpson, Jared T.
and Loman, Nicholas J.
and Loose, Matthew},
title={{Nanopore Sequencing and Assembly of a Human Genome with Ultra-Long Reads}},
journal={Nature Biotechnology},
year={2018},
month={Apr},
day={01},
volume={36},
number={4},
pages={338-345},
abstract={A human genome is sequenced and assembled de novo using a pocket-sized nanopore device.},
issn={1546-1696},
doi={10.1038/nbt.4060},
url={https://doi.org/10.1038/nbt.4060}
}

@Article{gong2019ultralong,
author={Gong, Liang
and Wong, Chee-Hong
and Idol, Jennifer
and Ngan, Chew Yee
and Wei, Chia-Lin},
title={{Ultra-Long Read Sequencing for Whole Genomic DNA Analysis}},
journal={JoVE},
year={2019},
month={Mar},
day={15},
publisher={MyJoVE Corp},
number={145},
pages={e58954},
keywords={Bioengineering; Nanopore DNA sequencing; ultra-long reads; third-generation DNA sequencing; whole genome sequence analysis; high molecular weight DNA extraction; tagmentation based DNA library construction},
abstract={Third generation single-molecule DNA sequencing technologies offer significantly longer read length that can facilitate the assembly of complex genomes and analysis of complex structural variants. Nanopore platforms perform single-molecule sequencing by directly measuring the current changes mediated by DNA passage through the pores and can generate hundreds of kilobase (kb) reads with minimal capital cost. This platform has been adopted by many researchers for a variety of applications. Achieving longer sequencing read lengths is the most critical factor to leverage the value of nanopore sequencing platforms. To generate ultra-long reads, special consideration is required to avoid DNA breakages and gain efficiency to generate productive sequencing templates. Here, we provide the detailed protocol of ultra-long DNA sequencing including high molecular weight (HMW) DNA extraction from fresh or frozen cells, library construction by mechanical shearing or transposase fragmentation, and sequencing on a nanopore device. From 20-25 {\textmu}g of HMW DNA, the method can achieve N50 read length of 50-70 kb with mechanical shearing and N50 of 90-100 kb read length with transposase mediated fragmentation. The protocol can be applied to DNA extracted from mammalian cells to perform whole genome sequencing for the detection of structural variants and genome assembly. Additional improvements on the DNA extraction and enzymatic reactions will further increase the read length and expand its utility.},
issn={1940-087X},
doi={10.3791/58954},
url={https://www.jove.com/t/58954},
url={https://doi.org/10.3791/58954}
}






@Article{rang2018squiggle,
author={Rang, Franka J.
and Kloosterman, Wigard P.
and de Ridder, Jeroen},
title={{From Squiggle to Basepair: Computational Approaches for Improving Nanopore Sequencing Read Accuracy}},
journal={Genome Biology},
year={2018},
month={Jul},
day={13},
volume={19},
number={1},
pages={90},
abstract={Nanopore sequencing is a rapidly maturing technology delivering long reads in real time on a portable instrument at low cost. Not surprisingly, the community has rapidly taken up this new way of sequencing and has used it successfully for a variety of research applications. A major limitation of nanopore sequencing is its high error rate, which despite recent improvements to the nanopore chemistry and computational tools still ranges between 5{\%} and 15{\%}. Here, we review computational approaches determining the nanopore sequencing error rate. Furthermore, we outline strategies for translation of raw sequencing data into base calls for detection of base modifications and for obtaining consensus sequences.},
issn={1474-760X},
doi={10.1186/s13059-018-1462-9},
url={https://doi.org/10.1186/s13059-018-1462-9}
}


@article{luo2018neural,
  title={{Neural Architecture Optimization}},
  author={Luo, Renqian and Tian, Fei and Qin, Tao and Chen, Enhong and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{xie2021weight,
  title={{Weight-Sharing Neural Architecture Search: A Battle to Shrink the Optimization Gap}},
  author={Xie, Lingxi and Chen, Xin and Bi, Kaifeng and Wei, Longhui and Xu, Yuhui and Wang, Lanfei and Chen, Zhengsu and Xiao, An and Chang, Jianlong and Zhang, Xiaopeng and others},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={9},
  pages={1--37},
  year={2021},
  publisher={ACM New York, NY}
}
@article{zoph2016neural,
  title={{Neural Architecture Search with Reinforcement Learning}},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}

@inproceedings{real2017large,
  title={{Large-Scale Evolution of Image Classifiers}},
  author={Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V and Kurakin, Alexey},
  booktitle={International Conference on Machine Learning},
  pages={2902--2911},
  year={2017},
  organization={PMLR}
}

@inproceedings{xie2017genetic,
  title={{Genetic CNN}},
  author={Xie, Lingxi and Yuille, Alan},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1379--1388},
  year={2017}
}


@inproceedings{szegedy2017inception,
  title={{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
  booktitle={Thirty-first AAAI conference on artificial intelligence},
  year={2017}
}

@article{hochreiter1998vanishing,
  title={{The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions}},
  author={Hochreiter, Sepp},
  journal={International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  volume={6},
  number={02},
  pages={107--116},
  year={1998},
  publisher={World Scientific}
}


@article{werbos1990backpropagation,
  title={{Backpropagation Through Time: What It Does and How To Do It}},
  author={Werbos, Paul J},
  journal={Proceedings of the IEEE},
  volume={78},
  number={10},
  pages={1550--1560},
  year={1990},
  publisher={IEEE}
}

@article{agarap2018deep,
  title={{Deep Learning Using Rectified Linear Units (ReLU)}},
  author={Agarap, Abien Fred},
  journal={arXiv},
  year={2018}
}

@misc{brevitas,
  author       = {Alessandro Pappalardo},
  title        = {Xilinx/brevitas},
  year         = {2021},
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.3333552},
  url          = {https://doi.org/10.5281/zenodo.3333552}
}

@inproceedings{bucilua2006model,
  title={{Model Compression}},
  author={Buciluǎ, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={535--541},
  year={2006}
}


@article{hinton2015distilling,
  title={{Distilling the Knowledge in a Neural Network}},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}

@inproceedings{ioffe2015batch,
  title={{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@inproceedings{kriman2020quartznet,
  title={{QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions}},
  author={Kriman, Samuel and Beliaev, Stanislav and Ginsburg, Boris and Huang, Jocelyn and Kuchaiev, Oleksii and Lavrukhin, Vitaly and Leary, Ryan and Li, Jason and Zhang, Yang},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6124--6128},
  year={2020},
  organization={IEEE}
}

@article{majumdar2021citrinet,
  title={{Citrinet: Closing the Gap Between Non-Autoregressive and Autoregressive End-to-End Models for Automatic Speech Recognition}},
  author={Majumdar, Somshubra and Balam, Jagadeesh and Hrinchuk, Oleksii and Lavrukhin, Vitaly and Noroozi, Vahid and Ginsburg, Boris},
  journal={arXiv preprint arXiv:2104.01721},
  year={2021}
}

@article{gulati2020conformer,
  title={{Conformer: Convolution-Augmented Transformer for Speech Recognition}},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  journal={arXiv preprint arXiv:2005.08100},
  year={2020}
}

@article{van2018third,
  title={{The Third Revolution in Sequencing Technology}},
  author={Van Dijk, Erwin L and Jaszczyszyn, Yan and Naquin, Delphine and Thermes, Claude},
  journal={Trends in Genetics},
  volume={34},
  number={9},
  pages={666--681},
  year={2018},
  publisher={Elsevier}
}
@article{ardui2018single,
  title={{Single Molecule Real-Time (SMRT) Sequencing Comes of Age: Applications and Utilities for Medical Diagnostics}},
  author={Ardui, Simon and Ameur, Adam and Vermeesch, Joris R and Hestand, Matthew S},
  journal={Nucleic acids research},
  volume={46},
  number={5},
  pages={2159--2168},
  year={2018},
  publisher={Oxford University Press}
}

@article{kchouk2017generations,
  title={{Generations of Sequencing Technologies: From First to Next Generation}},
  author={Kchouk, Mehdi and Gibrat, Jean-Francois and Elloumi, Mourad},
  journal={Biology and Medicine},
  volume={9},
  number={3},
  year={2017},
  publisher={HATASO Enterprises LLC}
}

@article{weirather2017comprehensive,
  title={{Comprehensive Comparison of Pacific Biosciences and Oxford Nanopore Technologies and Their Applications to Transcriptome Analysis}},
  author={Weirather, Jason L and de Cesare, Mariateresa and Wang, Yunhao and Piazza, Paolo and Sebastiano, Vittorio and Wang, Xiu-Jie and Buck, David and Au, Kin Fai},
  journal={F1000Research},
  volume={6},
  year={2017},
  publisher={Faculty of 1000 Ltd}
}

@article{jain2016oxford,
  title={{The Oxford Nanopore MinION: Delivery of Nanopore Sequencing to the Genomics Community}},
  author={Jain, Miten and Olsen, Hugh E and Paten, Benedict and Akeson, Mark},
  journal={Genome biology},
  volume={17},
  number={1},
  pages={1--11},
  year={2016},
  publisher={Springer}
}

@inproceedings{graves2006connectionist,
  title={{Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks}},
  author={Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={369--376},
  year={2006}
}

@misc{kldivloss,
  title = {{KLDivLoss}, \url{https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html}
}}

@misc{nni,
  title = {{NNI}, \url{https://github.com/microsoft/nni}
}}

@misc{nnmetercode,
    author = {Microsoft Research nn-Meter Team},
    title = {{nn-Meter: Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices}},
    year = {2021},
    url = {https://github.com/microsoft/nn-Meter},
}

@article{baskin2021uniq,
  title={Uniq: Uniform noise injection for non-uniform quantization of neural networks},
  author={Baskin, Chaim and Liss, Natan and Schwartz, Eli and Zheltonozhskii, Evgenii and Giryes, Raja and Bronstein, Alex M and Mendelson, Avi},
  journal={ACM Transactions on Computer Systems (TOCS)},
  volume={37},
  number={1-4},
  pages={1--15},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{branton2008potential,
  title={The potential and challenges of nanopore sequencing},
  author={Branton, Daniel and Deamer, David W and Marziali, Andre and Bayley, Hagan and Benner, Steven A and Butler, Thomas and Di Ventra, Massimiliano and Garaj, Slaven and Hibbs, Andrew and Huang, Xiaohua and others},
  journal={Nature biotechnology},
  volume={26},
  number={10},
  pages={1146--1153},
  year={2008},
  publisher={Nature Publishing Group}
}


@misc{torch_modules,
  title = {{TORCH.NN}, \url{https://pytorch.org/docs/stable/nn.html}
}}

@misc{pytorch,
  title = {{PyTorch}, \url{https://pytorch.org/}
}}

@inproceedings{singh2019napel,
  title={Napel: Near-memory computing application performance prediction via ensemble learning},
  author={Singh, Gagandeep and G{\'o}mez-Luna, Juan and Mariani, Giovanni and Oliveira, Geraldo F and Corda, Stefano and Stuijk, Sander and Mutlu, Onur and Corporaal, Henk},
  booktitle={2019 56th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2019},
  organization={IEEE}
}

@inproceedings{10.1145/3470496.3527442,
author = {Singh, Gagandeep and Nadig, Rakesh and Park, Jisung and Bera, Rahul and Hajinazar, Nastaran and Novo, David and G\'{o}mez-Luna, Juan and Stuijk, Sander and Corporaal, Henk and Mutlu, Onur},
title = {Sibyl: Adaptive and Extensible Data Placement in Hybrid Storage Systems Using Online Reinforcement Learning},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527442},
doi = {10.1145/3470496.3527442},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {320–336},
numpages = {17},
keywords = {machine learning, hybrid systems, reinforcement learning, hybrid storage systems, data placement, solid-state drives (SSDs)},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{singh2019low,
  title={Low precision processing for high order stencil computations},
  author={Singh, Gagandeep and Diamantopoulos, Dionysios and Stuijk, Sander and Hagleitner, Christoph and Corporaal, Henk},
  booktitle={International Conference on Embedded Computer Systems},
  pages={403--415},
  year={2019},
  organization={Springer}
}

@inproceedings{singh2020nero,
  title={NERO: A near high-bandwidth memory stencil accelerator for weather prediction modeling},
  author={Singh, Gagandeep and Diamantopoulos, Dionysios and Hagleitner, Christoph and G{\'o}mez-Luna, Juan and Stuijk, Sander and Mutlu, Onur and Corporaal, Henk},
  booktitle={2020 30th International Conference on Field-Programmable Logic and Applications (FPL)},
  pages={9--17},
  year={2020},
  organization={IEEE}
}