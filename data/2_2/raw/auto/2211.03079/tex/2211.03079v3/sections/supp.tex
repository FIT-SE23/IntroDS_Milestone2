\onecolumn
\setcounter{secnumdepth}{3}
% \def\@cite#1{({#1})} %square to rounded - PS
\clearpage
\begin{center}
\textbf{\LARGE Supplementary Material for\\ \ltitle}
\end{center}
%%%%%%%%%% Merge with supplemental materials %%%%%%%%%%
%%%%%%%%%% Prefix a "S" to all equations, figures, tables and reset the counter %%%%%%%%%%
\setcounter{section}{0}
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}
\makeatletter
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thesection}{S\arabic{section}}
% \renewcommand{\thetheorem}{S\arabic{theorem}}
% \renewcommand{\citenumfont}[1]{S#1}

\newcommand{\TextUnderscore}{\rule{.4em}{.4pt}}



\section{Sensitivity to Skip Connection} 
\label{suppsec:skipconnection}



Many state-of-the-art deep learning-based basecallers~\cite{wick2019performance,neumann2022rodan,konishi2021halcyon,xu2021fast,lou2020helix,perevsini2021nanopore,pages2022comprehensive,ambernas_zhang2021automated} incorporate skip connections to improve their basecalling accuracy. Figure~\ref{supfig:skip_sensitivity} shows the accuracy of \bon using two different configurations of skip connections (\texttt{s1} and \texttt{s2}) and one configuration without any skip connections (\texttt{s3}) and compares it to the baseline \bon architecture. In \texttt{s1} configuration, we reduce the number of repeats in each block to one, while in \texttt{s2} configuration, we use only one block with maximum channel size, maximum kernel size, and maximum number of repeats.%, and \texttt{s3}: sub-divide blocks with each block consisting of two repeat. 
  \begin{figure}[h]
  \centering
  \includegraphics[width=0.65\linewidth,trim={0.2cm 0.25cm 0.2cm 0cm},clip]{images/skip_connection_sensitivity_3.pdf}
%   \vspace{-15pt}
  \caption{Basecaller sensitivity to skip connections.}
  \label{supfig:skip_sensitivity}
%   \vspace{-12pt}
\end{figure}

For \texttt{s3} configuration, we manually remove all the skip connections from each block in \bon. We also annotate the change in model parameters compared to the baseline model. \bon architecture comprises several blocks, each consisting of a time channel separable convolution sub-block (referred to as repeat). We make two major observations. First, the number sub-block we provide skip connection plays an important role. In \texttt{s1} configuration, we observe that by using only one repeat, we reduce the accuracy by 2.84\% with 66.7\% lower model parameters, while by merging all the blocks into one big block in \texttt{s2} configuration, we observe 8.75\% lower accuracy with $96.2\%$ higher model parameters. Second, manually removing all the skip connections in \texttt{s3} configuration leads to $40.7\%$ lower model parameters at the expense of a $3.88\%$ loss in accuracy. This performance degradation is because, during neural network training, these connections provide a direct path for propagating the error through the layers and dealing with the vanishing gradient problem, allowing deep networks to learn properly and converge during training. Therefore, manual removal of skip connections can lead to lower basecalling performance. We conclude that skip connections are critical for basecalling accuracy.
% \newpage
\section{Hyper-parameter Tuning for \strim}
\label{suppsec:Hyper_skipclip}
In Figure~\ref{supfig:kd_hyper}, we show the effect of two critical hyper-parameters (alpha ($\alpha$) and temperature ($\tau$)) on \strim.  We observe that as we raise $\alpha$ while keeping $\tau$ constant, the basecaller accuracy increases. At higher $\alpha$, \strim gives more importance to the student loss than the distillation loss during the backward pass. We use $\alpha$ =0.9 throughout our experiments.

\begin{figure}[h]
  \centering
\includegraphics[width=0.6\linewidth,trim={0.2cm 0.25cm 0.4cm 0cm},clip]{images/kd_hyper3.pdf}
%   \vspace{-15pt}
  \caption{Sensitivity of \strim to hyper-parameters alpha ($\alpha$) and temperature ($\tau$).}
  \label{supfig:kd_hyper}
  \vspace{12pt}
\end{figure}

For $\tau$, we experiment with values ranging from 0.5 to 5.0. Increasing $\tau$ provides more knowledge from the teacher network for a student network to absorb. We observe at $\tau$=2, \strim provides the highest accuracy. Further increasing $\tau$ does not provide benefits because the student network cannot absorb knowledge provided by the teacher network.

\section{Comparison to More Accurate Basecallers}
\label{suppsec:guppy_compare}
Our goal is to make basecalling highly efficient and fast by building the first framework for specializing and optimizing machine learning-based basecaller. Currently, we focus on CNN-based basecallers because: (1) they are the most widely used basecallers, and  (2)  the fundamental multiply-accumulate (MAC) operation in a CNN model is amenable to hardware acceleration, unlike the operations in RNN-based basecallers. \go{\texttt{Bonito\_CRF}'s super high accuracy} (\gp) model is an RNN-based basecaller that provides more accuracy than \gpf at the expense of a \go{much larger} model. \sr{We compare} the \sr{overall} basecalling \sr{accuracy} \sr{of} \mech with \sr{that of the} baseline basecaller\sr{s} in terms of   model parameters, model size, and basecalling throughput in Figure~\ref{supfig:model_compare_all}(a), \ref{supfig:model_compare_all}(b), and \ref{supfig:model_compare_all}(c), respectively.


 \begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{images/model_compare_AMD_CRF_SUP_1.pdf}
  \vspace{-15pt}
 \caption{Comparison of \hc{average} basecalling  accuracy for \mechmp with baseline basecaller in terms of:  (a) model parameters, (b) model size, and (c) \hc{average}  basecalling throughput.}
  \label{supfig:model_compare_all}
%   \vspace{-12pt}
\end{figure}
 \input{tables/model_compare.tex}
In addition to our previous observations from Figure~\ref{fig:model_compare}, we make three new observations from Figure~\ref{supfig:model_compare_all} and Table~\ref{suptab:compare}. \go{First, \mechmp has 35.42$\times$ the performance of the highly-accurate  \gp.}
 %First, compared to \mechmp,  \gp provides 2.96\% more accuracy at the cost of 9.27$\times$ lower basecalling throughput.  
 Second, \gp uses 7.52$\times$,	36.96$\times$,	2.77$\times$, and	8.14$\times$ model parameters leading to a model size of 7.53$\times$,	36.93$\times$,	2.77$\times$, and	19.22$\times$ compared to \cc, \gpf, \bon, and \mechmp, respectively. Third, \gp is 5.37\% more accurate than its throughout-optimized version, \gpf, which provides 22.71$\times$ higher basecalling performance. We conclude that the high accuracy of a basecaller comes at a substantial cost in terms of lower throughput due to the higher number of model parameters and model size.


% \clearpage

% \end{document}

\let\noopsort\undefined
\let\printfirst\undefined
\let\singleletter\undefined
\let\switchargs\undefined
% \let\citex\undefined

% \bibliographystylesupp{IEEEtran}
% \bibliographysupp{ref}