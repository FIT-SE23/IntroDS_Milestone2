% \vspace{-17pt}
\section{Discussion} \label{sec:discussion}
% \srr{This normally includes the discussion and conclusion. We don't have them separated. We summarize what we did without copy/paste from other places and then discuss other potential applications and benefits that are not discussed in the paper. We discuss future work and such as well..}
We are witnessing a tremendous transformation in %genomics and 
high-throughput sequencing to significantly advance omics and  other life sciences. %and \sr{the} exponential accumulation of high-throughput sequencing data. 
The bioinformatics community has developed a multitude of software tools to leverage increasingly large and complex sequencing datasets. Deep learning models have been especially powerful in modeling basecalling. Basecalling is the most fundamental computational step in the high-throughput sequencing pipeline. Modern basecallers generally employ convolution neural networks to extract features from raw genomic sequences. However, designing a basecaller comes with a cost that a neural network model can have many different computational elements making the neural network tuning a major problem. 
At present, the vast majority of deep learning-based basecallers are manually tuned by computational biologists through manual trial and error, which is time-consuming. To a large extent, basecallers are being designed to provide higher accuracy without considering the compute demands of such networks. Such an approach leads to computationally complex basecallers that impose a substantial barrier to performing end-to-end time-sensitive genomic analyses.
This vast dependence of computational biologists \sr{and biomedical researchers} on these deep learning-based models creates a critical need to find efficient basecalling architectures optimized for performance. %speeding it up and making it much more efficient is fundamentally critical to genomics and general omics research and development.  % The increasing dependence of biomedical scientists on these powerful tools creates a critical need for faster and more efficient computational tools. %{m2v}. %These data enable the study of diverse molecular mechanisms and biological systems through a quantitative lens.
%, and we believe \mech provides a breakthrough advance in this direction

 To address this challenge, we developed: (a) \nas: an automatic architecture search framework that jointly searches for computation blocks in basecaller and best bit-with precision for each neural network layer, and (b) \strim: a dynamic skip removal module to remove hardware-unfriendly skip connections to greatly reduce resource and storage requirements without any loss in basecalling accuracy. We demonstrate the capabilities of \nas and \strim by designing \mech, the first hardware-optimized basecaller that provides both accuracy and inference efficiency. %\nas for efficiently designing optimal deep learning-based genomic basecallers. 
%  Our \nas framework automatically evaluates millions of neural network options while simultaneously searching for the best precision for weights and activations. While  \strim removes the skip connections present in modern basecallers to greatly reduce resource and storage requirements without any loss in basecalling accuracy. 

During our evaluation, we ran \nas for 96 GPU hours to sample architectures from our search space. Using complete sampling to evaluate all the 1.8$\times$10$^{32}$ viable options would take at least $\sim$4.3$\times$10$^{33}$ GPU hours. Thus, \nas accelerates the basecaller architecture search to develop high-performance basecalling architectures.   The final model architecture  can be further fine-tuned for other hyperparameters~\cite{singh2019napel,10.1145/3470496.3527442}, such as learning rate and batch size (for example, with grid search or neural architecture search). %in this study, we did not fine-tune these, to ensure fair comparisons between architectures. 
Throughout our experiments, we build general-purpose basecalling models by training and testing the model using an official, open-source ONT dataset that consists of a mix of different species. We did not specialize basecalling models for a specific specie. Past works, such as~\cite{wick2019performance}, show that higher basecalling accuracy can be achieved by building species-specific models.

As future work,    \nas can be extended in two ways: (1) evaluate advance model architectures (such as RNN, transformer, etc.), and (2) perform more fine-grain quantization. First, extending \nas to other model architectures is important for researchers to quickly evaluate different computational elements. We focus on convolution-based networks because matrix multiplication is the fundamental operation in such networks that is easily amenable to hardware acceleration.  As the field of machine learning is rapidly evolving, it is non-trivial for researchers to adapt their models with the latest deep learning techniques. Second, currently, we perform mixed precision quantization, where every layer is quantized to a different domain. In the future, we can quantize every dimension of the weights to different precision. Such an approach would increase the design space of neural network architectural options to many folds.  \nas enables easy integration to explore such options automatically. Thus, \nas is easily extensible and alleviates the designerâ€™s burden in exploring and finding sophisticated basecallers for different hardware configurations.

For \strim, we demonstrate its   applicability on  basecalling only, while there are other genome sequencing tasks where deep learning models with skip connections are actively being developed, such as predicting the effect of genetic variations~\cite{alipanahi2015predicting,ambernas_zhang2021automated}, detecting replication dynamics~\cite{boemo2021dnascent}, and predicting super-enhancers~\cite{sabba2021residual}. In Supplementary ~\ref{suppsec:skipconnection}, we show the effect of manual skip removal, where we manually remove all the skip connections at once. We observe that the basecaller achieves 90.55\% accuracy (4.08\% lower than the baseline model with skip connections). By manual skip removal, the basecaller is unable to recover the loss in accuracy because CNN-based basecallers are sensitive  to skip connections. Therefore, \strim provides a mechanism to develop hardware-friendly deep learning models for other genomic tasks.

We would explore two future directions for pruning a basecaller. First,  currently, we perform one-shot pruning, whereby we prune the model once and then fine-tune the model until convergence. Another approach could be to perform iterative pruning, where after every training epoch, we can re-prune the model using certain pruning criteria. Such an approach would further evaluate the fine-grained pruning limit of a basecaller. Second, an interesting future direction would be to combine multiple pruning techniques, e.g., structured channel pruning with structured group pruning (where we maintain the structure of the tensors without causing sparsity). Such an approach could lead to higher pruning ratios without substantial accuracy loss. 
  
 Our extensive evaluation of real genomic organisms shows that \mech provides the ability to basecall quickly, accurately, and efficiently enough to scale the analysis, leading to  $\sim$6.88$\times$ reductions in model size with 2.94$\times$ fewer neural network model parameters.  We observe researchers building larger and larger  basecallers in an attempt to gain more accuracy without heeding to the disproportionately higher amount of power  these basecallers are consuming. Moreover,  none of the previous basecallers~\cite{wick2019performance,neumann2022rodan,konishi2021halcyon,xu2021fast,lou2020helix,perevsini2021nanopore,pages2022comprehensive,ambernas_zhang2021automated} have been optimized for mixed-precision execution to reduce energy consumption. As energy usage is proportional to the size of the network, energy-efficient basecalling is essential to enable the adoption of more and more sophisticated basecallers.  We hope that our open-source implementations of \nas and \strim inspire future work and ideas in genomics and general omics research and development.
  
%   that can help researchers develop optimized basecallers without requiring machine learning expertise . 






