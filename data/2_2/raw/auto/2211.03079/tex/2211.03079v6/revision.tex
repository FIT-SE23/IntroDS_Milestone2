\onecolumn
\nobalance 
%\section*{{\Large AUTHORS' RESPONSE and SUMMARY OF CHANGES}}

\renewcommand{\thetable}{\arabic{table}}
\renewcommand{\thefigure}{\arabic{figure}}
\renewcommand\rev[1]{{\color{blue}{#1}}}

\setcounter{section}{0}
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}
\makeatletter
\renewcommand{\theequation}{\arabic{equation}}
\renewcommand{\thetable}{\arabic{table}}
\renewcommand{\thefigure}{\arabic{figure}}
\renewcommand{\thesection}{\arabic{section}}
% We thank the reviewers for their valuable feedback. We have addressed all comments. We first list the major changes we made to the paper in this revision. Then, we respond to each reviewer's comments and discuss how we have addressed them in the paper.  \\

\begin{center}
    \LARGE{\textbf{Summary of Changes and Response to Reviewers for:}}\\
    \LARGE{\ltitle}
\end{center}

\vspace{10pt}
We thank the reviewers very much for their thoughtful reviews of our manuscript. We also thank the editor-in-chief, Kevin Pang, for the time and effort. 

This document summarizes the improvements that we have made to our paper. The revision is centered around the reviewer's comments. We have addressed all comments as thoroughly as possible. We will further refine the text and improve the clarity of the paper. \hgb{Our modifications are highlighted in blue font in the revised manuscript}.

We first list the major changes we made to the paper in this revision. We then discuss common reviewer comments, followed by individual reviewer's comments, and provide a detailed explanation of the changes we have made. Within each section, each text box addresses one or a set of related comments. The silver boxes list pointers to reviewer comments and response(s) that the text box addresses in an effort to keep this manuscript self-contained and easy to follow.  

\section*{{MAJOR CHANGES TO THE PAPER}}

Based on the reviewersâ€™ feedback, the following is a list of the significant changes we have made to the paper:
\begin{enumerate}

\item \textbf{[Evaluation Using the Human Genome] } To underscore the applicability of our approach beyond bacterial species, we have incorporated an evaluation using the human genome HG002 {in Section~\sref{subsubsection:down_read_mapping}}. This comprehensive assessment with a complex and highly relevant dataset highlights the practical utility of the \framework framework on a critical genomics workload.

\item \textbf{[Experiments on Other Platforms] } We expanded our evaluation by conducting experiments on the NVIDIA A40 GPU-based system {in Supplementary Section 
\textbf{S5 Evaluation on Other Hardware
Platforms}%\sref{suppsec:other_platforms}
}, ensuring the adaptability and robustness of \framework across various hardware environments. This broader platform assessment enhances the applicability and versatility of our approach.


\item \textbf{[Code Open-Sourcing] } We have fully open-sourced the code for \framework at: \url{https://github.com/Xilinx/neuralArchitectureReshaping}. This initiative promotes transparency, encourages community engagement, and allows researchers to {relatively} quickly implement and build upon our methods, fostering the field's growth.



\item \textbf{[Streamlining the Entire Text] } We have condensed and streamlined the entire manuscript to provide a more concise and reader-friendly presentation. Our revised manuscript conforms to the style for Genome Biology Methods articles.  By removing unnecessary redundancy and focusing on the most critical information, we ensure that the core concepts and findings are easily accessible and comprehensible, enhancing the clarity and impact of our research.

\end{enumerate}


\section{Common Comments}
\subsection{Evaluation using human genome}
\label{subsection:cc1}
\boxbegin 
\noindent\emph{\textbf{[Rev. 2]}  The study mainly evaluated nanopore R9.4 reads from nine prokaryotic species. A more comprehensive result might be achieved by using other benchmark datasets (e.g., HG002)}

\noindent\emph{\textbf{[Rev. 3]}  It would be useful to apply and measure the performance and quality for human genome (or a chromosome perhaps?)
}
\boxend
Thank you for your valuable feedback regarding evaluating \mech on human genome data, particularly using the human HG002 dataset. We appreciate your suggestion and recognize the importance of expanding the scope of our evaluation.

In our revised version, we have added new experiments using HG002 for all the evaluated basecallers. This evaluation allows us to assess the performance and quality of our framework in a context that is of significant interest to the broader scientific community. 

We updated Section \sref{sec:data_avail}, with details on reads and reference for the human dataset:

\yboxbegin
    \Paste{CC1/1}
\yboxend

We also updated Section~\sref{sec:evaluation} Table 3 with details on the human dataset:
% \yboxbegin
\setcounter{table}{2}
\input{rev_tables/read_dataset}
% \yboxend


We updated all our results to include the human genome. We make the following four changes in Section~\sref{sec:results}.
First, Section~\sref{subsection:results_perf_compare} includes performance results on all the evaluated basecallers with the Human HG002 dataset. We observe \mechmp is the highest performing basecaller and improves average performance by 364.89$\times$, 14.25$\times$, 128.13$\times$, 81.58$\times$, and 3.77$\times$ over \cc, \gpf, \bon, \sac, and \dor, respectively. 

 \yboxbegin
 \renewcommand{\thefigure}{\arabic{figure}}
\setcounter{figure}{5}
\begin{figure}[H]
  \centering
\includegraphics[width=0.8\linewidth]{images/2023.11.01_basecalling_speed_amd_1.pdf}
  % \vspace{-13pt}
  \captionof{figure}{Performance comparison of \mech (using floating-point precision (\mechfp) and mixed-precision (\mechmp))~\sr{and five state-of-the-art basecallers on AMD MI210. The y-axis is on a logarithmic scale.}}
  % \label{fig:perf}
\end{figure}
 \yboxend
 
Second, we updated Section \sref{subsection:results_base_acc} to demonstrate the basecalling accuracy of all the evaluated basecallers on Human HG002.

 \yboxbegin
  \renewcommand{\thefigure}{\arabic{figure}}
\setcounter{figure}{6}
\begin{figure}[H]
  \centering
\includegraphics[width=0.8\linewidth]{images/2023.11.01_basecalling_accuracy_2.pdf} %{images/CRF/read_identity_aie_CRF_2.pdf}
  \vspace{-15pt}
  \captionof{figure}{\gs{Basecalling accuracy} comparison of \mech (using floating-point precision (\mechfp) and mixed-precision (\mechmp)).}
  % \label{fig:identity}
%   \vspace{-12pt}
\end{figure}
 \yboxend

Third, we updated Section \sref{subsubsection:down_denovo} to show the accuracy, completeness, and contiguity of assemblies we generate using our evaluated basecaller in terms of Genome Fraction (\%), Average Identity (\%), Assembly Length, Average GC (\%), NG50, Total Indels, Indel Ratio (\%), and Quality Value (QV) in Table 1.

\input{rev_tables/downstream_human}

Fourth, we updated Section \sref{subsubsection:down_read_mapping},
 where we compare the total number of base mismatches, the total number of mapped bases, the total number of mapped reads, and the total number of unmapped reads.

\yboxbegin
 \renewcommand{\thefigure}{\arabic{figure}}
\setcounter{figure}{6}
\begin{figure}[H]
% \setcounter{figure}{6}
\centering
\begin{subfigure}[h]{0.5\textwidth}
  \centering
  \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0cm 0.2cm},clip]{images/2023.11.01.samtools_mismatch_hg002_1.pdf}
  %{images/CRF/samtools_mismatch_CRF_1.pdf}
%   \caption{High-end and middle-end (\textsf{H\&M}).
  \vspace{-0.6cm}
% \caption{\label{fig:sam_mis}}
\end{subfigure}%
\begin{subfigure}[h]{0.5\textwidth}
  \centering
  \vspace{-0.04cm}
  \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0.2cm 0.2cm}, clip]{images/2023.11.01.samtools_mapped_bases_hg002_1.pdf}%{images/CRF/samtools_mapped_bases_CRF_1.pdf}
%   \caption{High-end and low-end (\textsf{H\&L}).
  \vspace{-0.6cm}
\end{subfigure}

\begin{subfigure}[h]{0.49\textwidth}
  \centering
  \vspace{0.2cm}
  \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0.2cm 0.2cm}, clip]{images/2023.11.01.samtools_read_map_hg002_1.pdf} %{images/CRF/samtools_read_map_CRF_1.pdf}
%   \caption{High-end and low-end (\textsf{H\&L}).
  \vspace{-0.6cm}
\end{subfigure}
\begin{subfigure}[h]{0.5\textwidth}
  \centering
  \vspace{-0.04cm}
  \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0.2cm 0.2cm}, clip]{images/2023.11.01.samtools_read_unmap_hg002_1.pdf}%{images/CRF/samtools_read_unmap_CRF_1.pdf}
%   \caption{High-end and low-end (\textsf{H\&L}).
  \vspace{-0.6cm}
\end{subfigure}
\captionof{figure}{Comparison of \mech (using floating-point precision (\mechfp) and mixed-precision (\mechmp)) for normalized (a) mismatches, (b) bases mapped, (c) reads mapped, and (d) reads unmapped. 
% \label{fig:samtool}
}
% \vspace{-0.5cm}
\end{figure}
\yboxend

 In our revised manuscript, we use normalized values instead of raw values for read mapping analysis to avoid the average values (\texttt{AVG} in Figure \ref{fig:samtool}(a), \ref{fig:samtool}(b), \ref{fig:samtool}(c), and \ref{fig:samtool}(d)) being biased toward the large human genome (i.e., HG002 has upto 67$\times$ more reads than our previously evaluated bacterial species).   We normalize the total number of base mismatches and the total number of mapped bases using the total number of bases in the reads, while for the total number of mapped reads and the total number of unmapped reads, we normalize using the total number of reads. We updated Section \sref{sec:evaluation} to include the above information:

\yboxbegin
\Paste{CC1/2}
\yboxend

Based on our experiments on HG002, we add the following new observation in Section~\sref{subsubsection:down_read_mapping}:
\yboxbegin
\Paste{CC1/3}
\yboxend

 \subsection{Using NVIDIA GPUs and extending to other platforms}
\label{subsection:cc2}
\boxbegin 
\noindent\emph{\textbf{[Rev. 2]}  The work was mainly conducted on AMD GPU and XILINX accelerator platforms, which are not commonly used in most labs, where X86+Nvidia GPUs are more commonly used at the current time point. Although the proposed method is not platform-specific, authors are expected to demonstrate the performance of their framework on other different platforms.}

\noindent\emph{\textbf{[Rev. 3]}  It might also be good to define performance on other GPUs which, I think, would be relevant for users. Was the performance measured on NVIDIA/ARM?
}
\boxend
\renewcommand{\thesection}{S\arabic{section}}
\setcounter{section}{4}
Thank you for highlighting the importance of demonstrating the performance of our framework on a broader range of platforms, especially the commonly used NVIDIA GPUs.  To address the generalization comment, we make two significant changes. First, we have added new experiments in Supplementary Section \textbf{S5 Evaluation on Other Hardware
Platforms}%\sref{suppsec:other_platforms}
, where we measure results on NVIDIA A40 GPU. We have also updated our hardware setup to use the newer AMD generation of AMD GPU, MI210, to have a fair comparison between the two evaluated GPUs.

\yboxbegin
\renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{figure}{6}
\Paste{CC2/1}
\begin{figure}[H]
% \yboxbegin
  \centering
  \includegraphics[width=0.8\linewidth]{images/2023.11.01_basecalling_speed_nvidia_1.pdf}%{images/model_compare_AMD_CRF_1.pdf}
  % \vspace{-15pt}
    \captionof{figure}{Performance comparison of \mech (using floating-point precision (\mechfp) and mixed-precision (\mechmp))~\sr{and five state-of-the-art basecallers on NVIDIA A40~\cite{a40}. The y-axis is on a logarithmic scale.}}
  % \label{fig:model_compare_nvidia}
%   \vspace{-12pt}
% \yboxend
\end{figure}
\yboxend

We refer to our new results in Section~\sref{subsection:results_perf_compare} as follows:

\yboxbegin
\Paste{CC1.2/2}
\yboxend

 Second, in our recently open-sourced \framework code~\cite{rubiconCode}, we provide installation scripts for both AMD [R1] and NVIDIA GPUs [R2]. \\

\noindent[R1] \url{https://github.com/Xilinx/neuralArchitectureReshaping/blob/main/requirements_rocm.txt}

\noindent[R2] \url{https://github.com/Xilinx/neuralArchitectureReshaping/blob/main/requirements_cuda.txt}

We believe that our updated evaluation on a different platform provides a robust assessment of \framework and ensures our framework's efficacy and efficiency across different popular hardware configurations.

\renewcommand{\thesection}{\arabic{section}}
\setcounter{section}{1}
\setcounter{subsection}{2}
 \subsection{Counter-intuitive accuracy axis of Figure 8c and Figure S3c}
\label{subsection:cc3}
\boxbegin 
\noindent\emph{\textbf{[Rev. 1]}   Figure S3c, seems a bit counter-intuitive to have basecalling accuracy axis flipped.}

\noindent\emph{\textbf{[Rev. 3]}   Figure 8c. It is odd to see a decrease in accuracy on the axis (which makes it hard to follow).
}
\boxend
The basecalling accuracy axis in Figure 8c and Figure S3c has been adjusted, which are Figure 5(a) and Figure S6 in the revised manuscript, respectively.

 \yboxbegin
 \renewcommand{\thefigure}{\arabic{figure}}
\setcounter{figure}{4}
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/2023.11.01.model_compare_pareto_1.pdf}%{images/model_compare_AMD_CRF_1.pdf}
%   \vspace{-15pt}
  \captionof{figure}{Comparison of \hc{average} basecalling  throughput for \mechmp with state-of-the-art basecallers in terms of:  (a) average  basecalling accuracy, (b) model parameters, and (c) model size. \mechmp provides higher compute performance with lower model size when compared to \mechfp because of the mixed-precision computation.}
  % \label{fig:model_compare}
%   \vspace{-12pt}
\end{figure}
 \yboxend


\yboxbegin
\renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{figure}{5}
\begin{figure}[H]
 % \yboxbegin
  \centering
  \includegraphics[width=1\linewidth]{images/2023.11.01.model_compare_pareto_all_1.pdf}%{images/model_compare_AMD_CRF_SUP_1.pdf}
  \vspace{-5pt}
 \captionof{figure}{Comparison of \hc{average} basecalling  throughput for \mechmp with baseline basecaller in terms of:  (a) average  basecalling accuracy, (b) model parameters, and (c)  model size.}
  % \label{supfig:model_compare_all}
%   \vspace{-12pt}
% \yboxend
\end{figure}
\yboxend

\subsection{Motivational data}
\label{subsection:cc4}
\boxbegin 
\noindent\emph{\textbf{[Rev. 1]}  The authors state: For example, Bonito\_CRF-fast, a throughput-optimized recurrent neural network-based version of Bonito [45] basecaller from ONT, takes $\sim$6 hours to basecall a 3 Gbps (Giga base pairs) human genome on a powerful server-grade GPU. I am a bit surprised by how slow that is, since in this published benchmark by ONT (\url{https://aws.amazon.com/blogs/hpc/benchmarking-the-oxford-nanopore-technologies-basecallers-on-aws/}), they show that it can take at the slowest 62 hours to basecall 90 Gbps of data, which would translate that you would need at max 2 hours (probably less since the benchmark uses the high accuracy model, not the fast one) to basecall 3 Gbps. This is with a reasonable hardware setup: 1GPU with 16Gb VRAM, 16Gb RAM and 4 CPUs. I would encourage the authors to revise why there are such discrepancies between their results and the ones from ONT.}

\noindent\emph{\textbf{[Rev. 3]}  It might be good to explain the specifics of data being discussed in the introduction. For example, I was not sure if the authors meant basecalling 3 Gb of sequence (1x human genome) takes 6 hours using Bonito. }
\boxend

Earlier motivational numbers relied on an older version of {the} bonito basecaller. The benchmark results in the shared link (\url{https://aws.amazon.com/blogs/hpc/benchmarking-the-oxford-nanopore-technologies-basecallers-on-aws/})  are based on different basecallers from Oxford Nanopore Technologies (ONT)  (closed-source Guppy and their newer throughput-optimized basecaller, Dorado). We have reviewed the benchmarking data provided by Oxford ONT and updated our motivation to better explain the specifics of the data in Section \sref{sec:introduction} accordingly.

\yboxbegin
\Paste{IR1.2/1}
\yboxend

\hfill \break

 \section{Individual Comments}

\subsection*{Reviewer 1}
\boxbegin 
\noindent\emph{\textbf{[IR1.1]}  I would suggest the authors to further motivate in which cases having a faster or more efficient basecaller would make an impact and perhaps include a cost (saving) calculation. There is some motivation in the discussion, but I believe this motivation should also be made in the introduction.
}
% \hfill \break
%   \hfill \break
\boxend

We appreciate your insights regarding the motivation and potential impact of improving basecalling speed. We have carefully considered your suggestions and have made the following two revisions to address these concerns.

First, we have revised Section \sref{sec:introduction} to explicitly outline scenarios where a faster or more efficient basecaller would substantially impact. This includes emphasizing cases such as field sequencing and adaptive sampling, where the rapid basecalling of data is critical due to limited hardware availability or the need for real-time decision-making. We now highlight the potential benefits in various contexts, including the re-basecalling of existing datasets with higher accuracy models and the resulting implications for downstream analysis.

\yboxbegin
\Paste{IR1.1/1}
\yboxend

Second, we have included runtime estimates of basecalling in the complete genome sequencing pipeline in Section \sref{sec:introduction}:

\yboxbegin
\Paste{IR1.1/2}
\yboxend

By incorporating these changes, our revised manuscript effectively communicates the importance of faster and more efficient basecalling in specific practical scenarios, aligning with your valuable feedback.

\boxbegin 
\noindent\emph{\textbf{[IR1.2]}  The authors state: For example, Bonito\_CRF-fast, a throughput-optimized recurrent neural network-based version of Bonito [45] basecaller from ONT, takes $\sim$6 hours to basecall a 3 Gbps (Giga base pairs) human genome on a powerful server-grade GPU. I am a bit surprised by how slow that is, since in this published benchmark by ONT (\url{https://aws.amazon.com/blogs/hpc/benchmarking-the-oxford-nanopore-technologies-basecallers-on-aws/}), they show that it can take at the slowest 62 hours to basecall 90 Gbps of data, which would translate that you would need at max 2 hours (probably less since the benchmark uses the high accuracy model, not the fast one) to basecall 3 Gbps. This is with a reasonable hardware setup: 1GPU with 16Gb VRAM, 16Gb RAM and 4 CPUs. I would encourage the authors to revise why there are such discrepancies between their results and the ones from ONT.
}
\boxend
We have addressed this comment as a part of Common Comments~\ref{subsection:cc4}.
% Earlier motivational numbers relied on an older version of {the} bonito basecaller. The benchmark results in the shared link (\url{https://aws.amazon.com/blogs/hpc/benchmarking-the-oxford-nanopore-technologies-basecallers-on-aws/})  are based on different basecallers from Oxford Nanopore Technologies (ONT)  (closed-source Guppy and their newer throughput-optimized basecaller, Dorado). We have reviewed the benchmarking data provided by Oxford ONT and updated our motivation in Section \sref{sec:introduction} accordingly.

% \yboxbegin
% \Paste{IR1.2/1}
% \yboxend

\boxbegin 
\noindent\emph{\textbf{[IR1.3]}  There is a lack of information on the methods section regarding the processing of the data before and after basecalling. Raw nanopore signals can be hundreds of thousands long which makes it not feasible to use as input directly to the neural network. The nanopore raw signal is normalized (not discussed in the manuscript), chunked  (not discussed in the manuscript) into small pieces (usually overlapping) and then fed into the basecaller. Then, the output of the neural network is stitched back together (not discussed in the manuscript) and decoded (not discussed in the manuscript). Some parameters, like chunk size, overlap and decoding method (greedy, beam-search) are completely omitted in the methods.
}
\boxend
To address this comment, we have revised Section \sref{sec:evaluation} in three ways. 
First, we provide a detailed explanation of the preprocessing steps for raw nanopore signals. This includes normalizing and chunking the signals into manageable, overlapping segments, preparing them for input into the neural network.

\yboxbegin
\Paste{IR1.3/1}
\yboxend

Second, we have now elaborated on the post-processing steps after basecalling, explicitly discussing the stitching together of neural network output segments and the subsequent decoding process to obtain the final basecalled sequences.  We also mention our used decoding method (i.e., beam-search).

\yboxbegin
\Paste{IR1.3/2}
\yboxend

Third, we provide a description of critical parameters such as chunk size and overlap. Their significance and impact on the overall basecalling process are now clearly outlined in the methods section.

\yboxbegin
\Paste{IR1.3/3}
\yboxend

\boxbegin 
\noindent\emph{\textbf{[IR1.4]}  The introduction is quite verbose. I certainly see that this intro would be necessary for the average GB reader, as many computational biologists will be unfamiliar with many of these concepts. However, I believe some of it should either be shortened or moved to the methods, to speed up the transition to the results. For example, SkipClip requires 4 paragraphs to describe, which in my opinion, is the simplest of the approaches to explain. The introduction also uses 4 figure panels, I would suggest having a single panel that quickly summarizes the different approaches, and provide more detailed figures in the supplementary material.}
\boxend

We appreciate your suggestions for streamlining and improving the clarity of the introductory section. In our revised manuscript, we have made the following three changes. First, we use a single informative figure in Section~\sref{sec:introduction} and have moved the detailed figures and descriptions to the supplementary material for reference. Second, we have streamlined the description of \strim in Supplementary Section~\ref{supsec:skip} to provide a more concise yet comprehensive understanding. Third, we updated our manuscript template to the Genome Biology Methods template (as requested by the editor) and have significantly shortened the abstract (100 words) to make it more concise and expedite the transition to the results. We will
further refine the text and improve the clarity of the paper for the final camera-ready version.

\boxbegin 
\noindent\emph{\textbf{[IR1.5]}  Although the main goal of the manuscript is the development of accurate and very fast basecallers; and for this reason the comparison of RUBICALL is to also fast basecallers. Despite this, it would be good to have an idea of what would be the "maximum" achievable performance. This is done on Figure S3, where Bonito SUP is included; however, the performance of Bonito SUP is not mentioned anywhere in the main text, which is a missed opportunity. 
}
\boxend
We thank the reviewer for this comment. We updated the main section \sref{sec:introduction} to mention Bonito-SUP explicitly.

\yboxbegin
\Paste{IR1.5}
\yboxend

\boxbegin 
\noindent\emph{\textbf{[IR1.6]}  I might have missed something, but it is not entirely clear why for some analysis only RUBICALL is compared against the other basecallers, and not RUBICALL-FP and RUBICALL-MP. For example Fig9 has both, but Fig10 has only RUBICALL. It would be best to have both in all figures. If there's a good reason for that not to happen, then properly annotate if RUBICALL means RUBICALL-MP or RUBICALL-FP.
}
\boxend
\mechfp is simply the floating-point variant of \mechmp. Since \mechfp and \mechmp use the same model architecture and weights, \mechmp, with its mixed precision architecture, provides the same accuracy as \mechfp. Therefore, both the basecallers lead to the same results, and we refer to them as \mech.  In \sref{subsection:results_base_acc}, we already explicitly mention the above information to provide clarity and transparency in the representation of the results. In our revised manuscript, we keep \mechfp and \mechmp separate in Figures 7 and Figure 8 to maintain consistency, which were Figure 10 and Figure 11 in our initial submission, respectively.


 \yboxbegin
  \renewcommand{\thefigure}{\arabic{figure}}
\setcounter{figure}{6}
\begin{figure}[H]
  \centering
\includegraphics[width=0.8\linewidth]{images/2023.11.01_basecalling_accuracy_2.pdf} %{images/CRF/read_identity_aie_CRF_2.pdf}
  \vspace{-15pt}
  \captionof{figure}{\gs{Basecalling accuracy} comparison of \mech (using floating-point precision (\mechfp) and mixed-precision (\mechmp)).}
  % \label{fig:identity}
%   \vspace{-12pt}
\end{figure}
 \yboxend



\yboxbegin
 \renewcommand{\thefigure}{\arabic{figure}}
\setcounter{figure}{6}
\begin{figure}[H]
% \setcounter{figure}{6}
\centering
\begin{subfigure}[h]{0.5\textwidth}
  \centering
  \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0cm 0.2cm},clip]{images/2023.11.01.samtools_mismatch_hg002_1.pdf}
  %{images/CRF/samtools_mismatch_CRF_1.pdf}
%   \caption{High-end and middle-end (\textsf{H\&M}).
  \vspace{-0.6cm}
% \caption{\label{fig:sam_mis}}
\end{subfigure}%
\begin{subfigure}[h]{0.5\textwidth}
  \centering
  \vspace{-0.04cm}
  \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0.2cm 0.2cm}, clip]{images/2023.11.01.samtools_mapped_bases_hg002_1.pdf}%{images/CRF/samtools_mapped_bases_CRF_1.pdf}
%   \caption{High-end and low-end (\textsf{H\&L}).
  \vspace{-0.6cm}
\end{subfigure}

\begin{subfigure}[h]{0.49\textwidth}
  \centering
  \vspace{0.2cm}
  \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0.2cm 0.2cm}, clip]{images/2023.11.01.samtools_read_map_hg002_1.pdf} %{images/CRF/samtools_read_map_CRF_1.pdf}
%   \caption{High-end and low-end (\textsf{H\&L}).
  \vspace{-0.6cm}
\end{subfigure}
\begin{subfigure}[h]{0.5\textwidth}
  \centering
  \vspace{-0.04cm}
  \includegraphics[width=\linewidth,trim={0.2cm 0.2cm 0.2cm 0.2cm}, clip]{images/2023.11.01.samtools_read_unmap_hg002_1.pdf}%{images/CRF/samtools_read_unmap_CRF_1.pdf}
%   \caption{High-end and low-end (\textsf{H\&L}).
  \vspace{-0.6cm}
\end{subfigure}
\captionof{figure}{Comparison of \mech (using floating-point precision (\mechfp) and mixed-precision (\mechmp)) for normalized (a) mismatches, (b) bases mapped, (c) reads mapped, and (d) reads unmapped. 
% \label{fig:samtool}
}
% \vspace{-0.5cm}
\end{figure}
\yboxend


\boxbegin 
\noindent\emph{\textbf{[IR1.7]}  References to Figure 1 and 2 use numeric systems, instead of the traditional letter system. References should read (Fig 1a, Fig1b, etc.) 
}
\boxend
References to Figures 1 and 2 have been updated to use the traditional letter system. In the revised manuscript, Figure 2 is Figure S1, while Figure 1 is still Figure 1.

\boxbegin 
\noindent\emph{\textbf{[IR1.8]}  Figure 13 and figure 15 need text explaining the figure.
}
\boxend
We updated our explanation of Figure 13 (Figure~\ref{fig:strim_analysis} in the revised manuscript) as follows:

\yboxbegin
\Paste{IR1.8/1}
\yboxend

We updated our explanation of Figure 15 (Figure~\ref{fig:explainability} in the revised manuscript) as follows:

\yboxbegin
\Paste{IR1.8/2}
\yboxend

\boxbegin 
\noindent\emph{\textbf{[IR1.9]}  Figure S1 denotes the change in number of parameters in gray inside the plot frame box. I find it a bit confusing since it seems that the baseline model has 94.63 parameters, but that value is the accuracy. I suggest to move the \#parameters as well as the values outside the plot frame box. 
}
\boxend
In Figure S1, which is Figure~\ref{supfig:skip_sensitivity} in the revised manuscript, the number of parameters and values have been moved outside the plot frame box to avoid confusion.
    \yboxbegin
\renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{figure}{2}
    \begin{figure}[H]
  \centering
\includegraphics[width=0.65\linewidth,trim={0.2cm 0.25cm 0.2cm 0cm},clip]{images/skip_connection_sensitivity_4.pdf}
%   \vspace{-15pt}
  \caption{Basecaller sensitivity to skip connections.}
  % \label{supfig:skip_sensitivity}
%   \vspace{-12pt}
\end{figure}
\yboxend

\boxbegin 
\noindent\emph{\textbf{[IR1.10]}  Figure S2 I assume also talks about validation accuracy in Bonito\_CTC architecture, might be best to also explicitly mention that.
}
\boxend
Text for Figure S2, which is Figure~\ref{supfig:kd_hyper} in the revised manuscript, now explicitly mentions that it is related to validation accuracy in the \bon architecture.
\yboxbegin
\Paste{IR1.10}
\yboxend

\boxbegin 
\noindent\emph{\textbf{[IR1.11]}  Figure S3c, seems a bit counter-intuitive to have basecalling accuracy axis flipped.
}
\boxend
As mentioned in {Common Comments}~\ref{subsection:cc3}, the basecalling accuracy axis in Figure S3c, which is Figure~\ref{supfig:model_compare_all} in the revised manuscript, has been adjusted.

\boxbegin 
\noindent\emph{\textbf{[IR1.12]}  Table S1, the basecalling accuracy metrics for \mechfp and \mechmp are exactly the same, this might be true, but perhaps double-check this is not a copy-paste error.
}
\boxend
As discussed in IR1.6, \mechfp and \mechmp use the same model architecture and weights. We ensure that \mechmp, with its mixed precision architecture, provides the same accuracy as \mechfp. Therefore, both the basecallers lead to the same results.  Table S1 has been double-checked to ensure that the basecalling accuracy metrics for \mechfp and \mechmp are accurate and not a copy-paste error.


\boxbegin 
\noindent\emph{\textbf{[IR1.13]}  Figure numbers are incorrect: there is no figure 6 nor 11. 
}
\boxend
Thank you for noticing this error. Figure 11 was marked as Figure 12, while Figure 6 was already correct. {We} corrected {the} figure numbering to ensure {that there are} no references to {a} non-existent Figure 11.

\boxbegin 
\noindent\emph{\textbf{[IR1.14]}  Table 1 has some values in bold, the meaning of these (I assume it's best performing) is not explained in the table text.
}
\boxend
{We e}xplained the meaning of bold values in Table 1, specifying that they indicate the best-performing values. We updated the text in Section \sref{subsubsection:down_denovo} as follows:

\yboxbegin
\Paste{IR1.14}
\yboxend

\boxbegin 
\noindent\emph{\textbf{[IR1.15]}  Table 1, for assembly length I assume the value closest to the real assembly length (based on the reference) is best. However, the bold number does not always correspond to that. For example, for Haemophilus haemolyticus M1C132\_1, \mech is bold, but the values of \dor and \gpf are closest to the reference assembly length. 
}
\boxend
{We c}larified the criteria for bold values in Table 1 regarding assembly length, ensuring they correspond to the closest values to the reference assembly length. {We have made the value of \gpf bold for Haemophilus haemolyticus M1C132\_1.} We have double-checked all the other values.

\boxbegin 
\noindent\emph{\textbf{[IR1.16]}  Table 1, Klebsiella pneumonia INF032 and Serratia marcescens 17-147-1671, the assembly length and NG50 for \sac values are not formatted properly as they lack the thousands comma separators.
}
\boxend

{We have} properly formatted the assembly length and NG50 values with thousands comma separators for Klebsiella pneumonia INF032 and Serratia marcescens 17-147-1671 in Table 1.

\boxbegin 
\noindent\emph{\textbf{[IR1.17]}  Table 1, for many cases the Assembly length and NG50 values are exactly the same. This could be correct, but I would encourage the authors to re-check just in case these are not copy-paste mistakes.
}
\boxend
We have reviewed the accuracy of Assembly length and NG50 values in Table 1 to prevent copy-paste mistakes. {We confirm that the Assembly length and NG50 values are exactly the same for the bacterial species.}


\newpage
\subsection*{Reviewer 2}
\boxbegin 
\noindent\emph{\textbf{[IR2.1]}  Generalization: The work was mainly conducted on AMD GPU and XILINX accelerator platforms, which are not commonly used in most labs, where X86+Nvidia GPUs are more commonly used at the current time point. Although the proposed method is not platform-specific, authors are expected to demonstrate the performance of their framework on other different platforms. 
}
\boxend
Please refer to {Common Comments}~\ref{subsection:cc2}  for our newly added results on using NVIDIA A40 GPU.

\boxbegin 
\noindent\emph{\textbf{[IR2.2]}  Pipeline Component: The two major components of the proposed method involve creating a compact network structure for a basecalling model while preserving the model's efficacy. Could step-2 not be resolved by step-1, considering that the skip-connection could also be treated as a model component, and step-1 aims to find the best model structure? Does this indicate that the QABAS in step-1 can only search for certain specific model architectures? 
Or to say, are steps 1 and 2 addressing the same problem with different tools (NAS, distillation)?
}
\boxend

Thank you for your insightful comment regarding the relationship between the two major components of our proposed method, \nas (Quantization-Aware Basecalling Neural Architecture Search) and \strim. 

\nas and \strim indeed aim to address the overarching goal of creating a compact network structure for basecalling without compromising its accuracy. However, they approach this goal from different perspectives and utilize different tools to achieve complementary optimizations. We would like to highlight two critical points because of which we cannot merge the two methods.

First, skip connections play a vital role in preventing vanishing gradients and ensuring proper training of the basecalling model~\cite{szegedy2017inception}. By maintaining the skip connections in the architecture during the initial NAS phase, we guarantee that the model's training is stable and effective. It is only after the \nas and the final training of the selected topology that we introduce \strim. At this stage, we apply \strim to gradually remove the skip connections in a controlled manner. If we were to combine \nas and \strim into a single step, we would risk not finding the optimal solution achieved by our two-step approach. This separation allows us to benefit from the strengths of both techniques -- \nas in optimizing the architecture for hardware efficiency and \strim in gradually removing skip connections -- to achieve a more robust and effective basecalling architecture.
 
Second, \nas focuses on tailoring the basecalling neural network architecture for a specific hardware acceleration platform while optimizing the bit-width precision for each layer WITHOUT a teacher network.  Using NAS techniques, \nas explores various architectural configurations and identifies the most suitable network architecture for the given hardware. \strim, on the other hand, leverages knowledge distillation to train a smaller network (student) without skip connections to mimic a pre-trained larger network (teacher) that utilizes skip connections. The teacher network provides an upper bound on the achievable accuracy. It is essential to note that: (1) \nas, while capable of incorporating Step 1, may not inherently focus on skip connection removal and lead to an architecture with skip connections, and (2)  in \nas, we do not restrict ourselves to a specific teacher network architecture, hence the final network is not bounded by teacher's accuracy. \strim's knowledge distillation approach specifically addresses skip connection removal, which may not be efficiently addressed by the architecture-focused \nas alone.

In summary, both steps aim to achieve a compact and efficient basecalling model, but they do so through different tools and approaches (NAS and knowledge distillation), ensuring a comprehensive optimization process.

% We hope this clarifies the distinction and synergy between \nas and \strim in addressing the challenges of basecalling while enhancing hardware efficiency. 
{To better clarify this  distinction and synergy between \nas and \strim, we have added the below discussion point in Section~\sref{sec:discussion}}:

\yboxbegin
\Paste{IR2.2}
\yboxend


\boxbegin 
\noindent\emph{\textbf{[IR2.3]}  Application Scenario: The proposed framework is optimized with hardware constraints in mind, but this optimization incurs a computational cost. Different end users may use different hardware. Although the optimized model can be highly efficient in the decoding stage, users are required to optimize the neural network model according to their specific hardware and then re-train the basecaller. Can the authors provide a more universal hardware setting for direct deployment? Besides being hardware-friendly, user-friendliness is also a crucial consideration.
}
\boxend
Thank you for your valuable feedback. We appreciate your understanding of the framework's flexibility in adapting to different hardware configurations. Indeed, users are encouraged to tailor the hardware setting to their specific devices and requirements to maximize performance. While we optimize our framework for our target hardware, i.e., AIE, we are aware that hardware choices can vary among end users. To accommodate this, we have designed our framework using the open-source nn-Meter (\url{https://github.com/microsoft/nn-Meter})  tool in a way that allows users to freely configure and adapt the hardware settings according to their particular devices by means of the \texttt{applied\_hardware} flag in \framework. nn-Meter offers a wide range of hardware models for various devices, enabling users to select or build custom models that suit their hardware configurations. This integration enhances the adaptability and user-friendliness of our framework, allowing for efficient optimization and deployment across diverse hardware environments. Second, in our open-source \framework code~\cite{rubiconCode}, we have comprehensive documentation and tutorial notebooks that guide users through the process of setting up and optimizing the framework for various hardware configurations~. This will ensure that users have the necessary resources to implement the basecaller according to their specific needs efficiently.

{To address this comment, we have added the following explanation in the Supplementary~\sref{supsec:qabas}:}

\yboxbegin
\Paste{IR2.3}
\yboxend

\boxbegin 
\noindent\emph{\textbf{[IR2.4]}  Evaluation of Datasets: The study mainly evaluated nanopore R9.4 reads from nine prokaryotic species. A more comprehensive result might be achieved by using other benchmark datasets (e.g., HG002) and the latest R10.4 chemistry with publicly available datasets.
}
\boxend

Please refer to {Common Comments}~\ref{subsection:cc1} for our new results on HG002. 

Regarding using R10 chemistry datasets: Currently, ONT does not provide an appropriate public training dataset for R10 chemistry. They only provide in-house trained R10 models, which cannot be used to have a consistent evaluation of all the basecallers. Since R9 and R10 chemistry use different generations of nanopore technologies (e.g., different pore protein and read length), we cannot use R9-trained models for inference on R10 sequenced datasets. Therefore, we use only the available R9 chemistry training dataset from ONT and perform inference on R9 chemistry datasets. We had already raised an issue to ONT  on October 25, 2021 (\url{https://github.com/nanoporetech/bonito/issues/195}).

We have updated our paragraph related to \textbf{Basecalling reads} in Section~\sref{sec:evaluation} as follows:

\yboxbegin
\Paste{IR2.4}
\yboxend
\newpage
\subsection*{Reviewer 3}
\boxbegin 
\noindent\emph{\textbf{[IR3.1]}  It might be good to explain the specifics of data being discussed in the introduction. For example, I was not sure if the authors meant basecalling 3 Gb of sequence (1x human genome) takes 6 hours using Bonito. 
}
\boxend
We have addressed this comment to provide specifics of the data being discussed as a part of Common Comments~\ref{subsection:cc4}.
% We have updated our writing to provide specifics of the data being discussed in Section \sref{sec:introduction}:

% \yboxbegin
% \Paste{IR1.2/1}
% \yboxend

\boxbegin 
\noindent\emph{\textbf{[IR3.2]}  It might also be good to define performance on other GPUs which, I think, would be relevant for users. Was the performance measured on NVIDIA/ARM?
}
\boxend
Please refer to {Common Comments}~\ref{subsection:cc2} for our newly added results on using NVIDIA A40 GPU.

\boxbegin 
\noindent\emph{\textbf{[IR3.3]}  I would not call bonito as state-of-the-art. It was the R\&D basecaller that ONT released to the community. However, the SOTA for Nanopore at the time would have been Guppy (and now a new version of Dorado). I appreciate the source code for those is not open. I do, however, feel that the performance (in runtime, speed, and quality) would benefit from being measured relative to those for users in the community.  
}
\boxend
Thank you very much for your insightful comment. We already provide runtime measurements for {the} Dorado basecaller {v0.4.0} in all our evaluation results (Section~\sref{sec:results}). In our revised manuscript, we have ensured that Bonito is not referred to as a state-of-the-art basecaller. %and (2) updated measurements relative to Dorado.

\boxbegin 
\noindent\emph{\textbf{[IR3.4]}  Figure 8c. It is odd to see a decrease in accuracy on the axis (which makes it hard to follow). 
}
\boxend
Please refer to Common Comments~\ref{subsection:cc3}. We have updated Figure 8c, which is Figure~\ref{fig:model_compare}(a) in the revised manuscript, with a proper axis.

\boxbegin 
\noindent\emph{\textbf{[IR3.5]}  It may also be worth breaking down the performance comparisons (possibly as a supplementary figure) to compare fast vs. fast and higher accuracy (e.g. Bonito-CTC, or sup) with high accuracy models. 
}
\boxend 

We thank the reviewer for this feedback. We have updated Table~\ref{suptab:compare} in supplementary Section~\ref{suppsec:guppy_compare} with an additional column where we compare the main characteristics of different basecallers in terms of accuracy and performance.


 \input{rev_tables/model_compare}

 Based on this new column in Table~\ref{suptab:compare}, we added the following text in Section~\ref{suppsec:guppy_compare}:
 \yboxbegin
\Paste{IR3.5}
\yboxend


\boxbegin 
\noindent\emph{\textbf{[IR3.6]}  It would be useful to compare under and over represented sequences (kmers) for the reads and resulting assemblies. 
}
\boxend

{We thank the reviewer for this feedback. We added a new Supplementary Section~\sref{suppsec:kmer_analysis} where we perform k-mer frequency analysis for reads and assemblies.}

\renewcommand{\thesection}{S\arabic{section}}
\setcounter{section}{6}
\renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{figure}{7}
\yboxbegin
\Paste{IR3.6}
\yboxend

\yboxbegin
\renewcommand{\thetable}{S\arabic{table}}
\setcounter{table}{2}
\input{rev_tables/kmer_compare}
\yboxend

We added a reference to this above analysis in Section~\sref{sec:results} as follows:

\yboxbegin
\Paste{IR3.6/2}
\yboxend
\boxbegin 
\noindent\emph{\textbf{[IR3.7]}  The QVs should be compared as well to ascertain that \mech data based assemblies are not only covering higher percent of the genome but also better in consensus quality. 
}
\boxend
We acknowledge the importance of measuring the reliability of the basecalls. In our revised manuscript, we added new experiments to include the Quality Value (QV) score of the assembly in Table~\ref{tab:downstream} in Section~\sref{subsubsection:down_denovo}. 

\input{rev_tables/downstream_qv}

We use Inspector~\cite{chen2021accurate} to calculate the QV score using the generated assembly by a basecaller for a specific read. We updated Section \sref{sec:evaluation} to provide details on our methodology:
\yboxbegin
\Paste{IR3.7/1}
\yboxend

The high QV score of \mech confirms that \mech assemblies provide reliability by covering a higher percentage of the genome and better consensus quality. Based on these experiments with QV score, we added the following observation in Section~\sref{subsubsection:down_denovo}:

\yboxbegin
\Paste{IR3.7/2}
\yboxend  




\boxbegin 
\noindent\emph{\textbf{[IR3.8]} It would be useful to apply and measure the performance and quality for human genome (or a chromosome perhaps?).  
}
\boxend

Please refer to {Common Comments}~\ref{subsection:cc1} for our new results on Human HG002.

\boxbegin 
\noindent\emph{\textbf{[IR3.9]}  If the \# mapped reads and \# bases mapped are comparable across the basecallers, does it suggest that other basecallers include can call unalignable sequence either within a read or (more likely) as additional reads?
}
\boxend
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thesection}{S\arabic{section}}
\setcounter{section}{5}
We added additional analysis on mapped reads and mapped bases in our new Supplementary Section~\ref{suppsec:read_lenght_analysis}. 
\yboxbegin
 \Paste{IR3.9}
 \input{rev_tables/read_mapping_compare}
\yboxend

We added a reference to the above new Supplementary section in Section~\sref{subsubsection:down_read_mapping}:

\yboxbegin
 \Paste{IR3.9/2}
\yboxend
 
\boxbegin 
\noindent\emph{\textbf{[IR3.10]}  How do indel errors compare between these basecallers?
}
\boxend

We have added two new metrics to our Table 1: (a) Total Indels: the total number of indels in all aligned bases in the assembly, and (b) Indel Ratio (\%): the ratio of indels to assembly length. We updated Section \sref{sec:evaluation} to provide details on these two metrics as additional metrics to measure the assembly accuracy:
\yboxbegin
\Paste{IR3.10/1}
\yboxend

We updated  Section \sref{subsubsection:down_denovo} as follows. First, we mention that the best-performing basecallers have the lower values:
\yboxbegin
\Paste{IR3.10/2}
\yboxend
Second, we updated Table 1 to include  Total Indels and  Indel Ratio:

\input{rev_tables/downstream_indel}



Third, based on these two new metrics, we added a new observation in Section \sref{subsubsection:down_denovo} that \mech consistently provides lower errors or structural variants.
\yboxbegin
\Paste{IR3.10/3}
\yboxend

\boxbegin 
\noindent\emph{\textbf{[IR3.11]}   What species are the training data from?
}
\boxend

To enhance reproducibility, we use an open-source ONT training dataset of $\sim$12.2 GiB, which can be downloaded through the official ONT Bonito repository~\cite{bonito} using the command `bonito download ---training'. The dataset comprises 1,221,470 reads, all sequenced from complete genomes. Although ONT has made the dataset publicly available, the specific DNA species contained within it has yet to be disclosed. A previous work [R3] calculated an approximate list of 496 unique taxonomic IDs using the Kraken2 taxonomic classification system in this ONT training dataset.  To have a fair comparison, we use this ONT dataset to train all the basecallers. 

\noindent[R3] {Larsen ACM, Knudsen CA, Hansen MN. Palamut - An Expansion of the Bonito basecaller using language models. Thesis. 2020. Available at:} \url{https://projekter.aau.dk/projekter/files/334904330/MI104F20_Speciale___Paper__21_.pdf}

We updated details on training data in Section~\sref{sec:evaluation} as follows:

\yboxbegin
\Paste{IR3.11}
\yboxend

\boxbegin 
\noindent\emph{\textbf{[IR3.12]}   I did not see where the explanation and application for hardware coupling and optimization was. It is talked about in the paper, but I was unable to see the implementation or details. Could the authors please clarify?
}
\boxend
Thank you for bringing up the need for a more detailed explanation and application of hardware coupling and optimization in our framework. 

In our \nas method, hardware coupling and optimization are fundamental to achieving efficient basecalling architecture tailored to specific hardware platforms. We employ a combination of hardware-aware optimization techniques and customization strategies to ensure optimal performance. Here's a more detailed explanation of our approach:

\textbf{Hardware-aware Optimization Techniques:}
We incorporate optimization techniques that consider the unique characteristics and constraints of the target hardware. These techniques include memory usage, computation capabilities, and bit-width optimization at each neural network layer. In doing so, we tailor the neural network architecture and computation to align with the hardware's capabilities.

\textbf{Customization for Target Hardware:}
We provide users with the flexibility to customize the framework for their target hardware (using the \texttt{applied\_hardware} flag in \framework~\cite{rubiconCode}) by adjusting hardware-specific parameters. Different hardware provides different latencies for the same layers chosen from the \nas search space. The user can use the \texttt{reference\_latency} flag in \nas to guide the search of basecalling architecture to find an architecture that meets certain latency constraints.  This coupling of target hardware latency ensures the basecaller architecture is finely tuned to operate optimally on the intended hardware.

We addressed this comment by updating Section \sref{supsec:qabas} as follows. First, we add the following to the benefit of using bit-width optimization to the \nas search space.
\yboxbegin
\Paste{IR3.12/1}
\yboxend

Second, we provide details on the flexibility of \framework in Section \sref{supsec:qabas} as follows:
\yboxbegin
\Paste{IR3.12}
\yboxend

\boxbegin 
\noindent\emph{\textbf{[IR3.13]}  I appreciate the code being shared but will say that figshare is an odd choice. Is there a plan to also share via GitHub?
}
\boxend

Thank you for your feedback regarding the sharing of code associated with our work. We have now successfully completed the AMD/Xilinx open-source process and have fully open-sourced our code at \url{https://github.com/Xilinx/neuralArchitectureReshaping/}.


\boxbegin 
\noindent\emph{\textbf{[IR3.14]}  As a suggestion, I think a Jupyter notebook might be a nice addition to the codebase.
}
\boxend

Thank you for this suggestion. We have added several notebooks and documentation on our open-source git repo (\url{https://github.com/Xilinx/neuralArchitectureReshaping/tree/main/notebook}). We plan to add more notebook examples on a rolling basis.

