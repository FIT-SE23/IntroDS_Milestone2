% \vspace{-17pt}
\section{Discussion} \label{sec:discussion}
% \srr{This normally includes the discussion and conclusion. We don't have them separated. We summarize what we did without copy/paste from other places and then discuss other potential applications and benefits that are not discussed in the paper. We discuss future work and such as well..}
We are witnessing a tremendous transformation in %genomics and 
high-throughput sequencing to significantly advance omics and  other life sciences. %and \sr{the} exponential accumulation of high-throughput sequencing data. 
The bioinformatics community has developed a multitude of software tools to leverage increasingly large and complex sequencing datasets. Deep learning models have been especially powerful in modeling basecalling. 

\hy{\mbox{\head{Importance of basecalling}} Basecalling is the most fundamental computational step in the high-throughput sequencing pipeline. It is a critical problem in the field of genomics, and it has a significant impact on downstream analyses, such as variant calling and genome assembly. Improving the efficiency of basecalling has the potential to reduce the cost and time required for genomic analyses, which has practical implications for real-world applications. \mbox{\mech} offers a valuable alternative for researchers and practitioners who seek a balance between accuracy and speed. By maintaining competitive accuracy levels while significantly improving speed, our framework addresses the needs of various applications with stringent time constraints, ultimately benefiting a broader range of users. We believe that RUBICON provides a significant improvement over existing methods, and it has practical implications for the genomics community. 
} 


\hy{\mbox{\head{Need to improve the throughput of basecallers}} Increasing throughput and reducing model size is critical because of the following three reasons. First, current basecallers already have high accuracy, but biologists do not pay attention to the throughput implications of using large deep learning-based models~\mbox{\cite{alser2022molecules}.}} We observe researchers building larger and larger  basecallers in an attempt to gain more accuracy without heeding to the disproportionately higher amount of power  these basecallers are consuming. Moreover,  none of the previous basecallers~\cite{wick2019performance,neumann2022rodan,konishi2021halcyon,xu2021fast,lou2020helix,perevsini2021nanopore,pages2022comprehensive,ambernas_zhang2021automated} have been optimized for mixed-precision execution to reduce energy consumption. As energy usage is proportional to the size of the network, energy-efficient basecalling is essential to enable the adoption of more and more sophisticated basecallers. \hy{Second, speed is critical in certain applications and use cases, particularly those that require real-time or near-real-time processing. \mbox{\framework} addresses these needs by focusing on hardware optimization and efficient implementation, ultimately enabling faster basecalling and potentially opening up new possibilities for applications with stringent time constraints. Third, as deep learning techniques and hardware continue to evolve, the balance between accuracy and speed/energy will remain an important aspect of model development. \mbox{\framework} provides a foundation for future research and innovation in hardware-friendly deep learning models for genomic basecalling.}

\hy{\mbox{\head{Evaluating \framework on other platforms}} All the state-of-the-art basecallers and RUBICON use high-level libraries, such as PyTorch or TensorFlow, which abstract the hardware architecture and provide a unified interface for deep learning computations.  These libraries work out-of-the-box for AMD GPUs and are equally optimized for them. Currently, high-level libraries do not provide capabilities to exploit low-precision tensor cores available on the latest GPUs. As a result, existing basecallers take advantage of comparable architectural capabilities regardless of the specific GPU employed.   Therefore, the hardware and software optimizations are at the same level for all supported GPU-based platforms. 
}

\hy{\mbox{\head{Automating basecaller generation process}}} Modern basecallers generally employ convolution neural networks to extract features from raw genomic sequences. However, designing a basecaller comes with a cost that a neural network model can have many different computational elements making the neural network tuning a major problem. 
At present, the vast majority of deep learning-based basecallers are manually tuned by computational biologists through manual trial and error, which is time-consuming. To a large extent, basecallers are being designed to provide higher accuracy without considering the compute demands of such networks. Such an approach leads to computationally complex basecallers that impose a substantial barrier to performing end-to-end time-sensitive genomic analyses.
This vast dependence of computational biologists \sr{and biomedical researchers} on these deep learning-based models creates a critical need to find efficient basecalling architectures optimized for performance. %speeding it up and making it much more efficient is fundamentally critical to genomics and general omics research and development.  % The increasing dependence of biomedical scientists on these powerful tools creates a critical need for faster and more efficient computational tools. %{m2v}. %These data enable the study of diverse molecular mechanisms and biological systems through a quantitative lens.
%, and we believe \mech provides a breakthrough advance in this direction

  %\nas for efficiently designing optimal deep learning-based genomic basecallers. 
%  Our \nas framework automatically evaluates millions of neural network options while simultaneously searching for the best precision for weights and activations. While  \strim removes the skip connections present in modern basecallers to greatly reduce resource and storage requirements without any loss in basecalling accuracy. 
During our evaluation, we ran \nas for 96 GPU hours to sample architectures from our search space. Using complete sampling to evaluate all the 1.8$\times$10$^{32}$ viable options would take at least $\sim$4.3$\times$10$^{33}$ GPU hours. Thus, \nas accelerates the basecaller architecture search to develop high-performance basecalling architectures.   The final model architecture  can be further fine-tuned for other hyperparameters~\cite{singh2019napel,10.1145/3470496.3527442}, such as learning rate and batch size (for example, with grid search or neural architecture search). %in this study, we did not fine-tune these, to ensure fair comparisons between architectures. 
Throughout our experiments, we build general-purpose basecalling models by training and testing the model using an official, open-source ONT dataset that consists of a mix of different species. We did not specialize basecalling models for a specific specie. Past works, such as~\cite{wick2019performance}, show that higher basecalling accuracy can be achieved by building species-specific models.

\hy{\mbox{\head{Extending \framework}} \framework's modular design allows for the incorporation of additional layers or techniques, such as RNN, LSTM, and Transformers, to potentially increase accuracy further.}  \hy{We focus on convolution-based networks because: (a) matrix multiplication is the fundamental operation in such networks that is easily amenable to hardware acceleration, (b) the training and inference of RNN and LSTM models inherently involve sequential computation tasks, which poses a challenge for their acceleration on contemporary hardware such as GPUs and field-programmable gate arrays (FPGAs)~\mbox{\cite{nurvitadhi2016accelerating,singh_fpga-based_2021,singh2023sparta,singh2022accelerating,cali_segram_2022,singh2018review,singh2019near,gomez2023evaluating,singh2021modeling}}, and (c) transformer-based models are typically composed of multiple fully connected layers, which can be supported in \mbox{\framework} by modifying convolutional layers for improved computational efficiency and performance \mbox{\cite{umuroglu2017finn}}.} As future work,    \nas can be extended in two ways: (1) evaluate advance model architectures (such as RNN, transformer, etc.), and (2) perform more fine-grain quantization. First, extending \nas to other model architectures is important for researchers to quickly evaluate different computational elements. As the field of machine learning is rapidly evolving, it is non-trivial for researchers to adapt their models with the latest deep learning techniques. Second, currently, we perform mixed precision quantization, where every layer is quantized to a different domain. In the future, we can quantize every dimension of the weights to different precision. Such an approach would increase the design space of neural network architectural options to many folds.  \nas enables easy integration to explore such options automatically. Thus, \nas is easily extensible and alleviates the designerâ€™s burden in exploring and finding sophisticated basecallers for different hardware configurations. We would explore two future directions for pruning a basecaller. First,  currently, we perform one-shot pruning, whereby we prune the model once and then fine-tune the model until convergence. Another approach could be to perform iterative pruning, where after every training epoch, we can re-prune the model using certain pruning criteria. Such an approach would further evaluate the fine-grained pruning limit of a basecaller. Second, an interesting future direction would be to combine multiple pruning techniques, e.g., structured channel pruning with structured group pruning (where we maintain the structure of the tensors without causing sparsity). Such an approach could lead to higher pruning ratios without substantial accuracy loss. 

\hy{\mbox{\head{Importance of \mech beyond basecalling}}} For \strim, we demonstrate its   applicability on  basecalling only, while there are other genome sequencing tasks where deep learning models with skip connections are actively being developed, such as predicting the effect of genetic variations~\cite{alipanahi2015predicting,ambernas_zhang2021automated}, detecting replication dynamics~\cite{boemo2021dnascent}, and predicting super-enhancers~\cite{sabba2021residual}. In \fgb{Additional file 1:} Section S1, we show the effect of manual skip removal, where we manually remove all the skip connections at once. We observe that the basecaller achieves 90.55\% accuracy (4.08\% lower than the baseline model with skip connections). By manual skip removal, the basecaller is unable to recover the loss in accuracy because CNN-based basecallers are sensitive to skip connections. Therefore, \strim provides a mechanism to develop hardware-friendly deep learning models for other genomic tasks.

\Copy{IR2.2}{\ogb{\head{Separation between \nas and \strim} Both \nas and \strim share the overarching objective of creating a compact basecalling network without compromising accuracy. However, they approach this goal from distinct perspectives and employ different optimization tools. The following three points justify the separation of the two methods. First, skip connections are integral to stable model training, and by retaining them during the initial \nas phase, we ensure effective training of the final basecalling network. The subsequent application of \strim allows for the controlled removal of skip connections, contributing to a more robust solution.  Second,  \nas might find an architecture with skip connections, whereas \strim employs knowledge distillation for skip connection removal, addressing a specific aspect not efficiently handled by \nas alone.  Third, unlike \strim, \nas tailors the neural network architecture for hardware efficiency without relying on a teacher network. The teacher network provides an upper bound on the achievable accuracy. Therefore, this two-step approach optimally combines the strengths of NAS and knowledge distillation, ensuring a comprehensive and effective optimization process for a compact and efficient basecalling model.}}

% \ogb{More Hardware Scenarios: While we optimize the framework for our target hardware (AIE), we integrate the open-source nn-Meter tool, allowing users to freely configure hardware settings through the \texttt{applied\_hardware} flag in \framework. This integration enhances adaptability, enabling efficient optimization and deployment across different hardware environments.}
% \hy{\mbox{\head{Fair evaluation of evaluated basecallers}} }
  

  
%   that can help researchers develop optimized basecallers without requiring machine learning expertise . 






