\section{Related Work}\label{sec:related_work}

\subsection{Optimization model-based Multi-agent Task Allocation}\label{sec:model_based_task_allocation}

Multi-agent task allocation is the problem of determining which agents should execute a given task to achieve the overall system goals \cite{gerkey2004formal, korsah2013comprehensive}.
% The task allocation is further defined by the requirements on agent coordination, heterogeneity of agents and tasks, and dependency between different tasks.

Both learning-based \cite{elfakharany2020towards, baker2019emergent, li2020graph, wang2020mobile, wu2021impact} and optimization model-based frameworks \cite{banks2020multi, aksaray2015distributed, fu2020heterogeneous, fu2021robust, ravichandar2020strata, prorok2017impact, messing2022grstaps, liemhetcharat2011modeling, liemhetcharat2012weighted} have been applied to the task allocation problem. Learning-based frameworks usually require low computational costs once trained and achieve high performance in similar test environments. However, they are often relegated to simulation environments since they require a large amount of training data and can exhibit safety issues during the early training process if trained in the real world. Optimization model-based frameworks are often generalizable to unseen task scenarios (subject to modeling assumptions) and can provide optimality bounds.

Optimization frameworks models the relationship between the tasks and agents.
In \cite{klee2015graph, nicolescu2003natural, galindo2008robot, torreno2017cooperative},
% \cite{klee2015graph, nicolescu2003natural, ekvall2008robot, niekum2012learning, grollman2010incremental, hayes2015effective, galindo2008robot, aeronautiques1998pddl, torreno2017cooperative}
a task is represented as a graph of states (nodes) and actions (edges) so that graph search based algorithms can be applied to find the best sequence of actions for agents.
In \cite{banks2020multi, aksaray2015distributed}, the task constraints are specified using linear temporal logic. Requirements on agent actions are represented as a conjunction of logical expressions.
It is common to assume that agents possess sets of discrete capabilities and that tasks require specific quantities of a given capability to successfully complete an objective. 
The works of \cite{fu2020heterogeneous, fu2021robust} apply such a capability model, where the agent's capabilities are represented as random distributions.
Then, they solve a discrete stochastic optimization to obtain the routes, schedules, and task assignment plans.
The authors of \cite{ravichandar2020strata, prorok2017impact} utilize similar task requirement models, but formulate the problem as an optimal control-style continuous optimization to generate the desired capability distribution at each time step.
A similar capability model is also applied in \cite{messing2022grstaps}.
The authors of \cite{liemhetcharat2011modeling, liemhetcharat2012weighted} represent the dependency between agents in a task using a capability-based graph model and generate a plan using a distributed auction-based method.

Most optimization frameworks for multi-agent task allocation assume the modeling parameters (e.g. task requirements, agent capabilities) are known a priori.
However, this assumption may not hold true in practice where agent capability variations may be hard to predict or determine.
Therefore, the ability to learn the modeling parameters from simulation or experimental data will provide an important contribution to optimization model-based task allocation frameworks.

\subsection{Learning Optimization Models from Data}

Optimization model-based frameworks require a small amount of training data and can be applied to new scenarios with little or no additional training or parameter tuning.
There are many examples from previous works focused on learning the parameters in an optimization model for single-robot manipulation and navigation.
\cite{klee2015graph} and \cite{nicolescu2003natural} leverage a learning from demonstration approach to determine a graph of actions for a new manipulation task. 
Authors of \cite{chou2020explaining, vazquez2018learning} embed linear temporal logic in their optimization model to constrain robot navigation and manipulation trajectories, and develop corresponding methods to learn the temporal logic specifications from demonstrations.

Multi-agent task allocation, on the other hand, has a few previous related works. The authors of \cite{liemhetcharat2011modeling, liemhetcharat2012weighted} model an agent's capabilities as a vector and the dependency between agents for a specific task as a synergy graph. To learn their model, they randomly initialize the synergy graph, and iteratively optimize an objective to estimate the capability values and locally perturb the graph structure to fit the training data.

As mentioned in Sec. \ref{sec:model_based_task_allocation}, the works of \cite{fu2020heterogeneous, fu2021robust, ravichandar2020strata, prorok2017impact, messing2022grstaps} start from the agent capability model and represent the task requirements as a vector of thresholds on each capability type, which are embedded as constraints in the optimization. We extend these models through our proposed framework to enable the learning of agent capability values and task requirement thresholds from simulation or experimental data.
As the learned capability-based model can be regarded as a set of linear constraints, the constraints can then be embedded easily in these existing capability-based models or other optimization-based task allocation frameworks.

