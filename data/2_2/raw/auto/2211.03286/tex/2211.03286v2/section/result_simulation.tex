\section{Simulation Application}\label{sec:result_simulation}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/simulation/simulation_environment.png}
    \caption{A screenshot of the multi-agent simulation environment. The robot team is conducting a `find and pick' task.
    }
    \label{fig:simulation_environment}
\end{figure}

In this section, we apply our learning framework to a robotic exploration and manipulation problem and show how the agent capabilities and task requirements are learned. 
Multiple types of tasks are involved in the example to show that the framework generalizes to different tasks.

The simulation environment is developed in Gazebo, and the communication between robots and sensors is established using ROS. A screenshot is shown in Fig. \ref{fig:simulation_environment}. There are five types of agents, and their qualitative capabilities are described in \tableref{} \ref{tab:real_agent_capability}. There are five types of tasks as shown in \tableref{} \ref{tab:real_task_requirement}.
Once the team configuration is determined for a specific task, a simple intra-team strategy (greedy algorithms) will guide the agents to complete the task cooperatively. As this paper focuses on inter-team scheduling and team configuration planning, the simple intra-team strategy is not described here.



% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]
  \centering
  \caption{Qualitative agent capabilities. A screenshot of the five robots is shown in Fig. \ref{fig:simulation_environment} (left to right: agent type 1-5). }
    \begin{tabular}{l|p{0.25\linewidth}p{0.45\linewidth}}
    \toprule
    \multicolumn{1}{c|}{Agents} & Perception & Manipulation \\
    \midrule
    1: Smallbot 1 & The range is 1 m.     & None. \\
    2: Smallbot 2 & The range is 1.5 m.     & None. \\
    3: Largebot 1 & The range is 1 m.     & None. \\
    4: Largebot 2 & The range is 1 m.     & Only picks light blocks. Picks light blocks faster than largebot 3. \\
    5: Largebot 3 & The range is 1.5 m.     & Picks both light and heavy blocks. \\
    \bottomrule
    \end{tabular}%
  \label{tab:real_agent_capability}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]
  \centering
  \caption{Qualitative task requirements. Perception is not required for the picking tasks as the block locations are known. The size of a task region is set to 6\(\times\)6 meters.}
    \begin{tabular}{l|p{0.25\linewidth}p{0.4\linewidth}}
    \toprule
    \multicolumn{1}{c|}{Tasks} & Perception & Manipulation \\
    \midrule
    1: Explore & Cover the region.     & None. \\
    2: Pick (light) & None.     & Pick four light blocks at known locations. \\
    3: Pick (mixed) & None.     & Pick two light and two heavy blocks at known locations. \\
    4: Pick (heavy) & None.     & Pick four heavy blocks at known locations. \\
    5: Find and pick & Find a block.     & Pick the found light block. \\
    \bottomrule
    \end{tabular}%
  \label{tab:real_task_requirement}%
\end{table}%

\subsection{Learning the Capability Model}

Considering the physical size of the task area, we add a constraint that a team for a task can contain at most four agents. There are 125 valid teams within the configuration space of \(\mathbb{N}^5\).
Considering the additional requirements\footnote{Task 2 is only relevant to the largebots 2 and 3. Task 3 requires at least one largebot 3 in the team. Task 4 is only relevant to largebot 3. Task 5 requires at least one agent with the manipulation capability.}, the number of valid teams for the five tasks are 125, 14, 10, 4, and 91, respectively. Among these possible team configurations, we chose to evaluate the performance of 20, 14, 10, 4, and 25 team configurations within the five tasks.

For each task \(i = 1, \cdots, 5\) and team configuration \(\mathbf{y}_i^l\) (\(l\) is the label for a team configuration), define the task performance as \(f_i(\mathbf{y}_i)\). Here, it is the task completion time.
\(f_i(\mathbf{y}_i)\) is stochastic because the completion time is influenced by the localization, control, communication uncertainties, and environmental setup.
Therefore, instead of \(f_i(\mathbf{y}_i) \leq f_i^*\), we use \eqref{eqn:prob_task_requirement} to determine whether a team is valid.
\begin{align}
    P(f_i(\mathbf{y}_i) \leq f_i^*) \geq 80 \% \label{eqn:prob_task_requirement}
\end{align}
To check the condition in \eqref{eqn:prob_task_requirement}, we randomly generate five cases for each \((i, l)\) pair (with different block locations) and evaluate the task completion time. If at least four of the task completion times are smaller than the task time threshold \(f_i^*\), the team is considered valid.
The statistics of the task completion time are listed in \tableref{} \ref{tab:task_statistics}.


% Table generated by Excel2LaTeX from sheet 'paper'
\begin{table}[t]
  \centering
  \caption{Task completion time and threshold selection for the tasks. The unit for the \{min, max, mean, thres\} are seconds. A specific team configuration \(\mathbf{y}_i\) is considered valid if the completion time \(f_i(\mathbf{y}_i)\) is smaller than the threshold. The error rate calculates the proportion of falsely predicted samples using the learning model.}
    \begin{tabular}{l|ccccc|c}
    \toprule
    \multicolumn{1}{c|}{Tasks} & Sample & Min   & Max   & Mean  & Thres & Error \\
    \midrule
    1: Explore & 20    & 85    & 245   & 156   & 190   & 0\% \\
    2: Pick (light) & 14    & 44    & 211   & 99    & 110   & 0\% \\
    3: Pick (mixed) & 10    & 66    & 252   & 117   & 130   & 0\% \\
    4: Pick (heavy) & 4     & 70    & 203   & 140   & 140   & 0\% \\
    5: Find and pick & 25    & 44    & 111   & 74    & 90    & 4\% \\
    \bottomrule
    \end{tabular}%
  \label{tab:task_statistics}%
\end{table}%

We apply the learning model described in Sec. \ref{sec:learning_model} to this case study to learn a set of agent capabilities and task requirements to approximate the boundary between valid and invalid teams.
We define five capability types (1: perception, 2: manipulation related to light blocks,  3: manipulation related to heavy blocks, 4: perception 2, 5: manipulation 2 related to light blocks).
The prior information is the sparsity pattern of the capability and requirement matrices:
which values in \tableref{}s \ref{tab:learned_cap}-\ref{tab:learned_req} are nonzero.
Note that capabilities 4-5 are copies of 1-2 related to the find-and-pick task. Adding the two additional capabilities makes the capability-based model more expressive and results in lower prediction error.
The practical meaning is that a specific capability of an agent (e.g. perception) can change with regard to different tasks.


The learned values with the threshold listed in \tableref{} \ref{tab:task_statistics} are shown in \tableref{}s \ref{tab:learned_cap}-\ref{tab:learned_req}. The values indicate that 1) smallbot2 and largebot3 have larger perception capabilities and 2) largebot 2 has a larger manipulation capability when picking light blocks. This is consistent with their qualitative capability in \tableref{} \ref{tab:real_agent_capability}.
The prediction error with the selected threshold is shown in \tableref{} \ref{tab:task_statistics}. A parametric study has been done and shown that similar capability and task requirements values can be learned with an over \(\pm 50\) perturbation of the selected threshold.


% Table generated by Excel2LaTeX from sheet 'paper'
\begin{table}[t]
  \centering
  \caption{Learned agent capabilities \(A^\transpose\).}
    \begin{tabular}{c|ccccc}
    \toprule
    Agents & 1: P  & 2: ML & 3: MH & 4: P2 & 5: ML2 \\
    \midrule
    1: Smallbot 1 & 1     & \cellcolor[rgb]{ .647,  .647,  .647}0 & \cellcolor[rgb]{ .647,  .647,  .647}0 & 1     & \cellcolor[rgb]{ .647,  .647,  .647}0 \\
    2: Smallbot 2 & 2     & \cellcolor[rgb]{ .647,  .647,  .647}0 & \cellcolor[rgb]{ .647,  .647,  .647}0 & 1     & \cellcolor[rgb]{ .647,  .647,  .647}0 \\
    3: Largebot 1 & 1     & \cellcolor[rgb]{ .647,  .647,  .647}0 & \cellcolor[rgb]{ .647,  .647,  .647}0 & 1     & \cellcolor[rgb]{ .647,  .647,  .647}0 \\
    4: Largebot 2 & 1     & 2     & \cellcolor[rgb]{ .647,  .647,  .647}0 & 1     & 1 \\
    5: Largebot 3 & 2     & 1     & 1     & 1     & 1 \\
    \bottomrule
    \end{tabular}%
    \vspace{-0.2cm}
  \label{tab:learned_cap}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'paper'
\begin{table}[t]
  \centering
  \caption{Learned task requirements. Each row indicates a vector \(\mathbf{b}_i^\transpose\).}
    \begin{tabular}{l|ccccc}
    \toprule
    \multicolumn{1}{c|}{Tasks} & 1: P  & 2: ML & 3: MH & 4: P2 & 5: ML2 \\
    \midrule
    1: Explore & 2     & \cellcolor[rgb]{ .647,  .647,  .647}0 & \cellcolor[rgb]{ .647,  .647,  .647}0 & \cellcolor[rgb]{ .647,  .647,  .647}0 & \cellcolor[rgb]{ .647,  .647,  .647}0 \\
    2: Pick (light) & \cellcolor[rgb]{ .647,  .647,  .647}0 & 4     & \cellcolor[rgb]{ .647,  .647,  .647}0 & \cellcolor[rgb]{ .647,  .647,  .647}0 & \cellcolor[rgb]{ .647,  .647,  .647}0 \\
    3: Pick (mixed) & \cellcolor[rgb]{ .647,  .647,  .647}0 & 3     & 1     & \cellcolor[rgb]{ .647,  .647,  .647}0 & \cellcolor[rgb]{ .647,  .647,  .647}0 \\
    4: Pick (heavy) & \cellcolor[rgb]{ .647,  .647,  .647}0 & \cellcolor[rgb]{ .647,  .647,  .647}0 & 3     & \cellcolor[rgb]{ .647,  .647,  .647}0 & \cellcolor[rgb]{ .647,  .647,  .647}0 \\
    5: Find and pick & \cellcolor[rgb]{ .647,  .647,  .647}0 & \cellcolor[rgb]{ .647,  .647,  .647}0 & \cellcolor[rgb]{ .647,  .647,  .647}0 & 2     & 1 \\
    \bottomrule
    \end{tabular}%
  \label{tab:learned_req}%
\end{table}%

In summary, the learning model can be applied to a practical problem to learn meaningful capabilities and task requirements with low prediction errors. The learning process is not sensitive to threshold selection.


\subsection{Task Allocation with the Learned Capabilities}\label{sec:result_task_allocation_simulation}

In this section, we insert the learned agent capabilities \(A\) and task requirements \(\mathbf{b}_i\) into the task allocation framework described in Sec. \ref{sec:task_allocation_model} and solve a practical allocation problem to generate the team, routes, and schedules for the tasks.

In Fig. \ref{fig:task_allocation_simulation}, five heterogeneous tasks (chosen from \tableref{} \ref{tab:real_task_requirement}) are distributed in a 30\(\times\)30 meters outdoor area. The tasks should be completed within a required time duration once proper agent teams reach the task region. The shortest paths to travel between any two task locations are computed and the corresponding energy and time cost are generated. We have four agents from all types in \tableref{} \ref{tab:real_agent_capability} and the team size for any task should be smaller than four. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/simulation/task_allocation_simulation.pdf}
    \caption{A multi-agent task allocation problem consisting of five heterogeneous sub-tasks. An agent from type \(i\) is marked as \(v_i\) in the figure. The teams for the tasks are shown as follows. Task 1: \(v_2\), task 2: two \(v_4\), task 3 and 4: three \(v_5\), task 5: one \(v_2\) and one \(v_4\).
    }
    \label{fig:task_allocation_simulation}
\end{figure}

By applying the task allocation model in Sec. \ref{sec:task_allocation_model} using the learned agent capabilities and requirements in \tableref{}s \ref{tab:learned_cap}-\ref{tab:learned_req}, a teaming plan is generated to minimize a weighted sum of energy and time while satisfying all task requirements. The planned routes and teams are shown in Fig. \ref{fig:task_allocation_simulation}. According to the solution, the planner chooses one agent 2, two agents 4, and three agents 5 to complete the five tasks.

The teaming and routing plan in Fig. \ref{fig:task_allocation_simulation} is evaluated in simulation.
The total time for % the
agents to complete all tasks and travel back to the depot is 317 seconds.
The task time threshold in \tableref{} \ref{tab:task_statistics} are all maintained.
As the task constraints are learned indoor (Fig. \ref{fig:simulation_environment}) and applied in a slightly different setup in an outdoor region, it shows the learned constraints can be applied to similar but different setups. Overall, the results validate that the task requirement learning and task allocation framework can work together and be applied to solve practical problems in simulation.


