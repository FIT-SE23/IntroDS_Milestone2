\section{Proof for Theorem \ref{thm:ais-dp}}\label{sec:proof:thm:ais-dp}

For readability we will restate the theorem statement
      \begin{theorem}
             For any time $\timestep$, any realisation $\sts_\timestep$ of $\State_\timestep$, $\action_\timestep$ of $\Action_\timestep$, let $\history_\timestep = (\sts_{1:\timestep}, \action_{1:\timestep-1})$, and $\ais_\timestep = \aisfunction_\timestep(\history_\timestep)$.
             The worst case difference between $\valuefunction^{\star}$ and $\valuefunction^{\policy{}}_{\timestep}$ is bounded as:
            \begin{align}
                \Delta
                \le 2 \frac{\varepsilon + \discount\delta \kappa_{\mathfrak{F}}(\hat \valuefunction^{\mu}, \hat{f})}{1 - \discount},
            \end{align}
        where, $\kappa_{\mathfrak{F}}(\hat \valuefunction, \hat f) = \sup_{\feature, \action}\rho_{\mathfrak{F}}(\hat \valuefunction(\hat f(\cdot, \feature, \action)))$. and $\rho_{\mathfrak{F}}(\cdot)$ is the Minkowski functional associated with the IPM $\ipm$ as defined in \eqref{eq:minkowski-functional}.
        \end{theorem}
 \begin{proof}\label{proof:thm:ais-dp}
    For this proof we will use the following convention: For a generic history $\history_\timestep \in \historyspace_\timestep$, we assume that $\history_\timestep  = (\sts_{1:\timestep}, \action_{1:\timestep-1})$, moreover, note that $\ais_\timestep = \aisfunction_\timestep(\history_\timestep)$.
    
    
   Now from \eqref{eq:ipm-function-diff}, and \Cref{def:ais} for any $\action_\timestep, \sts_\timestep, \ais_\timestep$:
        \begin{align}
            \nonumber  \max_{\history \in \historyspace_\timestep, \action_{\timestep} \in \actionspace}\bigg\vert \cost (\sts_\timestep, \action_\timestep) - \hat \cost(\ais_\timestep, \action_\timestep)\bigg\vert &\leq \epsilon.\\
            \max_{\history \in \historyspace_\timestep, \action_{\timestep} \in \actionspace}\bigg\vert \sum_{\sts_{\timestep+1} \in \statespace} \bigg(\transition(\sts_{\timestep+1}\vert \sts_\timestep,\action_\timestep)\hat{\valuefunction}^{\mu}(\hat{f}(\sts_{_\timestep+1}, \ais_\timestep, \action_\timestep)) - \hat{\transition}(\sts_{\timestep+1}\vert \ais_\timestep,\action_\timestep)\hat{\valuefunction}^{\mu}(\hat{f}(\sts_{\timestep+1}, \ais_\timestep, \action_\timestep))\bigg)\bigg\vert &\leq \delta \rho_{\mathfrak{F}}(\hat{\valuefunction}(\hat f(\cdot, \feature_\timestep, \action_\timestep))).\label{eq:delta}
        \end{align}
       Now using triangle inequality we get:
        \begin{align}
            \Vert \valuefunction^{\star}(\sts_\timestep) - \valuefunction_{\timestep}^{\pi}(\history_\timestep)\Vert_{\infty}
            &\stackrel{(a)}{\leq} \underbrace{\Vert \valuefunction^{\star}(\sts_\timestep) - \hat \valuefunction^{\mu}(\ais_\timestep) \Vert_{\infty}}_{\text{term 1}} +  \underbrace{\Vert \valuefunction_{\timestep}^{\policy{}}(\history_\timestep) - \hat\valuefunction^{\mu}(\ais_\timestep) \Vert_{\infty}}_{\text{term 2}},
        \end{align}
        where $(a)$ follows from triangle inequality.
        
        
        We will now proceed by bounding terms 1 and 2 separately
        
        \paragraph{Bounding term 1:}
        % Using inequality $\max f(x) \leq \max \vert f(x) - g(x)\vert + \max g(x)$ we get:
        \begin{align}
           \Vert \valuefunction^{\star}(\sts_\timestep) - \hat \valuefunction^{\mu}(\feature_\timestep) \Vert_\infty
            &\leq \max_{\history \in \historyspace_\timestep}\bigg \vert\max_{\action_{\timestep} \in \actionspace}\bigg[ Q^{\star}(\sts_\timestep, \action_\timestep) - \hat Q^{\mu}(\feature_\timestep, \action_\timestep)\bigg]\bigg\vert, \label{eq:vq-diff-1}
        \end{align}
        Therefore, for any action $\action_\timestep$ 
        \begin{align*}
             \max_{\history \in \historyspace_\timestep}\bigg \vert\max_{\action_{\timestep} \in \actionspace}\bigg[Q^\star(\sts_\timestep, \action_\timestep) - \hat Q^{\mu}(\feature_\timestep, \action_\timestep)\bigg]\bigg\vert 
             &= \max_{\history \in \historyspace_\timestep}\bigg \vert \max_{\action_{\timestep} \in \actionspace}\bigg[\cost(\sts_{\timestep},\action_\timestep) + \discount\sum_{\sts_{\timestep+1} \in \statespace} \transition(\sts_{\timestep+1} \vert \sts_\timestep,\action_\timestep)\valuefunction^{\star}(\sts_{\timestep+1})\\
             &-  \hat\cost(\ais_\timestep,\action_\timestep) - \discount\sum_{\sts_{\timestep+1} \in \statespace} \hat{\transition}(\sts_{\timestep+1} \vert \ais_\timestep,\action_\timestep)\hat \valuefunction^{\mu}(\hat{f}(\sts_{\timestep+1},\ais_\timestep, \action_\timestep))\bigg]\bigg\vert\\
             &\stackrel{(a)}{\leq} \epsilon + \max_{\history \in \historyspace_\timestep, \action_{\timestep} \in \actionspace} \bigg\vert \discount\sum_{\sts_{\timestep+1} \in \statespace} \transition(\sts_{\timestep+1} \vert \sts_\timestep,\action_\timestep)\valuefunction^{\star}(\sts_{\timestep+1})
             - \discount \sum_{\sts_{\timestep+1} \in \statespace}\transition(\sts_{\timestep+1}\vert \sts_\timestep,\action_\timestep)\hat{\valuefunction}^{\mu}(\hat{f}(\sts_{\timestep+1}, \ais_\timestep, \action_\timestep)) \bigg\vert\\
             &+ \max_{\history \in \historyspace_\timestep, \action_{\timestep} \in \actionspace} \bigg\vert \discount\sum_{\sts_{\timestep+1} \in \statespace}\transition(\sts_{\timestep+1}\vert \sts_\timestep,\action_\timestep)\hat{\valuefunction}^{\mu}(\hat{f}(\sts_{\timestep+1}, \ais_\timestep, \action_\timestep)) - \discount\sum_{\sts_{\timestep+1} \in \statespace} \hat{\transition}(\sts_{\timestep+1} \vert \ais_\timestep,\action_\timestep)\hat \valuefunction^{\mu}(\hat{f}(\sts_{\timestep+1},\ais_\timestep, \action_\timestep))\bigg\vert\\
            &\stackrel{(b)}{\leq} \epsilon + \discount \Vert (\valuefunction^{\star}(\sts_\timestep) -\hat{\valuefunction}^{\mu}(\ais_\timestep))\Vert_\infty  +\discount \delta \rho_{\mathfrak{F}}(\hat{\valuefunction}^{\mu}(\hat f(\cdot, \feature_\timestep, \action_\timestep))),\\
            % &\leq \frac{\epsilon + \delta_{\mathfrak{F}}\rho_{\mathfrak{F}}(\hat{\valuefunction})\kappa_{\mathfrak{F}}(\hat f)}{(1-\discount)}.
        \end{align*}
        where $(a)$ from triangle inequality and $(b)$ is due to \eqref{eq:delta}. Now defining $\kappa_{\mathfrak{F}}(\hat \valuefunction, \hat f) = \sup_{\feature, \action}\rho_{\mathfrak{F}}(\hat \valuefunction(\hat f(\cdot, \feature, \action)))$, and substituting the above result in \eqref{eq:vq-diff-1} we get
        \begin{align}
            \vert \valuefunction^{\star}(\sts_\timestep) - \hat \valuefunction^{\mu}(\feature_\timestep) \vert 
            &\leq \frac{\varepsilon + \discount\delta \kappa_{\mathfrak{F}}(\hat \valuefunction^{\mu}, \hat{f})}{1 - \discount}\label{eq:term1}.
        \end{align}
        \paragraph{Bounding term 2:}
        % Using inequality $\max f(x) \leq \max \vert f(x) - g(x)\vert + \max g(x)$ we get:
        \begin{align}
            \Vert \valuefunction_{\timestep}^{\policy{}}(\history_\timestep) - \hat \valuefunction^{\mu}(\feature_\timestep) \Vert_\infty
            &\leq \max_{\history \in \historyspace_\timestep}\bigg \vert\max_{\action_{\timestep} \in \actionspace}\bigg[ Q_{\timestep}^{\policy{}}(\history_\timestep, \action_\timestep) - \hat Q^{\mu}(\feature_\timestep, \action_\timestep)\bigg]\bigg\vert,  \label{eq:vq-diff-2}
        \end{align}
        Therefore, for any action $\action_\timestep$ 
        \begin{align*}
           \max_{\history \in \historyspace_\timestep}\bigg \vert\max_{\action_{\timestep} \in \actionspace}\bigg[ Q^{\policy{}}(\history_\timestep, \action_\timestep) - \hat Q^{\mu}(\feature_\timestep, \action_\timestep)\bigg]\bigg\vert
            &=\max_{\history \in \historyspace_\timestep}\bigg \vert \max_{\action_{\timestep} \in \actionspace}\bigg[\cost(\sts_{\timestep},\action_\timestep) + \discount\sum_{\sts_{\timestep+1} \in \statespace} \transition(\sts_{\timestep+1} \vert \sts_\timestep,\action_\timestep)\valuefunction_{\timestep+1}^{\policy{}}(\history_{\timestep+1}) \\
            &-  \hat\cost(\ais_\timestep,\action_\timestep) - \discount\sum_{\sts_{\timestep+1} \in \statespace} \hat{\transition}(\sts_{\timestep+1} \vert \ais_\timestep,\action_\timestep)\hat \valuefunction^{\mu}(\hat{f}(\sts_{\timestep+1},\ais_\timestep, \action_\timestep))\bigg]\bigg\vert\\
            &\stackrel{(a)}{\leq} \epsilon + \max_{\history \in \historyspace_\timestep, \action_{\timestep} \in \actionspace} \bigg\vert \discount\sum_{\sts_{\timestep+1} \in \statespace} \transition(\sts_{\timestep+1} \vert \sts_\timestep,\action_\timestep)\valuefunction_{\timestep+1}^{\policy{}}(\history_{\timestep+1})
            - \discount \sum_{\sts_{\timestep+1} \in \statespace}\transition(\sts_{\timestep+1}\vert \sts_\timestep,\action_\timestep)\hat{\valuefunction}^{\mu}(\hat{f}(\sts_{\timestep+1}, \ais_\timestep, \action_\timestep)) \bigg\vert\\
            &+ \max_{\history \in \historyspace_\timestep, \action_{\timestep} \in \actionspace} \bigg\vert \discount\sum_{\sts_{\timestep+1} \in \statespace}\transition(\sts_{\timestep+1}\vert \sts_\timestep,\action_\timestep)\hat{\valuefunction}^{\mu}(\hat{f}(\sts_{\timestep+1}, \ais_\timestep, \action_\timestep))
            - \discount\sum_{\sts_{\timestep+1} \in \statespace} \hat{\transition}(\sts_{\timestep+1} \vert \ais_\timestep,\action_\timestep)\hat \valuefunction^{\mu}(\hat{f}(\sts_{\timestep+1},\ais_\timestep, \action_\timestep))\bigg\vert\\
            &\stackrel{(b)}{\leq} \epsilon + \discount \Vert (\valuefunction^{\policy{}}(\history_\timestep) -\hat{\valuefunction}^{\mu}(\ais_\timestep))\Vert_\infty  +\discount \delta \rho_{\mathfrak{F}}(\hat{\valuefunction}^{\mu}(\hat f(\cdot, \feature_\timestep, \action_\timestep))),\\
            % &\leq \frac{\epsilon + \delta_{\mathfrak{F}}\rho_{\mathfrak{F}}(\hat{\valuefunction})\kappa_{\mathfrak{F}}(\hat f)}{(1-\discount)}.
        \end{align*}
        where $(a)$ is from triangle inequality,  $(b)$ is due to \eqref{eq:delta}, with $\kappa_{\mathfrak{F}}(\hat \valuefunction, \hat f) = \sup_{\feature, \action}\rho_{\mathfrak{F}}(\hat \valuefunction(\hat f(\cdot, \feature, \action)))$, and substituting the above result in \eqref{eq:vq-diff-2} we get
        \begin{align}
            \Vert \valuefunction^{\policy{}}_{\timestep}(\history_\timestep) - \hat \valuefunction^{\mu}(\feature_\timestep) \Vert_\infty
            &\leq \frac{\varepsilon + \discount\delta \kappa_{\mathfrak{F}}(\hat \valuefunction^{\mu}, \hat{f})}{1 - \discount}\label{eq:term2}.
        \end{align}
        
        The final result then follows by adding \eqref{eq:term1} and \eqref{eq:term2}.
       \end{proof}
       
\section{Proof for Corollary \ref{THM:TV-BOUND}}\label{sec:tv-proof}
    \begin{lemma}\label{lem:tv}
        If $\hat\valuefunction$ is the optimal value function of the MDP $\hat{\mdp}$ induced by the process $\{\Feature_{\timestep}\}_{\timestep \geq 0}$, then
        \begin{align}
            \spn(\hat\valuefunction) &\leq \frac{\spn(\hat \cost)}{1 - \discount}.
        \end{align}
    \end{lemma}
    \begin{proof}
        The result follows by observing that the per-step reward $ \hat \cost  (\Feature_\timestep, \Action_\timestep) \in [\min(\hat \cost), \max(\hat \cost) ]$. Therefore $\max(\hat\valuefunction) \leq \max(\hat \cost)$ and  $\min(\hat\valuefunction) \geq \min(\hat \cost)$.
    \end{proof}
    
    \begin{corollary}
             If the function class $\mathfrak{F}$ is $\mathfrak{F}^{\text{TV}}$, then $\Delta$ defined in \eqref{eq:sup-v} is upper bounded as:
             \begin{align}
             \Delta
                \le  \frac{2\epsilon}{1-\discount} +\frac{\discount\delta \spn(\hat\cost)} {(1-\discount)^2},
            \end{align}
        \end{corollary}
        \begin{proof}
            From \Cref{def:tv-dist} we know that for the Total variation distance $\rho_{\mathfrak{F}^{\text{TV}}}(\hat\valuefunction)$ = $\spn(\hat\valuefunction)$ and $\kappa(\hat f) = 1$. The result in the corollary then follows from \Cref{lem:tv}.
        \end{proof}
       
\section{Proof for Corollary \ref{THM:LIP-BOUND}} \label{sec:w-proof}
    \begin{definition}\label{def:lipdist}
        For any Lipschitz function $f:(\aisspace, d_{\Ais}) \to (\real, \vert \cdot \vert)$, and probability measures $\nu_1$, and $\nu_2$ on $(\aisspace, d_{\Ais})$
        \begin{align}
            \bigg\vert \int_{\aisspace} f d\nu_1 - \int_{\aisspace} f d\nu_2 \bigg\vert \leq \Vert f \Vert_{L}. d_{\mathfrak{F}^{\text{W}}}(\nu_1, \nu_2) \leq L_{f}d_{\mathfrak{F}^{\text{W}}}(\nu_1, \nu_2),
        \end{align}
        where $L_f$ is the Lipschitz constant of $f$ and $d_{\mathfrak{F}^{\text{W}}}$ is the Wasserstein distance.
    \end{definition}
    % Note that the process $\{\Feature_{\timestep}\}_{\timestep \geq 0}$ is a 
    \begin{definition}\label{def:lipmdp}
        Let $d$ be a metric on the AIS/Feature space $\aisspace$. The MDP $\hat{\mdp}$ induced by the process $\{\Feature_{\timestep}\}_{\timestep \geq 0}$ is said to be $(L_{\hat\cost}, L_{\hat\transition})$ - Lipschitz if for any $\Feature_1, \Feature_2 \in \aisspace$, the reward $\hat{\cost}$ and transition $\hat{\transition}$ of $\hat{\mdp}$ satisfy the following:
        \begin{align}
            \bigg\vert \cost(\Feature_1, \Action) - \cost(\Feature_2, \Action)\bigg\vert &\leq L_{\hat{\cost}} d(\Feature_1, \Feature_2)\\
            d_{\mathfrak{F}^{\text{W}}}(\hat\transition(\cdot \vert\Feature_1,\Action), \hat\transition(\cdot\vert\Feature_2,\Action) &\leq L_{\hat \transition} d(\Feature_1, \Feature_2),
        \end{align}
    \end{definition}
    where $d_{\mathfrak{F}^{\text{W}}}$ is the Wasserstein or the Kantorovitch-Rubinstein distance.
    % \begin{lemma}\label{lem:w-lip}
    %     If the MDP $\hat{\mdp}$ induced by the $\{\Feature_{\timestep}\}_{\timestep \geq 0}$ is $(L_{\hat \cost}, L_{\hat \transition})$ - Lipschitz and $\hat\valuefunction$ is the optimal value function of $\hat{\mdp}$, then:
    %     \begin{align}
    %         L_{\hat \valuefunction} \leq \frac{(1-\discount)L_{\hat\cost}}{(1-\discount L_{
    %         \hat \transition})}.
    %     \end{align}
    % \end{lemma}
    
    \begin{lemma}\label{lem:lipq}
        Let $\hat \valuefunction: \aisspace \to \real$ be $L_{\hat \valuefunction}$ continuous. Define:
        \begin{align*}
            \hat Q(\ais,\action) &= \hat \cost(\ais, \action) + \discount\sum_{\sts'}\hat\transition(
            \sts'\vert \ais, \action)\hat \valuefunction(\hat f(\sts', \ais, \action).
        \end{align*}
        Then $\hat Q$ is $(L_{\hat \cost} + \discount L_{\hat \valuefunction} L_{\hat f} L_{\hat \transition})$-Lipschitz continuous.
    \end{lemma}
    \begin{proof}
        For any action $\action$
        \begin{align}
          \bigg \vert \hat Q(\ais_1, \action) -\hat Q(\ais_2, \action) \bigg\vert &\stackrel{(a)}{\leq} \bigg\vert \hat\cost(\ais_1, \action) - \hat\cost(\ais_2, \action) \bigg\vert + \discount \bigg\vert \sum_{\sts'}\hat \transition(\sts'\vert \ais_1, \action) \hat \valuefunction(\hat f(\sts',\ais_1, \action)) - \hat \transition(\sts' \vert \ais_2, \action) \hat \valuefunction(\hat f(\sts',\ais_2, \action)) \bigg\vert \\
          &\stackrel{(b)}{\leq} (L_{\hat\cost} + \discount L_{\hat \valuefunction} L_{\hat f} L_{\hat \transition}) d(\ais_1, \ais_2),
        \end{align}
        where $(a)$ due to triangle inequality, and $(b)$ follows form \Cref{def:lipdist}, \Cref{def:lipmdp}, and because $\Vert a \circ b \Vert_{L} \leq \Vert a\Vert_{L} \cdot \Vert b \Vert_{L}$.
    \end{proof}
    
    \begin{lemma}\label{lem:qv}
     Let $\hat Q: \aisspace \times \actionspace \to \real$ be $L_{\hat Q}$- Lipschitz continuous, Define
     \begin{align*}
         \hat \valuefunction(\ais) = \max_{\action_{\timestep} \in \actionspace}\hat Q(\ais, \action).
     \end{align*}
     Then $\hat \valuefunction$ is $L_{\hat Q}$ Lipschitz
    \end{lemma}
    \begin{proof}
    Consider $\ais_1, \ais_2 \in \aisspace$, and let $\action_1$ and $\action_2$ denote the corresponding optimal action. Then, 
        \begin{align}
            \hat \valuefunction(\ais_1) - \hat\valuefunction(\ais_2) &= \hat Q(\ais_1, \action_1) - \hat Q(\ais_2, \action_2)\\
            &\stackrel{(a)}{\leq} \hat Q(\ais_1, \action_2) - \hat Q(\ais_2, \action_2)\\
            &\stackrel{(b)}{\leq}L_{\hat Q}d(\ais_1, \ais_2),
        \end{align}
        By symmetry,
        \begin{align*}
             \hat\valuefunction(\ais_2) - \hat \valuefunction(\ais_1) &\leq L_{\hat Q}d(\ais_1, \ais_2).
        \end{align*}
        Therefore,
        \begin{align*}
            \bigg \vert  \hat \valuefunction(\ais_1) - \hat\valuefunction(\ais_2) \bigg\vert &\leq L_{\hat Q}d(\ais_1, \ais_2).
        \end{align*}
    \end{proof}
    
    \begin{lemma}\label{lem:infv}
        Consider the following dynamic program defined in \eqref{eq:ais-dp}:\footnote{We have added $\timestep$ as a subscript to denote the computation time \ie, the time at which the respective function is updated.}
         \begin{align*}
                \hat Q_\timestep(\ais_\timestep, \action_\timestep) &= \hat \cost(\ais_\timestep, \action_\timestep) + \discount \sum_{\sts_\timestep \in \statespace}
                \hat \transition(\sts_\timestep|\ais_\timestep,\action_\timestep) \hat \valuefunction(\hat{f}(\ais_\timestep,\sts_\timestep,\action_\timestep)), \ \forall \feature \in \featurespace, \action \in \actionspace \\
                \hat \valuefunction_\timestep(\ais_\timestep) &= \max_{\action \in \actionspace} \hat Q_\timestep(\ais_\timestep,\action_\timestep), \ \forall \feature \in \featurespace \label{eq:ais-dp-2}
            \end{align*}
        Then at any time $\timestep$, we have:
        \begin{align*}
            L_{\hat \valuefunction_{\timestep+1}} & = L_{\hat\cost} + \discount L_{\hat \transition}L_{\hat f}L_{\hat \valuefunction_\timestep}.
        \end{align*}
    \end{lemma}
    
    \begin{proof}
        We prove this by induction. At time $\timestep =1$ $\hat Q_{1}(\ais, \action) = \hat \cost(\ais, \action)$, therefore $ L_{\hat Q_{1}} = L_{\hat \cost}$. Then according to \Cref{lem:qv}, $\hat \valuefunction_1$ is Lipschitz with Lipschitz constant $L_{\hat \valuefunction_1} = L_{\hat Q_1} = L_{\hat\cost}$. This forms the basis of induction. Now assume that at time $\timestep$,  $\hat \valuefunction_{\timestep}$ is $L_{\hat \valuefunction_\timestep}$- Lipschitz. By \Cref{lem:lipq} $\hat Q_{\timestep+1}$ is $L_{\hat \cost} + \discount L_{\hat f}, L_{\hat \transition} L_{\hat \valuefunction_{\timestep}}$. Therefore by \Cref{lem:qv}, $\hat \valuefunction_{(\timestep+1)}$ is Lipschitz with constant:
        \begin{align*}
            L_{\hat\valuefunction_{\timestep+1}} &= L_{\hat \cost} + \discount L_{\hat f}L_{\hat \transition}L_{\hat \valuefunction_\timestep}.
        \end{align*}
    \end{proof}
    
    \begin{theorem}\label{thm:thmlipv}
        Given any $(L_{\hat\cost}, L_{\hat\transition})$- Lipschitz MDP, if $\discount L_{
        \hat \transition}L_{\hat f} \leq 1$, then the infinite horizon $\discount$-discounted value function $\hat \valuefunction$ is Lipschitz continuous with Lipschitz constant 
        \begin{align*}
            L_{\hat \valuefunction} &= \frac{L_{\hat \cost}}{1 - \discount L_{\hat f}L_{\hat \transition}}.
        \end{align*}
    \end{theorem}
    \begin{proof}
        Consider the sequence of $L_{\timestep} = L_{\hat \valuefunction_{\timestep}}$ values. For simplicity write $\alpha = \discount L_{\hat \transition}L_{\hat f}$. Then the sequence $\{L_{
        \timestep}\}_{\timestep \geq 1}$ is given by : $L_1 = L_{\hat \cost}$ and for $\timestep \geq 1$,
        \begin{align*}
            L_{\timestep+1} &= L_{\hat \cost} + \alpha L_{\timestep}, \\
            \text{Therefore,} \\
            L_{\timestep} &= L_{\hat \cost} + \alpha L_{\hat \cost}+ \ldots + \alpha_{\timestep+1} = \frac{1 - \alpha^{\timestep}}{1 - \alpha}L_{\hat\cost}.
        \end{align*}
        This sequence converges if $\vert \alpha \vert \leq 1$. Since $\alpha$ is non-negative, this is equivalent to $\alpha\leq 1$, which is true by hypothesis. Hence $L_\timestep$ is a convergent sequence. At convergence, the limit $L_{\hat \valuefunction}$ must satisfy the fixed point of the recursion relationship introduced in \Cref{lem:infv}, hence,
        \begin{align*}
            L_{\hat \valuefunction}&= L_{\hat\cost} + \discount L_{\hat f}L_{\hat \transition}L_{\hat \valuefunction}.
        \end{align*}
        Consequently, the limit is equal to,
        \begin{align*}
            L_{\hat \valuefunction} = \frac{L_{\hat \cost}}{1 - \discount L_{\hat f}L_{\hat \transition}}.
        \end{align*}
    \end{proof}
     \begin{corollary}
             If $\discount L_{
        \hat \transition}L_{\hat f} \leq 1$ and the function class $\mathfrak{F}$ is $\mathfrak{F}^{\text{W}}$, then $\Delta$ as defined in \eqref{eq:sup-v} is upper bounded as:
            \begin{align}
               \Delta
                \le  \frac{2\epsilon}{(1-\discount)} + \frac{2\discount\delta L_{\hat \cost} }{(1- \discount)(1-\discount L_{\hat f}L_{\hat\transition})},
            \end{align}
        \end{corollary}
        \begin{proof}
            The proof follows from the observation that for $d_{\mathfrak{F}^{\text{W}}}$, $\rho_{\mathfrak{F}^{\text{W}}}$ = $L_{\hat \valuefunction}$, and then using the result from \Cref{thm:thmlipv}.
        \end{proof}
        
\section{Algorithmic Details}
    \subsection{Choice of an IPM:}
        \subsubsection{MMD}\label{sec:mmd-details}
            One advantage of choosing $\ipm$ as the MMD distance is that unlike the Wasserstein distance, its computation does not require solving an optimisation problem. Another advantage is that we can leverage some of their properties to further simplify our computation, as follows:
        
            \begin{proposition}[Theorem 22 ~\citep{sejdinovic}] \label{prop-ipm1}
                Let $\mathcal{X} \subseteq \real^{m}$, and $d_{\mathcal{X},p}: \mathcal{X}\times \mathcal{X} \to \real$ be a metric given by $d_{\mathcal{X},p}(x,x') = \Vert x - x'\Vert^{p}_{2}$, for $p \in (0,2]$. Let $k_p :\mathcal{X} \times \mathcal{X} \to \real$ be any kernel given:
                \begin{align}
                    k_p (x,x') &= \frac{1}{2}(d_{\mathcal{X},p}(x,x_0) + d_{\mathcal{X},p}(x',x_0) - d_{\mathcal{X},p}(x,x')),
                \end{align}
                where $x_0 \in \mathcal{X}$ is arbitrary, and let $\mathcal{U}_p$ be a RKHS kernel with kernel $k_p$ and $\mathfrak{F}_p = \{f \in \mathcal{U}_p : \Vert f \Vert_{\mathcal{U}_p} \geq 1  \}$. Then for any distributions $\nu_1$, $\nu_2 \in \Delta{\mathcal{X}}$, the IPM can be expressed as:
              \begin{align}
                    \ipm(\nu_1,\nu_2) &= \bigg(\expecun{}[d_{\mathcal{X},p}(X_1, W_1)] - \frac{1}{2}\expecun{}[d_{\mathcal{X},p}(X_1, X_2)] - \frac{1}{2}\expecun{}[d_{\mathcal{X},p}(W_1, W_2)]\bigg)^{\frac{1}{2}},\label{eq:prop-ipm1}
                \end{align}
                where $X_1,X_2$, and $W_1,W_2$ are i.i.d. samples from $\nu_1$ and $\nu_2$ respectively.
            \end{proposition}
            The main implication of \Cref{prop-ipm1} is that, instead of using \eqref{eq:prop-ipm1}, for $p\in (0,2]$ we can use the following as a surrogate for $d_{\mathfrak{F}_p}$:
                \begin{align}
                     \int_\mathcal{X}\int_\mathcal{X}\Vert x_1 - w_1 \Vert_{2}^{p}\nu_1(dx_1)\nu_2(dw_1) - \frac{1}{2}\int_\mathcal{X}\int_\mathcal{X}\Vert w_1 - w_2 \Vert_{2}^{p}\nu_{2}(dw_1)\nu_{2}(dw_1). \label{eq:ipm-surrogate}
                \end{align}
                Moreover, according to \citet{Sriperumbudur} for n identically and independently distributed (i.i.d) samples $\{X_i\}_{i=0}^{n} \sim \nu_1$ an unbiased estimator of \eqref{eq:ipm-surrogate} is given as:
                \begin{align}
                    \frac{1}{n}\sum_{i=1}^{n}\int_\mathcal{X}\Vert X_i -w_1\Vert_{2}^{p}\nu_1 d(w_1)  - \frac{1}{2}\int_\mathcal{X}\int_\mathcal{X}\Vert w_1 - w_2 \Vert_{2}^{p}\nu_1(dw_1)\nu_2(dw_2). \label{eq:ipm-surrogate-2}
                \end{align}
            
            We implement a simplified version of the surrogate loss in \eqref{eq:ipm-surrogate-2} as follows:
            \begin{proposition}[~\citep{ais-1}]\label{prop-imp2}
            %  \begin{proposition}\label{prop-imp2}
                Given the setup in \cref{prop-ipm1} and $p=2$, Let $\nu_2(\aisparams)$ be a parametric distribution with mean $m$ and let $X \sim \nu_1$, then the gradient $\grad_\aisparams (m_\aisparams - 2X)^{\top}m_\aisparams$ is an unbiased estimator of $\grad_\aisparams d_{\mathfrak{F}_{2}}(\alpha, \nu_\aisparams)^{2}$
            \end{proposition}
            \begin{proof}
                Let $X_1,X_2 \sim \nu_1$, and $W_1,W_2 \sim \nu_2(\aisparams)$
                \begin{align}
                    \therefore \grad_{\aisparams} d_{\mathfrak{F}_2}(\nu_1,\nu_2(\aisparams))^2 &= \grad_{\aisparams} \bigg[\expecun{}\Vert X_1 - W_1 \Vert_{2}^{2} - \frac{1}{2}\expecun{}\Vert X_1 - X_2 \Vert_{2}^{2} - \frac{1}{2}\expecun{}\Vert W_1 - W_2 \Vert_{2}^{2}\bigg ]\\
                    &\stackrel{(a)}{=}\grad_\aisparams \bigg[ \expecun{}\Vert W_1\Vert_{2}^{2} - 2\expecun{}\Vert X_1\Vert^{\top}\expecun{}\Vert W_1 \Vert \bigg]\label{eq:mmd-grad}, 
                \end{align}
                where $(a)$ follows from the fact that $X$ does not depend on $\aisparams$, which simplifies the implementation of the MMD distance. 
            \end{proof}
          In this way we can simplify the computation of $d_\mathfrak{F}$ using a parametric stochastic kernel approximator and MMD metric.
           
          Note that when are trying to approximate a continuous distribution we can readily use the loss function \eqref{eq:mmd-grad} as long as the mean $m_\aisparams$ of $\nu_2(\aisparams)$ is given in closed form. The AIS loss is then given as:
          \begin{align}
                 \aisloss(\aisparams) &= \frac{1}{\Timestep}\sum_{t = 0}^{\Timestep}\bigg( \lambda (f_{\hat{\cost}}(\Ais_{\timestep}, \Action_\timestep; \aisparams)  - \cost(\State_\timestep, \Action_\timestep))^{2} + (1-\lambda)(m^{\State_\timestep}_{\aisparams} - 2\State_\timestep)^{\top}m^{\State_\timestep}_{\aisparams}  \bigg),\label{eq:mmd-ais-loss-2}
            \end{align}
            where $m^{\State_\timestep}_{\aisparams}$ is obtained using the from the transition approximator, ~\ie, the mapping $f_{\hat\transition}(\aisparams): \aisspace \times \actionspace \to \real$.
        
        \subsubsection{Wasserstein Distance}\label{sec:wass-details}
        The the KL-divergence between two densities $\nu_1$ and $\nu_2$ on for any $X \in \mathcal{X} \subset \real^{m} $ is defined as:
        \begin{align}
            d_{\text{KL}}(\nu_1 \Vert \nu_2) &= \int_{\mathcal X} \log(\nu_1(x))\nu_1(dx) - \int_{\mathcal X} \log(\nu_2(x))\nu_1(dx)
        \end{align}
        Moreover, if $\mathcal{X}$ is bounded space with diameter $D$, then the relation between the Wasserstein distance $d_{\mathfrak{F}^{\text{W}}}$, Total variation distance $d_{\mathfrak{F}^{\text{TV}}}$, and the KL divergence is given as :
        \begin{align}
            d_{\mathfrak{F}^{\text{W}}}(\nu_1, \nu_2)\leq D d_{\mathfrak{F}^{\text{TV}}}(\nu_1, \nu_2)\stackrel{(a)}{\leq} \sqrt{2d_{\text{KL}}(\nu_1\Vert \nu_2)},
        \end{align}
        where, $(a)$ follows from the Pinsker's inequality. Note that in \eqref{eq:pgt-loss} we use $d_{\mathfrak{F}}^{2}$. Therefore, we can use a (simplified) KL-divergence based surrogate objective given as:
        \begin{align}
            \int_{\mathcal{X}}\log(\nu_2(x;\aisparams)) \nu_1(dx),
        \end{align}
        where we have dropped the terms which do not depend on $\aisparams$. Note that the above expression is same as the cross entropy between $\nu_1$ and $\nu_2$ which can be effectively computed using samples.
        In particular, if we get $\Timestep$ i.i.d samples from $\nu_1$, then, 
        \begin{align}
            \frac{1}{\Timestep}\sum_{i=0}^{\Timestep}\log(\nu_2(x_i;\aisparams))\label{eq:klwass}
        \end{align}
        is an unbiased estimator of $\int_{\mathcal X}\log(\nu_2(x;\aisparams)) \nu_1(dx)$.
        
        The KL divergence based AIS loss is then given as:
          \begin{align}
                \aisloss(\aisparams) &= \frac{1}{\Timestep}\sum_{t = 0}^{\Timestep}\bigg( \lambda (f_{\hat{\cost}}(\Ais_{\timestep}, \Action_\timestep; \aisparams)  - \cost(\State_\timestep, \Action_\timestep))^{2} + (1-\lambda)\log(\hat \transition(\State_\timestep;\aisparams))  \bigg),\label{eq:w-ais-loss-2}
            \end{align}
            
    %   \subsection{Actor Critic Algorithm} \label{sec:AC}
    %     As mentioned previously, similar to \Cref{sec:pgt} we can also design an AIS-based Actor-Critic algorithm. As such, in addition to a parameterised policy $\policy(\cdot; \actorparams)$ and AIS generator $(\aisfunction_\timestep(\cdot;\aisparams), \hat f, \hat r, \hat\transition)$ we can also a parameterised critic $\hat\valuefunction(\cdot;\criticparams):\featurespace \to \real$, where $\criticparams$ are the parameters for the critic. The performance of policy $\mu(\cdot;\actorparams)$ is then given by $\performance(\actorparams, \aisparams, \criticparams)$. According to policy gradient theorem~\citep{pgt,baxter-bartlett} the gradient of $\performance(\actorparams, \aisparams, \criticparams)$, is given as:
    %     \begin{align}
    %         \grad_\actorparams \performance(\actorparams, \aisparams, \criticparams) &= \expecun{}\bigg[ \grad_{\actorparams}\log(\mu(\cdot;\actorparams))\hat{\valuefunction}(\cdot;\criticparams)\bigg].
    %     \end{align}
    %     And for a trajectory of length $\Timestep$, we approximate it as:
    %     \begin{align}
    %         \hat{\grad}_\actorparams \performance(\actorparams, \aisparams, \criticparams) &= \frac{1}{\Timestep}\sum_{\timestep =1}^{\Timestep}\bigg[ \grad_{\actorparams}\log(\mu(\cdot;\actorparams))\hat{\valuefunction}(\cdot;\criticparams)\bigg].
    %     \end{align}
    %     The parameters $\criticparams$ can be learnt by optimising the temporal difference loss given as:
    %     \begin{align}
    %         \loss_{\text{TD}}(\actorparams, \aisparams, \criticparams) &= \frac{1}{\Timestep}\sum_{\timestep=0}^{\Timestep}\texttt{smoothL1}(\hat{\valuefunction}(\Feature_\timestep;\criticparams) - \cost(\Feature_\timestep,\Action_\timestep) - \discount \hat{\valuefunction}(\Feature_{\timestep+1};\criticparams)).
    %     \end{align}
    %      The parameters $\{(\aisparams_i, \actorparams_i, \criticparams_i  )\}_{i \geq 1}$ can then be updated using a multi-timescale stochastic approximation algorithm as follows:
    %      \begin{subequations}\label{eq:ac-update}
    %         \begin{align}
    %             \aisparams_{i+1} &= \aisparams_i + \aislr_i \grad_\aisparams\aisloss(\aisparams_i)\label{eq:ac-ais-update},\\
    %             \criticparams_{i+1} &= \criticparams_i + \criticlr_i \grad_{\criticparams}\loss_{\text{TD}}(\actorparams_i, \aisparams_i, \criticparams_i)\label{eq:ac-critic-update},\\
    %             \actorparams_{i+1} &= \actorparams_i + \actorlr_i \hat{\grad}_{\actorparams}\performance(\actorparams_{i},\aisparams_{i},\criticparams)\label{eq:ac-actor-update},
    %         \end{align}
    %      \end{subequations}
    %       where the step-size $\{\aislr_{i}\}_{i \geq 0}$,  $\{\criticlr_{i}\}_{i \geq 0}$ and $\{\actorlr_{i}\}_{i \geq 0}$ satisfy the standard conditions $\sum_{i} \aislr_{i} = \infty$, $\sum_{i}\aislr_{i}^{2} < \infty$, $\sum_{i} \criticlr_{i} = \infty$, $\sum_{i}\criticlr_{i}^{2} < \infty$, $\sum_{i} \actorlr_{i} = \infty$ and $\sum_{i}\actorlr_{i}^{2}< \infty$ respectively. Moreover, one can ensure that the AIS generator converges first, followed by the critic and the actor by choosing an appropriate step-sizes such that, $\lim_{i \to \infty} \frac{\actorlr_{i}}{\aislr_{i}} = 0$ and $\lim_{i \to \infty} \frac{\criticlr_{i}}{\actorlr_{i}} = 0$.
    
    
    % \subsection{Convergence analysis}\label{sec:convergence}
    
    % In this section we will discuss the convergence of the AIS-based policy gradient in \Cref{sec:pgt} as well as Actor-Critic algorithm presented in the previous subsection. The proof of convergence relies on multi-timescale stochastic approximation \citet{borkar2008stochastic} under conditions similar to the standard conditions for convergence of policy gradient algorithms with function approximation stated below, therefore it would suffice to provide a proof sketch.

    % \noindent\begin{assumption} \label{assumption-1}
    %     \begin{enumerate}
    %         \item \label{a.1.1}The values of step-size parameters $\aislr, \actorlr$ and $\criticlr$ (for the actor critic algorithm) are set such that the timescales of the updates for $\aisparams$, $\actorparams$, and $\criticparams$ (for Actor-Critic algorithm) are separated, ~\ie, $\aislr_{\timestep} \gg \actorlr_{\timestep}$, and for the Actor-Critic algorithm $\aislr_{\timestep} \gg \criticlr_\timestep \gg \actorlr_{\timestep}$, $\sum_{i} \aislr_{i} = \infty$, $\sum_{i}\aislr_{i}^{2} < \infty$, $\sum_{i} \criticlr_{i} = \infty$, $\sum_{i}\criticlr_{i}^{2} < \infty$, $\sum_{i} \actorlr_{i} = \infty$ and $\sum_{i}\actorlr_{i}^{2}< \infty$, $\lim_{i \to \infty} \frac{\actorlr_{i}}{\aislr_{i}} = 0$ and $\lim_{i \to \infty} \frac{\criticlr_{i}}{\actorlr_{i}} = 0$, 
    %         \item \label{a.1.2}The parameters $\aisparams$, $\actorparams$ and $\criticparams$ (for Actor-Critic algorithm) lie in a convex, compact and closed subset of Euclidean spaces.
    %         \item \label{a.1.3}The gradient $\grad_{\aisparams}\aisloss$ is Lipschitz in $\aisparams_{\timestep}$, and $\hat \grad_{\actorparams}\performance(\actorparams,\aisparams)$ is Lipschitz in $\actorparams_{\timestep},~\text{and}~\aisparams_{\timestep}$. Whereas for the Actor-Critic algorithm the gradient of the TD loss $ \grad_{\criticparams}\loss_{\text{TD}}(\aisparams, \actorparams, \criticparams)$ and the policy gradient $\hat \grad_{\actorparams} \performance(\aisparams, \actorparams, \criticparams)$ is Lipschitz in $(\aisparams_\timestep, \actorparams_\timestep, \criticparams_\timestep)$.
    %         \item \label{a.1.4}All the estimates of all the gradients $\grad_{\aisparams}\aisloss$, $\grad_{\actorparams}\performance(\actorparams,\aisparams)$, $ \grad_{\criticparams}\loss_{\text{TD}}(\aisparams, \actorparams, \criticparams)$ and are unbiased with bounded variance\footnote{This assumption is only satisfied in tabular MDPs.}.
    %     \end{enumerate}
    % \end{assumption}

    %     \begin{assumption} \label{assumption-2}
    %         \begin{enumerate}
    %             \item \label{a.2.1}The ordinary differential equation (ODE) corresponding to \eqref{eq:actor-update} is locally asymptotically stable.
    %             \item \label{a.2.2}The ODEs corresponding to \eqref{eq:ais-update} is globally asymptotically stable.
    %             \item For the Actor-Critic algorithm, the ODE corresponding to \eqref{eq:ac-critic-update} is globally asymptotically stable and has a fixed point which is Lipschitz in $\actorparams$.
    %         \end{enumerate}
    %     \end{assumption}
    %     \begin{theorem}\label{thm:convergence}
    %     Under \cref{assumption-1,assumption-2}, along any sample path, almost surely we have the following:
    %     \begin{enumerate}
    %         \item The iteration for $\aisparams$ in \eqref{eq:ais-update} converges to an AIS generator that minimises the $\aisloss$.
    %         \item The iteration for $\actorparams$ in \eqref{eq:actor-update} converges to a local maximum of the performance $\performance(\aisparams^\star,\actorparams)$ where $\aisparams^\star$, and $\criticparams^\star$ (for Actor Critic) are the converged value of $\aisparams$, $\criticparams$.
    %         \item For the Actor-Critic algorithm the iteration for $\criticparams$ in \eqref{eq:ac-critic-update} converges to critic that minimises the error with respect to the true value function.
    %     \end{enumerate}
    %     \end{theorem}
    %     \begin{proof}
    %     The proof for this theorem follows the technique used in \citep{Leslie2004ReinforcementLI,borkar2008stochastic}. Due to the specific choice of learning rate the AIS-generator is updated at a faster time-scale than the actor, therefore it is ``quasi static'' with respect to to the actor while the actor observes a ``nearly equilibriated'' AIS generator. Similarly in the case of the Actor-Critic algorithm the AIS generator observes a stationary critic and actor, whereas the critic and actor see ``nearly equilibriated'' AIS generator. The Martingale difference condition (A3) of \citet{borkar2008stochastic} is satisfied due to \cref{a.1.4} in \cref{assumption-1}. As such since our algorithm satisfies all the four conditions by \citep[page35]{Leslie2004ReinforcementLI}, \citep[Theorem 23]{Borkar1997StochasticAW}, the result then follows by combining the theorem on \citep[page 35]{Leslie2004ReinforcementLI}\citep[Theorem 23]{borkar2008stochastic} and \citep[Theorem 2.2]{Borkar1997StochasticAW}.
    %     \end{proof}

        
\section{Experimental Details}\label{sec:experiment-details}
    \begin{table}[!htbp]
        \begin{center}
            \begin{tabular}{|c|l|c|}
                \hline
                \multirow{9}{*}{Common}&Optimiser& Adam \\
                & Discount Factor $\discount$ & 0.99 \\
                % & GAE parameter $\lambda$& 0.95\\
                &Inital standard deviation for the policy &0.0\\
                & PPO-Epochs & 12\\
                & Clipping Coefficient & 0.2\\
                & Entropy-Regulariser& 0\\
                & Batch Size & 512\\
                & Episode Length & 2048\\
                \hline
                \multirow{3}{*}{AIS generator}& History Compressor& GRU \\
                & Hidden layer dimension & 256 \\
                & Step size & 1.5e-3\\
                & $\lambda$ & 0.3\\
                \hline
                \multirow{3}{*}{Actor}
                & Step size & 3.5e-4\\
                & No of hidden layers& 1 \\
                & Hidden layer Dimension& 32\\
                % \hline
                % \multirow{3}{*}{Critic} 
                % & Step size & 1.5e-3\\
                % & No of hidden layers& 1 \\
                % & Hidden layer Dimension& 32\\
                \hline
            \end{tabular}
        \end{center}
        \caption{Hyperparameters}\label{tab:hyperparams}
    \end{table}
    
   
        
    \subsection{Environments}
         Our algorithms are evaluated on MuJoCo~\citep[mujoco-py version 2.0.2.9 ]{Todorov2012MuJoCoAP} via OpenAI gym~\citep[version 0.17.1]{gym} interface, using the v2 environments. The environment, state-space, action space, and reward function are not modified or pre-processed in any way for easy reproducibility and fair comparison with previous results. Each environment runs for a maximum of 2048 time steps or until some termination condition and has a multi-dimensional action space with values in the range of (-1, 1), except for Humanoid which uses the range of (-0.4, 0.4). 
         
         
    \subsection{Hyper-parameters}
        \Cref{tab:hyperparams} contains all the hyper-parameters used in our experiments.
        Both the policy and AIS networks are trained with Adam optimiser~\citep{adam}, with a batch size of 512. We follow~\citet{Raichuk2021WhatMF}'s recommended protocol for training on-policy policy based methods, and perform 12 PPO updates after every policy evaluation subroutine. To ensure separation of time-scales the step size of the AIS generator and the policy network is set to $1.5\text{e}^{-3}$ and $3.5\text{e}^{-4}$ respectively. Hyper-parameters of our approach are searched over a grid of values, but an exhaustive grid search is not carried out due to prohibitive computational cost. We start with the recommended hyper-parameters for the baseline implementations and tune them further around promising values by an iterative process of performing experiments and observing results. 
        
        For the state-based RNN baseline we have tuned the learning rate over a grid of values starting from 1e-4 to 4e-4 and settled on 3.5e-4 as it achieved the best performance. Similarly the hidden layer size set to 256 as it is observed to achieve best performance. For the feed-forward baselines we use the implementation by OpenAI baselines~\citep{baselines} with their default hyper-parameters. 
        
        % \subsection{Modifications to baselines and their hyper-parameters}
        % Note that the methods in \citep{Dreamer, Pla-Net} are designed for pixel-based control tasks and cannot be readily used for continuous control tasks in this paper. To help them process real-valued state vectors, we replace the convolutional and deconvolutional layers in their architectures by fully connected layers. We observed that feed-forward layers of size 256 for PlaNET~\citep{Pla-Net}, and 300 for Dreamer~\citep{Dreamer} produced the best results for both the methods. For PlaNET~\citep{Pla-Net} we used the default hyper-parameters and varied the learning rate over a grid of values starting from 1e-4 to 4e-4. We observed that this method achieved best performance at the learning rate of 3e-3. For Dreamer~\citep{Dreamer} we used the default hyper-parameters and varied the learning rate over a grid of values starting from 6e-4, 6e-5 and 6e-5 respectively to 1e-3, 1e-4, 1e-4 respectively. We observed that this method achieved the best performance at 7.5e-4, 9e-5 and 8.5e-5 respectively. 
        
        % The architecture proposed in this paper is similar to the VariBAD~\citep{VariBad} method proposed by \citeauthor{VariBad}. The main motivation of \citeauthor{VariBad} was evaluate VariBAD for multi-task RL, however we believe that the specifics of this method are relevant to both the ideas presented in this paper and the single-task RL setup. In our experiments we use the code base provided by \citeauthor{VariBad} without any modifications and observe that the method performs on-par with several other baselines. 
        
        % The performance of the proposed approach on the remaining MuJoCo environments is shown in \cref{fig:comb-results-3}.
        % \begin{figure*}[!htbp]
        %     \includegraphics[width=\linewidth]{Results/combined-3.pdf}
        %     \caption{Additional Results averaged over 50 runs with $\pm$1 \texttt{std-dev}} \label{fig:comb-results-3}
        % \end{figure*}
        
    % \subsection{Type of MMDs}
    %     The MMD distance given by \eqref{eq:mmd-grad} in \Cref{sec:mmd-details}, can be computed using different types of characteristic kernels (for a detailed review see ~\citep{Sriperumbudur,NIPS2009_685ac8ca,sejdinovic}). In this paper we consider computing \eqref{eq:mmd-grad} using the Laplace, Gaussian and energy distance kernels. The performance of the proposed approach under different kernels is shown in \cref{fig:MMD-res}. It can be observed that for the continuous control tasks in the MuJoCo suite, the energy distance yields better performance, and therefore we implement \cref{eq:mmd-grad} using the energy distance for all the experiments.
        
    %     \begin{figure*}[!htbp]
    %         \includegraphics[width=\linewidth]{Results/MMD.pdf}
    %         \caption{Comparison of different MMDs, averaged over 50 runs with $\pm$1 \texttt{std-dev}} \label{fig:MMD-res}
    %     \end{figure*}
        
    % \subsection{MMD vs KL}
    %     Next, we compare the performance of our method under MMD (energy distance)-based AIS loss in \eqref{eq:mmd-ais-loss} and KL-based AIS loss given in \eqref{eq:w-ais-loss}. From \cref{fig:Wass-res}, one can observe that for the Mujoco tasks, MMD-based loss leads to better performance.
    
       
        
    %     \begin{figure*}[!htbp]
    %         \includegraphics[width=\linewidth]{Results/wass.pdf}
    %         \caption{Comparison of Wasserstein vs MMDs, averaged over 50 runs with $\pm$1 \texttt{std-dev}} \label{fig:Wass-res}
    %     \end{figure*}
        
    % \subsection{Modifications to algorithms}
        
%     \section{The Machine Learning Reproducibility Checklist}
%     \begin{compactitem}
%         \item[1.] A clear description of the mathematical setting, algorithm, and/or model: Yes
%         \item[2.] A clear explanation of any assumptions: Yes
%         \item[3.] An analysis of the complexity (time, space, sample size) of any algorithm: No, however analysis of approximation error is provided.
%         \item[4.] A clear statement of the claim: Yes
%         \item[5.] A complete proof of the claim: Yes
%         \item[6.] The relevant statistics, such as number of examples: Yes
%         \item[7.] The details of train / validation / test splits: Not Applicable
%         \item[8.] An explanation of any data that were excluded, and all pre-processing step: Not Applicable
%         \item[9.] A link to a downloadable version of the dataset or simulation environment: Not Applicable
%         \item[10.] For new data collected, a complete description of the data collection process, such as
% instructions to annotators and methods for quality control: Not Applicable
%         \item[11.] Training code: No, code will be made public after the paper is accepted or in the arxiv version.
%         \item[12.] The range of hyper-parameters considered, method to select the best hyper-parameter
% configuration, and specification of all hyper-parameters used to generate results]: Yes
%         \item[13.] The exact number of training and evaluation runs: Yes
%         \item[14.] A clear definition of the specific measure or statistics used to report results: Yes
%         \item[15.] A description of results with central tendency (e.g. mean) \& variation (e.g. error bars): Yes
%         \item[16.] The average run-time for each result, or estimated energy cost: No.
%         \item[17.] A description of the computing infrastructure used: No.
%     \end{compactitem}



          

        