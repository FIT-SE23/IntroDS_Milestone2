\section{Background and Motivation}\label{sec:background}
        \begin{figure*}[!ht]
          \centering
          \begin{minipage}{0.75\linewidth}
          \subfigure[$P(0)$]{
            \begin{tikzpicture}[thick,scale=0.65]
              \node [agent] at (0, 0) (0) {$0$};
              \node [agent] at (1, 0) (1) {$1$}; 
              \node [agent] at (0, 1) (3) {$3$};
              \node [agent] at (1, 1) (2) {$2$};
              \path[->]
                    (0) edge node[below] {$0.5$} (1)
                    (1) edge node[right] {$0.5$} (2)
                    (2) edge node[above] {$0.5$} (3)
                    (3) edge node[left]  {$0.5$} (0)
                    (0) edge[loop left] node {$0.5$} (0)
                    (1) edge[loop right] node {$0.5$} (1)
                    (2) edge[loop right] node {$0.5$} (2)
                    (3) edge[loop left] node {$0.5$} (3)
                    ;
            \end{tikzpicture}}
           \hfill
          \subfigure[$P(1)$]{
            \centering
            \begin{tikzpicture}[thick,scale=0.75]
              \node [agent] at (0, 0) (0) {$0$};
              \node [agent] at (1, 0) (1) {$1$}; 
              \node [agent] at (0, 1) (3) {$3$};
              \node [agent] at (1, 1) (2) {$2$};
              \path[<-]
                    (0) edge node[below] {$0.5$} (1)
                    (1) edge node[right] {$0.5$} (2)
                    (2) edge node[above] {$0.5$} (3)
                    (3) edge node[left]  {$0.5$} (0)
                    (0) edge[loop left] node {$0.5$} (0)
                    (1) edge[loop right] node {$0.5$} (1)
                    (2) edge[loop right] node {$0.5$} (2)
                    (3) edge[loop left] node {$0.5$} (3)
                    ;
            \end{tikzpicture}}
            \hfill
            \subfigure[$P(2)$]{%
                  \begin{tikzpicture}[thick,scale=0.65]
                  \node [agent] at (0, 0) (0) {$0$};
                  \node [agent] at (1, 0) (1) {$1$}; 
                  \node [agent] at (0, 1) (3) {$3$};
                  \node [agent] at (1, 1) (2) {$2$};
                  \path[->]
                        (0) edge [bend right] node[below] {$0.5$} (1)
                        (1) edge [bend right] node[right] {$0.5$} (2)
                        (2) edge [bend right] node[above] {$0.5$} (3)
                        (3) edge [bend right] node[left]  {$0.5$} (0)
                        (0) edge [bend right] node[right] {} (3)
                        (3) edge [bend right] node[below] {} (2)
                        (2) edge [bend right] node[left]  {} (1)
                        (1) edge [bend right] node[above]  {} (0);
                \end{tikzpicture}}
                \caption{The transition probability model for the example of}
          \label{fig:P}
          \end{minipage}%
          \begin{minipage}{0.25\linewidth}
          \centering
            \begin{tikzpicture}[thick,scale=0.9]
              \node [agent] at (0, 0) (0) {$0$};
              \node [agent] at (1, 0) (1) {$1$}; 
              \node [agent] at (0, 1) (3) {$3$};
              \node [agent] at (1, 1) (2) {$2$};
              \path[->]
                    (0) edge node[below] {$0.5$} (1)
                    (1) edge[bend right] node[right] {$0.5$} (2)
                    (2) edge[bend right]  node[left] {$0.5$} (1)
                    (3) edge node[left]  {$0.5$} (0)
                    (3) edge node[above] {$0.5$} (2)
                    (0) edge[loop left] node {$0.5$} (0)
                    (1) edge[loop right] node {$0.5$} (1)
                    (2) edge[loop right] node {$0.5$} (2)
                    (3) edge[loop left] node {$0.5$} (3)
                    ;
            \end{tikzpicture}
          \caption{The transition probability of the optimal policy}
          \label{fig:optimal}
          \end{minipage}
        \end{figure*}

    % \subsection{Function approximation in MDPs}
    
        Consider an MDP $ \mdp = \langle \statespace, \actionspace, \transition, \cost, \discount \rangle$ where $\statespace$ denotes the state space, $\actionspace$ denotes the action space, $\transition$ denotes the controlled transition matrix, $\cost \colon \statespace \times \actionspace \to \real$ denotes the per-step reward, and $\discount \in (0,1)$ denotes the discount factor.
                
        The performance of a randomised (and possibly history-dependent) policy $\policy$
        starting from a start state $\sts_{0}$ is measured by the value function, defined as:
        \begin{equation}
          \valuefunction^{\policy}(\sts_0) = \expecun{}^{\policy}\bigg[\sum_{\timestep=0}^{\infty}\discount^{\timestep}\cost(\State_{\timestep},\Action_{\timestep}) \bigg| \State_{0} = \sts_{0}\bigg].
        \end{equation}
        A policy maximising $\valuefunction^{\policy}(\sts_0)$ over all (randomized and possibly history dependent) policies is called the optimal policy with respect to initial state $s_0$ and is denoted by $\policy^{\star}$\footnote{This notion can easily be extended to start state distributions.}.
                
        In many applications, $\statespace$ and $\actionspace$ are combinatorially large or uncountable, which makes it intractable to compute the optimal policy. 
        Most practical RL algorithms overcome this hurdle by using function approximation where the state is mapped to a feature space $\featurespace$ using a state abstraction function $\basis:\statespace \to \featurespace$. In Deep-RL algorithms, the last layer of the network is often viewed as a feature vector. These feature vectors are then used as an approximate state for approximating the value function $\hat\valuefunction: \featurespace \to \real$ and/or computing an approximately optimal policy $\mu:\featurespace \to \Delta(\actionspace)$~\citep{Sutton+Barto:1998} (where $\Delta(\actionspace)$ denotes the set of probability distribution over actions). We use $\tilde{\mu} = \mu \circ \basis$ ~\ie, $\tilde{\mu} = \mu(\phi(\cdot))$ to denote the ``flattened" policy that maps states to action distributions.
        
        % The features $\Feature_\timestep$ are then used as an approximate state for computing the value function $\hat\valuefunction: \featurespace \to \real$ and/or the policy $\mu:\featurespace \to \Delta(\actionspace)$~\citep{Sutton+Barto:1998} (where $\Delta(\actionspace)$ denotes the set of probability distribution over actions). We denote by $\mu \circ \phi $ as the ``flattened" policy that maps states to action distributions, by composing the feature abstraction and the policy acting on it. In Deep-RL algorithms, the output of the last layer of the network is often viewed as the feature vector $\Feature_{\timestep}$.
        
        % A well known yet often overlooked fact about function approximation is that the features used as a proxy state may not satisfy the controlled Markov property \ie, in general,
        A well known fact about function approximation is that the features that are used as an approximate state may not satisfy the controlled Markov property \ie, in general, 
        \[
            \prob(\Feature_{\timestep+1} \mid \Feature_{1:\timestep}, \Action_{1:\timestep}) \neq
            \prob(\Feature_{\timestep+1} \mid \Feature_\timestep, \Action_\timestep).
        \]

        To see the implications of this fact, consider the toy MDP depicted in Fig.~\ref{fig:P}, with $ \statespace = \{0, 1, 2, 3\}$, $\actionspace =
        \{0, 1, 2\}$, $\{\transition_{\sts, \sts'}(\action)\}_{\action \in \actionspace}$, and $r(0) = r(1) = -1$, $r(2) = 1$, $r(3) = -K$, where $K$ is a large positive number.
       Given the reward structure the objective of the policy is to try to avoid state~$3$ and keep the agent at state~$2$ as much as possible. It is easy to see that the optimal policy is 
            \[
              \pi^\star(0) = 0, \quad
              \pi^\star(1) = 0, \quad
              \pi^\star(2) = 1,
              \text{ and}\quad
              \pi^\star(3) = 2.
            \]
        The Markov chain induced by the optimal policy is shown in Fig.~\ref{fig:optimal}. 
        Note that if the initial state is not state~$3$ then an agent will never visit that state under the optimal policy. Furthermore, any policy which cannot prevent the agent from visiting state~$3$ will have a large negative value and, therefore, cannot be optimal.
        Now suppose the feature space $\featurespace = \{0, 1\}$. It is easy to see that for any Markovian feature-abstraction $\policyencoder{} \colon \statespace \to \featurespace$, no policy $\hat \pi \colon \featurespace \to \actionspace$ can prevent the agent from visiting state~$3$. Thus, the best policy when using Markovian feature abstraction will perform significantly worse than the optimal policy (which has direct access to the state).
    
        However, it is possible to construct a history-based feature-abstraction  $\policyencoder$ and a history-based control policy $\hat \pi$ that works with $\phi$ and is of the same quality as $\pi^\star$. For this, consider the following \emph{codebooks} (where the entries denoted by a dot do not matter):
        \begin{align*}
          F(1) &= \begin{bmatrix}
            0 & 1 & \cdot & \cdot \\
            \cdot & 0 & 1 & \cdot \\
            \cdot & \cdot & 0 & 1 \\
            1 & \cdot & \cdot & 0
          \end{bmatrix}, 
          &
          F(2) &= \begin{bmatrix}
            1 & \cdot & \cdot & 0 \\
            0 & 1 & \cdot & \cdot \\
            \cdot & 0 & 1 & \cdot \\
            \cdot & \cdot & 0 & 1
          \end{bmatrix} ,
           \\
          F(3) &= \begin{bmatrix}
            \cdot & 0 & \cdot & 1 \\
            0 & \cdot & 1 & \cdot \\
            \cdot & 0 & \cdot & 1 \\
            0 & \cdot & 1 & \cdot
          \end{bmatrix},
          &
          D(0) &= \begin{bmatrix}
            0 & 1 \\
            1 & 2 \\
            2 & 3 \\
            3 & 0 
          \end{bmatrix} ,
          \\
          D(1) &= \begin{bmatrix}
            3 & 0 \\
            0 & 1 \\
            1 & 2 \\
            2 & 3 
          \end{bmatrix} ,
          &
          D(2) &= \begin{bmatrix}
            1 & 3 \\
            0 & 2 \\
            1 & 3 \\
            0 & 2 
          \end{bmatrix} .
        \end{align*}
        Now consider the following feature-abstraction:
        \[
          Z_t =  F_{S_{t-1}, S_t}(A_{t-1}),
        \]
        and a control policy $\mu$ which is a finite state machine with memory $M_t$ that is updated as:
        \[
          M_t = D_{M_{t-1}, Z_t}(A_{t-1})
        \]
        and chooses the actions according to:
        \[
          \quad
          A_t = \pi(M_t),
        \]
        where $\pi \colon \statespace \to \Delta(\actionspace)$ is any pre-specified reference policy. It can be verified that if the system starts from a known initial state then $\mu \circ \basis = \pi$. 
         Thus, if we choose the reference policy $\pi=\pi^\star$, then
        the agent will never visit state~$3$ under $\mu \circ \basis$, in contrast to Markovian feature-abstraction policies where (as we argued before) state~$3$ is always visited.
        
        From the above example, we can see that when system dynamics are known, one can exploit the problem structure to design history-based feature abstractions which outperform their memoryless counterparts. However, such constructions are not feasible in the general learning setup, where the system dynamics are unknown. 
        % One way to overcome this issue is to let the policy $\policy{}$ be a function of all the information available to the system at time $\timestep$  ~\ie, $\policy{}_\timestep: \historyspace_{\timestep} \to \Delta(\actionspace)$ where, $\historyspace_\timestep \define \{\statespace^{1:\timestep},\featurespace^{1:\timestep-1}, \actionspace^{1:\timestep-1}\}$. 
        We can overcome this issue by letting the policy be a function of the history of state, actions and observations observed until time $\timestep$, \ie, $\policy{}_\timestep: \historyspace_{\timestep} \to \Delta(\actionspace)$ where, $\historyspace_\timestep \define \{\statespace^{1:\timestep},\featurespace^{1:\timestep-1}, \actionspace^{1:\timestep-1}\}$. 
        
        % Since the environment is an MDP, the state $\State_\timestep$ is  a sufficient statistic and the loss of information happens when $\State_\timestep$ is mapped to $\Feature_\timestep$. This is a subtle yet critical distinction between POMDPs and function approximation in MDPs. In POMDPs the state signal is unavailable, and the system only sees a observation vector $\Feature_\timestep$. Whereas, in function approximation setup, we can observe the state signal perfectly and partial observability is induced due to feature abstraction. Therefore, we can discard the the history of states and focus on the policies of the following form $\policy{}_\timestep: \historyspace_{\timestep} \to \Delta(\actionspace)$ where, $\historyspace_\timestep \define \{\statespace,\featurespace^{1:\timestep-1}, \actionspace^{1:\timestep-1}\}$.   
        
        As the size of history grows with time $t$ and it is computationally and conceptually difficult to use it for deriving a dynamic programming decomposition. One solution is to map the history to a fixed dimensional representation and use it as a state for designing a dynamic program. One can think of such a representation as an information state.  Due to lossy compression a prefect information state may be hard to obtain, but we an construct an approximate information state (AIS) such that it satisfies the controlled Markov property and captures the sufficient information for identifying the policy. In the next section we will show how to construct and AIS and use it for obtaining a dynamic program, which can then be solved, either exactly or approximately, to obtain a policy with a bounded approximation error. 
        
        % One can think of this representation as an information state ~\citep{kwakernaakh.1965,infostate-Witsenhausen,Striebel1965SufficientSI,Bohlin1970InformationPF,Davis1972InformationSF,Kumar1986StochasticSE}. 
        
        
    \section{Main Results}\label{sec:main}
    
        As mentioned previously, we can use the history of state, action and observations to learn the feature abstractions, but it possible to simplify this information structure further. Note that, as the environment is an MDP, the state $\State_\timestep$ is a sufficient statistic. The loss of information happens when $\State_\timestep$ is mapped to $\Feature_\timestep$. This is a subtle yet critical distinction between POMDPs and function approximation in MDPs. In POMDPs the state signal is unavailable, and the system only sees a observation vector $\Feature_\timestep$. Whereas, in function approximation setup, we can observe the state signal perfectly and partial observability is induced due to feature abstraction. Therefore, we can discard the the history of states and focus on the policies of the following form $\policy{}_\timestep: \historyspace_{\timestep} \to \Delta(\actionspace)$ where, $\historyspace_\timestep \define \{\statespace,\featurespace^{1:\timestep-1}, \actionspace^{1:\timestep-1}\}$. Despite this simplification the size of history is growing with time and history compression is still required. As such, we will now proceed towards defining the AIS for MDPs to address all of the aforementioned issues. 

      Given an MDP $\mdp$ and a feature space $\aisspace$, let $\historyspace_t = \statespace \times \featurespace^{\timestep-1} \times \actionspace^{\timestep-1}$ denote the space of all histories $(\State_\timestep, \Feature_{1:t}, \Action_{1:t-1})$ up to time~$t$. We are interested in learning history-based feature abstraction functions $\{ \aisfunction_t \colon \historyspace_t \to \aisspace \}_{t \ge 1}$ and a policy $\mu \colon \aisspace \to \Delta(\actionspace)$ such that $\policy{} = \{\policy{}_\timestep\}_{\timestep \ge 1}$, where $\policy{}_\timestep = \mu \circ \aisfunction_\timestep$, is approximately optimal. 

        
        Before defining the main model we will start with some necessary technical definitions.
        
        \subsection{Integral probability metrics (IPM)}
            \begin{definition}[~\citep{ipm}]
                    Let $(\mathcal{E},\mathcal{G})$ be a measurable space and $\mathfrak{F}$ denote a class of uniformly bounded measurable functions on $(\mathcal{E},\mathcal{G})$. The integral probability metric between two probability distributions  $\nu_1, \nu_2 \in \mathcal{P}(\mathcal{E})$ with respect to the function class $\mathfrak{F}$ is defined as:
                    \begin{align}
                        \ipm(\nu_1, \nu_2) &= \sup_{f \in \mathfrak{F}}\bigg| \int_{\mathcal{E}}f d\nu_1  - \int_{\mathcal{E}}f d\nu_2\bigg|.\label{eq:def-ipm}
                    \end{align}
                    
                    For any function $f$ (not necessarily in $\mathfrak{F}$), the Minkowski functional $\rho_{\mathfrak{F}}$ associated with the $\ipm$ is given as:
                    \begin{align}
                        \rho_{\mathfrak{F}}(f)&\define \inf\{\rho \in \real_{+}: \rho^{-1}f \in \mathfrak{F}\}.\label{eq:minkowski-functional}
                    \end{align}
                    
                    \Cref{eq:minkowski-functional}, implies that that for any function $f$:
                    \begin{align}
                         \bigg|\int_{\mathcal{E}}fd\nu_1 - \int_{\mathcal{E}}fd\nu_2 \bigg|\leq \rho_{\mathfrak{F}}(f)\ipm(\nu_1,\nu_2) \label{eq:ipm-implication}
                     \end{align}
                    
            \end{definition}
            
            \noindent Some examples of IPMs are:
            \begin{example}\label{def:tv-dist}
                If $\mathfrak{F}$ is chosen as $\mathfrak{F}^{\text{TV}}\define \{\frac{1}{2}\spn(f) = \frac{1}{2}(\max(f)- \min(f))\}$, then $\ipm$ is the total variation distance, and its Minkowski functional is: $\rho_{\mathfrak{F}}(f) = \frac{1}{2}\spn(f)$. 
            \end{example}
            \begin{example}\label{def:kr-dist}
                 If $\mathcal{E}$ is a metric space and $\mathfrak{F}$ is chosen as $\mathfrak{F}^{W} \define \{f: L_f \leq 1 \}$(where $L_f$ denotes the Lipschitz constant of $f$ with respect to the metric on $\mathcal{E}$), then $\ipm$ is the Wasserstein or the Kantorovich distance. The Minkowski function for the Wasserstein distance is a $\rho_{\mathfrak{F}^W}(f) = L_f$
            \end{example}
            
            \begin{example}\label{def:mmd-dist}
                Let $\mathcal{U}$ be a reproducing kernel Hilbert space (RKHS) of real-valued functions on $\mathcal{E}$ and $\mathfrak{F}$ is choosen as $\mathfrak{F}^{MMD} \define \{f\in \mathcal{U}: \Vert f \Vert _{\mathcal{U}} \leq 1 \}$, (where $\Vert \cdot \Vert_{\mathcal{U}}$ denotes the RKHS norm), then $\ipm$ is the Maximum Mean Discrepancy (MMD) distance and its Minkowski functional is $\rho_{\mathfrak{F}^{\text{MMD}}}(f) = \Vert f\Vert _{\mathcal{U}}$

            \end{example}
 
            
        \subsection{Approximate information state}
        
        \begin{definition}\label{def:state-update}
                A family of history-based feature abstraction functions  $\{\aisfunction_\timestep: \historyspace^{\timestep} \to \featurespace\}_{\timestep \geq 1}$ are said to update recursively if there exists an function $\hat f: \featurespace \times \statespace \times \actionspace \to \featurespace $ such that the process $\{\Feature_\timestep\}_{\timestep\geq 1}$ satisfies:
                \begin{align}
                    \Feature_{\timestep +1} = \hat f(\Feature_\timestep, \State_\timestep, \Action_\timestep),
                \end{align}
                where, $\Feature_\timestep = \aisfunction_\timestep(\State_{\timestep}, \Feature_{1:\timestep-1} \Action_{1:\timestep-1})$.
        \end{definition}
        
        \begin{definition}\label{def:ais}
            Given a family of history based feature abstraction functions $\{\aisfunction_\timestep: \historyspace^{\timestep} \to \featurespace\}_{\timestep \geq 1}$, the features $\Feature_\timestep = \aisfunction(\State_{\timestep}, \Feature_{1:\timestep-1}, \Action_{1:\timestep-1})$ are said to be $(\epsilon, \delta)$-Approximate information state with respect to a function space $f$, if there exist: 
            \begin{itemize}
                \item Reward approximation function: $\hat r: \featurespace \times \actionspace \to \real$
                \item Approximate Transition kernel: $\hat\transition: \featurespace \times \actionspace \to \Delta(\statespace)$
            \end{itemize}
            The tuple $(\hat r ,\hat\transition)$ is called an $(\epsilon, \delta)$- AIS approximator if the AIS satisfies the following properties:
            
            \begin{property}\label{p1}
                \textbf{\textit{Sufficient for approximate performance evaluation:}}
                \begin{align}
                  \forall \timestep \, \  | \cost(\State_{\timestep}, \Action_{\timestep})
                                            - \hat{\cost}(\Feature_\timestep, \Action_\timestep)| \leq \epsilon.
                 \label{def:p1}
                \end{align}
            \end{property}
            \begin{property} \label{p2b}
            \textbf{\textit{Is sufficient for predicting future states approximately:}} 
            \begin{align}
                \forall \timestep \ , \ d_{\mathfrak{F}}(\transition(\cdot \vert \State_\timestep, \Action_\timestep), \hat \transition(\cdot\vert \Feature_\timestep, \Action_\timestep)) &\leq \delta_{d_{\mathfrak{F}}}.
                % \text{with }\kappa_{\ipm}(\hat{f}_\timestep) &\define \sup_{\history_\timestep, \action_\timestep} \bigg[ \kappa_{\ipm}(\hat{f}_\timestep (\aisfunction_\timestep (\history_\timestep), \cdot, \action_\timestep))\bigg].
            \end{align}
        \end{property}
            
        \end{definition}
        \Cref{def:state-update}, \Cref{p1,p2b} help us ensure that the history compression (approximately) satisfies the properties of state so that we can use it for designing dynamic program as follows:
        % \subsubsection{AIS based dynamic program}
        \begin{theorem}\label{thm:ais-dp}
            Given an $(\epsilon, \delta)$ AIS approximator $(\hat \transition, \hat \cost)$, and history-based abstraction functions $\{\aisfunction_\timestep: \historyspace^\timestep \to \featurespace\}_{\timestep \geq 1}$ that update as per $\hat f$, define the following: 
            % Given a state-update function $\hat{f}$ and an AIS generator $\{\aisfunction_\timestep, \hat \transition_\timestep, \hat \cost_\timestep\}_{\timestep = 1}^{\infty}$, where $\aisfunction_\timestep$ is defined based on $\hat{f}$, define the following:
            
            \begin{align}
                \hat Q(\ais, \action) &= \hat \cost(\ais, \action) + \discount \sum_{\sts \in \statespace}
                \hat \transition(\sts|\ais,\action) \hat \valuefunction(\hat{f}(\ais,\sts,\action)), \\
                \hat \valuefunction(\ais) &= \max_{\action \in \actionspace} \hat Q(\ais,\action),\label{eq:ais-dp}
            \end{align}
            and let $\mu \colon \aisspace \to \Delta(\actionspace)$ be any policy such that for any $\ais \in \aisspace$,
            \[
                \text{Supp}(\mu(\ais)) \subseteq 
                \arg\max_{\action \in \actionspace} \hat Q(\ais,\action).
            \]
            Define the policy $\policy{} = \{\policy_\timestep\}_{\timestep = 1}^{\infty}$ where $\policy_\timestep = \mu \circ \aisfunction_\timestep$. Then, loss of performance incurred by $\policy{}$ is bounded as follows:
            \begin{align}
                \valuefunction^{\policy{}^\star}(\sts_0) - \hat{\valuefunction}^{\policy{}}(\feature_0)
                \le \frac{\varepsilon + \discount\delta_{d_{\mathfrak{F}}} \rho_{\mathfrak{F}}( \hat\valuefunction^{\policy{}}) \kappa_{\mathfrak{F}}(\hat{f})}{1 - \discount},\label{eq:ais-bound}
            \end{align}

            where $\kappa_{\mathfrak{F}}(\hat{f}) \define \sup_{\history_\timestep, \action_\timestep} \bigg[ \rho_{\mathfrak{F}}(\hat{f} (\aisfunction_\timestep (\history_\timestep), \cdot, \action_\timestep))\bigg]$.
        \end{theorem}
        Proof in \Cref{sec:proof:thm:ais-dp}
        
        
        {\color{blue}{Proof and Remarks to be discussed with Aditya.}}
        
        In \eqref{eq:ais-bound}, $\epsilon$ and $\delta$, capture the worst case error incurred by the AIS when predicting instantaneous reward $\cost$ and approximating the transition distribution of the ground MDP $\mdp$. Therefore, resulting suboptimality bound helps us quantify us the loss in performance due to feature abstraction. At the same time, the IPM used for measuring the distance between the approximate and true transition distribution also influences the bound via $\rho_{\mathfrak{F}}(\hat\valuefunction)$ and $\kappa_{\mathfrak{F}}(\hat f)$. In the following corollaries we will show how $\rho_{\mathfrak{F}}$ and $\kappa_{\mathfrak{F}}(\hat f)$ take specific forms according the choice of the IPM.
        
         \begin{corollary}\label{thm:tv-bound}
             If the function class $\mathfrak{F}$ is $\mathfrak{F}^{\text{TV}}$, then the result in \Cref{thm:ais-dp} is:
             \begin{align}
                \valuefunction^{\policy{}^\star}(\sts_0) - \hat{\valuefunction}^{\policy{}}(\feature_0)
                \le  \frac{\epsilon + \discount\delta \spn(\hat\valuefunction) \spn(\hat f)} {(1-\discount)},
            \end{align}
        \end{corollary}
        
        
        \begin{corollary}\label{thm:lip-bound}
            If the function class $\mathfrak{F}$ is $\mathfrak{F}^{\text{W}}$, then the result in \Cref{thm:ais-dp} is:
            \begin{align}
                \valuefunction^{\policy{}^\star}(\sts_0) - \hat{\valuefunction}^{\policy{}}(\feature_0)
                \le  \frac{\epsilon + \discount\delta L_{\hat\valuefunction} L_{\hat{f}}}{(1-\discount)},
            \end{align}
            where $L_{\hat\valuefunction}$ and $L_{\hat{f}}$ denote the Lipschitz constants of $\hat V$ and $\hat f$ respectively.
        \end{corollary}
        
        \begin{corollary}\label{thm:mmd-bound}
             If the function class $\mathfrak{F}$ is $\mathfrak{F}^{\text{MMD}}$, then the result in \Cref{thm:ais-dp} is:
             \begin{align}
                \valuefunction^{\policy{}^\star}(\sts_0) - \hat{\valuefunction}^{\policy{}}(\feature_0)
                \le  \frac{\epsilon + \discount\delta \Vert\hat\valuefunction \Vert_{\mathcal{U}} \Vert \hat{f} \Vert_{\mathcal{U}}} {(1-\discount)},
            \end{align}
            where similar to \Cref{def:mmd-dist}, $\mathcal{U}$ is a RKHS space and  $\Vert\cdot\Vert_{\mathcal{U}}$ is the norm associated with it.
        \end{corollary}
        
        Additionally, all the constants in \eqref{eq:ais-bound} are influenced by how one constructs and obtains an AIS. Since the an AIS is primarily a compression of history, we can learn it using a carefully designed data-driven algorithm. Such as AIS can then be used for approximately solving dynamic program in \eqref{eq:ais-dp} using RL.
        In the following section we will show how one can design a multi-timescale RL algorithm for simultaneously learning the AIS and policy $\policy{}$.