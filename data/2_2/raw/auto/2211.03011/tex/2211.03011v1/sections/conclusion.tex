\section{Conclusion and future work}\label{sec:conclusion}
    This paper presents the design and analysis of a principled approach for learning history-based policies for controlling MDPs. We believe that our approximation bounds can be helpful for practitioners to study the effect of some of their design choices on the solution quality. On the practical side, the proposed algorithm shows favourable results on high-dimensional control tasks. Note that one can also use the bounds in \Cref{thm:ais-dp} to analyse the approximation error of other history-based methods. However, since some of these algorithms do not satisfy \Cref{def:ais}, the resulting approximation error might be arbitrarily large. Such blow-ups in the approximation error could be because the bound itself is loose or the optimality gap is large. This would depend on the specifics of the methods and remains to be investigated. As such, a sharper analysis of the approximation error by factoring in the specific design choices of other methods is an interesting direction for future research. Another interesting direction would be to conduct a thorough empirical evaluation exploring the design choices of history compression functions.