\section{Related Work}\label{sec:litreview}

    The development of RL algorithms with memory-based feature abstractions has been an active area of research, and most existing algorithms have tackled this problem using non-parametric methods like Nearest neighbour~\citep{Bentley1975MultidimensionalBS,Friedman1977AnAF,PENG1995438}, Locally-weighted regression~\citep{Baird1993ReinforcementLW,locallyweighedatekson,Moore1997EfficientLW}, and Kernel-based regression~\citep{Connell1987LearningTC,Dietterich2001BatchVF,kbrl,Xu2006KernelLT,Bhat2012NonparametricAD,BarretoAndr2016PracticalKR}. Despite their solid theoretical footing, these methods, have limited applicability as they are difficult to scale to high-dimensional state and action spaces. More recently, several methods that propose using recurrent neural networks for learning history-based abstractions have enjoyed considerable success in complex computer games~\citep{rnn-hausknecht,JaderbergMCSLSK17,impala,DBLP:conf/iclr/GruslysDAPBM18,ha} however most of these methods have been designed for partially observable environments where use of history-based methods is often necessary. To the best of our knowledge, the only other work where a history-based RL algorithm is used for controlling a MDP is presented by \citet{openai2019learning}. In this work the authors show that using an LSTM-based agent architecture results in superior performance for the object reorientation using robotic arms. However, the authors do not provide a theoretical analysis of their method. 
    % The algorithmic framework in \Cref{sec:algorithm} can be 
    \subsection{ Bisimulation metrics}
     On the theoretical front, our work is closely related to state aggregation techniques based on bisimulation metrics proposed by~\citet{Givan2003EquivalenceNA,Ferns2004MetricsFF,Ferns2011BisimulationMF}. The bisimulation metric is the  fixed point of an operator on the space of semi-metrics defined over the state space of an MDP with Lipschitz value functions. Apart from state aggregation, bisimulation metrics have been used for feature discovery~\citep{Comanici2011BasisFD,Ruan2015RepresentationDF}, and transfer learning~\citep{Castro2010UsingBF}. However, computational impediments have prevented their broad adoption. Our work can be viewed as an alternative to bisimulation for the analysis of history-based state abstractions and deep RL methods. Our work can also be thought of as extension of the DeepMDP framework~\citep{deepmdp} to history-based policies and direct policy search methods. %{\bf{TODO: Verify this with Aditya and Doina}}.
     \subsection{AIS and Agent state}
     The notion of AIS is closely related to the epistemic state recently proposed by \citet{bvrstate}. An epistemic state is a bounded representation of the history. It is updated recursively as the agent collects more information, and is represented as an environment proxy $\Upsilon$ which is learnt by optimising a target/objective function $\chi$. Since $\Upsilon$ is a random variable, its entropy $\mathbb{H}(\Upsilon)$ is used to represent system's uncertainty about the environment. The framework proposed in this paper can considered as a practical way of constructing the system epistemic state where, the AIS $\Ais_\timestep$ represents both the epistemic state and the environment proxy $\Upsilon$, $\aisloss$ represents  $\chi$, and instead of entropy, the constants $\epsilon$, and $\delta$ represent the systems uncertainty about the environment. The study of the AIS framework in the regret minimisation paradigm could help establish a relationship between the $\epsilon$, $\delta$, and $\mathbb{H}(\Upsilon)$, thereby helping designers develop principled algorithms which synthesise ideas like information directed sampling for direct policy search algorithms.
    \subsection{Analysis of RL algorithms with attention mechanism}
    Recently, there has been considerable interest in developing RL algorithms which use attention mechanism/transformer architectures~\citep{BahdanauCB14,Xu2015ShowAA} for learning feature abstractions~\citep{Zambaldi2019DeepRL,Mott2019TowardsIR,Sorokin2015DeepAR,Oh,RitterFSSBR21,ParisottoSRPGJJ20,chen,LoyndFcSH20,TangNH20,PritzelUSBVHWB17}. Attention mechanism extract task relevant information from historical observations and can be used instead of RNNs for processing sequential data~\citep{vaswani}. As we do not impose a functional from on the history compression function $\aisfunction_\timestep(\cdot)$ in \cref{def:ais}, any attention mechanism can be interpreted as history compression function, and one can construct a valid information state by ensuring that the output of the attention mechanism satisfies (P1) and (P2). That being said, even without optimising $\aisloss$, the approximation bound in \cref{thm:ais-dp} still applies for RL algorithms with attention mechanisms, with the caveat that the constants $\epsilon$, and $\delta$ may be arbitrarily large. A thorough empirical analysis of the effect of different attention mechanisms, and the AIS loss on the on the error constants $\epsilon$, and $\delta$ could help us gain a better understanding of the way in which such design choices could influence the learning process.
    \subsection{AIS for POMDPs}
      The concept of an AIS used in this paper is similar to the idea of AIS for POMDPs ~\citep{ais-2, ais-1}. Moreover, the literature also contains several other methods which have enjoyed empirical success in using history-based policies for controlling POMDPs \citep{Isbell,hutte-1,hutter-2,Schaefer2007ARC,Dreamer,Pla-Net}. In principle, one can use any of these methods for controlling MDPs. However, this does not immediately provide a tight bound for the approximation error. The MDP model has more structure than POMDPs, and our goal in this paper is to use this fact to present a tighter analysis of the approximation error.