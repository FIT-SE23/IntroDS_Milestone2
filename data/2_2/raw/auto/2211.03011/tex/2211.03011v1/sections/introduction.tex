\section{Introduction}\label{sec:intro}
State abstraction and function approximation are vital components used by reinforcement learning (RL) algorithms to efficiently solve complex control problems  when exact computations are intractable due to large state and action spaces. Over the past few decades, state abstraction in RL has evolved from the use of pre-determined and problem-specific features~\cite{CritesB95, TsitsiklisR96,ndp,Sutton+Barto:1998,SinghLKW02,activesensing, ProperT06} to the use of adaptive basis functions learnt by solving an isolated regression problem~\citep{kbrl,autobasis-MenacheMS05,6-keller,Petrik07}, and more recently to the use of neural network-based Deep-RL algorithms that embed state abstraction in successive layers of a neural network~\citep{Barto2004SynthesisON,BellemareDDTCRS19}.
 
Feature abstraction results in information loss, and the resulting state features might not satisfy the controlled Markov property, even if this property is satisfied by the corresponding state ~\citep{Sutton+Barto:2018}. One approach to counteract the loss of the Markov property is to generate the features using the history of state-action pairs, and empirical evidence suggests that using such history-based features are beneficial in practice~\citep{openai2019learning}. However, a theoretical characterisation of history-based Deep-RL algorithms for fully observed Markov Decision Processes (MDPs) is largely absent form the literature. 

% However, besides the general Partially Observable(PO)-MDP assumptions, the theoretical characterisation of history-based RL algorithms for controlling MDPs is absent from the literature.
In this paper, we bridge this gap between theory and practise by providing a theoretical analysis of history-based RL agents acting in a MDP.
% This paper introduces a theoretical framework to analyse the use of history-based feature abstraction in RL agents acting in Markov Decision Processes (MDPs). 
Our approach adapts the notion of approximate information state (AIS) for POMDPs proposed in \cite{ais-1,ais-2} to feature abstraction in MDPs, and we develop a theoretically grounded policy search algorithm for history-based feature abstractions and policies. 
% and evaluate it on continuous control tasks. 


The rest of the paper is organised as follows: In \cref{sec:background},
following a brief review of feature-based abstraction, we motivate the need for using history-based feature abstractions. In \cref{sec:main}, we present a formal model for the co-design of the feature abstraction and control policy, derive a dynamic program using the AIS. We also derive bounds on the quality of approximate solutions to this dynamic program. 
In \Cref{sec:algorithm} we build on these approximation bounds to develop an RL algorithm for learning a history-based state representation and control policy. In \cref{sec:experiments}, we present an empirical evaluation of our proposed algorithm on continuous control tasks. Finally, we discuss related work in \cref{sec:litreview} and conclude with future research directions in \cref{sec:conclusion}.




