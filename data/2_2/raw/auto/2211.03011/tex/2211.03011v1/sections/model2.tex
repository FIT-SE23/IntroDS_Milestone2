\section{Background and Motivation}\label{sec:background}

    % \subsection{Function approximation in MDPs}
    
        Consider an MDP $ \mdp = \langle \statespace, \actionspace, \transition, \cost, \discount \rangle$ where $\statespace$ denotes the state space, $\actionspace$ denotes the action space, $\transition$ denotes the controlled transition matrix, $\cost \colon \statespace \times \actionspace \to \real$ denotes the per-step reward, and $\discount \in (0,1)$ denotes the discount factor.
                
        The performance of a randomised (and possibly history-dependent) policy $\policy$
        starting from a start state $\sts_{0}$ is measured by the value function, defined as:
        \begin{equation}
          \valuefunction^{\policy}(\sts_0) = \expecun{}^{\policy}\bigg[\sum_{\timestep=1}^{\infty}\discount^{\timestep-1}\cost(\State_{\timestep},\Action_{\timestep}) \bigg| \State_{0} = \sts_{0}\bigg].
        \end{equation}
        A policy maximising $\valuefunction^{\policy}(\sts_0)$ over all (randomised and possibly history dependent) policies is called the optimal policy with respect to initial state $s_0$ and is denoted by $\policy^{\star}$.
        %\footnote{This notion can easily be extended to start state distributions.}.
                
        In many applications, $\statespace$ and $\actionspace$ are combinatorially large or uncountable, which makes it intractable to compute the optimal policy. 
        Most practical RL algorithms overcome this hurdle by using function approximation where the state is mapped to a feature space $\featurespace$ using a state abstraction function $\basis:\statespace \to \featurespace$. In Deep-RL algorithms, the last layer of the network is often viewed as a feature vector. These feature vectors are then used as an approximate state for approximating the value function $\hat\valuefunction: \featurespace \to \real$ and/or computing an approximately optimal policy $\mu:\featurespace \to \Delta(\actionspace)$~\citep{Sutton+Barto:1998} (where $\Delta(\actionspace)$ denotes the set of probability distribution over actions). Therefore, the mapping from state to distribution of actions is given by the ``flattened'' policy $\tilde{\mu} = \mu \circ \basis$ ~\ie, $\tilde{\mu} = \mu(\phi(\cdot))$.
        
        % The features $\Feature_\timestep$ are then used as an approximate state for computing the value function $\hat\valuefunction: \featurespace \to \real$ and/or the policy $\mu:\featurespace \to \Delta(\actionspace)$~\citep{Sutton+Barto:1998} (where $\Delta(\actionspace)$ denotes the set of probability distribution over actions). We denote by $\mu \circ \phi $ as the ``flattened" policy that maps states to action distributions, by composing the feature abstraction and the policy acting on it. In Deep-RL algorithms, the output of the last layer of the network is often viewed as the feature vector $\Feature_{\timestep}$.
        
        % A well known yet often overlooked fact about function approximation is that the features used as a proxy state may not satisfy the controlled Markov property \ie, in general,
        A well known fact about function approximation is that the features that are used as an approximate state may not satisfy the controlled Markov property \ie, in general, 
        \[
            \prob(\Feature_{\timestep+1} \mid \Feature_{1:\timestep}, \Action_{1:\timestep}) \neq
            \prob(\Feature_{\timestep+1} \mid \Feature_\timestep, \Action_\timestep).
        \]
        
        % \begin{figure}[!h]
        %   \centering
        %   \begin{minipage}{\linewidth}
        %   \subfigure[$P(0)$]{
        %     \begin{tikzpicture}[thick,scale=0.9]
        %       \node [agent] at (0, 0) (0) {$0$};
        %       \node [agent] at (1, 0) (1) {$1$}; 
        %       \node [agent] at (0, 1) (3) {$3$};
        %       \node [agent] at (1, 1) (2) {$2$};
        %       \path[->]
        %             (0) edge node[below] {$0.5$} (1)
        %             (1) edge node[right] {$0.5$} (2)
        %             (2) edge node[above] {$0.5$} (3)
        %             (3) edge node[left]  {$0.5$} (0)
        %             (0) edge[loop left] node {$0.5$} (0)
        %             (1) edge[loop right] node {$0.5$} (1)
        %             (2) edge[loop right] node {$0.5$} (2)
        %             (3) edge[loop left] node {$0.5$} (3)
        %             ;
        %     \end{tikzpicture}}
        %   \hfill
        %   \subfigure[$P(1)$]{
        %     % \centering
        %     \begin{tikzpicture}[thick,scale=0.9]
        %       \node [agent] at (0, 0) (0) {$0$};
        %       \node [agent] at (1, 0) (1) {$1$}; 
        %       \node [agent] at (0, 1) (3) {$3$};
        %       \node [agent] at (1, 1) (2) {$2$};
        %       \path[<-]
        %             (0) edge node[below] {$0.5$} (1)
        %             (1) edge node[right] {$0.5$} (2)
        %             (2) edge node[above] {$0.5$} (3)
        %             (3) edge node[left]  {$0.5$} (0)
        %             (0) edge[loop left] node {$0.5$} (0)
        %             (1) edge[loop right] node {$0.5$} (1)
        %             (2) edge[loop right] node {$0.5$} (2)
        %             (3) edge[loop left] node {$0.5$} (3)
        %             ;
        %     \end{tikzpicture}}
        %     \hfill
        %     \subfigure[$P(2)$]{%
        %           \begin{tikzpicture}[thick,scale=0.9]
        %           \node [agent] at (0, 0) (0) {$0$};
        %           \node [agent] at (1, 0) (1) {$1$}; 
        %           \node [agent] at (0, 1) (3) {$3$};
        %           \node [agent] at (1, 1) (2) {$2$};
        %           \path[->]
        %                 (0) edge [bend right] node[below] {$0.5$} (1)
        %                 (1) edge [bend right] node[right] {$0.5$} (2)
        %                 (2) edge [bend right] node[above] {$0.5$} (3)
        %                 (3) edge [bend right] node[left]  {$0.5$} (0)
        %                 (0) edge [bend right] node[right] {} (3)
        %                 (3) edge [bend right] node[below] {} (2)
        %                 (2) edge [bend right] node[left]  {} (1)
        %                 (1) edge [bend right] node[above]  {} (0);
        %         \end{tikzpicture}}
        %     \hfill
        %     \subfigure[$P_{\policy{}}$]{
        %     \begin{tikzpicture}[thick,scale=0.9]
        %       \node [agent] at (0, 0) (0) {$0$};
        %       \node [agent] at (1, 0) (1) {$1$}; 
        %       \node [agent] at (0, 1) (3) {$3$};
        %       \node [agent] at (1, 1) (2) {$2$};
        %       \path[->]
        %             (0) edge node[below] {$0.5$} (1)
        %             (1) edge[bend right] node[right] {$0.5$} (2)
        %             (2) edge[bend right]  node[left] {$0.5$} (1)
        %             (3) edge node[left]  {$0.5$} (0)
        %             (3) edge node[above] {$0.5$} (2)
        %             (0) edge[loop left] node {$0.5$} (0)
        %             (1) edge[loop right] node {$0.5$} (1)
        %             (2) edge[loop right] node {$0.5$} (2)
        %             ;
        %     \end{tikzpicture}
        %     % \caption{}
        %     % \label{fig:optimal}
        %     }
        %   \label{fig:P}
        %   \caption{The transition probability for an example MDP}
        %   \end{minipage}%
        % \end{figure}
        
        \begin{figure}[!h]
          \centering
          \begin{minipage}{\linewidth}
          \subfloat[$P(0)$]{
            \begin{tikzpicture}[thick,scale=0.9]
              \node [agent] at (0, 0) (0) {$0$};
              \node [agent] at (1, 0) (1) {$1$}; 
              \node [agent] at (0, 1) (3) {$3$};
              \node [agent] at (1, 1) (2) {$2$};
              \path[->]
                    (0) edge node[below] {$0.5$} (1)
                    (1) edge node[right] {$0.5$} (2)
                    (2) edge node[above] {$0.5$} (3)
                    (3) edge node[left]  {$0.5$} (0)
                    (0) edge[loop left] node {$0.5$} (0)
                    (1) edge[loop right] node {$0.5$} (1)
                    (2) edge[loop right] node {$0.5$} (2)
                    (3) edge[loop left] node {$0.5$} (3)
                    ;
            \end{tikzpicture}
            \label{fig:P(1)}}
           \hfill
          \subfloat[$P(1)$]{
            % \centering
            \begin{tikzpicture}[thick,scale=0.9]
              \node [agent] at (0, 0) (0) {$0$};
              \node [agent] at (1, 0) (1) {$1$}; 
              \node [agent] at (0, 1) (3) {$3$};
              \node [agent] at (1, 1) (2) {$2$};
              \path[<-]
                    (0) edge node[below] {$0.5$} (1)
                    (1) edge node[right] {$0.5$} (2)
                    (2) edge node[above] {$0.5$} (3)
                    (3) edge node[left]  {$0.5$} (0)
                    (0) edge[loop left] node {$0.5$} (0)
                    (1) edge[loop right] node {$0.5$} (1)
                    (2) edge[loop right] node {$0.5$} (2)
                    (3) edge[loop left] node {$0.5$} (3)
                    ;
            \end{tikzpicture}
            \label{fig:P(2)}}
            \hfill
            \subfloat[$P(2)$]{%
                  \begin{tikzpicture}[thick,scale=0.9]
                  \node [agent] at (0, 0) (0) {$0$};
                  \node [agent] at (1, 0) (1) {$1$}; 
                  \node [agent] at (0, 1) (3) {$3$};
                  \node [agent] at (1, 1) (2) {$2$};
                  \path[->]
                        (0) edge [bend right] node[below] {$0.5$} (1)
                        (1) edge [bend right] node[right] {$0.5$} (2)
                        (2) edge [bend right] node[above] {$0.5$} (3)
                        (3) edge [bend right] node[left]  {$0.5$} (0)
                        (0) edge [bend right] node[right] {} (3)
                        (3) edge [bend right] node[below] {} (2)
                        (2) edge [bend right] node[left]  {} (1)
                        (1) edge [bend right] node[above]  {} (0);
                \end{tikzpicture}
                \label{fig:P(3)}}
            \hfill
            \subfloat[$P_{\policy{}}$]{
            \begin{tikzpicture}[thick,scale=0.9]
              \node [agent] at (0, 0) (0) {$0$};
              \node [agent] at (1, 0) (1) {$1$}; 
              \node [agent] at (0, 1) (3) {$3$};
              \node [agent] at (1, 1) (2) {$2$};
              \path[->]
                    (0) edge node[below] {$0.5$} (1)
                    (1) edge[bend right] node[right] {$0.5$} (2)
                    (2) edge[bend right]  node[left] {$0.5$} (1)
                    (3) edge node[left]  {$0.5$} (0)
                    (3) edge node[above] {$0.5$} (2)
                    (0) edge[loop left] node {$0.5$} (0)
                    (1) edge[loop right] node {$0.5$} (1)
                    (2) edge[loop right] node {$0.5$} (2)
                    ;
            \end{tikzpicture}
            \label{fig:optimal}
            }
          \end{minipage}%
          \caption{The transition probability for an example MDP}
        \end{figure}


        To see the implications of this fact, consider the toy MDP depicted in~\cref{fig:P(1),fig:P(2),fig:P(3)}, with $ \statespace = \{0, 1, 2, 3\}$, $\actionspace =
        \{0, 1, 2\}$, $\{\transition_{\sts, \sts'}(\action)\}_{\action \in \actionspace}$, and $r(0) = r(1) = -1$, $r(2) = 1$, $r(3) = -K$, where $K$ is a large positive number.
       Given the reward structure the objective of the policy is to try to avoid state~$3$ and keep the agent at state~$2$ as much as possible. It is easy to see that the optimal policy is 
            \[
              \pi^\star(0) = 0, \quad
              \pi^\star(1) = 0, \quad
              \pi^\star(2) = 1,
              \text{ and}\quad
              \pi^\star(3) = 2.
            \]
            
        Note that if the initial state is not state~$3$ then an agent will never visit that state under the optimal policy. Furthermore, any policy which cannot prevent the agent from visiting state~$3$ will have a large negative value and, therefore, cannot be optimal.
        Now suppose the feature space $\featurespace = \{0, 1\}$. It is easy to see that for any Markovian feature-abstraction $\policyencoder{} \colon \statespace \to \featurespace$, no policy $\hat \pi \colon \featurespace \to \actionspace$ can prevent the agent from visiting state~$3$. Thus, the best policy when using Markovian feature abstraction will perform significantly worse than the optimal policy (which has direct access to the state).
    
        However, it is possible to construct a history-based feature-abstraction  $\policyencoder$ and a history-based control policy $\hat \pi$ that works with $\phi$ and is of the same quality as $\pi^\star$. For this, consider the following \emph{codebooks} (where the entries denoted by a dot do not matter):    
            
        % \begin{wrapfigure}{rt}{0.25\linewidth}
        %   \begin{minipage}{\linewidth}
        %   \centering
        %     \begin{tikzpicture}[thick,scale=0.9]
        %       \node [agent] at (0, 0) (0) {$0$};
        %       \node [agent] at (1, 0) (1) {$1$}; 
        %       \node [agent] at (0, 1) (3) {$3$};
        %       \node [agent] at (1, 1) (2) {$2$};
        %       \path[->]
        %             (0) edge node[below] {$0.5$} (1)
        %             (1) edge[bend right] node[right] {$0.5$} (2)
        %             (2) edge[bend right]  node[left] {$0.5$} (1)
        %             (3) edge node[left]  {$0.5$} (0)
        %             (3) edge node[above] {$0.5$} (2)
        %             (0) edge[loop left] node {$0.5$} (0)
        %             (1) edge[loop right] node {$0.5$} (1)
        %             (2) edge[loop right] node {$0.5$} (2)
        %             % (3) edge[loop left] node {$0.5$} (3)
        %             ;
        %     \end{tikzpicture}
        %   \caption{The transition probability of $\pi^\star$}
        %   \label{fig:optimal}
        %   \end{minipage}
        %   \vspace{-3mm}
        % \end{wrapfigure}
        
        The Markov chain induced by the optimal policy is shown in~\Cref{fig:optimal}. 
        Now define
        \begin{align*}
          F(1) &= \begin{bmatrix}
            0 & 1 & \cdot & \cdot \\
            \cdot & 0 & 1 & \cdot \\
            \cdot & \cdot & 0 & 1 \\
            1 & \cdot & \cdot & 0
          \end{bmatrix}, 
          &
          F(2) &= \begin{bmatrix}
            1 & \cdot & \cdot & 0 \\
            0 & 1 & \cdot & \cdot \\
            \cdot & 0 & 1 & \cdot \\
            \cdot & \cdot & 0 & 1
          \end{bmatrix} ,
          &
          F(3) &= \begin{bmatrix}
            \cdot & 0 & \cdot & 1 \\
            0 & \cdot & 1 & \cdot \\
            \cdot & 0 & \cdot & 1 \\
            0 & \cdot & 1 & \cdot
          \end{bmatrix},
          \\
          D(0) &= \begin{bmatrix}
            0 & 1 \\
            1 & 2 \\
            2 & 3 \\
            3 & 0 
          \end{bmatrix} ,
          &
          D(1) &= \begin{bmatrix}
            3 & 0 \\
            0 & 1 \\
            1 & 2 \\
            2 & 3 
          \end{bmatrix} ,
          &
          D(2) &= \begin{bmatrix}
            1 & 3 \\
            0 & 2 \\
            1 & 3 \\
            0 & 2 
          \end{bmatrix} .
        \end{align*}
        
      
        and consider the feature-abstraction policy
        \(
          Z_t =  F_{S_{t-1}, S_t}(A_{t-1})
        \)
        and a control policy $\mu$ which is a finite state machine with memory, where the memory $M_t$ that is updated as
        \(
          M_t = D_{M_{t-1}, Z_t}(A_{t-1})
        \)
        and the action $A_t$ is chosen as
        \(
          A_t = \pi(M_t),
        \)
        where $\pi \colon \statespace \to \Delta(\actionspace)$ is any pre-specified reference policy. It can be verified that if the system starts from a known initial state then $\mu \circ \basis = \pi$. 
         Thus, if we choose the reference policy $\pi=\pi^\star$, then
        the agent will never visit state~$3$ under $\mu \circ \basis$, in contrast to Markovian feature-abstraction policies where (as we argued before) state~$3$ is always visited.
        
        In the above example, we used the properties of the system dynamics and the reward function to design a history-based feature abstraction which outperforms memoryless feature abstractions. We are interested in developing such history-based feature abstractions using a learning framework when the system model is not known. We present such a construction in the next section.
        
        %From the above example, we can see that when system dynamics are known, one can exploit the problem structure to design history-based feature abstractions which outperform their memoryless counterparts.   However, such constructions are not feasible in the general learning setup, where the system dynamics are unknown. 
        % One way to overcome this issue is to let the policy $\policy{}$ be a function of all the information available to the system at time $\timestep$  ~\ie, $\policy{}_\timestep: \historyspace_{\timestep} \to \Delta(\actionspace)$ where, $\historyspace_\timestep \define \{\statespace^{1:\timestep},\featurespace^{1:\timestep-1}, \actionspace^{1:\timestep-1}\}$. 
        %We can overcome this issue by letting the policy be a function of the history of state, actions and observations observed until time $\timestep$, \ie, $\policy{}_\timestep: \historyspace_{\timestep} \to \Delta(\actionspace)$ where, $\historyspace_\timestep \define \{\statespace \times \featurespace \times \actionspace\}$. 
        
        % Since the environment is an MDP, the state $\State_\timestep$ is  a sufficient statistic and the loss of information happens when $\State_\timestep$ is mapped to $\Feature_\timestep$. This is a subtle yet critical distinction between POMDPs and function approximation in MDPs. In POMDPs the state signal is unavailable, and the system only sees a observation vector $\Feature_\timestep$. Whereas, in function approximation setup, we can observe the state signal perfectly and partial observability is induced due to feature abstraction. Therefore, we can discard the the history of states and focus on the policies of the following form $\policy{}_\timestep: \historyspace_{\timestep} \to \Delta(\actionspace)$ where, $\historyspace_\timestep \define \{\statespace,\featurespace^{1:\timestep-1}, \actionspace^{1:\timestep-1}\}$.   
        
        %As the size of history grows with time $t$ and it is computationally and conceptually difficult to use it for deriving a dynamic programming decomposition. One solution is to map the history to a fixed dimensional representation and use it as a state for designing a dynamic program. One can think of such a representation as an information state~\citep{kwakernaakh.1965,infostate-Witsenhausen,Striebel1965SufficientSI,Bohlin1970InformationPF,Davis1972InformationSF,Kumar1986StochasticSE}.  Due to lossy compression a prefect information state may be hard to obtain, but we an construct an approximate information state (AIS) such that it satisfies the controlled Markov property and captures the sufficient information needed for identifying a good policy. In the next section we show how to construct and AIS and use it for obtaining a dynamic program to obtain a policy with a bounded approximation error. 
        
        
        
    \section{Approximation bounds for history-based feature abstraction}\label{sec:main}
         
        The approximation results of our framework depend on the properties of metrics on probability spaces. We start with a brief overview of a general class of metrics known as Integral Probability Measures (IPMs)~\citep{ipm}; many of the commonly used metrics on probability spaces such as total variation (TV) distance, Wasserstein distance, and maximum-mean discrepency (MMD) are instances of IPMs. We then derive a general approximation bound that holds for general IPMs, and then specialize the bound to specific instances (TV, Wassserstein, and MMD).
        
        %In particular, we derive results for Total Variation (TV) distance, Wasserstein/Kantorovich-Rubinstein distance and Maximum-Mean Discrepancy (MMD) distance. All of these metrics are instances of Integral Probability Measures (IPMs)~\citep{ipm}---a class of metrics that have a dual characterisation. Viewing all the above metrics through the lens of IPMs allows us to derive a general approximation bound that holds for all the instances of IPMs, and then specialise the bound to specific instances \ie, TV, Wasserstein and MMD distance. Before defining our main model we will briefly describe the important properties of IPMs necessary for deriving our results.
        
        \subsection{Integral probability metrics (IPM)}
            \begin{definition}[~\citep{ipm}]\label{def:ipm}
                    Let $(\mathcal{E},\mathcal{G})$ be a measurable space and $\mathfrak{F}$ denote a class of uniformly bounded measurable functions on $(\mathcal{E},\mathcal{G})$. The integral probability metric between two probability distributions  $\nu_1, \nu_2 \in \mathcal{P}(\mathcal{E})$ with respect to the function class $\mathfrak{F}$ is defined as:
                    \begin{align}
                        \ipm(\nu_1, \nu_2) &= \sup_{f \in \mathfrak{F}}\bigg| \int_{\mathcal{E}}f d\nu_1  - \int_{\mathcal{E}}f d\nu_2\bigg|.\label{eq:def-ipm}
                    \end{align}
                    
                    For any function $f$ (not necessarily in $\mathfrak{F}$), the Minkowski functional $\rho_{\mathfrak{F}}$ associated with the metric $\ipm$ is defined as:
                    \begin{align}
                        \rho_{\mathfrak{F}}(f)&\define \inf\{\rho \in \real_{\geq 0}: \rho^{-1}f \in \mathfrak{F}\}.\label{eq:minkowski-functional}
                    \end{align}
                    
                    Eq.~\eqref{eq:minkowski-functional}, implies that that for any function $f$:
                    \begin{align}
                         \bigg|\int_{\mathcal{E}}fd\nu_1 - \int_{\mathcal{E}}fd\nu_2 \bigg|\leq \rho_{\mathfrak{F}}(f)\ipm(\nu_1,\nu_2). \label{eq:ipm-implication}
                     \end{align}\label{eq:ipm-function-diff}
                    
            \end{definition}
            \noindent In this paper, we use the following IPMs:
            \begin{compactitem}
                \item[1.]\label{def:tv-dist}{\bf Total Variation Distance}: If $\mathfrak{F}$ is chosen as $\mathfrak{F}^{\text{TV}} \define \{\frac{1}{2}\spn(f)$ = $\frac{1}{2}(\max(f)- \min(f))\}$, then $\ipm$ is the total variation distance, and its Minkowski functional is $\rho_{\mathfrak{F}^{\text{TV}}}(f) = \frac{1}{2}\spn(f)$. 
                \item[2.]\label{def:kr-dist}{\bf Wasserstein/Kantorovich-Rubinstein Distance}: If $\mathcal{E}$ is a metric space and $\mathfrak{F}$ is chosen as $\mathfrak{F}^{W} \define \{f: L_f \leq 1 \}$ (where $L_f$ denotes the Lipschitz constant of $f$ with respect to the metric on $\mathcal{E}$), then $\ipm$ is the Wasserstein or the Kantorovich distance. The Minkowski function for the Wasserstein distance is $\rho_{\mathfrak{F}^W}(f) = L_f$.
                \item[3.] \label{def:mmd-dist}{\bf Maximum Mean Discrepancy (MMD) Distance}: Let $\mathcal{U}$ be a reproducing kernel Hilbert space (RKHS) of real-valued functions on $\mathcal{E}$ and $\mathfrak{F}$ is choosen as $\mathfrak{F}^{MMD} \define \{f\in \mathcal{U}: \Vert f \Vert _{\mathcal{U}} \leq 1 \}$, (where $\Vert \cdot \Vert_{\mathcal{U}}$ denotes the RKHS norm), then $\ipm$ is the Maximum Mean Discrepancy (MMD) distance and its Minkowski functional is $\rho_{\mathfrak{F}^{\text{MMD}}}(f) = \Vert f\Vert _{\mathcal{U}}$.
            \end{compactitem}
 
        % As mentioned previously, we can use the history of state, action and observations to learn the feature abstractions, but it possible to simplify this information structure further. Note that, as the environment is an MDP, the state $\State_\timestep$ is a sufficient statistic. The loss of information happens when $\State_\timestep$ is mapped to $\Feature_\timestep$. This is a subtle yet critical distinction between POMDPs and function approximation in MDPs. In POMDPs the state signal is unavailable, and the system only sees a observation vector $\Feature_\timestep$. Whereas, in function approximation setup, we can observe the state signal perfectly and partial observability is induced due to feature abstraction. Therefore, we can discard the the history of states and focus on the policies of the following form $\policy{}_\timestep: \historyspace_{\timestep} \to \Delta(\actionspace)$ where, $\historyspace_\timestep \define \{\statespace,\featurespace^{1:\timestep-1}, \actionspace^{1:\timestep-1}\}$. Despite this simplification the size of history is growing with time and history compression is still required. As such, we will now proceed towards defining the AIS for MDPs to address all of the aforementioned issues. 
        
        \subsection{Approximate information state}
            
            Given an MDP $\mdp$ and a feature space $\aisspace$, let $\historyspace_\timestep = \statespace \times  \actionspace $ denote the space of all histories $(\State_{1:\timestep}, \Action_{1:\timestep-1})$ up to time~$t$, where $\State_{1:\timestep}$ is a shorthand notation for the history of states $(\State_1,\ldots, \State_\timestep)$, and similar interpretation holds for $\Action_{1:\timestep}$. We are interested in learning history-based feature abstraction functions $\{ \aisfunction_\timestep \colon \historyspace_\timestep \to \aisspace \}_{\timestep \ge 1}$ and a time homogenous policy $\mu \colon \aisspace \to \Delta(\actionspace)$ such that the flattened policy $\policy{} = \{\policy{}_\timestep\}_{\timestep \ge 1}$, where $\policy{}_\timestep = \mu \circ \aisfunction_\timestep$, is approximately optimal.
            
            % We are interested in practical history-based feature abstractions, which often tend to be implemented as RNNs. So, we consider abstractions which are recursively updatable as defined below:
        
            Since the feature abstraction approximates the state, its quality depends on how well it can be used to approximate the per step reward and predict the next state. We formalise this intuition in definition below.
            
            
            \begin{definition}\label{def:state-update}
                A family of history-based feature abstraction functions  $\{\aisfunction_\timestep: \historyspace_{\timestep} \to \featurespace\}_{\timestep \geq 1}$ are said to be \emph{recursively updatable} if there exists an update function $\hat f: \featurespace \times \statespace \times \actionspace \to \featurespace $ such that the process $\{\Feature_\timestep\}_{\timestep\geq 1}$, where $\Feature_\timestep = \aisfunction_\timestep(\State_{1:\timestep}, \Action_{1:\timestep-1})$, satisfies:
                \begin{equation}
                    \Feature_{\timestep +1} = \hat f(\Feature_\timestep, \State_{\timestep+1}, \Action_\timestep).\quad \timestep \geq 1
                \end{equation}
            \end{definition}
            \begin{definition}\label{def:ais}
                Given a family of history based recursively updatable feature abstraction functions $\{\aisfunction_\timestep: \historyspace_{\timestep} \to \featurespace\}_{\timestep \geq 1}$, the features $\Feature_\timestep = \aisfunction_\timestep(\State_{1:\timestep}, \Action_{1:\timestep-1})$ are said to be \emph{$(\epsilon, \delta)$-approximate information state} (AIS) with respect to a function space $\mathfrak{F}$ if there exist: (i)~a reward approximation function $\hat r: \featurespace \times \actionspace \to \real$, and (ii)~an approximate transition kernel $\hat\transition: \featurespace \times \actionspace \to \Delta(\statespace)$ such that $\Ais$ satisfies the following properties:
            \begin{compactitem}
                \item[(P1)] \label{p1} Sufficient for approximate performance evaluation: for all $\timestep$,
                \begin{equation}
                     | \cost(\State_{\timestep}, \Action_{\timestep})
                                            - \hat{\cost}(\Feature_\timestep, \Action_\timestep)| \leq \epsilon.
                 \label{def:p1}
                \end{equation}
                \item[(P2)]\label{p2b} Sufficient for predicting future states approximately: for all $\timestep$
                \begin{equation}
                 d_{\mathfrak{F}}(\transition(\cdot \vert \State_\timestep, \Action_\timestep), \hat \transition(\cdot\vert \Feature_\timestep, \Action_\timestep)) \leq \delta.
                % \text{with }\kappa_{\ipm}(\hat{f}_\timestep) &\define \sup_{\history_\timestep, \action_\timestep} \bigg[ \kappa_{\ipm}(\hat{f}_\timestep (\aisfunction_\timestep (\history_\timestep), \cdot, \action_\timestep))\bigg].
                \end{equation}
            \end{compactitem}
        \end{definition}
        We call the tuple $(\hat r ,\hat\transition)$ as an $(\epsilon, \delta)$-AIS approximator. Note that similar definitions have appeared in other works \eg, latent state \citep{deepmdp}, and approximate information state for for POMDPs \citep{ais-1,ais-2}. However, in \citep{deepmdp} it is assumed that the feature abstractions are memory-less and the discussion is restricted to Wasserstein distance. The key difference from the POMDP model in \citep{ais-1,ais-2} is that the in POMDPs the observation $\Feature_\timestep$ is a pre-specified function of the state while in the proposed model $\Feature_\timestep$ depends on our choice of feature abstraction.
        
        As such, our key insight is that an AIS-approximator of a recursively updatable history-based feature abstraction can be used to define a dynamic program. In particular, given a history-based abstraction function $\{\aisfunction_\timestep: \historyspace_\timestep \to \featurespace\}_{\timestep \geq 1}$ which is recursively updatable using $\hat f$ and an $(\epsilon, \delta)$ AIS-approximator $(\hat \transition, \hat \cost)$, we can define the following dynamic programming decomposition: 
        
        For any $\feature_\timestep \in \featurespace, \ \action_\timestep \in \actionspace$
        \begin{subequations}\label{eq:ais-dp}
            \begin{align}
                   \hat Q(\ais_\timestep, \action_\timestep) = \hat \cost(\ais_\timestep, \action_\timestep) + \discount \sum_{\sts_{\timestep+1} \in \statespace}
                    \hat \transition(\sts_{\timestep+1}|\ais_\timestep,\action_\timestep) \hat \valuefunction(\hat{f}(\ais_\timestep,\sts_{\timestep+1},\action_\timestep));&&
                    \hat \valuefunction(\ais_\timestep) = \max_{\action_\timestep \in \actionspace} \hat Q(\ais_\timestep,\action_\timestep), \quad\forall \feature_\timestep \in \featurespace 
            \end{align}
        \end{subequations}
        \begin{definition}\label{def:policy}
            Define $\mu \colon \aisspace \to \Delta(\actionspace)$ be any policy such that for any $\ais \in \aisspace$,
            \begin{align}
                \support(\mu(\ais)) \subseteq 
                \arg\max_{\action \in \actionspace} \hat Q(\ais,\action).\label{eq:policy}
            \end{align}
         Since $\mu$ is a policy from the feature space to actions, we can use it to define a policy from the history of the state action pairs to actions as:
         \begin{align}
             \policy{}_{\timestep}(\sts_{1:\timestep}, \action_{1:\timestep-1}) \define \mu(\aisfunction_{\timestep}(\sts_{1:\timestep}, \action_{1:\timestep-1})) \label{eq:policy-definition}
         \end{align}
        \end{definition}
        Therefore, the dynamic program defined in \eqref{eq:ais-dp} indirectly defines a history-based policy $\policy{}$. The performance of any such history-based policy is given by the following dynamic program: 
        
        For any $\feature \in \featurespace, \ \action \in \actionspace$
        \begin{subequations}
            \begin{align}
                     Q^{\policy{}}_{\timestep}(\history_\timestep, \action_\timestep) = \cost(\sts_\timestep, \action_\timestep) + \discount \sum_{\sts_{\timestep+1} \in \statespace}
                    \transition(\sts_{\timestep+1}|\sts_\timestep,\action_\timestep) \valuefunction_{\timestep+1}^{\policy{}}(\history_{\timestep+1}); && 
                     \valuefunction_{\timestep}^{\policy{}}(\history_\timestep) = \max_{\action \in \actionspace}  Q_{\timestep}^{\policy{}}(\history_\timestep,\action_\timestep), \label{eq:hist-dp}
            \end{align}
        \end{subequations}
        We want to quantify the loss in performance when using the history based policy $\policy$. Note that since $\valuefunction_\timestep^\policy$ is not time-homogeneous, we need to compute the worst-case difference between $\valuefunction^{\star}$ and $\valuefunction_\timestep^\policy$, which is given by:
        \begin{equation}
            \Delta \define\sup_{\timestep \geq 0}\sup_{\history_\timestep = (\sts_{1:\timestep}, \action_{1:\timestep}) \in \historyspace_\timestep} \vert \valuefunction^{\star}(\sts_\timestep) - \valuefunction^{\policy{}}_{\timestep}(\history_\timestep)\vert, \label{eq:sup-v}
        \end{equation}
        
        Our main approximation result is the following:
    
        % A natural question which then arises is that \emph{how far from optimal is $\policy{}$?}
        % We answer this question in the following result:
        \begin{theorem}\label{thm:ais-dp}
            % For any time $\timestep$, any realisation $\sts_\timestep$ of $\State_\timestep$, $\action_\timestep$ of $\Action_\timestep$, let $\history_\timestep = (\sts_{1:\timestep}, \action_{1:\timestep-1})$, and $\ais_\timestep = \aisfunction_\timestep(\history_\timestep)$.
             The worst case difference between $\valuefunction^{\star}$ and $\valuefunction^{\policy{}}_{\timestep}$ is bounded by
            %     \begin{align}
            %     \Delta &\define \sup_{\timestep \geq 0}\sup_{\history_\timestep = (\sts_{1:\timestep}, \action_{1:\timestep}) \in \historyspace_\timestep} \vert \valuefunction^{\star}(\sts_\timestep) - 
            %      \valuefunction^{\policy{}}(\ais_\timestep)\vert. \label{eq:sup-v}
            %     \end{align}
            % Then 
            \begin{equation}
                \Delta
                \le 2 \frac{\varepsilon + \discount\delta \kappa_{\mathfrak{F}}(\hat \valuefunction^{\mu}, \hat{f})}{1 - \discount},\label{eq:ais-bound}
            \end{equation}
        where $\kappa_{\mathfrak{F}}(\hat \valuefunction^{\mu}, \hat f)$ = $ \sup_{\feature, \action}\rho_{\mathfrak{F}}(\hat \valuefunction^{\mu}(\hat f(\cdot, \feature, \action)))$, $\rho_{\mathfrak{F}}(\cdot)$ is the Minkowski functional associated with the IPM $\ipm$ as defined in \eqref{eq:minkowski-functional}.
        \end{theorem}
        Proof in \Cref{sec:proof:thm:ais-dp}
        
        % The above result can be interpreted as follows. Given any history based feature abstraction function and a parametric family $\mathfrak{R}(\aisparams)$ and $\mathfrak\transition(\aisparams)$ of functions (where $\aisparams$ are the parameters) we can find the best AIS approximation as: 
        % \begin{align}
        %     \hat \cost^{\star} &= \arginf_{\hat \cost(\cdot;\aisparams) \in \mathfrak\Cost(\aisparams)} \Vert \cost - \hat \cost(\cdot; \aisparams)\Vert_\infty.
        % \end{align}
        % and 
        % \begin{align}
        %     \hat \transition^{\star} &= \arginf_{\hat \transition(\cdot; \aisparams) \in \mathfrak\transition( \aisparams)}\sup_{\timestep \geq 0}\sup_{\history_\timestep \in \historyspace_\timestep}\ipm(\transition(\cdot\vert\sts_\timestep,\action_\timestep), \hat\transition(\cdot\vert \ais_\timestep, \action_\timestep, \aisparams)).
        % \end{align}
        % Then define
        % \begin{align}
        %     \epsilon &= \Vert \cost - \hat \cost^{\star}(\cdot;\aisparams)\Vert_{\infty}.\\
        %     \delta &= \sup_{\timestep \geq 0}\sup_{\history_\timestep \in \historyspace_\timestep} \ipm(\transition\cdot\vert\sts_\timestep,\action_\timestep), \hat\transition^{\star}(\cdot\vert\ais_\timestep,\action_\timestep, \aisparams)).
        % \end{align}
        % Then, by construction $(\hat \cost^{\star}, \hat \transition^{\star})$ is an $(\epsilon, \delta)$ AIS-approximator, and is the best approximation within a parametric family $\mathfrak\Cost(\aisparams), \mathfrak\transition(\aisparams)$. \Cref{thm:ais-dp} then provides the approximate error in choosing a history based policy based on $(\hat \cost^{\star}, \hat \transition^{\star})$, which is obtained using \eqref{eq:policy}.
        
        Some salient features of the bound are as follows:
        First, the bound depends on the choice of metric on probability spaces. Different IPMs will result in a different value of $\delta$ and also a different value of $\kappa_{\mathfrak F}(\hat{\valuefunction}^{\mu}, \hat f)$. Second, the bound depends on the properties of $\hat{\valuefunction}^{\mu}$. For this reason we call it an instance dependent bound. Sometimes, it is desirable to have bounds which do not require solving the dynamic program in \eqref{eq:ais-dp}. We present such bounds as below, note that these ``instance independent'' bounds are the derived by upper bounding $\kappa_{\mathfrak{F}}(\hat{\valuefunction}^{\mu}, \hat f)$. Therefore, these are looser than the upper bound in \Cref{thm:ais-dp}
        
        % In \eqref{eq:ais-bound}, $\epsilon$ and $\delta$, capture the worst case error incurred by the AIS when predicting instantaneous reward $\cost$ and approximating the transition distribution of the ground MDP $\mdp$. Therefore, resulting suboptimality bound helps us quantify us the loss in performance due to feature abstraction. At the same time, the IPM used for measuring the distance between the approximate and true transition distribution also influences the bound via $\kappa_{ \mathfrak{F}}(\hat \valuefunction^{\mu}, \hat f)$. In the following corollaries we will show how $\kappa_{ \mathfrak{F}}(\hat \valuefunction^{\mu}, \hat f)$ takes a specific form according the choice of the IPM.
        
         \begin{corollary}\label{THM:TV-BOUND}
             If the function class $\mathfrak{F}$ is $\mathfrak{F}^{\text{TV}}$, then $\Delta$ as defined in \eqref{eq:sup-v} is upper bounded as:
             \begin{align}
              \Delta
                \le  \frac{2\epsilon}{(1-\discount)} +  \frac{\discount\delta \spn(\hat\cost)} {(1-\discount)^2}.
            \end{align}
        \end{corollary}
        
            Proof in Appendix \ref{sec:tv-proof}
        
        \begin{corollary}\label{THM:LIP-BOUND}
            Let $L_{\hat \cost}$ and $L_{\hat{\transition}}$ denote the Lipschitz constants of the approximate reward function $\hat \cost$ and approximate transition function $\hat \transition$ respectively, and $L_{\hat f}$ is the uniform bound on the Lipschitz constant of $\hat f$ with respect to the state $\State_\timestep$.
            If $\discount L_{\hat \transition}L_{\hat f} \leq 1$ and the function class $\mathfrak{F}$ is $\mathfrak{F}^{\text{W}}$, then $\Delta$ as defined in  \eqref{eq:sup-v} is upper bounded as:
            \begin{align}
                \Delta
                \le  \frac{2\epsilon}{(1-\discount)} + \frac{2\discount\delta L_{\hat \cost} }{(1- \discount)(1-\discount L_{\hat f}L_{\hat\transition})}.
            \end{align}
        \end{corollary}

            Proof in Appendix \ref{sec:w-proof}

        \begin{corollary}\label{thm:mmd-bound}
             If the function class $\mathfrak{F}$ is $\mathfrak{F}^{\text{MMD}}$, then $\Delta$ as defined in \eqref{eq:sup-v} is upper bounded as:
             \begin{align}
              \Delta
                \le  2 \frac{\epsilon + \discount\delta\kappa_{\mathcal{U}}(\hat \valuefunction, \hat f) } {(1-\discount)},
            \end{align}
            where $\mathcal{U}$ is a RKHS space,  $\Vert\cdot\Vert_{\mathcal{U}}$ its associated norm and $\kappa_{\mathcal{U}}(\hat \valuefunction, \hat f) = \sup_{\feature, \action}\Vert(\hat \valuefunction(\hat f(\cdot, \feature, \action)))\Vert_{\mathcal{U}}$.
        \end{corollary}
        \begin{proof}
            The proof follows from the properties of MMD described previously. 
        \end{proof}
        
        In the following section we will show how one can use these theoretical insights to design a policy search algorithm.
        