\section{Background and Motivation}\label{sec:background}
        \begin{figure*}[!ht]
          \centering
          \begin{minipage}{0.75\linewidth}
          \subfigure[$P(0)$]{
            \begin{tikzpicture}[thick,scale=0.65]
              \node [agent] at (0, 0) (0) {$0$};
              \node [agent] at (1, 0) (1) {$1$}; 
              \node [agent] at (0, 1) (3) {$3$};
              \node [agent] at (1, 1) (2) {$2$};
              \path[->]
                    (0) edge node[below] {$0.5$} (1)
                    (1) edge node[right] {$0.5$} (2)
                    (2) edge node[above] {$0.5$} (3)
                    (3) edge node[left]  {$0.5$} (0)
                    (0) edge[loop left] node {$0.5$} (0)
                    (1) edge[loop right] node {$0.5$} (1)
                    (2) edge[loop right] node {$0.5$} (2)
                    (3) edge[loop left] node {$0.5$} (3)
                    ;
            \end{tikzpicture}}
           \hfill
          \subfigure[$P(1)$]{
            \centering
            \begin{tikzpicture}[thick,scale=0.75]
              \node [agent] at (0, 0) (0) {$0$};
              \node [agent] at (1, 0) (1) {$1$}; 
              \node [agent] at (0, 1) (3) {$3$};
              \node [agent] at (1, 1) (2) {$2$};
              \path[<-]
                    (0) edge node[below] {$0.5$} (1)
                    (1) edge node[right] {$0.5$} (2)
                    (2) edge node[above] {$0.5$} (3)
                    (3) edge node[left]  {$0.5$} (0)
                    (0) edge[loop left] node {$0.5$} (0)
                    (1) edge[loop right] node {$0.5$} (1)
                    (2) edge[loop right] node {$0.5$} (2)
                    (3) edge[loop left] node {$0.5$} (3)
                    ;
            \end{tikzpicture}}
            \hfill
            \subfigure[$P(2)$]{%
                  \begin{tikzpicture}[thick,scale=0.65]
                  \node [agent] at (0, 0) (0) {$0$};
                  \node [agent] at (1, 0) (1) {$1$}; 
                  \node [agent] at (0, 1) (3) {$3$};
                  \node [agent] at (1, 1) (2) {$2$};
                  \path[->]
                        (0) edge [bend right] node[below] {$0.5$} (1)
                        (1) edge [bend right] node[right] {$0.5$} (2)
                        (2) edge [bend right] node[above] {$0.5$} (3)
                        (3) edge [bend right] node[left]  {$0.5$} (0)
                        (0) edge [bend right] node[right] {} (3)
                        (3) edge [bend right] node[below] {} (2)
                        (2) edge [bend right] node[left]  {} (1)
                        (1) edge [bend right] node[above]  {} (0);
                \end{tikzpicture}}
                \caption{The transition probability model for the example of}
          \label{fig:P}
          \end{minipage}%
          \begin{minipage}{0.25\linewidth}
          \centering
            \begin{tikzpicture}[thick,scale=0.9]
              \node [agent] at (0, 0) (0) {$0$};
              \node [agent] at (1, 0) (1) {$1$}; 
              \node [agent] at (0, 1) (3) {$3$};
              \node [agent] at (1, 1) (2) {$2$};
              \path[->]
                    (0) edge node[below] {$0.5$} (1)
                    (1) edge[bend right] node[right] {$0.5$} (2)
                    (2) edge[bend right]  node[left] {$0.5$} (1)
                    (3) edge node[left]  {$0.5$} (0)
                    (3) edge node[above] {$0.5$} (2)
                    (0) edge[loop left] node {$0.5$} (0)
                    (1) edge[loop right] node {$0.5$} (1)
                    (2) edge[loop right] node {$0.5$} (2)
                    (3) edge[loop left] node {$0.5$} (3)
                    ;
            \end{tikzpicture}
          \caption{The transition probability of the optimal policy}
          \label{fig:optimal}
          \end{minipage}
        \end{figure*}

    % \subsection{Function approximation in MDPs}
    
        Consider an MDP $ \mdp = \langle \statespace, \actionspace, \transition, \cost, \discount \rangle$ where $\statespace$ denotes the state space, $\actionspace$ denotes the action space, $\transition$ denotes the controlled transition matrix, $\cost \colon \statespace \times \actionspace \to \real$ denotes the per-step reward, and $\discount \in (0,1)$ denotes the discount factor.
                
        The performance of a randomised (and possibly history-dependent) policy $\policy$
        starting from a start state $\sts_{0}$ is measured by the value function, defined as:
        \begin{equation}
          \valuefunction^{\policy}(\sts_0) = \expecun{}^{\policy}\bigg[\sum_{\timestep=0}^{\infty}\discount^{\timestep}\cost(\State_{\timestep},\Action_{\timestep}) \bigg| \State_{0} = \sts_{0}\bigg].
        \end{equation}
        A policy maximising $\valuefunction^{\policy}(\sts_0)$ over all (randomized and possibly history dependent) policies is called the optimal policy with respect to initial state $s_0$ and is denoted by $\policy^{\star}$\footnote{This notion can easily be extended to start state distributions.}.
                
        In many applications, $\statespace$ and $\actionspace$ are combinatorially large or uncountable, which makes it intractable to compute the optimal policy. 
        Most practical RL algorithms overcome this hurdle by using function approximation where the state is mapped to a feature space $\featurespace$ using a state abstraction function $\basis:\statespace \to \featurespace$. In Deep-RL algorithms, the last layer of the network is often viewed as a feature vector. These feature vectors are then used as an approximate state for approximating the value function $\hat\valuefunction: \featurespace \to \real$ and/or computing an approximately optimal policy $\mu:\featurespace \to \Delta(\actionspace)$~\citep{Sutton+Barto:1998} (where $\Delta(\actionspace)$ denotes the set of probability distribution over actions). We use $\tilde{\mu} = \mu \circ \basis$ ~\ie, $\tilde{\mu} = \mu(\phi(\cdot))$ to denote the ``flattened" policy that maps states to action distributions.
        
        % The features $\Feature_\timestep$ are then used as an approximate state for computing the value function $\hat\valuefunction: \featurespace \to \real$ and/or the policy $\mu:\featurespace \to \Delta(\actionspace)$~\citep{Sutton+Barto:1998} (where $\Delta(\actionspace)$ denotes the set of probability distribution over actions). We denote by $\mu \circ \phi $ as the ``flattened" policy that maps states to action distributions, by composing the feature abstraction and the policy acting on it. In Deep-RL algorithms, the output of the last layer of the network is often viewed as the feature vector $\Feature_{\timestep}$.
        
        % A well known yet often overlooked fact about function approximation is that the features used as a proxy state may not satisfy the controlled Markov property \ie, in general,
        A well known fact about function approximation is that the features that are used as an approximate state may not satisfy the controlled Markov property \ie, in general, 
        \[
            \prob(\Feature_{\timestep+1} \mid \Feature_{1:\timestep}, \Action_{1:\timestep}) \neq
            \prob(\Feature_{\timestep+1} \mid \Feature_\timestep, \Action_\timestep).
        \]

        To see the implications of this fact, consider the toy MDP depicted in Fig.~\ref{fig:P}, with $ \statespace = \{0, 1, 2, 3\}$, $\actionspace =
        \{0, 1, 2\}$, $\{\transition_{\sts, \sts'}(\action)\}_{\action \in \actionspace}$, and $r(0) = r(1) = -1$, $r(2) = 1$, $r(3) = -K$, where $K$ is a large positive number.
       Given the reward structure the objective of the policy is to try to avoid state~$3$ and keep the agent at state~$2$ as much as possible. It is easy to see that the optimal policy is 
            \[
              \pi^\star(0) = 0, \quad
              \pi^\star(1) = 0, \quad
              \pi^\star(2) = 1,
              \text{ and}\quad
              \pi^\star(3) = 2.
            \]
        The Markov chain induced by the optimal policy is shown in Fig.~\ref{fig:optimal}. 
        Note that if the initial state is not state~$3$ then an agent will never visit that state under the optimal policy. Furthermore, any policy which cannot prevent the agent from visiting state~$3$ will have a large negative value and, therefore, cannot be optimal.
        Now suppose the feature space $\featurespace = \{0, 1\}$. It is easy to see that for any Markovian feature-abstraction $\policyencoder{} \colon \statespace \to \featurespace$, no policy $\hat \pi \colon \featurespace \to \actionspace$ can prevent the agent from visiting state~$3$. Thus, the best policy when using Markovian feature abstraction will perform significantly worse than the optimal policy (which has direct access to the state).
    
        However, it is possible to construct a history-based feature-abstraction  $\policyencoder$ and a history-based control policy $\hat \pi$ that works with $\phi$ and is of the same quality as $\pi^\star$. For this, consider the following \emph{codebooks} (where the entries denoted by a dot do not matter):
        \begin{align*}
          F(1) &= \begin{bmatrix}
            0 & 1 & \cdot & \cdot \\
            \cdot & 0 & 1 & \cdot \\
            \cdot & \cdot & 0 & 1 \\
            1 & \cdot & \cdot & 0
          \end{bmatrix}, 
          &
          F(2) &= \begin{bmatrix}
            1 & \cdot & \cdot & 0 \\
            0 & 1 & \cdot & \cdot \\
            \cdot & 0 & 1 & \cdot \\
            \cdot & \cdot & 0 & 1
          \end{bmatrix} ,
           \\
          F(3) &= \begin{bmatrix}
            \cdot & 0 & \cdot & 1 \\
            0 & \cdot & 1 & \cdot \\
            \cdot & 0 & \cdot & 1 \\
            0 & \cdot & 1 & \cdot
          \end{bmatrix},
          &
          D(0) &= \begin{bmatrix}
            0 & 1 \\
            1 & 2 \\
            2 & 3 \\
            3 & 0 
          \end{bmatrix} ,
          \\
          D(1) &= \begin{bmatrix}
            3 & 0 \\
            0 & 1 \\
            1 & 2 \\
            2 & 3 
          \end{bmatrix} ,
          &
          D(2) &= \begin{bmatrix}
            1 & 3 \\
            0 & 2 \\
            1 & 3 \\
            0 & 2 
          \end{bmatrix} .
        \end{align*}
        Now consider the following feature-abstraction:
        \[
          Z_t =  F_{S_{t-1}, S_t}(A_{t-1}).
        \]
        and a control policy $\mu$ which is a finite state machine with memory $M_t$ that is updated as:
        \[
          M_t = D_{M_{t-1}, Z_t}(A_{t-1}).
        \]
        and chooses the actions according to:
        \[
          \quad
          A_t = \pi(M_t).
        \]
        where $\pi \colon \statespace \to \Delta(\actionspace)$ is any pre-specified reference policy. It can be verified that if the system starts from a known initial state then $\mu \circ \basis = \pi$. 
         Thus, if we choose the reference policy $\pi=\pi^\star$, then
        the agent will never visit state~$3$ under $\mu \circ \basis$, in contrast to Markovian feature-abstraction policies where (as we argued before) state~$3$ is always visited.
        
        From the above example, we can see that when system dynamics are known, one can exploit the problem structure to design history-based feature abstractions which outperform their memoryless counterparts. However, such constructions are not feasible in the general learning setup, where the system dynamics are unknown. 
        % One way to overcome this issue is to let the policy $\policy{}$ be a function of all the information available to the system at time $\timestep$  ~\ie, $\policy{}_\timestep: \historyspace_{\timestep} \to \Delta(\actionspace)$ where, $\historyspace_\timestep \define \{\statespace^{1:\timestep},\featurespace^{1:\timestep-1}, \actionspace^{1:\timestep-1}\}$. 
        We can overcome this issue by letting the policy be a function of the history of state, actions and observations observed until time $\timestep$, \ie, $\policy{}_\timestep: \historyspace_{\timestep} \to \Delta(\actionspace)$ where, $\historyspace_\timestep \define \{\statespace \times \featurespace \times \actionspace\}$. 
        
        % Since the environment is an MDP, the state $\State_\timestep$ is  a sufficient statistic and the loss of information happens when $\State_\timestep$ is mapped to $\Feature_\timestep$. This is a subtle yet critical distinction between POMDPs and function approximation in MDPs. In POMDPs the state signal is unavailable, and the system only sees a observation vector $\Feature_\timestep$. Whereas, in function approximation setup, we can observe the state signal perfectly and partial observability is induced due to feature abstraction. Therefore, we can discard the the history of states and focus on the policies of the following form $\policy{}_\timestep: \historyspace_{\timestep} \to \Delta(\actionspace)$ where, $\historyspace_\timestep \define \{\statespace,\featurespace^{1:\timestep-1}, \actionspace^{1:\timestep-1}\}$.   
        
        As the size of history grows with time $t$ and it is computationally and conceptually difficult to use it for deriving a dynamic programming decomposition. One solution is to map the history to a fixed dimensional representation and use it as a state for designing a dynamic program. One can think of such a representation as an information state~\citep{kwakernaakh.1965,infostate-Witsenhausen,Striebel1965SufficientSI,Bohlin1970InformationPF,Davis1972InformationSF,Kumar1986StochasticSE}.  Due to lossy compression a prefect information state may be hard to obtain, but we an construct an approximate information state (AIS) such that it satisfies the controlled Markov property and captures the sufficient information for identifying the policy. In the next section we will show how to construct and AIS and use it for obtaining a dynamic program, which can then be solved, either exactly or approximately, to obtain a policy with a bounded approximation error. 
        



\section{Model}\label{sec:main}

        The approximation results of our framework depend on the properties of metrics on probability spaces. In particular, we derive results for Total Variation (TV) distance, Wasserstein/Kantorovich-Rubinstein distance and Maximum-Mean Discrepancy (MMD) distance. All of these metrics are instances of Integral Probability Measures (IPMs) -- a class of metrics that have a dual characterisation. Viewing all the above metrics through the lens of IPMs allows us to derive a general approximation bound that holds for all the instances of IPMs, and then specialise the bound to specific instances \ie, TV, Wasserstein and MMD distance. Before defining our main model we will briefly describe the important properties of IPMs necessary for deriving our results.
        
        % \subsection{Integral probability metrics (IPM)}
            \begin{definition}[~\citep{ipm}]\label{def:ipm}
                    Let $(\mathcal{E},\mathcal{G})$ be a measurable space and $\mathfrak{F}$ denote a class of uniformly bounded measurable functions on $(\mathcal{E},\mathcal{G})$. The integral probability metric between two probability distributions  $\nu_1, \nu_2 \in \mathcal{P}(\mathcal{E})$ with respect to the function class $\mathfrak{F}$ is defined as:
                    \begin{align}
                        \ipm(\nu_1, \nu_2) &= \sup_{f \in \mathfrak{F}}\bigg| \int_{\mathcal{E}}f d\nu_1  - \int_{\mathcal{E}}f d\nu_2\bigg|.\label{eq:def-ipm}
                    \end{align}
                    
                    For any function $f$ (not necessarily in $\mathfrak{F}$), the Minkowski functional $\rho_{\mathfrak{F}}$ associated with the $\ipm$ is given as:
                    \begin{align}
                        \rho_{\mathfrak{F}}(f)&\define \inf\{\rho \in \real_{\geq 0}: \rho^{-1}f \in \mathfrak{F}\}.\label{eq:minkowski-functional}
                    \end{align}
                    
                    \Cref{eq:minkowski-functional}, implies that that for any function $f$:
                    \begin{align}
                         \bigg|\int_{\mathcal{E}}fd\nu_1 - \int_{\mathcal{E}}fd\nu_2 \bigg|\leq \rho_{\mathfrak{F}}(f)\ipm(\nu_1,\nu_2). \label{eq:ipm-implication}
                     \end{align}\label{eq:ipm-function-diff}
                    
            \end{definition}
            \noindent We will now list the types of IPMs used in this paper with their Minkowski functionals.
            \begin{compactitem}
                \item[1.]\label{def:tv-dist}{\bf Total Variation Distance}: If $\mathfrak{F}$ is chosen as $\mathfrak{F}^{\text{TV}} \define \{\frac{1}{2}\spn(f) = \frac{1}{2}(\max(f)- \min(f))\}$, then $\ipm$ is the total variation distance, and its Minkowski functional is: $\rho_{\mathfrak{F}^{\text{TV}}}(f) = \frac{1}{2}\spn(f)$. 
                \item[2.]\label{def:kr-dist}{\bf Wasserstein/Kantorovich-Rubinstein Distance}: If $\mathcal{E}$ is a metric space and $\mathfrak{F}$ is chosen as $\mathfrak{F}^{W} \define \{f: L_f \leq 1 \}$(where $L_f$ denotes the Lipschitz constant of $f$ with respect to the metric on $\mathcal{E}$), then $\ipm$ is the Wasserstein or the Kantorovich distance. The Minkowski function for the Wasserstein distance is a $\rho_{\mathfrak{F}^W}(f) = L_f$.
                \item[3.] \label{def:mmd-dist}{\bf Maximum Mean Discrepancy (MMD) Distance}: Let $\mathcal{U}$ be a reproducing kernel Hilbert space (RKHS) of real-valued functions on $\mathcal{E}$ and $\mathfrak{F}$ is choosen as $\mathfrak{F}^{MMD} \define \{f\in \mathcal{U}: \Vert f \Vert _{\mathcal{U}} \leq 1 \}$, (where $\Vert \cdot \Vert_{\mathcal{U}}$ denotes the RKHS norm), then $\ipm$ is the Maximum Mean Discrepancy (MMD) distance and its Minkowski functional is $\rho_{\mathfrak{F}^{\text{MMD}}}(f) = \Vert f\Vert _{\mathcal{U}}$.
            \end{compactitem}

Consider a Markov decision process (MDP) $\mdp = \langle \statespace, \actionspace, \cost, \transition, \discount \rangle$ with state-space $\statespace$, action space $\actionspace$, reward function $\cost$, transition kernel $\transition$, and discount factor $\discount$.

Let $\{\aisfunction\}_{\timestep \geq 0}$ be a family of history-dependent feature abstraction functions. We say that the features $\Ais_\timestep = \aisfunction_\timestep (\State_{1:\timestep}, \Action_{1:\timestep-1})$ are an $(\epsilon, \delta)$ \emph{approximate information state} (AIS) if there exist a  reward approximation function $\hat \cost: \aisspace \times \actionspace \to \real$ and approximate transition kernel $\hat \transition: \aisspace \times \actionspace \to \Delta(\statespace)$ which satisfy the following properties: 
(P1)~\ul{Sufficient for approximate performance evaluation:} for all $\timestep$, $ \vert \cost(\State_{\timestep}, \Action_{\timestep})- \hat{\cost}(\Feature_\timestep, \Action_\timestep)\vert \leq \epsilon \label{def:p1}$, and 
(P2) \ul{Sufficient for predicting future states approximately:} for all $\timestep$, $ d_{\mathfrak{F}}(\transition(\cdot \vert \State_\timestep, \Action_\timestep), \hat \transition(\cdot\vert \Feature_\timestep, \Action_\timestep)) \leq \delta$, where $d_{\mathfrak{F}}$ is an integral probability measure (IPM)%\footnote{Given a class $\mathfrak{F}$ of uniformly measurable functions on a measurable space $(\mathcal{E}, \mathcal{G})$, the integral probability metric (IPM) between two probability distributions  $\nu_1, \nu_2 \in \mathcal{P}(\mathcal{E})$ with respect to the function class $\mathfrak{F}$ is given by: $\ipm(\nu_1, \nu_2) = \sup_{f \in \mathfrak{F}}\vert \int_{\mathcal{E}}f d\nu_1  - \int_{\mathcal{E}}f d\nu_2\vert$~\cite{ipm}.} 
 on $\Delta(\statespace)$. We call $(\hat{\cost}, \hat \transition)$ as an $(\epsilon, \delta)$-\emph{AIS generator}.
    
%The metric $d_{\mathfrak{F}}$ captures the distance between the MDP's transition probability distribution $\transition$, and the approximate transition distribution $\hat \transition$ based on $\Ais_\timestep$,  and $\Action_\timestep$. 
% Moreover, for any function $f$ (not necessarily in $\mathfrak{F}$), the Minkowski functional $\rho_{\mathfrak{F}}$ associated with the $\ipm$ is given as: $\rho_{\mathfrak{F}}(f)\define \inf\{\rho \in \real_{\geq 0}: \rho^{-1}f \in \mathfrak{F}\}\label{eq:minkowski-functional}$, this implies that for any function $f$: $\vert\int_{\mathcal{E}}fd\nu_1 - \int_{\mathcal{E}}fd\nu_2 \vert\leq \rho_{\mathfrak{F}}(f)\ipm(\nu_1,\nu_2) \label{eq:ipm-implication}$. 
    
    Consider a history-based feature map that is recursively updatable \ie, there exists an update function $\hat f$ such that $\Ais_{\timestep+1} = \hat f (\Ais_\timestep, \Action_\timestep, \State_{\timestep+1})$. Given such a recursively-updatable history based feature map and an $(\epsilon, \delta)$ AIS generator $(\hat \cost, \hat \transition)$, define the following dynamic program: 
    
    \begin{equation}
                \hat Q(\ais, \action) = \hat \cost(\ais, \action) + \discount \sum_{\sts \in \statespace}
                \hat \transition(\sts|\ais,\action) \hat \valuefunction(\hat{f}(\ais,\sts,\action)), \ \forall \feature \in \featurespace, \action \in \actionspace \qquad
                \hat \valuefunction(\ais) = \max_{\action \in \actionspace} \hat Q(\ais,\action), \ \forall \feature \in \featurespace \label{eq:ais-dp}
    \end{equation}
    
    Now, let $\mu \colon \aisspace \to \Delta(\actionspace)$ be any policy such that for any $\ais \in \aisspace$, $\support(\mu(\ais)) \subseteq \arg\max_{\action \in \actionspace} \hat Q(\ais,\action)\label{eq:policy}$.
    Note that $\mu$ is a policy from the feature space to actions, and we can use it to define a policy from the history of the state action pairs to actions as: $\policy{}_{\timestep}(\sts_{1:\timestep}, \action_{1:\timestep-1}) = \mu(\aisfunction_{\timestep}(\sts_{1:\timestep}, \action_{1:\timestep-1}))$, which then implies that $\policy{} = \{\policy{}_\timestep\}_{\timestep \geq 1}$ is a history-based policy. Thus the dynamic program defined in \eqref{eq:ais-dp} indirectly defines a history-based policy $\policy{}$. We provide the following bound on the performance of $\policy{}$ and the optimal (state-based) policy $\policy^*$. 
    %A natural question which then arises is that \emph{how can we bound the sub-optimality gap between $\policy^\star$ and $\policy{}$?} We answer this question in the following result:
    \begin{theorem}\label{thm:ais-dp}
      For any time $\timestep$, any realisation $\sts_\timestep$ of $\State_\timestep$, $\action_\timestep$ of $\Action_\timestep$, let $\history_\timestep = (\sts_{1:\timestep}, \action_{1:\timestep-1})$, and $\ais_\timestep = \aisfunction_\timestep(\history_\timestep)$. Additionally, let $\valuefunction^{\policy{}}$ be the value of using the history-based policy $\policy{}$ as defined in \eqref{eq:policy} to control the MDP $\mdp$.
      Define the worst case difference between $\valuefunction^{\star}$ and $\valuefunction^{\policy{}}$ as, $   \Delta = \sup_{\timestep \geq 0}\sup_{\history_\timestep = (\sts_{1:\timestep}, \action_{1:\timestep}) \in \historyspace_\timestep} \vert \valuefunction^{\star}(\sts_\timestep) - 
      \valuefunction^{\policy{}}(\ais_\timestep)\vert$.
      Then 
      \begin{equation}
        \Delta
        \le 2 \frac{\varepsilon + \discount\delta \kappa_{\mathfrak{F}}(\hat \valuefunction^{\mu}, \hat{f})}{1 - \discount},\label{eq:ais-bound}
      \end{equation}
      where, $\kappa_{\mathfrak{F}}(\hat \valuefunction^\mu, \hat f)$ 
      is a quantity that depends on the choice of the IPM $d_{\mathfrak{F}}$. 
    \end{theorem}

    We can further show the following.
    (a)~When $\ipm$ is the total variation distance, then 
        $\kappa_{\mathfrak{F}}(\hat \valuefunction^\mu, \hat f) = \spn(\hat \cost)/(1-\discount)$. 
    (b)~When $\ipm$ is the Wasserstein distance, then
    $\kappa_{\mathfrak{F}}(\hat \valuefunction^\mu, \hat f) = L_{\hat \cost}/(1-\discount L_{\hat f} L_{\hat \transition})$, where for any function~$f$, $L_f$ denotes its Lipschitz constant.  
    (c)~When $\ipm$ is the Maximum Mean Discrepancy (MMD) distance, 
    $\kappa_{\mathfrak{F}}(\hat \valuefunction^{\mu}, \hat{f})  = \sup_{\feature, \action}\Vert(\hat \valuefunction(\hat f(\cdot, \feature, \action)))\Vert_{\mathcal{U}}$, where $\| \cdot \|_{\mathcal{U}}$ is the RKHS norm.
    
    %If $\ipm$ is the Kantorovich-Rubinstein/Wasserstein distance, then its Minkowski functional is $\rho_{\mathfrak{F}^W}(f) = L_f$ where $L_f$ denotes the Lipschitz constant of any function $f$ with respect to the metric on $\mathcal{E}$, and $\kappa_{\mathfrak{F}}(\hat \valuefunction^{\mu}, \hat{f}) = L_{\hat \cost}/1 - \discount L_{\hat f}L_{\hat \transition}$. Finally, if $\ipm$ is the Maximum Mean Discrepancy (MMD) distance, then its Minkowski functional is $\rho_{\mathfrak{F}^{\text{MMD}}}(f) = \Vert f\Vert _{\mathcal{U}}$, where for any function $f$, $\Vert \cdot \Vert_{\mathcal{U}}$ denotes the RKHS norm, and $\kappa_{\mathfrak{F}}(\hat \valuefunction^{\mu}, \hat{f})  = \sup_{\feature, \action}\Vert(\hat \valuefunction(\hat f(\cdot, \feature, \action)))\Vert_{\mathcal{U}}$.
    
    The above results bound the worst case performance loss of the best feature-based policy given a particular AIS generator. The loss function can be used as an auxiliary loss to optimise the AIS generator. In the next section, we build on this idea to construct an RL algorithm.
    
       
         \begin{corollary}\label{THM:TV-BOUND}
             If the function class $\mathfrak{F}$ is $\mathfrak{F}^{\text{TV}}$, then $\Delta$ as defined in  \Cref{thm:ais-dp} is upper bounded as:
             \begin{align}
              \Delta
                \le  \frac{2\epsilon}{(1-\discount)} +  \frac{\discount\delta \spn(\hat\cost)} {(1-\discount)^2},
            \end{align}
        \end{corollary}
        
            Proof in Appendix \ref{sec:tv-proof}
        
        \begin{corollary}\label{THM:LIP-BOUND}
            If $\discount L_{\hat \transition}L_{\hat f} \leq 1$ and the function class $\mathfrak{F}$ is $\mathfrak{F}^{\text{W}}$, then $\Delta$ as defined in  \Cref{thm:ais-dp} is upper bounded as:
            \begin{align}
                \Delta
                \le  \frac{2\epsilon}{(1-\discount)} + \frac{2\discount\delta L_{\hat \cost} }{(1- \discount)(1-\discount L_{\hat f}L_{\hat\transition})},
            \end{align}
        \end{corollary}
        where $L_{\hat \cost}$ and $L_{\hat{\transition}}$ are the Lipschitz constants of the approximate reward function $\hat \cost$ and approximate transition function $\hat \transition$ respectively, and $L_{\hat f}$ is the uniform bound on the Lipschitz constant of $\hat f$ with respect to the state $\State_\timestep$.

            Proof in Appendix \ref{sec:w-proof}

        \begin{corollary}\label{thm:mmd-bound}
             If the function class $\mathfrak{F}$ is $\mathfrak{F}^{\text{MMD}}$, then $\Delta$ as defined in  \Cref{thm:ais-dp} is upper bounded as:
             \begin{align}
              \Delta
                \le  2 \frac{\epsilon + \discount\delta\kappa_{\mathcal{U}}(\hat \valuefunction, \hat f) } {(1-\discount)},
            \end{align}
            where similar to \Cref{def:mmd-dist}, $\mathcal{U}$ is a RKHS space,  $\Vert\cdot\Vert_{\mathcal{U}}$ its associated norm and $\kappa_{\mathcal{U}}(\hat \valuefunction, \hat f) = \sup_{\feature, \action}\Vert(\hat \valuefunction(\hat f(\cdot, \feature, \action)))\Vert_{\mathcal{U}}$.
        \end{corollary}
        \begin{proof}
            The proof follows in a straight forward way from the properties of MMD described previously. 
        \end{proof}
    
   
