    \begin{figure*}[!htbp]
      \includegraphics[width=\linewidth]{Results/combined-1.pdf}
      % \caption{This is a figure} \label{fig:comb-results-1}
      \includegraphics[width=\linewidth]{Results/combined-2.pdf}
      \caption{Empirical results averaged over 50 Monte Carlo runs with shaded regions showing the interquantile range.} \label{fig:comb-results-2}
    \end{figure*}
    
  

    
   
\section{Empirical evaluation}\label{sec:experiments}


    % Through our experiments, we seek to answer the following questions:
    % (1) Can history-based feature representations policies help improve the quality of solution found by a memory-less RL algorithms?
    % (2) In terms of the solution quality how does the proposed method compare with other methods which use memory augmented policies as well as reward and transition predictors?
    % (3) How does the choice of IPM affect the algorithms performance?

    % We answer question (1) by comparing our approach with the proximal policy gradient (PPO)~\citep{ppo} and the policy-gradient version of DeepMDP framework~\citep{deepmdp}. For question (2) we compare our approach with modified versions of PlaNet~\citep{Pla-Net}, Dreamer~\citep{Dreamer}, and VariBAD~\citep{VariBad}.
    
    % For question (3) we compare the performance of our method using different MMD kernels and KL-divergence based approximation of Wasserstein distance. All the approaches are evaluated on six continuous control tasks from the MuJoCo~\citep{Todorov2012MuJoCoAP} OpenAI-Gym suite. To ensure a fair comparison, the baselines and their respective hyper-parameter settings are taken from well tested stand-alone implementations provided by~\citet{baselines}. From an implementation perspective, our framework can be used to modify any off-the-shelf policy-gradient algorithm by simply replacing (or augmenting) the feature abstraction layers of the policy and/or value networks with recurrent neural networks (RNNs), trained with the appropriate losses, as outlined previously. In these experiments, we replace the fully connected layers in PPO's architecture with a Gated Recurrent Unit (GRU). For all the implementations, we initialise the hidden state of the GRU to zero at the beginning of the trajectory. This strategy simplifies the implementation and also allows for independent decorrelated sampling of sequences, therefore ensuring robust optimisation of the networks~\citep{rnn-hausknecht}. It is important to note that we can extend our framework to other policy gradient methods such as SAC~\citep{HaarnojaZAL18}, TD3~\citep{td3} or DDPG~\citep{ddpg}, after satisfying certain technical conditions. However, we leave these extensions for future work. Additional experimental details and results can be found in \Cref{sec:experiment-details}.
    
    % From \Cref{fig:comb-results-2} one can observe that AIS results in performance improvements in high-dimensional environments like Ant, Humanoid and Walker. Other memory-augmented methods like \citep{Pla-Net,VariBad,Dreamer} also performance better as compared to memory-less methods. Overall these results lend credence to our observation of using history-based policies. In some environments we observe that methods which use planning subroutines \citep{Pla-Net,Dreamer} accelerate learning in the initial stages but then converge to a sub-optimal policy. This could be because of the inaccuracies in the approximate dynamics model used in the planning loop. An interesting future direction would be use planning loop similar to \citep{Pla-Net,Dreamer} in \Cref{alg:ciac-a}.
    
    
    Through our experiments, we seek to answer the following questions:
    (1) Can history-based feature representation policies help improve the quality solution found by a memory-less RL algorithm?
    (2) In regards to the solution quality and sample complexity, how does the proposed method compare with other memory-augmented policies?
    (3) How does the choice of IPM affect the algorithms performance?
    
    
     We answer question (1) and (2) by comparing our approach with the proximal policy gradient (PPO) algorithm which uses feed-forward neural networks. For question (2), we compare our method with an LSTM-based PPO variant which learns the feature representation using the history of states $\State_{1:\Timestep}$ in a trajectory. For question (3) we compare the performance of our method using different MMD kernels and KL-divergence based approximation of Wasserstein distance. All the approaches are evaluated on six continuous control tasks from the MuJoCo~\citep{Todorov2012MuJoCoAP} OpenAI-Gym suite. To ensure a fair comparison, the baselines and their respective hyper-parameter settings are taken from well tested stand-alone implementations provided by~\citet{baselines}. From an implementation perspective, our framework can be used to modify any off-the-shelf policy-gradient algorithm by simply replacing (or augmenting) the feature abstraction layers of the policy and/or value networks with recurrent neural networks (RNNs), trained with the appropriate losses, as outlined previously. In these experiments, we replace the fully connected layers in PPO's architecture with a Gated Recurrent Unit (GRU). For all the implementations, we initialise the hidden state of the GRU to zero at the beginning of the trajectory. This strategy simplifies the implementation and also allows for independent decorrelated sampling of sequences, therefore ensuring robust optimisation of the networks~\citep{rnn-hausknecht}. It is important to note that we can extend our framework to other policy gradient methods such as SAC~\citep{HaarnojaZAL18}, TD3~\citep{td3} or DDPG~\citep{ddpg}, after satisfying certain technical conditions. However, we leave these extensions for future work. Additional experimental details and results can be found in \Cref{sec:experiment-details}.
    
    
%\subsection{Numerical Results} 
    
     \Cref{fig:comb-results-2} contains the results of our experiments averaged over 50 Monte-Carlo evaluations using MMD-based AIS loss in \eqref{eq:mmd-ais-loss}.  These results show that our algorithm improves over the performance of both the baselines, and the performance gain is significantly higher for high-dimensional environments like Humanoid and Ant. 
     It is worth noticing that the GRU baseline also outperforms the feed-forward baseline for most  environments. Overall, these findings lend credence to  history-based encoding policies as a way to improve the quality of the solution learnt by the RL algorithm.
    
    
    
    
    
     \begin{figure*}[!htbp]
        \includegraphics[width=\linewidth]{Results/MMD.pdf}
        \caption{Comparison of different MMDs, averaged over 50 runs} \label{fig:MMD-comp}
    \end{figure*}

    
    Note that the MMD distance given by \eqref{eq:mmd-grad} in \Cref{sec:mmd-details}, can be computed using different types of characteristic kernels (for a detailed review see ~\citep{Sriperumbudur,NIPS2009_685ac8ca,sejdinovic}). In this paper we consider computing \eqref{eq:mmd-grad} using the Laplace, Gaussian and energy distance kernels. In in \Cref{fig:MMD-comp} we compre the perfromance of our methods under different MMD kernels. It can be observed that for the continuous control tasks in the MuJoCo suite, the energy distance yields better performance, and therefore we implement \cref{eq:mmd-grad} using the energy distance for the results in \Cref{fig:comb-results-2}.
    
    Next, we compare the performance of our method under MMD (Energy distance kernel) and Wasserstein distance. From \Cref{fig:Wass-res} we observe that for continuous control tasks, use of MMDs result in better performance as compared to Wasserstein distance. 
    
        \begin{figure*}[!htbp]
            \includegraphics[width=\linewidth]{Results/wass.pdf}
            \caption{Comparison of Wasserstein vs MMDs, averaged over 50 runs.} \label{fig:Wass-res}
        \end{figure*}
    
    % \begin{figure}[!htbp]
    %         \includegraphics[width=\linewidth]{Results/dist-new-1.pdf}
    %         \caption{MMD vs KL for Half Cheetah} \label{fig:dist-1}
    % \end{figure}
    
    % \begin{figure}[!htbp]
    %         \includegraphics[width=\linewidth]{Results/dist-new-2.pdf}
    %         \caption{MMD vs KL for Ant } \label{fig:dist-2}
    % \end{figure}
    
    % \begin{figure}[!htbp]
    %         \includegraphics[width=\linewidth]{Results/dist-new-3.pdf}
    %         \caption{MMD vs KL for Walker} \label{fig:dist-3}
    % \end{figure}
    
    