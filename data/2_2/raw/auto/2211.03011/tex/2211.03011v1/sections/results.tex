    \begin{figure}[!htbp]
      \includegraphics[width=\linewidth]{Results/combined-1.pdf}
      % \caption{This is a figure} \label{fig:comb-results-1}
      \includegraphics[width=\linewidth]{Results/combined-2.pdf}
      \caption{PPO using feed-forward NN, GRU and AIS, averaged over 50 Monte Carlo runs with $\pm$1 \texttt{std-dev}.} \label{fig:comb-results-2}
    \end{figure}
\section{Empirical evaluation}\label{sec:experiments}

    Through our experiments, we seek to answer the following questions:
    (1) Can history-based feature representations policies help improve the quality of solution found by a memory-less RL algorithms?
    (2) In terms of the solution quality how does the proposed method compare with other methods which use memory augmented policies as well as reward and transition predictors?
    (3) How does the choice of IPM affect the algorithms performance?

    % Through our experiments, we seek to answer the following questions:
    % (1) Can history-based feature representation policies help improve the quality solution found by a memory-less RL algorithm?
    % (2) In regards to the solution quality and sample complexity, how does the proposed method compare with other memory-augmented policies?
    % (3) How does the choice of IPM affect the algorithms performance?
    
    We answer question (1) and (2) by comparing our approach with the proximal policy gradient (PPO) algorithm which uses feed-forward neural networks. For question (2), we compare our method with an LSTM-based PPO variant which learns the feature representation using the history of states $\State_{1:\Timestep}$ in a trajectory. For question (3) we compare the performance of our method using different MMD kernels and KL-divergence based approximation of Wasserstein distance. All the approaches are evaluated on six continuous control tasks from the MuJoCo~\citep{Todorov2012MuJoCoAP} OpenAI-Gym suite. To ensure a fair comparison, the baselines and their respective hyper-parameter settings are taken from well tested stand-alone implementations provided by~\citet{baselines}. From an implementation perspective, our framework can be used to modify any off-the-shelf policy-gradient algorithm by simply replacing (or augmenting) the feature abstraction layers of the policy and/or value networks with recurrent neural networks (RNNs), trained with the appropriate losses, as outlined previously. In these experiments, we replace the fully connected layers in PPO's architecture with a Gated Recurrent Unit (GRU). For all the implementations, we initialise the hidden state of the GRU to zero at the beginning of the trajectory. This strategy simplifies the implementation and also allows for independent decorrelated sampling of sequences, therefore ensuring robust optimisation of the networks~\citep{rnn-hausknecht}. It is important to note that we can extend our framework to other policy gradient methods such as SAC~\citep{HaarnojaZAL18}, TD3~\citep{td3} or DDPG~\citep{ddpg}, after satisfying certain technical conditions. However, we leave these extensions for future work. Additional experimental details and results can be found in \Cref{sec:experiment-details}.
    
    
%\subsection{Numerical Results} 
    
     \Cref{fig:comb-results-2} contains the results of our experiments averaged over 50 Monte-Carlo evaluations using MMD-based AIS loss in \eqref{eq:mmd-ais-loss}.  These results show that our algorithm improves over the performance of both the baselines, and the performance gain is significantly higher for high-dimensional environments like Humanoid and Ant. 
     It is worth noticing that the GRU baseline also outperforms the feed-forward baseline for most  environments. Overall, these findings lend credence to  history-based encoding policies as a way to improve the quality of the solution learnt by the RL algorithm.
    % \begin{table}
    %     \resizebox{\columnwidth}{!}
    %     {
    %      \begin{tabular}{||c |c| c| c |c ||} 
    %      \hline
    %      Environment & PPO(1e3) & PPO-RNN(1e3) & Ciac-a(1e3) & Ciac-b(1e3) \\ [0.3ex] 
    %      \hline\hline
    %      Half-Cheetah & 1.64$\pm$0.74 & 2.91$\pm$1.13 & 5.16$\pm$0.47 & 4.30$\pm$0.38 \\ 
    %      \hline
    %      Ant & 3.36$\pm$0.18 & 4.16$\pm$ 0.14 & 6.24.87$\pm$0.23 & 5.86$\pm$0.15\\
    %      \hline
    %      Humanoid & 1.03$\pm$0.15 & 4.16$\pm$ 0.39 & 6.29$\pm$0.64 & 6.02$\pm$0.48\\
    %      \hline
    %      Humanoid-Standup & 14.61$\pm$3.61 & 15.14$\pm$ 9.99 & 18.41$\pm$8.70 & 16.36$\pm$6.25\\
    %      \hline
    %      Walker & 2.93$\pm$0.52 & 2.66$\pm$0.77 & 6.24$\pm$0.33 & 5.15$\pm$0.33\\ [1ex] 
    %      \hline
    %     \end{tabular}
    %     }
    %     \caption{Performance of algorithms compared evaluated across 50 independent runs reported with $\pm 1 \ \mathsf{std\_dev}$}
    %     \label{table:results}
    % \end{table}
 