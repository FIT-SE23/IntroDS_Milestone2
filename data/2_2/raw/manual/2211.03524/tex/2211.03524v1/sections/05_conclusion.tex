\section{Conclusion}
In this paper, we propose methods to polish representation learning for the Multimodal Review Helpfulness Prediction task.  In particular, we aim to advance cross-modal relation representations by learning mutual information through contrastive learning. In order to further enhance our framework, we propose an adaptive weighting strategy to encourage flexibility in optimization. Moreover, we integrate a cross-modal interaction module to loose the modelâ€™s reliance on unalignment nature among modalities, continuing to refine multimodal representations. Our framework is able to outperform prior baselines and achieve state-of-the-art results on the MRHP problem. 