\section{Training Strategies}
\subsection{Adaptive Cross-modal Contrastive Learning}
In this section, we explain the formulation and adaptive pattern along with its derivation of our Cross-modal Contrastive Learning.

\noindent\textbf{Cross-modal Contrastive Learning} First of all, we extract hidden states of helpful product-review pairs. Second of all, hidden features are max-pooled to extract meaningful entries.
\begin{gather}
    \mathbf{h}^p = \text{MaxPool}(H^p), \, \mathbf{h}^r = \text{MaxPool}(H^r) \\
    \mathbf{v}^p = \text{MaxPool}(V^p), \, \mathbf{v}^r = \text{MaxPool}(V^r) 
\end{gather}
We formulate our contrastive learning framework taking positive and negative pairs from the above-mentioned cross-modal features. In our framework, we hypothesize that pairs established by modalities of the same sample are positive, whereas those formed by modalities of distinct ones are negative. 
\begin{equation}
    \mathcal{L}_{\text{CE}} = -\sum_{i=1}^{B} \text{sim}(\mathbf{t}^1_i, \mathbf{t}^2_i) +  \sum_{j=1, k=1, j \neq k}^{B} \text{sim}(\mathbf{t}_j^{1}, \mathbf{t}_k^{2})
\end{equation}
where $\mathbf{t}^1, \mathbf{t}^2 \in \{\mathbf{h}^p, \mathbf{h}^r, \mathbf{v}^p, \mathbf{v}^r\}$, and $B$ denotes the batch size in the training process.

\noindent\textbf{Adaptive Weighting} The standard contrastive objective suffers from inflexible optimization due to irrational gradient assignment to positive and negative pairs. As a result, to tackle the problem, we propose the Adaptive Weighting Strategy for our contrastive framework. Initially, we introduce weights $\epsilon^p$ and $\epsilon^n$ to represent distances from the optimum, then integrate them into positive and negative terms of our loss.
\begin{equation}
\begin{split}
    &\mathcal{L}_{\text{AdaptiveCE}} = -\sum_{i=1}^{B} \epsilon^p_i \cdot \text{sim}(\mathbf{t}^1_i, \mathbf{t}^2_i) \\
    &+ \sum_{j=1, k=1, j \neq k}^{B} \epsilon_{j,k}^n \cdot \text{sim}(\mathbf{t}_j^{1}, \mathbf{t}_k^{2})
\end{split}
\label{eq:adaptive_ce}
\end{equation}
where $\epsilon_i^p = [o^p - \text{sim}(\mathbf{t}^1_i, \mathbf{t}^2_i)]_+$ and  $\epsilon_{j,k}^n = [\text{sim}(\mathbf{t}^1_j, \mathbf{t}^2_k) - o^n]_+$. Investigating the intuition to determine the values for $o^p$ and $o^n$, we continue to conduct derivation and arrive in the following theorem

\begin{theorem} Adaptive Contrastive Loss (\ref{eq:adaptive_ce}) has the hyperspherical form: 
\begin{equation*}
    \begin{split}
       &\mathcal{L}_{\text{AdaptiveCE}} = \sum_{i=1}^{B} \left(\text{sim} (\mathbf{t}^1_i, \mathbf{t}^2_i) - \frac{o^p}{2}\right)^2 \\
       &+ \sum_{j=1, k=1, j \neq k}^{B} \left(\text{sim} (\mathbf{t}^1_j, \mathbf{t}^2_k) - \frac{o^n}{2}\right)^2 - C, \\
       &\quad \text{where} \, C > 0         
    \end{split}
\end{equation*}
\label{theorem:spherical}
\end{theorem}
We provide the proof for Theorem (\ref{theorem:spherical}) in the Appendix section. As a consequence, theoretically the contrastive objective arrives in the optimum when $\text{sim}(\mathbf{t}_i^1, \mathbf{t}_i^2) = \frac{o^p}{2}$ and $\text{sim}(\mathbf{t}_j^1, \mathbf{t}_k^2) = \frac{o^n}{2}$. Based upon this observation, in our experiments we set $o^p = 2$ and $o^n = 0$.

\subsection{Training Objective}
For the Review Helpfulness Prediction problem, the modelâ€™s parameters are updated according to the pairwise ranking loss as follows
\begin{equation}
    \mathcal{L}_{\text{ranking}} = \sum_i \text{max} (0, \beta - f(p_i, r^{+}) + f(p_i, r^{-}))
\end{equation}
where $r^{+}$and $r^{-}$ are random reviews in which $r^{+}$ possesses a higher helpfulness level than $r^{-}$. We jointly combine the contrastive goal with the ranking objective of the Review Helpfulness Prediction problem to train our model
\begin{equation}
    \mathcal{L} = \mathcal{L}_\text{AdaptiveCE} + \mathcal{L}_\text{ranking}
\end{equation}