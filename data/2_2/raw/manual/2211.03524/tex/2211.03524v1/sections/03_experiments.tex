\section{Experiments}
\begin{table}[ht]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|l|ccc}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Split}} & \multicolumn{3}{c}{\textbf{Category (Product / Review)}} \\ 
& & Clothing & Electronics. & Home \\
\midrule
 \multirow{2}{*}{Lazada} & Train \& Dev & 8K/130K & 5K/52K & 4K/16K  \\
  & Test & 2K/32K & 1K/13K & 1K/13K \\
\midrule
 \multirow{2}{*}{Amazon} & Train \& Dev & 16K/349K & 13K/325K & 18K/462K  \\
  & Test & 4K/87K & 3K/80K & 5K/111K \\
 \bottomrule
\end{tabular} }
\caption{
Statistics of MRHP datasets.}
\label{table:datasets}
\end{table}

\begin{table*}[t]
\centering
\resizebox{1\textwidth}{!}{
\begin{tabular}{|l|l|ccc|ccc|ccc|}
\toprule
\multirow{2}{*}{\textbf{Type}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{Clothing}} & \multicolumn{3}{c|}{\textbf{Electronics}} & \multicolumn{3}{c|}{\textbf{Home}} \\ 
 &  & \textbf{MAP} & \textbf{N@3} & \textbf{N@5} & \textbf{MAP} & \textbf{N@3} & \textbf{N@5} & \textbf{MAP} & \textbf{N@3} & \textbf{N@5} \\
\midrule
\multirow{4}{*}{Text-only} & BiMPM & 60.0 & 52.4 & 57.7 & 74.4 & 67.3 & 72.2 & 70.6 & 64.7 & 69.1 \\
 & EG-CNN & 60.4 & 51.7 & 57.5 & 73.5 & 66.3 & 70.8 & 70.7 & 63.4 & 68.5 \\
 & Conv-KNRM & 62.1 & 54.3 & 59.9 & 74.1 & 67.1 & 71.9 & 71.4 & 65.7 & 70.5 \\
 & PRH-Net & 62.1 & 54.9 & 59.9 & 74.3 & 67.0 & 72.2 & 71.6 & 65.2 & 70.0 \\
\midrule
\multirow{4}{*}{Multimodal} & SSE-Cross & 66.1 & 59.7 & 64.8 & 76.0 & 68.9 & 73.8 & 72.2 & 66.0 & 71.0 \\
 & DR-Net & 66.5 & 60.7 & 65.3 & 76.1 & 69.2 & 74.0 & 72.4 & 66.3 & 71.4 \\
 & MCR & 68.8 & 62.3 & 67.0 & 76.8 & 70.7 & 75.0 & 73.8 & 67.0 & 72.2 \\
 & \textbf{Our Model} & \textbf{70.3} & \textbf{64.7} & \textbf{69.0} & \textbf{78.2} & \textbf{72.4} & \textbf{76.5} & \textbf{75.2} & \textbf{68.8} & \textbf{73.7} \\
\bottomrule
\end{tabular} }
\caption{
Helpfulness Prediction results on Lazada-MRHP dataset.}
\label{table:lazada_results}
\end{table*}

\begin{table*}[t]
\centering
\resizebox{1\textwidth}{!}{
\begin{tabular}{|l|l|ccc|ccc|ccc|}
\toprule
\multirow{2}{*}{\textbf{Type}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{Clothing}} & \multicolumn{3}{c|}{\textbf{Electronics}} & \multicolumn{3}{c|}{\textbf{Home}} \\ 
 &  & \textbf{MAP} & \textbf{N@3} & \textbf{N@5} & \textbf{MAP} & \textbf{N@3} & \textbf{N@5} & \textbf{MAP} & \textbf{N@3} & \textbf{N@5} \\
\midrule
\multirow{4}{*}{Text-only} & BiMPM & 57.7 & 41.8 & 46.0 & 52.3 & 40.5 & 44.1 & 56.6 & 43.6 & 47.6 \\
 & EG-CNN & 56.4 & 40.6 & 44.7 & 51.5 & 39.4 & 42.1 & 55.3 & 42.4 & 46.7 \\
 & Conv-KNRM & 57.2 & 41.2 & 45.6 & 52.6 & 40.5 & 44.2 & 57.4 & 44.5 & 48.4 \\
 & PRH-Net & 58.3 & 42.2 & 46.5 & 52.4 & 40.1 & 43.9 & 57.1 & 44.3 & 48.1 \\
\midrule
\multirow{4}{*}{Multimodal} & SSE-Cross & 65.0 & 56.0 & 59.1 & 53.7 & 43.8 & 47.2 & 60.8 & 51.0 & 54.0 \\
 & DR-Net & 65.2 & 56.1 & 59.2 & 53.9 & 44.2 & 47.5 & 61.2 & 51.8 & 54.6 \\
 & MCR & 66.4 & 57.3 & 60.2 & 54.4 & 45.0 & 48.1 & 62.6 & 53.5 & 56.6 \\
 & \textbf{Our Model} & \textbf{67.4} & \textbf{58.6} & \textbf{61.6} & \textbf{56.5} & \textbf{47.6} & \textbf{50.8} & \textbf{63.5} & \textbf{54.6} & \textbf{57.8} \\
\bottomrule
\end{tabular} }
\caption{
Helpfulness Prediction results on Amazon-MRHP dataset.}
\label{table:amazon_results}
\end{table*}
\subsection{Datasets}
We evaluate our methods on two publicly available benchmark datasets for MRHP task: Lazada-MRHP and Amazon-MRHP.

\noindent\textbf{Lazada-MRHP} \cite{liu2021multi} consists of product items and artificial reviews on Lazada.com, an e-commerce platform in Southest Asia. All of the texts in the dataset are expressed in Indonesian.

\noindent\textbf{Amazon-MRHP} \cite{liu2021multi} is collected from Amazon.com, the large-scale international e-commerce platform. Product information and associated reviews are in English and extracted between 2016 and 2018.

Both datasets comprise 3 categories: (i) Clothing, Shoes \& Jewelry (Clothing), (ii) Electronics (Electronics), and (iii) Home \& Kitchen (Home). We present the statistics of them in Table \ref{table:datasets}.
\subsection{Implementation Details}
We use a 1-layer LSTM with hidden dimension size of 128. We initialize our word embedding with fastText embedding \cite{bojanowski2017enriching} for Lazada-MRHP dataset and 300-dimensional GloVe pretrained word vectors \cite{pennington2014glove} for Amazon-MRHP dataset. We set our multimodal attention module to have $D = 5$ attention layers. For the visual modality, we extract 2048-dimensional ROI features from each image and encode them into 128-dimensional vectors. Our entire model is trained end-to-end with Adam optimizer \cite{kingma2014adam} and batch size of 32. For the training objective, we set the value of the margin in the ranking loss to be 1.

\subsection{Baselines}
We compare our proposed architecture against the following baselines:
\begin{itemize}
    \item \textbf{BiMPM} \cite{wang2017bilateral}: a ranking model which encodes input sentences in two directions to ascertain the matching result.
    \item \textbf{Conv-KNRM} \cite{dai2018convolutional}: a CNN-based model which encodes n-gram of multiple lengths and uses kernel pooling to generate the final ranking score.
    \item \textbf{EG-CNN} \cite{chen2018cross}: a CNN-based model targeting data scarcity and OOV problem in RHP task via taking advantage of character-based representations and domain discriminators.
    \item \textbf{PRH-Net} \cite{fan2019product}: a baseline to predict helpfulness of a review by taking into consideration both product text and product metadata.
    \item \textbf{DR-Net} \cite{xu2020reasoning}: a cross-modality approach that models contrast in associated contexts by leveraging decomposition and relation modules.
    \item \textbf{SSE-Cross} \cite{abavisani2020multimodal}: multimodal model to fuse different modalities with stochastic shared embeddings.
    \item \textbf{MCR} \cite{liu2021multi}: a baseline model focusing on coherent reasoning.
\end{itemize}

\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|ccc|ccc|ccc|}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multicolumn{3}{c|}{\textbf{Clothing}} & \multicolumn{3}{c|}{\textbf{Electronics}} & \multicolumn{3}{c|}{\textbf{Home}} \\ 
 & \textbf{MAP} & \textbf{N@3} & \textbf{N@5} & \textbf{MAP} & \textbf{N@3} & \textbf{N@5} & \textbf{MAP} & \textbf{N@3} & \textbf{N@5} \\
\midrule
 Lazada & $4.48 \cdot 10^{-2}$ & $1.55 \cdot 10^{-2}$ & $3.93 \cdot 10^{-2}$ & $4.54 \cdot 10^{-3}$ & $1.05 \cdot 10^{-4}$ & $2.63 \cdot 10^{-3}$ & $1.09 \cdot 10^{-3}$ & $3.40 \cdot 10^{-2}$ & $3.68 \cdot 10^{-3}$ \\
 Amazon & $3.45 \cdot 10^{-2}$ & $4.22 \cdot 10^{-2}$ & $1.86 \cdot 10^{-2}$ & $4.37 \cdot 10^{-3}$ & $2.81 \cdot 10^{-2}$ & $3.04 \cdot 10^{-2}$ & $2.04 \cdot 10^{-3}$ & $3.30 \cdot 10^{-3}$ & $6.50 \cdot 10^{-3}$ \\
\bottomrule
\end{tabular} }
\caption{
Significance test of the results of our model against MCR model. }
\label{table:sig_tests}
\end{table*}

\subsection{Automatic Evaluation}
In Table \ref{table:lazada_results} and \ref{table:amazon_results}, we follow previous work \cite{liu2021multi} to report Mean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG@N) \cite{jarvelin2017ir} where $N = 3$ and $N = 5$. As it can be seen, multimodal approaches achieve better performance than text-only ones.

For Lazada-MRHP dataset, we achieve an absolute improvement of NDCG@3 of 2.4 points in Clothing, NDCG@5 of $1.5$ points in Electronics, and MAP of $1.4$ points in Home over the previous best method, which is MCR. In addition, our model also obtains better results than the best text-only RHP model, which is PRH-Net, with a gain of NDCG@3 of $9.8$ points in Clothing, NDCG@5 of $4.3$ points in Electronics, and MAP of $3.6$ points in Home. Those results prove that our method can produce reasonable rankings for associated reviews.

For Amazon dataset, which is written in English, our model outperforms MCR on all 3 categories, by NDCG@5 of $1.4$ points in Clothing, $2.7$ points in Electronics, and $1.2$ points in Home, respectively. These results have verified that our interaction module and optimization approach can come up with more useful multimodal fusion than previous state-of-the-art baselines, not only in English context but other language one as well. 

We also perform significance tests to evaluate the statistical significance of our improvement on two datasets Amazon-MRHP and Lazada-MRHP, and note p-values in Table \ref{table:sig_tests}. As shown in the table, all of the p-values are smaller than $0.05$, verifying the statistical significance in the enhancement of our method against prior best MRHP model, MCR \cite{liu2021multi}.

\subsection{Case Study}
In Table \ref{table:example}, we introduce an example of one product item and two reviews extracted from Electronics category of Amazon-MRHP dataset. Whereas MCR fails to predict relevant helpfulness scores, our model successfully produces sensible rankings for both of them. We hypothesize that our Multimodal Interaction module learns more meaningful representations and Adaptive Contrastive Learning framework acquires more logical hidden states of relations among input elements. Thus, our model is able to generate more rational outcomes.

\subsection{Ablation Study}
In this section, we proceed to study the impact of (1) Adaptive Contrastive Learning framework and (2) Cross-modal Interaction module.

\noindent\textbf{Adaptive Contrastive Learning} It is worth noting from Table \ref{table:ablation} that plainly integrating contrastive learning brings less enhancement to the performance, with the improvement of NDCG@3 dropping $0.53$ points in Lazada-MRHP dataset, NDCG@5 waning $0.84$ points in Amazon-MRHP dataset. Furthermore, completely removing contrastive objective hurts performance, as NDCG@3 score decreasing $0.77$ points in Lazada-MRHP, and MAP score declining $1.06$ points in Amazon-MRHP. We hypothesize that the model loses the ability to learn efficient representations for cross-modal relations.

\noindent\textbf{Cross-modal Interaction} In this ablation, we eliminate the cross-modal interaction module. As shown in Table \ref{table:ablation}, without the module, the improvement is downgraded, for instance, N@3 drops $1.89$ points in Lazada-MRHP dataset, MAP shrinks $1.39$ points in Amazon-MRHP dataset. It is hypothesized that without the module, the model is rigidly dependent upon the alignment nature among multimodal input elements, which brings about insensible modeling because in most cases, cross-modal elements are irrelevant to be bijectively mapped together.

\begin{table}[ht]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|l|ccc|}
\toprule
\textbf{Dataset} & \textbf{Model} & \textbf{MAP} & \textbf{N@3} & \textbf{N@5} \\ 
\midrule
 \multirow{4}{*}{Lazada} & Our Model & \textbf{78.15} & \textbf{72.43} & \textbf{76.49}  \\
  & - w/o Adaptive Weighting & 77.90 & 71.90 & 75.97 \\
  & - w/o Contrastive Objective & 77.69 & 71.66 & 75.85 \\
  & - w/o Cross-modal Module & 77.32 & 70.54 & 74.86 \\
\midrule
 \multirow{4}{*}{Amazon} & Our Model & \textbf{56.49} & \textbf{47.62} & \textbf{50.79}  \\
  & - w/o Adaptive Weighting & 56.03 & 46.98 & 49.95 \\
  & - w/o Contrastive Objective & 55.43 & 46.30 & 49.02 \\
  & - w/o Cross-modal Module & 55.10 & 45.67 & 48.50 \\
 \bottomrule
\end{tabular} }
\caption{
Ablation study in Electronics category of Lazada-MRHP and Amazon-MRHP datasets.}
\label{table:ablation}
\vspace{-10pt}
\end{table}
\subsection{Impact of Contrastive Learning on Cross-modal Relations}
\begin{table*}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|l|cc|cc|cc|}
\toprule
\multirow{2}{*}{\textbf{Label}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{Intra-modal}} & \multicolumn{2}{c|}{\textbf{Inter-modal}} & \multicolumn{2}{c|}{\textbf{Intra-review}} \\ 
 & & \textbf{CS} & \textbf{L2} & \textbf{CS} & \textbf{L2} & \textbf{CS} & \textbf{L2} \\
\midrule
\multirow{2}{*}{1} & MCR & 0.785 $\pm$ 0.002 & 3.852 $\pm$ 0.067 & 0.843 $\pm$ 0.002 & 11.719 $\pm$ 0.001 & 0.845 $\pm$ 0.002 & 14.631 $\pm$ 0.001 \\
 & Our Model & 0.875 $\pm$ 0.002 & 6.545 $\pm$ 0.007 & 0.957 $\pm$ 0.002 & 13.934 $\pm$ 0.027 & 0.953 $\pm$ 0.002 & 15.160 $\pm$ 0.036  \\
 \midrule
 \multirow{2}{*}{4} & MCR & 0.533 $\pm$ 0.004 & 1.014 $\pm$ 0.051 & 0.712 $\pm$ 0.010 & 9.476 $\pm$ 0.001 & 0.617 $\pm$ 0.001 & 8.519 $\pm$ 0.001 \\
 & Our Model & 0.433 $\pm$ 0.001 & 0.981 $\pm$ 0.005 & 0.564 $\pm$ 0.001 & 4.179 $\pm$ 0.017 & 0.538 $\pm$ 0.001 & 3.827 $\pm$ 0.020  \\
\bottomrule
\end{tabular} }
\caption{Intra-modal, Inter-modal, and Intra-review distances in Home category of Lazada-MRHP dataset.}
\label{table:lazada_cs_mse_dist}
\end{table*}

\begin{table*}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|l|cc|cc|cc|}
\toprule
\multirow{2}{*}{\textbf{Label}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{Intra-modal}} & \multicolumn{2}{c|}{\textbf{Inter-modal}} & \multicolumn{2}{c|}{\textbf{Intra-review}} \\ 
 & & \textbf{CS} & \textbf{L2} & \textbf{CS} & \textbf{L2} & \textbf{CS} & \textbf{L2} \\
\midrule
\multirow{2}{*}{1} & MCR & 0.785 $\pm$ 0.006 & 8.532 $\pm$ 0.292 & 0.686 $\pm$ 0.001 & 9.696 $\pm$ 0.300 & 0.880 $\pm$ 0.002 & 9.620 $\pm$ 0.217 \\
 & Our Model & 0.971 $\pm$ 0.001 & 10.663 $\pm$ 0.770 & 0.976 $\pm$ 0.001 & 13.234 $\pm$ 0.493 & 0.970 $\pm$ 0.001 & 12.222 $\pm$ 0.431 \\
 \midrule
 \multirow{2}{*}{4} & MCR & 0.697 $\pm$ 0.009 & 3.045 $\pm$ 0.139 & 0.624 $\pm$ 0.001 & 3.179 $\pm$ 0.830 & 0.781 $\pm$ 0.001	& 5.098 $\pm$ 0.636 \\
 & Our Model & 0.571 +- 0.001 & 1.572 +- 0.037 & 0.488 +- 0.001 & 1.460 +- 0.008 & 0.487 +- 0.001 & 3.555 +- 0.001 \\
\bottomrule
\end{tabular} }
\caption{Intra-modal, Inter-modal, and Intra-review distances in Home category of Amazon-MRHP dataset.}
\label{table:amazon_cs_mse_dist}
\end{table*}

Despite improved performances, it remains a quandary that whether the enhancement stems from more meaningful representations of input samples, which we hypothesize as a significant benefit of our contrastive learning framework. For deeper investigation, we decide to statistically measure distances among input samples using standard distance functions. Table \ref{table:lazada_cs_mse_dist} and \ref{table:amazon_cs_mse_dist} reveal the results of our experiment. In particular, we estimate the cosine distance (CS) and L2 distance (L2) between tokens of (1) product text - review text and product image - review image (intra-modal), (2) product text - review image and product image - review text (inter-modal), and (3) product text - product image and review text - review image (intra-review), then calculate the mean value of all samples. As it can be seen, our frameworks are more efficient in attracting elements of helpful pairs and repelling those of unhelpful pairs.