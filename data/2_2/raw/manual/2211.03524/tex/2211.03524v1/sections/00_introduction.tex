\section{Introduction}
Current e-commerce sites such as Amazon, Ebay, etc., construct review platforms to collect user feedback concerning their products. These platforms play a fundamental role in online transactions since they help future consumers collect useful reviews which assist them in deciding whether to make the purchase or not. Unfortunately, nowadays the number of user-generated reviews is overwhelming, raising doubts related to the relevance and veracity of reviews. Therefore, there is a need to verify the quality of reviews before publishing them to prospective customers. As a result, this inspires a recent surge of interest targeting the Review Helpfulness Prediction (RHP) problem.
\begin{table}[h!]
\centering
\begin{tabular}{p{0.9\linewidth}}
% \toprule
\textbf{Product Information} \\
\small{
The Cooks Standard 6-Quart Stainless Steel Stockpot with Lid is made with 18/10 stainless steel with an aluminum disc layered in the bottom. The aluminum disc bottom provides even heat distribution and prevents hot spots. Tempered glass lid with steam hole vent makes viewing food easy. Stainless steel riveted handles offer durability. Induction compatible. Works on gas, electric, glass, ceramic, etc. Oven safe to 500F, glass lid to 350F. Dishwasher safe.} \\
\includegraphics[width=0.3\linewidth]{images/product_images/pic_4_0.jpg} \includegraphics[width=0.3\linewidth]{images/product_images/pic_4_1.jpg} \\
\midrule
\textbf{Review 1} \\
\small{
I needed a stainless steel pot for canning my tomatoes.  I learned the hard way that you have to use a non-reactive pot or else your end result will be inedible (I thought I was using stainless steel but quickly realized it wasnt)  I headed to Amazon and came across this Cooks Standard SS Cookpot with cover and bought it after reading the reviews.  I have had it for just under a year and it still looks just as good as the day I bought it.  I couldn't be happier with my purchase!  Oh, and by the way, this one actually is stainless steel unlike the other pot I bought that said it was and wasn't.} \\
% Label score: \textbf{4} \\
% MCR score: \textbf{0.168} \\
% Our Model score: \textbf{4.651} \\
\midrule
\textbf{Review 2} \\
\small{
I ordered it on May 21st. What a waste of time and money.} \\
\includegraphics[width=0.3\linewidth]{images/review_images/pic_1_0.jpg} \\
% Label score: \textbf{1} \\
% MCR score: \textbf{3.637} \\
% Our Model score: \textbf{0.743} \\
% \bottomrule
\end{tabular}
\small
\begin{tabular}{ccc}
\toprule
& Review 1 & Review 2 \\
\midrule
Label score & 4 & 1 \\
MCR score & 0.168 & 3.637 \\
Our Model score & \textbf{4.651} & \textbf{0.743} \\
\bottomrule
\end{tabular}
\caption{Example of unreasonable predictions in the Multimodal Review Helpfulness Prediction task.}
\label{table:example}
\vspace{-5mm}
\end{table}

Two principal groups of early efforts focus on purely textual data. The first group follows feature engineering techniques, retrieving argument-based features \cite{liu2017using}, lexical features \cite{krishnamoorthy2015linguistic}, and semantic features \cite{kim2006automatically}, as input to their classifier. Inherently, their methods are labor-intensive and vulnerable to the typical issues of conventional machine learning methods. Instead of relying on manual features, the second group leverages deep neural models, for instance, RNN \cite{alsmadi2020predicting} and CNN \cite{chen2018cross}, to learn rich features automatically. Nonetheless, their approach is ineffective because the helpfulness of a review is not only contingent upon textual information but also other modalities.

To cope with the above issues, recent works \cite{liu2021multi,han2022sancl} proposed to utilize multi-modality via the Multi-perspective Coherent Reasoning (MCR) model. Hypothesizing that a review is helpful if it exhibits coherent text and images with the product information, those works take into account both textual and visual modality of the inputs, then estimate their coherence level to discern whether the reviews are \emph{helpful} or \emph{unhelpful}. However, the MCR model contains a detrimental drawback. Particularly, it aims to maximize the scores $s_p$ of positive (helpful) product-review pairs while minimizing those $s_n$ of negative (unhelpful) pairs. Hence, it was assumed that following the aforementioned manner would project features with similar semantics to stay close and those with disparate ones to be distant apart. Unfortunately, in multimodal learning, this was shown not to be the case, causing the model to learn ad-hoc representations \cite{zolfaghari2021crossclr}. This is one reason leading to unreasonable predictions of MCR in Table \ref{table:example}. As it can be seen, even though Review 1 closely relates to the product of ``\emph{6-Quart Stainless Steel Stockpot}'', the model classifies it as \emph{unhelpful}. In addition, the target of Review 2’s text content is vague because it does not specifically correspond to the ``\emph{Stockpot}''. In fact, it can be used for any product. Moreover, the image does not clearly show any hint of the ``\emph{Stockpot}'' as well. Despite such vagueness, the output of MCR for Review 2 is still \emph{helpful}. 

As a remedy to this problem, we propose Cross-modal Contrastive Learning to mine the mutual information of cross-modal relations in the input to capture more sensible representations. Nonetheless, plainly applying symmetric gradient pattern, which is similar to MCR that they assign equivalent penalty to $s_n$ and $s_p$, is inflexible. In cases that $s_p$ is small and $s_n$ is already negatively skewed, or both $s_p$ and $s_n$ are positively skewed, it is irrational to assign equivalent penalties to both $s_p$ and $s_n$. Last but not least, MCR directly leverages Coherent Reasoning, repeatedly enforcing alignment among modalities in the input. This ignores the unaligned nature of multimodal input, for example, images might only refer to a particular section in the text, hence do not completely align with the textual content. In consequence, strictly forming alignment can make the model learn inefficient multimodal representations  \cite{tsai2019multimodal}.

To overcome the above problems, we propose an adaptive scheme to accomplish the flexibility in the optimization of our contrastive learning stage. Finally, we propose to adopt a multimodal attention module that reinforces one modality’s high-level features with low-level ones of other modalities. This not only relaxes the alignment assumption but also informs one modality of information of others, encouraging refined representation learning.

In sum, our contributions are three-fold:
\begin{itemize}
    \item We propose an Adaptive Cross-modal Contrastive Learning for Review Helpfulness Prediction task by polishing cross-modal relation representations.
    \item We propose a Multimodal Interaction module which correlates modalities’ features without depending upon the alignment assumption.
    \item We conducted extensive experiments on two datasets for the RHP problem and found that our method outperforms other baselines which are both textual-only and multimodal, and obtains state-of-the-art results on those benchmarks.
\end{itemize}