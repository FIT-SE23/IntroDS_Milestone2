\section{Related Work}
\subsection{Review Helpfulness Prediction}
Past works that pursue Review Helpfulness Prediction (RHP) dilemma follow text-only approaches. In general, they extract salient information, for instance lexical \cite{krishnamoorthy2015linguistic}, argument \cite{liu2017using}, and emotional features \cite{martin2014prediction} from reviews. Subsequently, these features are fed to a standard classifier such as Random Forest \cite{louppe2014understanding} in order to produce the output score. Inspired by the meteoric development of computation resources, contemporary approaches seek to take advantage of deep learning techniques to tackle the RHP problem. For instance, \citet{wang2017bilateral} propose multi-perspective matching between review and product information via applying attention mechanism. Furthermore, \citet{chen2018cross, dai2018convolutional} adapt CNN models to learn textual representations in various views. 

In reality, review content are not only determined by texts but also other modalities. As a consequence, \citet{fan2019product} integrate metadata information of the target product into the prediction model. \citet{abavisani2020multimodal} filter out uninformative signals before fusing various modalities. Moreover, \citet{liu2021multi} perform coherent reasoning to ascertain the matching level between product and numerous review items. 

\subsection{Contrastive Estimation}
Different from architectural techniques such as Knowledge Distillation \cite{hinton2015distilling, hahn2019self, nguyen2022improving} or Variational AutoEncoder \cite{zhao2020neural, nguyen2021enriching, nguyen2021contrastive, wang2019topic}, Contrastive Learning has been introduced as a representation-based but universal mechanism to enhance natural language processing performance. Proposed by \citet{chopra2005learning}, Contrastive Learning has been widely adopted in myriad problems of Natural Language Processing (NLP). 

As an approach to polish text representations, \citet{gao2021simcse, zhang2021pairwise, liu2021dialoguecse, nguyen2021contrastive} employ contrastive loss to advance sentence embeddings and topic representations. For downstream tasks, \citet{cao2021cliff} propose negative sampling strategies to generate noisy output so that the model can learn to distinguish correct summaries from incorrect ones in Document Summarization. For Spoken Question Answering (SQA), \citet{you2021self} introduce augmentation algorithms in their contrastive learning stage so as to capture noisy-invariant representations of utterances. Additionally, \citet{ke2021classic} inherit the formulation of the contrastive objective to construct distillation loss which transfers knowledge of the previous task to the current one. Their proposals are to improve tasks in the Aspect Sentiment Classification domain. Unfortunately, despite the surge of interest in exercising contrastive learning for NLP, research works to adapt the method to the MRHP task have been scant.