@article{bapna2021slam,
  author = {Bapna, A. and Chung, YA and Wu, N. and others},
  title = {SLAM: A unified encoder for speech and language modeling via speech-text joint pre-training},
  year = {2021},
  journal = {arXiv preprint arXiv:2110.10329},
}
@article{rajpurkar2016squad,
  author = {Rajpurkar, P. and Zhang, J. and Lopyrev, K. and others},
  title = {Squad: 100,000+ questions for machine comprehension of text},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.05250},
}
@article{baevski2019effectiveness,
  author = {Baevski, A. and Auli, M. and Mohamed, A.},
  title = {Effectiveness of self-supervised pre-training for speech recognition},
  year = {2019},
  journal = {arXiv preprint arXiv:1911.03912},
}
@article{arora2022two,
  author = {Arora, S. and Dalmia, S. and Chang, X. and others},
  title = {Two-Pass Low Latency End-to-End Spoken Language Understanding},
  year = {2022},
  journal = {Interspeech},
}
@article{translation,
  author = {Wu, A. and Wang, C. and Pino, J. and others},
  title = {Self-Supervised Representations Improve End-to-End Speech Translation},
  year = {2020},
  journal = {Interspeech},
}
@inproceedings{arora2022espnet,
  author = {Arora, S. and Dalmia, S. and Denisov, P. and others},
  title = {{ESP}net-{SLU}: Advancing spoken language understanding through espnet},
  year = {2022},
  booktitle = {ICASSP},
}
@article{bapna2022mslam,
  author = {Bapna, A. and Cherry, C. and Zhang, Y. and others},
  title = {mSLAM: Massively multilingual joint pre-training for speech and text},
  year = {2022},
  journal = {arXiv preprint arXiv:2202.01374},
}
@article{baevski2020wav2vec,
  author = {Baevski, A. and Zhou, Y. and Mohamed, A. and others},
  title = {wav2vec 2.0: A framework for self-supervised learning of speech representations},
  year = {2020},
  journal = {NIPS},
  volume = {33},
}
@article{yang2021superb,
  author = {Yang, SW and Chi, PH and Chuang, YS and others},
  title = {{SUPERB}: Speech processing Universal PERformance Benchmark},
  year = {2021},
  journal = {Interspeech},
}
@inproceedings{bert,
  author = {Devlin, J. and Chang, MW and Lee, K. and others},
  title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year = {2019},
  booktitle = {NAACL},
}
@inproceedings{guo2021recent,
  author = {Guo, P. and Boyer, F. and Chang, X. and others},
  title = {Recent developments on espnet toolkit boosted by conformer},
  year = {2021},
  booktitle = {ICASSP},
}
@inproceedings{gomez2017self,
  author = {Gomez, L. and Patel, Y. and Rusinol, M. and others},
  title = {Self-supervised learning of visual features through embedding images into text topic spaces},
  year = {2017},
  booktitle = {CVPR},
}
@inproceedings{qian2022training,
  author = {Qian, T. and Shi, J. and Guo, S. and others},
  title = {Training Strategies for Automatic Song Writing: A Unified Framework Perspective},
  year = {2022},
  booktitle = {ICASSP},
}
@article{xue2022byt5,
  author = {Xue, L. and Barua, A. and Constant, N. and others},
  title = {By{T}5: Towards a token-free future with pre-trained byte-to-byte models},
  year = {2022},
  journal = {TACL},
}
@inproceedings{yeh2018unsupervised,
  author = {Yeh, CK and Chen, J. and Yu, C. and others},
  title = {Unsupervised Speech Recognition via Segmental Empirical Output Distribution Matching},
  year = {2018},
  booktitle = {ICLR},
}
@inproceedings{wang2021covost,
  author = {Wang, C. and Wu, A. and Gu, J. and others},
  title = {Co{V}o{ST} 2 and Massively Multilingual Speech Translation.},
  year = {2021},
  booktitle = {Interspeech},
}
@article{busso2008iemocap,
  author = {Busso, C. and Bulut, M. and Lee, CC and others},
  title = {{IEMOCAP}: Interactive emotional dyadic motion capture database},
  year = {2008},
  journal = {Language resources and evaluation},
}
@article{choi2018quac,
  author = {Choi, E. and He, H. and Iyyer, M. and others},
  title = {Qu{AC}: Question answering in context},
  year = {2018},
  journal = {arXiv preprint arXiv:1808.07036},
}
@article{liu2022towards,
  author = {Liu, AH and Hsu, WN and Auli, M. and others},
  title = {Towards End-to-end Unsupervised Speech Recognition},
  year = {2022},
  journal = {arXiv preprint arXiv:2204.02492},
}
@inproceedings{inaguma2020espnet,
  author = {Inaguma, H. and Kiyono, S. and Duh, K. and others},
  title = {{ESP}net-{ST}: All-in-One Speech Translation Toolkit},
  year = {2020},
  booktitle = {ACL},
}
@inproceedings{kahn2020libri,
  author = {Kahn, J. and Rivi{\`e}re, M. and Zheng, W. and others},
  title = {Libri-light: A benchmark for asr with limited or no supervision},
  year = {2020},
  booktitle = {ICASSP},
}
@inproceedings{xu2015show,
  author = {Xu, K. and Ba, J. and Kiros, R. and others},
  title = {Show, attend and tell: Neural image caption generation with visual attention},
  year = {2015},
  booktitle = {ICML},
}
@article{chang2022exploration,
  author = {Chang, KW and Tseng, WC and Li, SW and others},
  title = {An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks},
  year = {2022},
  journal = {Interspeech},
}
@article{trischler2016newsqa,
  author = {Trischler, A. and Wang, T. and Yuan, X. and others},
  title = {New{SQA}: A machine comprehension dataset},
  year = {2016},
  journal = {arXiv preprint arXiv:1611.09830},
}
@article{lin2022dual,
  author = {Lin, GT and Chuang, YS and Chung, HL and others},
  title = {{DUAL}: Textless Spoken Question Answering with Speech Discrete Unit Adaptive Learning},
  year = {2022},
  journal = {Interspeech},
}
@article{dinarelli2022toward,
  author = {Dinarelli, M. and Naguib, M. and Portet, F.},
  title = {Toward Low-Cost End-to-End Spoken Language Understanding},
  year = {2022},
  journal = {arXiv preprint arXiv:2207.00352},
}
@article{baevski2021unsupervised,
  author = {Baevski, A. and Hsu, WN and Conneau, A. and others},
  title = {Unsupervised speech recognition},
  year = {2021},
  journal = {NIPS},
}
@inproceedings{liu2020towards,
  author = {Liu, AH and Tu, T. and Lee, HY and others},
  title = {Towards unsupervised speech recognition and synthesis with quantized speech representation learning},
  year = {2020},
  booktitle = {ICASSP},
}
@inproceedings{tsai2022superb,
  author = {Tsai, HS and Chang, HJ and Huang, WC and others},
  title = {{SUPERB-SG}: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities},
  year = {2022},
  booktitle = {ACL},
}
@article{watanabe2017hybrid,
  author = {Watanabe, S. and Hori, T. and Kim, S. and others},
  title = {Hybrid CTC/attention architecture for end-to-end speech recognition},
  year = {2017},
  journal = {JSTSP},
}
@inproceedings{bastianelli2020slurp,
  author = {Bastianelli, E. and Vanzo, A. and Swietojanski, P. and others},
  title = {SLURP: A Spoken Language Understanding Resource Package},
  year = {2020},
  booktitle = {EMNLP},
}
@article{borgholt2021we,
  author = {Borgholt, L. and Havtorn, JD and Abdou, H. and others},
  title = {Do We Still Need Automatic Speech Recognition for Spoken Language Understanding?},
  year = {2021},
  journal = {arXiv preprint arXiv:2111.14842},
}
@inproceedings{chung2021splat,
  author = {Chung, YA and Zhu, C. and Zeng, M.},
  title = {{SPLAT}: Speech-Language Joint Pre-Training for Spoken Language Understanding},
  year = {2021},
  booktitle = {NAACL},
}
@article{phonemet5,
  author = {Hsu, CJ and Chung, HL and Lee, HY and others},
  title = {T5lephone: Bridging Speech and Text Self-supervised Models for Spoken Language Understanding via Phoneme level T5},
  year = {2022},
  journal = {arXiv preprint arXiv:2211.00586},
}
@book{yu2016automatic,
  author = {Yu, D. and Deng, L.},
  title = {Automatic speech recognition},
  year = {2016},
  volume = {1},
  publisher = {Springer},
}
@inproceedings{lai2021semi,
  author = {Lai, CI and Chuang, YS and Lee, HY and others},
  title = {Semi-supervised spoken language understanding via self-supervised speech and language model pretraining},
  year = {2021},
  booktitle = {ICASSP},
}
@inproceedings{chen2019completely,
  author = {Chen, KY and Tsai, CP and Liu, DR and others},
  title = {Completely Unsupervised Phoneme Recognition by a Generative Adversarial Network Harmonized with Iteratively Refined Hidden Markov Models.},
  year = {2019},
  booktitle = {Interspeech},
}
@inproceedings{chang2021exploration,
  author = {Chang, X. and Maekaku, T. and Guo, P. and others},
  title = {An exploration of self-supervised pretrained representations for end-to-end speech recognition},
  year = {2021},
  booktitle = {ASRU},
}
@inproceedings{huang2020leveraging,
  author = {Huang, Y. and Kuo, HK and Thomas, S. and others},
  title = {Leveraging unpaired text data for training end-to-end speech-to-intent systems},
  year = {2020},
  booktitle = {ICASSP},
}
