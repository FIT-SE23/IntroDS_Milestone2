{
    "elements": {
        "2211.03032_91712315ed3daaaaf7cfca5529a2da50": "Document Root 2211.03032",
        "2211.03032_f91c96473601e4a43c22c64d3b9b87e6": "[algorithm]",
        "2211.03032_104dc29c71f6583580826c6c186b9234": "[#1]#2",
        "2211.03032_6bc5d8e5e737c4dd07872b2bcf3ae2dd": "#1",
        "2211.03032_e353dbe42c8654f33588d4da0b517469": "Abstract",
        "2211.03032_436833da1629f5577d16a32a22f0ea64": "The study of decentralized learning or independent learning in cooperative multi-agent reinforcement learning has a history of decades",
        "2211.03032_6dd0f57dc208839ba586ec8b85f2683f": "Recently empirical studies show that independent PPO (IPPO) can obtain good performance, close to or even better than the methods of centralized training with decentralized execution, in several benchmarks",
        "2211.03032_e98e442015262c20b25163d13741f88a": "However, decentralized actor-critic with convergence guarantee is still open",
        "2211.03032_5c1320498a2faf4a55cc23d5f89910b1": "In this paper, we propose",
        "2211.03032_10c77ec2996d30c9dee297005dbdc54c": "(DPO), a decentralized actor-critic algorithm with monotonic improvement and convergence guarantee",
        "2211.03032_aa7b3cc6b73d98f655a077fb215a768a": "We derive a novel decentralized surrogate for policy optimization such that the monotonic improvement of joint policy can be guaranteed by each agent",
        "2211.03032_b7c1e1eb7c6cad3fb4247cf502dcbfe3": "optimizing the surrogate",
        "2211.03032_24fd634675e30ec3c16c6593efb7340e": "In practice, this decentralized surrogate can be realized by two adaptive coefficients for policy optimization at each agent",
        "2211.03032_e0aa77551432ef65bd9153e6ce15b58a": "Empirically, we compare DPO with IPPO in a variety of cooperative multi-agent tasks, covering discrete and continuous action spaces, and fully and partially observable environments",
        "2211.03032_5b04def1c6ab9153649f5146c7c046b7": "The results show DPO outperforms IPPO in most tasks, which can be the evidence for our theoretical results.",
        "2211.03032_0b79795d3efc95b9976c7c5b933afce2": "Introduction",
        "2211.03032_e457bad44372dadc697b33597c695e93": "In cooperative multi-agent reinforcement learning (MARL), centralized training with decentralized execution (CTDE) has been the primary framework",
        "2211.03032_c33dfb33eda12fa9a6063c9ea68b7163": "\\citep{MADDPG,COMA,VDN,QMIX,QPLEX,FOP,MAPPO}",
        "2211.03032_d01189478ab4d0fca879e822897f2e00": "Such a framework can settle the non-stationarity problem with the centralized value function, which takes as input the global information and is beneficial to the training process",
        "2211.03032_6c5846e1683aa50629d0340c9b9be12a": "Conversely, decentralized learning has been paid much less attention",
        "2211.03032_bf40af7a2723b5d4b44e066938e178be": "The main reason may be that there are few theoretical guarantees for decentralized learning and the interpretability is insufficient even if the simplest form of decentralized learning,",
        "2211.03032_8c0795eeedc7eb3a81f17e172c327a19": ", independent learning, can obtain good empirical performance in several benchmarks",
        "2211.03032_c0e4b7502ffa7e281ac1cd1cda767884": "\\citep{benchmark}",
        "2211.03032_1028f1399af1457283043081c0212cda": "However, decentralized learning itself still deserves attention as there are still many settings in which the global information cannot be accessed by each agent, and also for better robustness and scalability",
        "2211.03032_75e75e0204fad046c55965ce3d6c44f1": "\\citep{MARL-survey}",
        "2211.03032_a61f5956d054fdee9875e61520dfd747": "Moreover, the idea of decentralized learning is direct, comprehensible, and easy to realize in practice",
        "2211.03032_61ab0bf258da9555894fbf487d8fe704": "Independent Q-learning (IQL)",
        "2211.03032_a9229bdc2cab904ecc075b07bbb3d942": "\\citep{IDQN}",
        "2211.03032_cbb0a0e3d88b455b0b8e6c3a032c3622": "and independent PPO (IPPO)",
        "2211.03032_30f42dd9618548057535af7404dfede1": "\\citep{IPPO}",
        "2211.03032_6643f152a3bda52316cc9b5afd1cdec0": "are the straightforward decentralized learning methods for cooperative MARL, where each agent learns the policy by DQN",
        "2211.03032_e9019e42a1b58590ca39c2d1c916005a": "\\citep{DQN}",
        "2211.03032_7282cb56fdedf30e1aa5cfa30f8347f7": "and PPO",
        "2211.03032_b3a33df7dd0367f6cf69aa34b3c0c9ec": "\\citep{PPO}",
        "2211.03032_4b99ad4f32b999c31dae0b336712e6d9": "respectively",
        "2211.03032_c2b92150d8194f87f92e795ce5a365f0": "Empirical studies",
        "2211.03032_7edfda7b45b61b64c1e5a87587dba4bc": "\\citep{IPPO,MAPPO,benchmark}",
        "2211.03032_b87cb19a7aab05135766550cc41dcee2": "demonstrate that these two methods can obtain good performance, close CTDE methods",
        "2211.03032_8816d04b6ced742df833190d8663830f": "Especially, IPPO can outperform several CTDE methods in a few benchmarks, including MPE",
        "2211.03032_0460728ab83ff8cba7b4171c06611c61": "\\citep{MADDPG}",
        "2211.03032_e96d86039d433a8482f53b1508e3eb8a": "and SMAC",
        "2211.03032_5da6a11149c2ff4eb721eb079a89cfc3": "\\citep{SMAC}",
        "2211.03032_e93721a7b140f24f03d8c4d66a5c9a0c": ", which shows great promise for decentralized learning",
        "2211.03032_bc596e09fc965d8a5b787569928a2b2c": "Unfortunately, to the best of our knowledge, there is still no theoretical guarantee or rigorous explanation for IPPO, though there has been some study",
        "2211.03032_b184c6cdaddff7c8ba6c1713db6264ef": "\\citep{sun2022monotonic}",
        "2211.03032_73c43c1c0aacef650603a00e04125d99": "In this paper, we make a further step and propose",
        "2211.03032_84c40473414caf2ed4a7b1283e48bbf4": "(",
        "2211.03032_7be39d56a7e7a7c89759870337c9d624": "), a decentralized actor-critic method with monotonic improvement and convergence guarantee for cooperative multi-agent reinforcement learning",
        "2211.03032_30f614992a65bc3f1a15d6891c7d0e2e": "Similar to IPPO, DPO is actually",
        "2211.03032_aaaf519ed5223e75e3e922426acb64e6": ", because in DPO each agent optimizes its own objective individually and independently",
        "2211.03032_dbca265ef9b44d705716a2f86cb5be05": "However, unlike IPPO, such an independent policy optimization of DPO can guarantee the monotonic improvement of the joint policy",
        "2211.03032_e4a5ff7ffc4e06b32474d0cda6fc261f": "From the essence of fully decentralized learning, we first analyze Q-function in the decentralized setting and further show that the optimization objective of IPPO may not induce the joint policy improvement",
        "2211.03032_747752d616466f0fe5ff3088f1b2fb11": "Then, starting from the surrogate of TRPO",
        "2211.03032_281bef11f87b4453e64a3bd9d7278948": "\\citep{TRPO}",
        "2211.03032_457ba6a0e4dc17629529d2efd9406640": "and together considering the characteristics of fully decentralized learning, we introduce a",
        "2211.03032_558f1e6f61dbe543cbdbe10c37ae0247": "lower bound of joint policy improvement as the surrogate for decentralized policy optimization",
        "2211.03032_2d254cad7f533d73a599ec41aa980e08": "This surrogate can be naturally decomposed for each agent, which means each agent can optimize its individual objective to make sure that the joint policy improves monotonically",
        "2211.03032_3b960e777acb3a6ef35113baa4f5a636": "The idea of DPO is simple yet effective, and suitable for fully decentralized learning",
        "2211.03032_e9d4f1882b6beda93330cc45582ec07a": "Empirically, we compare DPO and IPPO in a variety of cooperative multi-agent tasks including a cooperative stochastic game, MPE",
        "2211.03032_5cc3cec5164f5ec70820836be8465c5d": ", multi-agent MuJoCo",
        "2211.03032_f5cf8856523c64fd5131060376754611": "\\citep{mamujoco}",
        "2211.03032_8b8bbd5728ee44be1050049581fd0a4f": ", and SMAC",
        "2211.03032_42bdf8fe4241735368e2acfe9535be37": ", covering discrete and continuous action spaces, and fully and partially observable environments",
        "2211.03032_364b97ae7784b00b7fb411b34c42e4ca": "The empirical results show that DPO performs better than IPPO in most tasks, which can be evidence for our theoretical results.",
        "2211.03032_aed402c3112b4749a9a98a72cbe9093d": "Related Work",
        "2211.03032_46bf109b62667b94f3818d1e03ee3650": "In cooperative MARL, centralized training with decentralized execution (CTDE) is the most popular framework",
        "2211.03032_c9f7f78998dcf1b5dc3c0383a01cf16c": "\\citep{MADDPG, MAAC, COMA, VDN, QMIX, QPLEX,FOP,mamujoco}",
        "2211.03032_73d14c9a206fa4f877b1c8e3eb4dc7c9": "CTDE algorithms can handle the non-stationarity problem in the multi-agent environment by the centralized value function",
        "2211.03032_2956f006448fa5cb2a9070fb5831dfbf": "One line of research in CTDE is value decomposition",
        "2211.03032_1a447e08b7ee35ed200e5d052a2eaad7": "\\citep{VDN,QMIX,QTRAN,Qatten,QPLEX}",
        "2211.03032_67ee2f64abc93c9c7be2dc5a507ac88d": ", where a joint Q-function is learned and factorized into local Q-functions by the relationship between optimal joint action and optimal local actions",
        "2211.03032_4f74e52b43944b6ce9b8ce8921c7b7a3": "Another line of research in CTDE is multi-agent actor-critic",
        "2211.03032_41b312409f529bb7f333c16f2e7144c7": "\\citep{COMA,MAAC,DOP,FOP,DMAC}",
        "2211.03032_8f7fc7c5fc1d7c540ef16a5927bee73f": ", where the centralized value function is learned to provide policy gradients for agents to learn stochastic policies",
        "2211.03032_bc0ebca39c67127903cc95bfeaa8fa10": "More recently, policy optimization has attracted much attention for cooperative MARL",
        "2211.03032_0d93969c834e54ad8a5f302fb3bd6b4a": "PPO",
        "2211.03032_2597c418d416dd9ba720ef7ec88d8e6a": "and TRPO",
        "2211.03032_597b9c0193aa3cdb1ef9e22ee411f319": "have been extended to multi-agent settings by MAPPO",
        "2211.03032_3caa2e055ad94a3110cd4be221a05399": "\\citep{MAPPO}",
        "2211.03032_906883b18812b49326ff2c95e67565fe": ", CoPPO",
        "2211.03032_fc17636d92606d4173a8612a42ba8038": "\\citep{COPPO}",
        "2211.03032_098b4db3a262d8205367b653b6f02c14": ", and HAPPO",
        "2211.03032_50374ba5b005f49a08a31122167ef4ab": "\\citep{HAPPO}",
        "2211.03032_73a8f940ccecf87f69e26fa5e016dcf5": "respectively via learning a centralized state value function",
        "2211.03032_ed023035184a5672cb3f5e9df5a471f8": "However, these methods are CTDE and thus not appropriate for decentralized learning.",
        "2211.03032_bf3f6366a017b8c80d5b104a913b4f83": "Independent learning",
        "2211.03032_1901ee77c153adf18b9e7f8eb03c16a0": "\\citep{CO-MARLreview}",
        "2211.03032_82120baba9940de761d4339cc8fa6193": "is the most straightforward approach for fully decentralized learning and has actually been studied in cooperative MARL since decades ago",
        "2211.03032_60070b8ae3fe7777814f965dd5ce5ba9": "The representatives are independent Q-learning (IQL)",
        "2211.03032_62e01c193d50615fe355a59ca80b119e": "\\citep{IQL,IDQN}",
        "2211.03032_edbe74f9ccdf1c4f92de622182cce209": "and independent actor-critic (IAC) as",
        "2211.03032_6a8068d7f79ccbf1f79a2b71e612e4de": "\\citet{COMA}",
        "2211.03032_a4376d6be42e65b1f745a8db1aa1e316": "empirically studied",
        "2211.03032_879f5682dd8582771c39a9ee6b5503ee": "These methods make agents directly execute the single-agent Q-learning or actor-critic algorithm individually",
        "2211.03032_d326bbf20615f6ed1dda7d4d0a27ab62": "The drawback of such independent learning methods is obvious",
        "2211.03032_60bfb97b8487805e4b1e97d278c8ca92": "As other agents are also learning, each agent interacts with a non-stationary environment, which violates the stationary condition of MDP",
        "2211.03032_81d8ff13e400696069dc3b4252d2f2f6": "Thus, these methods are not with any convergence guarantee theoretically, though IQL could obtain good performance in several benchmarks",
        "2211.03032_19703ccaab41c49f70e352399aa55788": "More recently, decentralized learning has also been specifically studied with communication",
        "2211.03032_e3a544caeb837629547b1e5415d10b7e": "\\citep{zhang2018fully,li2020f2a2}",
        "2211.03032_3e3defaaf3ba13b4b7c8c84315e0325d": "or parameter sharing",
        "2211.03032_b6060d33874e54910b586f3d6ee791e4": "\\citep{terry2020revisiting}",
        "2211.03032_4f91f358b6b3b0f740c51fa5277aa1bc": "However, in this paper, we consider fully decentralized learning as each agent independently learning its policy while being not allowed to communicate or share parameters as in",
        "2211.03032_4a1e563289fb75d466706d19becc5a23": "\\citet{IDQN,IPPO}",
        "2211.03032_aac34cdeedf9fc8aa86072c35c3b9cc0": "We will propose an algorithm with convergence guarantees in such a fully decentralized learning setting.",
        "2211.03032_0f0ecda859dc70c0e4e3675ad9d74beb": "TRPO",
        "2211.03032_e434536bff9c3c2691aa2b1ccaad4ad0": "is an important single-agent actor-critic algorithm that limits the policy update in a trust region and has a monotonic improvement guarantee by optimizing a surrogate objective",
        "2211.03032_db6f74f5d37661bbf4dba8ff7c0ee4f5": "is a practical but effective algorithm derived from TRPO, which replaces the trust region constraint with a simpler clip trick",
        "2211.03032_266f565b3e575cdd3d584553ab792479": "IPPO",
        "2211.03032_82276e2d61988c42bc0aad2d8c04fa87": "is a recent cooperative MARL algorithm in which each agent just learns with independent PPO",
        "2211.03032_423ec130107b2b22fc20bf50c213e4ec": "Though IPPO is still with no convergence guarantee, it obtains surprisingly good performance in SMAC",
        "2211.03032_c6cc787923e0a5a231c405e63e50cd45": "IPPO is further empirically studied by",
        "2211.03032_234bd832ef37c155663faebd4531d743": "\\citet{MAPPO,benchmark}",
        "2211.03032_5d089676392d57300173872779938a6a": "Their results show IPPO can outperform a few CTDE methods in several benchmark tasks",
        "2211.03032_a04bcf47dfed729859dcabd3ae9769f6": "These studies demonstrate the potential of policy optimization in fully decentralized learning, which we will focus on in this paper.",
        "2211.03032_4c3880bb027f159e801041b1021e88e8": "Method",
        "2211.03032_c547cc5d0ad23f5e1fd11403937a02fe": "From the perspective of policy optimization, in fully decentralized learning, we need to find an objective for each agent such that the joint policy improvement can be guaranteed by each agent independently and individually optimizing its own objective",
        "2211.03032_a20f40b2a90c6cd22a0d153a3c9bff28": "Thus, we propose a novel lower bound of the joint policy improvement to enable",
        "2211.03032_1270f34fa86c0f69b21ba359237e878f": "(DPO)",
        "2211.03032_27847feb74ee52b9e7980e799440e0f1": "In the following, we first discuss some preliminaries; then we analyze the critic of agent in fully decentralized learning; next we derive the lower bound and the proof for convergence; finally we introduce the practical algorithm of DPO.",
        "2211.03032_7ad034002c27ec941420ccdc527cfb38": "Preliminaries",
        "2211.03032_8a3546e2baef5f32e0ca0291171a65ec": "Decentralized partially observable Markov decision process is a general model for cooperative MARL",
        "2211.03032_2da515861492a86ca31b9ce498283068": "A Dec-POMDP is a tuple",
        "2211.03032_e95cd28c6a3810e0365430c657c5f15d": "$\\mathcal{G}=\\left\\{S,A,P,Y,O,I,N,r,\\gamma\\right\\}$",
        "2211.03032_5058f1af8388633f609cadb75a75dc9d": ".",
        "2211.03032_e257acd1ccbe7fcb654708f1a866bfe9": "$S$",
        "2211.03032_9b5ca9d71852a45f4390ac69dd1a3ed1": "is the state space,",
        "2211.03032_f9c4988898e7f532b9f826a75014ed3c": "$N$",
        "2211.03032_0c0b555c01f9949b891241640f746fcd": "is the number of agents,",
        "2211.03032_2917c679d53b0db00e3ed70559018e51": "$\\gamma \\in [0,1)$",
        "2211.03032_059c6d69d675fa6682a8b46626599f2b": "is the discount factor, and",
        "2211.03032_e8927b2b0b673f26d0224343f1641ede": "$I = \\{1,2\\cdots N\\}$",
        "2211.03032_8c4f3a79d451fb139b1b677b4188563b": "is the set of all agents.",
        "2211.03032_a4dbffcf1a359a61c5bf4415b0ed107e": "$A = A_1 \\times A_2 \\times \\cdots \\times A_N$",
        "2211.03032_b3686abf2d8ca5e943b0a47e42a9ec0f": "represents the joint action space, where",
        "2211.03032_4ebf880807deff5796460f39aea46f80": "$A_i$",
        "2211.03032_097e7026d1853e85e5101af129c07fa7": "is the individual action space for agent",
        "2211.03032_77a3b857d53fb44e33b53e4c8b68351a": "$i$",
        "2211.03032_071a26c74380aa4b26ff47f336eed4df": "$P(s^{\\prime} |s,\\bm{a} ): S \\times A \\times S \\to [0,1]$",
        "2211.03032_0260498a254fc67961289fbeba078921": "is the transition function, and",
        "2211.03032_f81d8e4f4d5f00dee3c14f2671c027aa": "$r(s,\\bm{a} ): S \\times A \\to \\mathbb{R}$",
        "2211.03032_bac4e2110b17ccf249cfcd768527d6cb": "is the reward function of state",
        "2211.03032_2d8cca33f0ee74986943da285a93a659": "$s \\in S$",
        "2211.03032_b1cbe6740a819f2d7fea85227b1fdc25": "and joint action",
        "2211.03032_bb1b18782b54e8fbab53f4f2e6ad6eff": "$\\bm{a} \\in A$",
        "2211.03032_91aac9730317276af725abd8cef04ca9": "$Y$",
        "2211.03032_ec5a92d657eba94fcd6d65f3ea32becf": "is the observation space, and",
        "2211.03032_479f598ce476926d00d024e05dd2e955": "$O(s,i):S \\times I \\to Y $",
        "2211.03032_298a47385c99448a9ec046d955b04f59": "is a mapping from state to observation for each agent",
        "2211.03032_e9d3178eefc7e430cc6608c9a30c6cff": "The objective of Dec-POMDP is to maximize",
        "2211.03032_7ce2f6919b55c0ab472dc0d56f92e7ce": "$J({\\bm{\\pi}}) = \\mathbb{E}_{\\bm{\\pi}}\\left[ \\sum_{t = 0} \\gamma^t r(s_t,\\bm{a}_t ) \\right],$",
        "2211.03032_d3ebc1984eb8c6cf0510de9a9a1fb86f": "thus we need to find the optimal joint policy",
        "2211.03032_1570d9e11abaf2fd85128728500413ee": "${\\bm{\\pi}}^{*} = \\arg\\max_{{\\bm{\\pi}}} J({\\bm{\\pi}})$",
        "2211.03032_0816751dcf9279046c7e856d705e63bc": "To settle the partial observable problem, history",
        "2211.03032_c802083e40cdda94f83e2570a7b57920": "$\\tau_i \\in \\mathcal{T}_i = (Y \\times A_i)^*$",
        "2211.03032_4c89860beda33c31f28e9ac9952dabe7": "is often used to replace observation",
        "2211.03032_48a4861be2e8aca4e1edec1fe6c910c9": "$o_i \\in Y$",
        "2211.03032_93437e830467a948ad72eae25822e4d6": "In fully decentralized learning, each agent",
        "2211.03032_840a24e03fb2455f822c106660677745": "independently learns an individual policy",
        "2211.03032_adc32a0d1e0220358ec233ea4df67aec": "$\\pi^i(a_i|\\tau_i)$",
        "2211.03032_94c4a9d86e87771f4204890c81517454": "and their joint policy",
        "2211.03032_7859e9f1b0faaea11a4a99c04a2738ab": "$\\bm{\\pi}$",
        "2211.03032_720a34aaf7b1f313edab61fda6ab1c76": "can be represented as the product of each",
        "2211.03032_e89b71cbae7cfafdc3ce65ffe06082d1": "$\\pi^i$",
        "2211.03032_787bd5bfdb8cb5b2f4367df05dca4b58": "Though each agent learns individual policy as",
        "2211.03032_2f1a3e6c23505941bab9464c3a571800": "in practice, in our analysis, we will assume that each agent could receive the state",
        "2211.03032_6f9bad7347b91ceebebd3ad7e6f6f2d1": "$s$",
        "2211.03032_88ca02ce1276fd88685be8d3b3b82dc0": ", because the analysis in partially observable environments is much more difficult and the problem may be undecidable in Dec-POMDP",
        "2211.03032_737129ba0e37846939bf10fe72c86d8e": "\\citep{undecideDECPOMDP}",
        "2211.03032_72ef2b3c05cad09db2786841539d6f91": "Moreover, the V-function and Q-function of the joint policy",
        "2211.03032_f0cf53c30791fded654d20b2b3b329f1": "are as follows,",
        "2211.03032_ff1ef9ddfb9118bdb4056111be81f998": "\\begin{align}\n\t\t& V^{{\\bm{\\pi}}}(s) = \\mathbb{E}_{\\bm{a} \\sim \\bm{\\pi}}\\left[ Q^{{\\bm{\\pi}}}(s,\\bm{a}) \\right] \\\\\n\t\t& Q^{{\\bm{\\pi}}}(s,\\bm{a}) = r(s,\\bm{a}) + \\gamma \\mathbb{E}_{s^\\prime \\sim P(\\cdot | s, \\bm{a} )}\\left[ V^{\\bm{\\pi}}(s^\\prime) \\right]. \\label{eq:joint-q}\n\t\\end{align}",
        "2211.03032_dc9bc62a4014316c0f6b5b5506663f3c": "In Dec-POMDP, we can still obtain a TRPO objective for the joint policy",
        "2211.03032_4cb767b954f64a57670a5989b6333e2b": "from the theoretical results in single-agent RL",
        "2211.03032_7d8aa5220ad25d1d0056dd7cf6397e85": ", which is referred to as the joint TRPO objective,",
        "2211.03032_33f1349eb4bcc83abbb57ce64217f50e": "\\begin{align}\n    & J({\\bm{\\pi}_{\\operatorname{new}} } )-J({\\bm{\\pi}_{\\operatorname{old}} } ) \\ge \\mathcal{L}^{\\operatorname{joint}}_{\\bm{\\pi}_{\\operatorname{old}}}(\\bm{\\pi}_{\\operatorname{new}}) - C \\cdot D_{\\operatorname{KL}}^{\\operatorname{max}}(\\bm{\\pi}_{\\operatorname{old}} \\| \\bm{\\pi}_{\\operatorname{new}}) \\label{eq:trpo-bound}\\\\\n    & {\\text{where}\\,\\,\\,}\\mathcal{L}^{\\operatorname{joint}}_{\\bm{\\pi}_{\\operatorname{old}}}(\\bm{\\pi}_{\\operatorname{new}}) = \\sum_{s} \\bm{\\rho}_{\\operatorname{old}}(s) \\sum_{\\bm{a}} \\bm{\\pi}_{\\operatorname{new}}(\\bm{a} | s) A_{{\\on{old}}}(s,\\bm{a}), \\label{eq:trpo_l}\n\\end{align}",
        "2211.03032_567904efe9e64d9faf3e41ef402cb568": "where",
        "2211.03032_bf534a7583f0a4288f3858f5c95d3e53": "$D_{\\operatorname{KL}}^{\\operatorname{max}}(\\bm{\\pi}_{\\operatorname{old}} \\| \\bm{\\pi}_{\\operatorname{new}}) = \\max_{s} D_{\\operatorname{KL}}(\\bm{\\pi}_{\\operatorname{old}}(\\cdot|s) \\| \\bm{\\pi}_{\\operatorname{new}}(\\cdot|s) ) $",
        "2211.03032_c0cb5f0fcf239ab3d9c1fcd31fff1efc": ",",
        "2211.03032_7f666b43383b2c8c31355ccea53e11d7": "$\\bm{\\rho}_{\\operatorname{old}}(s) = \\sum_{t = 0} \\gamma^t \\operatorname{Pr}(s_t = s| {\\bm{\\pi}_{\\operatorname{old}} })$",
        "2211.03032_abeaa9e137ed6a908615f0c412f7c491": "is the discounted stationary distribution of the state given",
        "2211.03032_23b1f852095e157691b6003ca8973139": "$\\bm{\\pi}_{\\operatorname{old}}$",
        "2211.03032_02d5df8138f662e6b332d60c629f1376": "$A_{{\\on{old}}}$",
        "2211.03032_ae3e80f13e17a2ecd81eee0bedbb7d53": "is the advantage function under",
        "2211.03032_86738b8aee15c70d7bde6f1ba7dcc096": "$\\bm{\\pi}_{\\on{old}}$",
        "2211.03032_8a0e2b549d223aba55a866c38d1c9275": ", and",
        "2211.03032_9b325b9e31e85137d1de765f43c0f8bc": "$C$",
        "2211.03032_fbc23d83994fb3f4e56f47869d7ee28c": "is a constant",
        "2211.03032_a52acd5c6d5fa94b31bd4f1df870e872": "The joint TRPO objective,",
        "2211.03032_5b5399dc6a69180549439f3f5add2513": ", RHS of",
        "2211.03032_13e97e9f4a55743a14e87ef5f48900d8": ", is a lower bound for the difference between the new joint policy",
        "2211.03032_0de04c6e11b924c4ba933177d13d5db1": "$\\bm{\\pi}_{\\operatorname{new}}$",
        "2211.03032_0682743df5648b7011b8585653ad7f54": "and the old joint policy",
        "2211.03032_ff8a18709286f805940ba085c0ae421b": "in term of expected return",
        "2211.03032_5571c7375745e3db22eb1b6e4905e491": "Therefore, we can use this objective as a surrogate, and maximizing this surrogate can guarantee that the policy is improving monotonically",
        "2211.03032_0f0cb3759d73f5a80429d8a8dd88ff51": "However, the joint TRPO objective cannot be directly optimized in fully decentralized learning as this objective is involved in the joint policy, which cannot be accessed in fully decentralized learning",
        "2211.03032_37d2872de6a1962e5196cfa3dc6cb929": "We will propose a new lower bound (surrogate) for",
        "2211.03032_fc3be43301d8bc53651636a917c0732c": "$J({\\bm{\\pi}_{\\operatorname{new}} } )-J({\\bm{\\pi}_{\\operatorname{old}} } )$",
        "2211.03032_d6456e32e8a4ca1b706c1228d8c50f8a": ", which can be optimized in fully decentralized learning",
        "2211.03032_e311856ed0cc3ef93a16ac615b684c96": "Before introducing our new surrogate, we need to first analyze the critic of agent in fully decentralized learning, which is referred to as decentralized critic.",
        "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44": "Decentralized Critic",
        "2211.03032_c6a6fc28a5b911126979ce6c1c826a65": "In fully decentralized learning, each agent learns independently from its own interactions with the environment",
        "2211.03032_493c13e375dafc98bb8d24db985db8e7": "Therefore, the Q-function of each agent",
        "2211.03032_648c8269b177a64d718ddd3a1517a3a5": "is actually the following formula:",
        "2211.03032_d68fbdd5939055fd3357b5ee40f40629": "\\begin{align}\n     Q^{\\pi^i}_{\\pi^{-i} }(s,a_i) = r_{\\pi^{-i} }(s,a_i) + \\gamma \\mathbb{E}_{a_{-i} \\sim \\pi^{-i}, s^\\prime \\sim P(\\cdot|s,a_i,a_{-i}), a_i^\\prime \\sim \\pi^i}[Q^{\\pi^i}_{\\pi^{-i}}(s^\\prime,a_i^\\prime)],\n\\end{align}",
        "2211.03032_7577abb1d43510396d998b03cdc055b1": "$r_{\\pi^{-i} }(s,a_i) = \\mathbb{E}_{\\pi^{-i}}[r(s,a_i,a_{-i})]$",
        "2211.03032_ecc0e97074ec6d374b64ff40816aa9cd": "$\\pi^{-i}$",
        "2211.03032_be5d5d37542d75f93a87094459f76678": "and",
        "2211.03032_bc85e4fea70c0fadf7421cac0b051af2": "$a_{-i}$",
        "2211.03032_3db35a4db6eafec06d0f6ed661a7e1ea": "respectively denote the joint policy and joint action of all agents expect agent",
        "2211.03032_80ab841985ff20e34c7623bc6cd45ce6": "If we take the expectation",
        "2211.03032_aef485e36b6756f13ee47d3b3561dabb": "$\\mathbb{E}_{a_{-i}^\\prime \\sim \\pi^{-i}(\\cdot|s^\\prime),a_{-i} \\sim \\pi^{-i}(\\cdot|s)}$",
        "2211.03032_54b9ea98fdc4a11de9a02675d69412c0": "over both sides of the Q-function of joint policy (",
        "2211.03032_e8d339e34296980200497148da022b6c": "), then we have",
        "2211.03032_9995f347aac1b769a3d393615cf66827": "\\begin{align*}\n    \\mathbb{E}_{\\pi^{-i}}[Q^{{\\bm{\\pi}}}(s,a_i,a_{-i})] = r_{\\pi^{-i} }(s,a_i) + \\gamma \\mathbb{E}_{a_{-i} \\sim \\pi^{-i}, s^\\prime \\sim P(\\cdot|s,a_i,a_{-i}),a_i^\\prime \\sim \\pi^i}\\left[ \\mathbb{E}_{\\pi^{-i}}[Q^{{\\bm{\\pi}}}(s^\\prime,a_i^\\prime,a_{-i}^\\prime)] \\right].\n\\end{align*}",
        "2211.03032_b0ddce9b08dd34deb66e48a4943e76d1": "We can see that",
        "2211.03032_a430ecc501bb01d022a23a2c430eb435": "$Q^{\\pi^i}_{\\pi^{-i} }(s,a_i)$",
        "2211.03032_42a3a2037c8227e9ff22f59093167dc3": "$\\mathbb{E}_{\\pi^{-i}}[Q^{{\\bm{\\pi}}}(s,a_i,a_{-i})]$",
        "2211.03032_707baf3a3f5cf3777fedcc61fa072dab": "satisfy the same iteration",
        "2211.03032_b457ace2bb5ce9b11aefa80a762e0cc6": "Moreover, we will show in the following that",
        "2211.03032_799b61690872d6c887679c689395a146": "are just the same",
        "2211.03032_2f5030b6b126a005be63d99add3b2147": "We first define an operator",
        "2211.03032_3a20859426a38f45ed62200aa48b8b51": "$\\Gamma^{\\pi^i}_{\\pi^{-i}}$",
        "2211.03032_47fde7a0db9d9bafacdb190ce4747ee1": "as follows,",
        "2211.03032_1eaed172f767d412aeaff3a01512cd1e": "\\begin{align*}\n    \\Gamma^{\\pi^i}_{\\pi^{-i}}Q(s,a_i) =  r_{\\pi^{-i} }(s,a_i) + \\gamma \\mathbb{E}_{a_{-i} \\sim \\pi^{-i}, s^\\prime \\sim P(\\cdot|s,a_i,a_{-i}),a_i^\\prime \\sim \\pi^i}[Q(s^\\prime,a_i^\\prime)].\n\\end{align*}",
        "2211.03032_1fd92975660e16837445e2c119d6228f": "Then we will prove that the operator",
        "2211.03032_11045e43379819e919d42e872934a0e2": "is a contraction",
        "2211.03032_396392595c5f68f79198d6a59d3e4c3b": "Considering any two individual Q-functions",
        "2211.03032_b3e07525c64893f724160bd7046f85d7": "$Q_1$",
        "2211.03032_d455ef8f60a853c27a0abd1e49898ea0": "$Q_2$",
        "2211.03032_0f4ad8ed6f8abf5056019865b8b13f87": ", we have:",
        "2211.03032_ddaa44618cf3a32ff9a3d1133d39848b": "\\begin{align*}\n    \\Vert \\Gamma^{\\pi^i}_{\\pi^{-i}}Q_1 - \\Gamma^{\\pi^i}_{\\pi^{-i}}Q_2 \\Vert_{\\infty} & = \\max_{s,a_i} \\gamma|\\mathbb{E}_{a_{-i} \\sim \\pi^{-i}, s^\\prime \\sim P(\\cdot|s,a_i,a_{-i}),a_i^\\prime \\sim \\pi^i}[Q_1(s^\\prime,a_i^\\prime) - Q_2(s^\\prime,a_i^\\prime)]  | \\\\\n    & \\le \\gamma \\mathbb{E}_{a_{-i} \\sim \\pi^{-i}, s^\\prime \\sim P(\\cdot|s,a_i,a_{-i}),a_i^\\prime \\sim \\pi^i}[\\max_{s^\\prime, a_i^\\prime}|Q_1(s^\\prime,a_i^\\prime) - Q_2(s^\\prime,a_i^\\prime)| ] \\\\\n    & = \\gamma \\max_{s^\\prime, a_i^\\prime}|Q_1(s^\\prime,a_i^\\prime) - Q_2(s^\\prime,a_i^\\prime)| \\\\\n    & = \\gamma \\Vert Q_1 - Q_2 \\Vert_{\\infty}.\n\\end{align*}",
        "2211.03032_a07a7cf4670477e71b1908d9190d2c92": "So the operator",
        "2211.03032_a5d431f3390b6c51d1a8a7d1851b3ef1": "$\\Gamma^{\\pi_i}_{\\pi_{-i}}$",
        "2211.03032_3882ff84d4940c1539ee37a9f1572cba": "has one and only one fixed point, which means",
        "2211.03032_f43e37843fa011a80bf07f5f9e720276": "\\begin{align*}\n    &Q^{\\pi^i}_{\\pi^{-i} }(s,a_i)=\\mathbb{E}_{\\pi^{-i}}[Q^{{\\bm{\\pi}}}(s,a_i,a_{-i})],\\\\\n    &V^{\\pi^i}_{\\pi^{-i} }(s)=\\mathbb{E}_{\\pi^{-i}}[V^{{\\bm{\\pi}}}(s)]=V^{{\\bm{\\pi}}}(s).\n\\end{align*}",
        "2211.03032_6e66e7779d3c682e1b4fcc61624abcd9": "With this well-defined decentralized critic, we can further analyze the objective of IPPO",
        "2211.03032_f310232493ed7bad711ee30be8f2754c": "In IPPO, the policy objective of each agent",
        "2211.03032_052708a7a998a756ed6846968fd7587a": "can be essentially formulated as follows:",
        "2211.03032_b30f95f29556d543afab549a1ffb487b": "\\begin{align}\n    &\\mathcal{L}^{i}_{\\bm{\\pi}_{\\operatorname{old}}}(\\pi^i_{\\operatorname{new}}) = \\sum_{s} \\bm{\\rho}_{\\operatorname{old}}(s) \\sum_{a_i} \\pi^i_{\\operatorname{new}}(a_i|s) A^i_{\\operatorname{old}}(s,a_i),\n    \\label{eq:init-obj}\\\\\n    &{\\text{where}\\,\\,\\,}A^i_{\\operatorname{old}}(s,a_i) = Q^{\\pi_{\\operatorname{old}}^i }_{\\pi_{\\operatorname{old}}^{-i} }(s,a_i) - \\mathbb{E}_{\\pi_{\\operatorname{old}}^i}[Q^{\\pi_{\\operatorname{old}}^i }_{\\pi_{\\operatorname{old}}^{-i} }(s,a_i)] = \\mathbb{E}_{\\pi^{-i}_{\\operatorname{old}}}[A_{\\operatorname{old}}(s,a_i,a_{-i})]. \\notag\n\\end{align}",
        "2211.03032_09d6630f15ebed1a68325026bef45812": "However,",
        "2211.03032_ac3c78a4068b296a9859c1fdd475492b": "is different from",
        "2211.03032_8beb6b66ad3958a11fd4b712dfad31b6": "in the joint TRPO objective",
        "2211.03032_5692c01e7909f7af05b5de728d25c01d": "Thus, directly optimizing",
        "2211.03032_85b8127d62ecc66825d28fca5e384c9a": "may not improve the joint policy, and thus cannot provide any guarantee for convergence, to the best of our knowledge",
        "2211.03032_cf47ef61d4d199a8ecdaacc66cad39c1": "Nevertheless, it seems that",
        "2211.03032_da32c036a0afce487964d965a38bb373": "$A^i_{\\operatorname{old}}(s,a_i)$",
        "2211.03032_519f207ec1d99060e9560686c626d1bb": "is the only advantage formulation that can be accessed by each agent in fully decentralized learning",
        "2211.03032_3c13a810a0494ddfade56f97e21f2a9c": "So, the policy objective of DPO will be derived on",
        "2211.03032_bd8b2e7146fe87a129042914b1ae7bc3": "but with modifications to guarantee convergence, and we will introduce the detail in the next section",
        "2211.03032_56dbb97fc91302bd8bcc7540436f8ea5": "In the following, we discuss how to compute this advantage in practice in fully decentralized learning",
        "2211.03032_d051c9590d956f2c787ebdc0952a9f4d": "As we need to calculate",
        "2211.03032_dcc905487342811937e7467b4bf7faab": "$A^i_{\\operatorname{old}}(s,a_i) =  \\mathbb{E}_{\\pi^{-i}_{\\operatorname{old}}}[r(s,a_i,a_{-i}) + \\gamma V^{\\bm{\\pi}_{\n\\operatorname{old}}}(s^\\prime) - V^{\\bm{\\pi}_{\n\\operatorname{old}}}(s)] $",
        "2211.03032_21844f38a52d3d39410cc0f724a9e3c1": "for the policy update, we can approximate",
        "2211.03032_23a58bf9274bedb19375e527a0744fa9": "with",
        "2211.03032_7361032a679e00bb41dc0cd2137918c8": "$\\hat{A}^i(s,a_i) = r + \\gamma V^{\\pi^i}_{\\pi^{-i}}(s^\\prime) - V^{\\pi^i}_{\\pi^{-i}}(s)$",
        "2211.03032_73f815a0fbeb6c6311ab29c632bbfd3a": ", which is an unbiased estimate of",
        "2211.03032_62e677139cea580ee831ffa38559ec4a": ", though it may be with a large variance",
        "2211.03032_039d72fc2b5172230852f09464a059d1": "In practice, we can follow the traditional idea in fully decentralized learning, and let each agent",
        "2211.03032_f3345f1e510541414cdfa41d864facb4": "independently learn an individual value function",
        "2211.03032_a26d42ec640fe54a9cf4cb45846f41cf": "$V^i(s)$",
        "2211.03032_c54cfe49fc0a021d13fcdc9bac86724b": "Then, we further have",
        "2211.03032_bf58637d0875fe280bd9bfaef798acdc": "$\\hat{A}^i(s,a_i) \\approx r + \\gamma V^i(s^\\prime) - V^i(s)$",
        "2211.03032_508be29dacdf019c6fc430af4bbf6e2c": "The loss for the decentralized critic is as follows:",
        "2211.03032_49df72e05810d70c8cc0833e46627cf2": "\\begin{equation}\n    \\mathcal{L}^i_{\\operatorname{critic}} = \\mathbb{E}\\left[ (V^i(s) - y_i)^2 \\right], \\quad \\text{where} \\,\\,  y_i = r + \\gamma V^i(s^\\prime) \\text{\\, or Monte Carlo return}.\n    \\label{eq:critic-obj}\n\\end{equation}",
        "2211.03032_ba7a442bd9dcf8d384851a79aa6c750e": "There may be some ways to improve the learning of this critic, which however is beyond the scope of our discussion.",
        "2211.03032_bdc8ac1715448e99be17fa11376a2028": "Decentralized Surrogate",
        "2211.03032_4ed3f61a0ca12db5cebe9d7b1eff6956": "We are ready to introduce the decentralized surrogate",
        "2211.03032_b14608dbe80247d6ace70aadd234f634": "First, we derive our novel lower bound of the joint policy improvement by the following theorem.",
        "2211.03032_800efb76c88ed758854b775e2a554cd3": "Suppose",
        "2211.03032_9e6dd5dfdd88d81a8f81d882e21385d9": "are two joint policies",
        "2211.03032_39abc0c3af755dc5d6ab8fd8bba6791d": "Then, the following bound holds:",
        "2211.03032_f2e130e07814efa3754bc46e3f645583": "\\begin{equation*}\n         J({\\bm{\\pi}_{\\operatorname{new}} } )-J({\\bm{\\pi}_{\\operatorname{old}} } ) \\ge \\frac{1}{N} \\sum_{i = 1}^{N} \\mathcal{L}^{i}_{\\bm{\\pi}_{\\operatorname{old}}}(\\pi^i_{\\operatorname{new}}) -  \\tilde{M} \\cdot \\sum_{i = 1}^N \\sqrt{D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi_{\\on{new}}^{i} )} -  C \\cdot \\sum_{i = 1}^N  D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi_{\\on{new}}^{i}),  \n    \\end{equation*}",
        "2211.03032_1728f22cf0a81ce0cd5ff044aca30682": "$\\tilde{M} = \\frac{2\\max_{s,\\bm{a}}|A_{\\on{old}}(s,\\bm{a})|}{1 - \\gamma}$",
        "2211.03032_407bfeb24d1036f525398ffe999e4da8": "$C = \\frac{4\\gamma\\max_{s,\\bm{a}}|A_{\\on{old}}(s,\\bm{a})|}{(1 - \\gamma)^2}$",
        "2211.03032_222c05434b3c7aee774e3405b9e9fe7f": "are two constants.",
        "2211.03032_be9aa1aa618366bbe27962a10e57fc23": "We first consider",
        "2211.03032_65dd75ca6665ccbba4d9984a25088718": "$\\mathcal{L}^{\\operatorname{joint}}_{\\bm{\\pi}_{\\operatorname{old}}}(\\bm{\\pi}_{\\operatorname{new}}) - \\mathcal{L}^{i}_{\\bm{\\pi}_{\\operatorname{old}}}(\\pi^i_{\\operatorname{new}})$",
        "2211.03032_88924c19517b83228a6cb51c3237718f": "According to",
        "2211.03032_712282a57f058d1fffe7532b71ff6971": ", we have the following equation:",
        "2211.03032_e746d83e9ad5ea3e662504c8e866d565": "\\begin{align*}\n        & \\mathcal{L}^{\\operatorname{joint}}_{\\bm{\\pi}_{\\operatorname{old}}}(\\bm{\\pi}_{\\operatorname{new}}) - \\mathcal{L}^{i}_{\\bm{\\pi}_{\\operatorname{old}}}(\\pi^i_{\\operatorname{new}}) \\\\\n        & = \\sum_{s} \\bm{\\rho}_{\\operatorname{old}}(s) \\sum_{a_i} \\pi^i_{\\operatorname{new}}(a_i|s) \\bigg( \\sum_{a_{-i}} \\pi_{\\operatorname{new}}^{-i}(a_{-i}|s)A_{\\on{old}}(s,a_i,a_{-i}) -  A_{\\operatorname{old}}^i(s,a_i) \\bigg) \\\\\n        & = \\mathbb{E}_{\\bm{\\rho}_{\\on{old}}}\\mathbb{E}_{\\pi^i_{\\on{new}}}\\bigg[ \\sum_{a_{-i}}  \\left( \\pi^{-i}_{\\on{new}}(a_{-i}|s) - \\pi^{-i}_{\\on{old}}(a_{-i}|s) \\right) A_{\\on{old}}(s,a_i,a_{-i})\\bigg].\n    \\end{align*}",
        "2211.03032_66d3ac9b51a69a057fb42ca8dece94e7": "Then, we have the following inequalities:",
        "2211.03032_11c981f03cdb3c9a469bf36492b32004": "\\begin{align}\n\t\t& |\\mathcal{L}^{\\operatorname{joint}}_{\\bm{\\pi}_{\\operatorname{old}}}(\\bm{\\pi}_{\\operatorname{\\on{new}}}) - \\mathcal{L}^{i}_{\\bm{\\pi}_{\\operatorname{old}}}(\\pi^i_{\\operatorname{new}})| \\notag\\\\\n\t\t&  \\le \\mathbb{E}_{\\bm{\\rho}_{\\on{old}}}\\mathbb{E}_{\\pi^i_{\\on{new}}}\\bigg[ \\sum_{a_{-i}} | \\pi^{-i}_{\\on{old}}(a_{-i}|s) - \\pi^{-i}_{\\on{new}}(a_{-i}|s) | \\ |A_{\\on{old}}(s,a_i,a_{-i})|\\bigg] \\notag\\\\\n\t\t& \\le \\mathbb{E}_{\\bm{\\rho}_{\\on{old}}}\\mathbb{E}_{\\pi^i_{\\on{new}}} \\bigg[ M \\sum_{a_{-i}} | \\pi^{-i}_{\\on{old}}(a_{-i}|s) - \\pi^{-i}_{\\on{new}}(a_{-i}|s) |  \\bigg] \\quad\\quad (M = \\max_{s,\\bm{a}}|A_{\\on{old}}(s,\\bm{a})|) \\notag\\\\\n\t\t& = 2M \\mathbb{E}_{\\bm{\\rho}_{\\on{old}}} \\left[ D_{\\operatorname{TV}}(\\pi_{\\on{old}}^{-i}(\\cdot|s)\\Vert \\pi_{\\on{new}}^{-i}(\\cdot|s)) \\right]  \\notag\\\\\n\t\t& \\le  \\frac{2M}{1-\\gamma}\\max_{s}   D_{\\operatorname{TV}}(\\pi_{\\on{old}}^{-i}(\\cdot|s)\\Vert \\pi_{\\on{new}}^{-i}(\\cdot|s))  \\notag\\\\\n\t\t& = \\tilde{M} D_{\\operatorname{TV}}^{\\max}(\\pi_{\\on{old}}^{-i} \\Vert \\pi_{\\on{new}}^{-i}) \\quad\\quad (\\tilde{M} = \\frac{2M}{1-\\gamma}) \\notag\\\\\n\t\t& \\le \\tilde{M} \\sqrt{D_{\\operatorname{KL}}^{\\max}(\\pi_{\\on{old}}^{-i} \\Vert \\pi_{\\on{new}}^{-i})} \\label{eq:tv-kl} \\\\\n\t\t& \\le \\tilde{M} \\sqrt{\\sum_{j \\not = i} D^{\\max}_{\\operatorname{KL}}(\\pi_{\\operatorname{old}}^{j} \\Vert \\pi_{\\operatorname{new}}^{j} ) }, \\label{eq:kl-sum}\n\t\\end{align}",
        "2211.03032_e4699885c23ee52ca02ebd084babdbf9": "is from the relationship between the total variance and KL-divergence that",
        "2211.03032_27f9e4d8bb9f8a74fa1db7117eb465a0": "$D_{\\operatorname{TV}}(p\\Vert q)^2 \\le D_{\\operatorname{KL}}(p\\Vert q)$",
        "2211.03032_03e2bd5112ed8d6f137440cdfd7851bf": "is a property of the KL-divergence, which can be obtained as follows,",
        "2211.03032_aa67a51c7cf35b85cff62fb05db600c3": "\\begin{align}\n        D_{\\operatorname{KL}}^{\\max}(\\bm{\\pi}_{\\operatorname{old}} || \\bm{\\pi}_{\\operatorname{new}}) & = \\max_s D_{\\operatorname{KL}}(\\bm{\\pi}_{\\operatorname{old}}(\\cdot|s) || \\bm{\\pi}_{\\operatorname{new}}(\\cdot|s)) \\notag \\\\\n        & =\\max_s  \\sum_i D_{\\operatorname{KL}}(\\pi^i_{\\operatorname{old}}(\\cdot|s) || \\pi^i_{\\operatorname{new}}(\\cdot|s)) \\notag \\\\\n        & \\le \\sum_i \\max_s D_{\\operatorname{KL}}(\\pi^i_{\\operatorname{old}}(\\cdot|s) || \\pi^i_{\\operatorname{new}}(\\cdot|s)) \\notag \\\\\n        & = \\sum_i D_{\\operatorname{KL}}^{\\max}(\\pi^i_{\\operatorname{old}} || \\pi^i_{\\operatorname{new}}). \\label{eq:kl-property}\n    \\end{align}",
        "2211.03032_5da618e8e4b89c66fe86e32cdafde142": "From",
        "2211.03032_74cb4cdf246f86d424bc1ec6553c5cf5": ", we can further obtain the following inequality,",
        "2211.03032_b0fa5db4b1a32c8a67fc2e2884bff409": "\\begin{equation}\n        \\mathcal{L}^{\\operatorname{joint}}_{\\bm{\\pi}_{\\operatorname{old}}}(\\bm{\\pi}_{\\operatorname{new}}) - \\mathcal{L}^{i}_{\\bm{\\pi}_{\\operatorname{old}}}(\\pi^i_{\\operatorname{new}}) \\ge -\\tilde{M} \\sqrt{\\sum_{j \\not = i} D^{\\max}_{\\operatorname{KL}}(\\pi_{\\on{old}}^{j} \\Vert \\pi_{\\on{new}}^{j} ) }. \\label{eq:coro}\n    \\end{equation}",
        "2211.03032_7e8a3d9412a480b23f68708d12a22f3f": "Next we will prove this theorem, starting from",
        "2211.03032_88ed5dc6a0002c127ff717d2d7f83fe4": "\\begin{align}\n\t& J({\\bm{\\pi}_{\\operatorname{new}} } )-J({\\bm{\\pi}_{\\operatorname{old}} } ) \\ge \\mathcal{L}^{\\operatorname{joint}}_{\\bm{\\pi}_{\\operatorname{old}}}(\\bm{\\pi}_{\\operatorname{new}}) - C \\cdot D_{\\operatorname{KL}}^{\\operatorname{\\max}}(\\bm{\\pi}_{\\operatorname{old}} || \\bm{\\pi}_{\\operatorname{new}}) \\notag \\\\\n\t& = \\frac{1}{N} \\sum_{i = 1}^{N} \\mathcal{L}^{\\operatorname{joint}}_{\\bm{\\pi}_{\\operatorname{old}}}(\\bm{\\pi}_{\\operatorname{new}}) -  C \\cdot D_{\\operatorname{KL}}^{\\operatorname{\\max}}(\\bm{\\pi}_{\\operatorname{old}} || \\bm{\\pi}_{\\operatorname{new}}) \\notag \\\\\n\t& \\label{eq:step1} \\ge \\frac{1}{N} \\sum_{i = 1}^{N} \\mathcal{L}^{i}_{\\bm{\\pi}_{\\operatorname{old}}}(\\pi^i_{\\operatorname{new}}) - \\frac{\\tilde{M}}{N}\\sum_{i = 1}^{N}\\sqrt{\\sum_{j \\not = i} D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{j} \\Vert \\pi_{\\on{new}}^{j} ) } -  C \\cdot D_{\\on{KL}}^{\\max}(\\bm{\\pi}_{\\on{new}}\\Vert\\bm{\\pi}_{\\on{old}}) \\\\\n\t& \\label{eq:step2} \\ge \\frac{1}{N} \\sum_{i = 1}^{N} \\mathcal{L}^{i}_{\\bm{\\pi}_{\\operatorname{old}}}(\\pi^i_{\\operatorname{new}}) - \\tilde{M} \\sqrt{\\frac{N-1}{N} \\sum_{i = 1}^N D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi_{\\on{new}}^{i} ) } -  C \\cdot D_{\\on{KL}}^{\\max}(\\bm{\\pi}_{\\on{new}}\\Vert\\bm{\\pi}_{\\on{old}})  \\\\ \n\t& \\label{eq:step3} \\ge \\frac{1}{N} \\sum_{i = 1}^{N} \\mathcal{L}^{i}_{\\bm{\\pi}_{\\operatorname{old}}}(\\pi^i_{\\operatorname{new}}) - \\tilde{M} \\sqrt{\\frac{N-1}{N} \\sum_{i = 1}^N D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi_{\\on{new}}^{i} ) } -  C \\cdot \\sum_{i = 1}^N  D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi_{\\on{new}}^{i} )  \\\\ \n\t& \\ge \\frac{1}{N} \\sum_{i = 1}^{N} \\mathcal{L}^{i}_{\\bm{\\pi}_{\\operatorname{old}}}(\\pi^i_{\\operatorname{new}}) - \\tilde{M} \\sqrt{ \\sum_{i = 1}^N D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi_{\\on{new}}^{i} ) } -  C \\cdot \\sum_{i = 1}^N  D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi_{\\on{new}}^{i} )  \\notag \\\\ \n\t& \\label{eq:step6} \\ge \\frac{1}{N} \\sum_{i = 1}^{N} \\mathcal{L}^{i}_{\\bm{\\pi}_{\\operatorname{old}}}(\\pi^i_{\\operatorname{new}}) -  \\tilde{M} \\cdot \\sum_{i = 1}^N \\sqrt{D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi_{\\on{new}}^{i} )} -  C \\cdot \\sum_{i = 1}^N  D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi_{\\on{new}}^{i} ). \n\t\\end{align}",
        "2211.03032_aac9135b48bda028b0c512263039cfbf": "The inequality",
        "2211.03032_2fc2fdbac26fb3077dfcbe40c3c38ff0": "is the direct application of the inequality",
        "2211.03032_b8ec7f4bae9dc26f466cc73ae7f151c4": "is from the Cauthy-Schwarz inequality,",
        "2211.03032_f210eb4a396145ab3fb56a6e83e74e83": "\\begin{align*}\n\t    \\sum_{i = 1}^{N}\\sqrt{\\sum_{j \\not = i} D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{j} \\Vert \\pi_{\\on{new}}^{j} ) } & \\le \\sqrt{N \\sum_{i = 1} \\sum_{j \\not = i} D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{j} \\Vert \\pi_{\\on{new}}^{j}}) \\\\\n\t    & = \\sqrt{N(N-1)\\sum_{i = 1}D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi_{\\on{new}}^{i}}).\n\t\\end{align*}",
        "2211.03032_de1e07d2e73389bf0e7352f392dd2a16": "is from",
        "2211.03032_516ac41c88be20e3926b09730b0baa17": ", while the inequality",
        "2211.03032_309acd31e99e792891849124861665ed": "is from the simple inequality",
        "2211.03032_c950bf568f73db830a6d5a28c99b691b": "$\\sqrt{\\sum_i{a_i}} \\le \\sum_i \\sqrt{a_i} \\,\\, (a_i \\ge 0 ,\\ \\forall i)$",
        "2211.03032_b36f83aff0f688e4d0e8dd2bb932aaaf": "The lower bound in Theorem",
        "2211.03032_fc0c9a2e86ebbd82b35a22043099a00a": "is dedicated to decentralized policy optimization, because it can be directly decomposed individually for each agent as a decentralized surrogate",
        "2211.03032_4c927f0757e6877db398f48f016f75b8": "From Theorem",
        "2211.03032_76d3f30508bb2c9b278434e11d92bc96": ", if we set the policy optimization objective of each agent",
        "2211.03032_f970e2767d0cfe75876ea857f92e319b": "as",
        "2211.03032_d06db2b315675fe3e1c2cadf7af9d1a6": "\\begin{equation}\n    \\pi_{\\operatorname{new}}^i = \\arg \\max_{\\pi^i} \\Big(\\frac{1}{N}\\mathcal{L}^{i}_{\\bm{\\pi}_{\\operatorname{old}}}(\\pi^i)- \\tilde{M} \\cdot \\sqrt{D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi^i )} - C \\cdot D^{\\max}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi^i) \\Big) ,\n    \\label{eq:de-obj}\n\\end{equation}",
        "2211.03032_b7e4cd0e85ee785de1cd742bfbef0126": "then we have",
        "2211.03032_1062ec0701845810beddf3efba7c2987": "$J({\\bm{\\pi}_{\\operatorname{new}} } ) \\ge J({\\bm{\\pi}_{\\operatorname{old}} })$",
        "2211.03032_26f947e26ecd1180afe973ca55981e31": "from TRPO",
        "2211.03032_ca4efa5ec17563c745100c851dd3cd01": "Moreover, as the objective",
        "2211.03032_47431ff9b236d30e23540f6f76240b21": "$J(\\bm{\\pi})$",
        "2211.03032_1ff95a647834e801064b7182ad42818b": "is bounded, the convergence of",
        "2211.03032_92740d4ac83595ad95a878c7b16609a3": "$\\{J(\\bm{\\pi}^t)\\}$",
        "2211.03032_263e1bfea9f1f1a006467f230dcb7b46": "is guaranteed, where",
        "2211.03032_306f13a1f075f2012efb33d2e87ad7e5": "$\\bm{\\pi}^t$",
        "2211.03032_70333d7078a9387f19c1c437e9e31034": "is the joint policy after",
        "2211.03032_4f4f4e395762a3af4575de74c019ebb5": "$t$",
        "2211.03032_e6251e20770f0f39e18a420376f2fe73": "iterations according to",
        "2211.03032_7002b245d017fe0595435a3c31cf57d7": "Note that this result is under the assumption that each agent can obtain the state, and in practice each agent will take the individual trajectory",
        "2211.03032_e7cdf5013524d24e01bb7ecb5878d45f": "$\\tau_i$",
        "2211.03032_91f2b4611b1bcdce55929d7fde5eedb0": "as the approximation to the state.",
        "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a": "Algorithm",
        "2211.03032_1e01aec4d3ef2402d271dbb2eac34a94": "DPO is with a simple idea that each agent optimizes the decentralized surrogate",
        "2211.03032_c53bb3e8082bd04ca2adc30ce251f89c": "However, we face the same trouble as TRPO that the constant",
        "2211.03032_d218b4d5df0580d68b6142a044223f26": "$\\tilde{M}$",
        "2211.03032_4ab7d2f300bb22508b2bd96eaae093c2": "are large and if we directly optimize this objective, then the step size of the policy update will be small",
        "2211.03032_fdca614a7186158fc68a97283dbe63d6": "To settle this problem, we absorb the idea of the adaptive coefficient in PPO",
        "2211.03032_f58f0f04c7d8105c4764796e22178def": "We use two adaptive coefficients",
        "2211.03032_02c9e98e7d9ece3a1b2a41ce24c5d531": "$\\beta_1^i$",
        "2211.03032_1ad63b19ba49298487a4eb18d5383643": "$\\beta_2^i$",
        "2211.03032_d2319d40766dfd50fde1770ae96f721a": "to replace the constant",
        "2211.03032_257937869572521fdd52716164057925": "and additionally replace the maximum KL-divergence with the average KL-divergence",
        "2211.03032_2c0cfefbd27f799756e0b064cddc4395": "In practice, we will actually optimize the following objective",
        "2211.03032_eb3131b4e1dafaf9559546feb9bf795f": "\\begin{equation}\n    \\pi_{\\operatorname{new}}^i = \\arg \\max_{\\pi^i} \\Big( \\frac{1}{N}\\mathcal{L}^{i}_{\\bm{\\pi}_{\\operatorname{old}}}(\\pi^i)- \\beta_1^i  \\sqrt{D^{\\on{avg}}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi^{i} )} - \\beta_2^i D^{\\on{avg}}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi^i ) \\Big),\n    \\label{eq:adaptive-obj}\n\\end{equation}",
        "2211.03032_204fe7ef0b41920bfe0382b396686c18": "$D^{\\on{avg}}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi^{i} ) = \\mathbb{E}_{s \\sim \\bm{\\pi_{\\on{old}}}}\\left[ D_{\\on{KL}}(\\pi_{\\on{old}}^{i}(\\cdot|s) \\Vert \\pi^{i}(\\cdot|s) ) \\right]$",
        "2211.03032_d8ba9e6953d566d49f4cbbcb404cac04": "As for the adaption of",
        "2211.03032_950f76abb7fa54caf7727fe22271766f": ", we need to define a hyperparameter",
        "2211.03032_28caa999d0925744d7982ff1a818d131": "$d_{target}$",
        "2211.03032_112683a6ddc004027c84f6edfafe8d40": ", which can be seen as a ruler for the average KL-divergence",
        "2211.03032_e8d6f4efc276fff3c7075c5e311acbf0": "$D^{\\on{avg}}_{\\on{KL}}(\\pi_{\\on{old}}^{i} \\Vert \\pi_{\\on{new}}^{i} )$",
        "2211.03032_fc02139c3d2d67731eeaf2dd3019204f": "for each agent",
        "2211.03032_786887572f6ef1c20f2d8177cb2f1639": "If",
        "2211.03032_1c8667bcfb412d6a3d70be4187e860a5": "is close to",
        "2211.03032_0be5e2220327ab8d0606f7dfe74366ba": ", then we believe current",
        "2211.03032_d765248e8fdfa4eb91bc0b058eb1ef4e": "are appropriate",
        "2211.03032_484803f778f0161d1cb3838241da9662": "exceeds",
        "2211.03032_ab1094af334394eaaaa5e115f3d9e664": "too much, we believe",
        "2211.03032_66c793daa0ec252e59902bb55d244066": "are small and need to increase and vice versa",
        "2211.03032_2c647b655fcc8e11c2a37b90a3e327d8": "In practice, we will use the following rule to update",
        "2211.03032_853ae90f0351324bd73ea615e6487517": ":",
        "2211.03032_66f384a0fe29fa3101361e5a0fb26f39": "\\begin{equation}\n    \\begin{aligned}\n     & \\text{If} \\  D^{\\on{avg}}_{\\on{KL}}(\\pi_{\\on{old}}^i \\Vert \\pi^i_{\\on{new}} ) >  d_{target}* \\delta , \\quad \\text{then} \\  \\beta^i_j \\leftarrow \\beta^i_j * \\omega \\quad \\forall j \\in \\{1,2\\} \\\\ \n    & \\text{If} \\  D^{\\on{avg}}_{\\on{KL}}(\\pi^i_{\\on{old}} \\Vert \\pi^i_{\\on{new}} ) <  d_{target} / \\delta , \\quad \\text{then} \\  \\beta^i_j \\leftarrow \\beta^i_j / \\omega \\quad \\forall j \\in \\{1,2\\}. \n    \\end{aligned}\n    \\label{eq:adaptive-rule}\n\\end{equation}",
        "2211.03032_b28a6e240ff9c0b97f3777bc67aa39ff": "We choose the constants",
        "2211.03032_57dffa21802cc3d2953e9ee63004de71": "$\\delta=1.5$",
        "2211.03032_ba03f681d92b887e1a770eaedca4944e": "$\\omega = 2$",
        "2211.03032_d6bf618d614c766d86193ac0c83606fa": "as the choice in PPO",
        "2211.03032_bcbb594c372346613098476f18b3ce6a": "As for the critic, we just follow the standard method in PPO",
        "2211.03032_238cc7623559f1b9e0f91db4a90c5920": "Then, we can have the fully decentralized learning procedure of DPO for each agent",
        "2211.03032_eae417273daa4f11bab257929fbffda6": "in Algorithm",
        "2211.03032_8c834d30bbe046c99321eb71949382e5": "[t]",
        "2211.03032_35dba5d75538a9bbe0b4da4422759a0e": "[1]",
        "2211.03032_10927a67903afd162cc29267d434a2bb": "select action",
        "2211.03032_fbb415fbf0c5be1c512757914e8a0b51": "$a_i \\sim \\pi^i(\\cdot|s)$",
        "2211.03032_68a973ee70c70e9391459fe065fa3ff5": "execute action",
        "2211.03032_65ed4b231dcf18a70bae40e50d48c9c0": "$a_i$",
        "2211.03032_4d474f59f0c4d970629e4ad31088176e": "and observe reward",
        "2211.03032_89f2e0d2d24bcf44db73aab8fc03252c": "$r$",
        "2211.03032_3f09298d18a487faee631ddace1f0f9d": "and next state",
        "2211.03032_ac5fc6bb75d77e4b6a16e3a7946496b3": "$s^{\\prime}$",
        "2211.03032_0788a6922bd5f9f130e7ed8980193bab": "collect",
        "2211.03032_8e5460b266eeffcbc8765576aa540418": "$\\langle s,a_i,r,s' \\rangle$",
        "2211.03032_32c30587cb6b2fdf0201b2b0ff79501a": "Update decentralized critic according to",
        "2211.03032_c6d01501684c50a5bdcb008b0bce0a09": "Update policy according to the surrogate",
        "2211.03032_06933067aafd48425d67bcb01bba5cb6": "Update",
        "2211.03032_93156b3a9ec50122adcae63cff800cf4": "$\\beta^i_1$",
        "2211.03032_736a063b5369f7c08dc04e85c1d4b681": "$\\beta^i_2$",
        "2211.03032_440a3bbc91716125aef7de35fc6e49ae": "according to",
        "2211.03032_6972391bc97af6f3885acaa10baaffd6": "The practical algorithm of DPO actually uses some approximations to the decentralized surrogate",
        "2211.03032_51545d33aa90316992b9c5465ae33cd6": "Most of these approximations are traditional practice in RL or with no alternative in fully decentralized learning yet",
        "2211.03032_097a4fa3b9a94508dfdd052653a8fd6f": "We admit that the practical algorithm may not maintain the theoretical guarantee",
        "2211.03032_a4b03bd947079c8ac3162311168c526a": "However, we need to argue that we go one step further to give a decentralized surrogate in fully decentralized learning with convergence guarantee",
        "2211.03032_d86066b0c6a7fd9664b769ae4b504d36": "We believe and expect that a better practical method can be found based on this objective in future work.",
        "2211.03032_4829262cecb9828817b33e0f9c907f91": "Experiments",
        "2211.03032_932237b32f854a053c74b89207c2e273": "In this section, we compare the practical algorithm of DPO with IPPO",
        "2211.03032_f12c623007c875d4da97685b58baa3f5": "in a variety of cooperative multi-agent environments, including a cooperative stochastic game, MPE",
        "2211.03032_9ac44d5ed136ebce1155daa80595fc3d": ", covering both discrete and continuous action spaces, and fully and partially observable environments",
        "2211.03032_6162253436a471e3b480d07068d9888b": "As we consider fully decentralized learning, in the experiments",
        "2211.03032_d55a7659447463fbaf7df5e350687f07": "as sharing parameters should be considered as centralized learning",
        "2211.03032_36c8d160c8583e7cbe1fa8f78785403c": "In all experiments, the network architectures and common hyperparameters of DPO and IPPO are the same for a fair comparison",
        "2211.03032_7adeccc27721de7ab090900939496a27": "More details about experimental settings and hyperparameters are available in Appendix",
        "2211.03032_f0bd75186528cacd658f796b8337e4b0": "Moreover, all the learning curves are from 5 random seeds and the shaded area corresponds to the 95",
        "2211.03032_b8f78ecb847bc34d3dd6f4920501fc97": "confidence interval.",
        "2211.03032_cd373ce12ecadec4ba0da47b8bae334a": "A Didactic Example",
        "2211.03032_aabd501209bb621299f9f1927ed1921a": "\\begin{figure}[t]\n\\vspace{-2mm}\n    \\centering\n    \\begin{subfigure}{0.37\\linewidth}\n        \\centering\n        \\includegraphics[width=.95\\linewidth]{figures/matrix_game_A.pdf}\n        \\vspace{-2mm}\n        \\caption{DPO compared with IPPO}\n        \\label{fig:matrix_performance}\n    \\end{subfigure}\n    \\begin{subfigure}{0.37\\linewidth}\n        \\centering\n        \\includegraphics[width=.95\\linewidth]{figures/matrix_game_B.pdf}\n        \\vspace{-2mm}\n        \\caption{the influence of $d_{{target}}$}\n        \\label{fig:target}\n    \\end{subfigure}\n    \\caption{Empirical studies of DPO on the didactic example: (a) learning curve of DPO compared with IPPO and the global optimum; (b) the influence of different values of $d_{{target}}$ on DPO, x-axis is environment steps.}\n    \\label{fig:matrix}\n    %\\vspace{-2mm}\n\\end{figure}",
        "2211.03032_514c5300d34b272ea166e89722a02e5b": "First, we use a cooperative stochastic game as a didactic example",
        "2211.03032_af474c73deb478d2b9b5b5dd1a571d67": "The cooperative stochastic game is with 100 states, 6 agents and each agent has 5 actions",
        "2211.03032_ee7b90e5b6a3772c5d33cded5f76d8d3": "All the agents share a joint reward function",
        "2211.03032_e8759403a34e23fc508eb2846b808fba": "The reward function and the transition probability are both generated randomly",
        "2211.03032_1986749af4b913cc785600804f134daa": "This stochastic game has a certain degree of complexity which is helpful to distinguish the performance of DPO and IPPO",
        "2211.03032_51c007088db7726413d7e7ae82b19728": "On the other hand, this environment is tabular which means training in this environment is fast and we can do ablation studies efficiently",
        "2211.03032_4d35e757b9552f721c6542ca766bb961": "Moreover, we can find the global optimum by dynamic programming to compare with in this game",
        "2211.03032_4ee39a2789d074c02f4512443e59f2b8": "The learning curves in Figure",
        "2211.03032_b51c5209b8eede99f6f98f931c23c111": "show that DPO performs better than IPPO and learns a better solution in this environment",
        "2211.03032_4a92b6ae9d4ba8dc2fdb1225a2d91ef9": "The fact that DPO learns a sub-optimal solution agrees with our theoretical result",
        "2211.03032_d70c37f51740a5b03e27653e6f7b961e": "However, the sub-optimal solution found by DPO is still away from the global optimum",
        "2211.03032_17f780e8668ab4a3e660c3bff1a2105e": "This means that there is still improvement space",
        "2211.03032_bf0a330db46f07ada5eb6f73aaebb46b": "On the other hand, we study the influence of the hyperparameter",
        "2211.03032_133c4f787e87d708e777c2a01faa6cb6": "$d_{{target}}$",
        "2211.03032_18ebe717682630c89581d20a195374f2": "on DPO",
        "2211.03032_23b868f41949ab56c790ffd6e86403e0": "We choose",
        "2211.03032_dab5902ebe39ffe2330e5265fe1c3306": "$d_{{target}} = 0.001,0.01,0.1,1$",
        "2211.03032_251d4ffd0f3c57a1b350236d5d5880ea": "The empirical results are shown in Figure",
        "2211.03032_2c7a7be8c53bf19e7d6949c12da5e366": "We find that when",
        "2211.03032_079fe1dcb4caa12f29b14392d1354093": "is small, the coefficient",
        "2211.03032_15ef3b23ef739e47090fa0825bf9d390": "$\\beta_1$",
        "2211.03032_2cae3bbfffb6ab2858054ba28bfcba80": "$\\beta_2$",
        "2211.03032_a582908ccc70f2cf7b3f6734058e8887": "are more likely to be increased and the step size of the policy update is limited",
        "2211.03032_5bc4f929ef8135a715d3fa584e68d08a": "So for the case that",
        "2211.03032_e89907fe33fb5becd26ff5b133e0d1a4": "$d_{{target}} = 0.001,0.01$",
        "2211.03032_30e0f9d4bbe45fb1cc62805b28482f86": ", the performance of DPO is relatively low",
        "2211.03032_eb96aab2148055ca3e2c533126477c5a": "And when",
        "2211.03032_5df1581a5a30e698f431cd34d4ada81d": "is large, the policy update may be out of the trust region",
        "2211.03032_9e1cba125ff8b3813163826030bfb190": "This can be witnessed by the fluctuating learning curve of the case",
        "2211.03032_49ac13a76fc79da81f1606aba5176d2e": "$d_{{target}} = 1$",
        "2211.03032_47fb50b771f731d801c88793368347ec": "So we need to choose an appropriate value for",
        "2211.03032_ebb202de48bb08f2f6cf62952dcb9bb0": "and in this environment we choose",
        "2211.03032_10b37bc4b45c36e89abc5f8602729d92": "$d_{{target}} = 0.1$",
        "2211.03032_172c7f5a9649309400f1095cb978bd1f": ", which is also the learning curve of DPO in Figure",
        "2211.03032_019adb4198575d6f89d9ccdd88bac2c5": "We found that the appropriate value for",
        "2211.03032_8eea3e9f5e983760681f0d6cd8fe20cc": "changes in different environments",
        "2211.03032_59df6fe776a939108b9f2a186ab3d988": "In the following, we keep",
        "2211.03032_24b0757de192585f7a74fb5f1bac2748": "to be the same for tasks of the same environment",
        "2211.03032_14c62b788db94b0b338de9d7688c8014": "There may be some better choices for",
        "2211.03032_10e3868c1bf97282778005cadb2b2a6f": ", but it is a bit time-consuming and out of the range of our discussion.",
        "2211.03032_93eda4801aa7098be93d51b921fc4c72": "MPE",
        "2211.03032_dba755585539b1a852ed6d3b64bba072": "\\begin{figure}[t]\n    \\vspace{-2mm}\n    \\centering\n    \\includegraphics[width=.95\\textwidth]{figures/MPE_performance.pdf}\n    \\vspace{-2mm}\n    \\caption{Learning curve of DPO compared with IPPO in 5-agent simple spread, 5-agent line control, and 5-agent circle control in MPE, where x-axis is environment steps.}\n    \\label{fig:mpe}\n    \\vspace{-2mm}\n\\end{figure}",
        "2211.03032_f641029bf4cdd92681e2e7c16b21a445": "MPE is a popular environment in cooperative MARL",
        "2211.03032_28014961a866a8cf9860705b542ce7c7": "MPE is a 2D environment and the objects in MPE environment are either agents or landmarks",
        "2211.03032_f5887093866db214c5809cf87f97cd16": "Landmark is a part of the environment, while agents can move in any direction",
        "2211.03032_e997fbf8d4e245410b225a3907b8cf67": "With the relation between agents and landmarks, we can design different tasks",
        "2211.03032_557fabc831fefd512f3bb2a47b0f9871": "We use the discrete action space version of MPE and the agents can accelerate or decelerate in the direction of x-axis or y-axis",
        "2211.03032_6407be9e0f56eae66428e90d960f9987": "We choose MPE for its partial observability",
        "2211.03032_87dc5a86ccc79c48bcc85e0bf20d2ad4": "We take",
        "2211.03032_e5e7cce7a4b22df5b88fdceae8564a04": "$d_{{target}} = 0.01$",
        "2211.03032_83476fbc3bd5f99bdaabbadd899f3d4c": "for all MPE tasks",
        "2211.03032_0b6dbf1bf9fb5c93829f007ee7e23f3c": "The MPE tasks we used for the experiments are simple spread, line control, and circle control which are originally used in",
        "2211.03032_55e75eebee71644699ce17673ff5ae78": "\\citet{transfer}",
        "2211.03032_63e5bc1038148c0ef71fa56598bc70a1": "In our experiments, we set the number of agents",
        "2211.03032_c3bf56131b9241bc4287f97b8ab2c631": "$N = 5$",
        "2211.03032_b56ecd7f3d749e393a972068beef03ec": "in all these three tasks",
        "2211.03032_5c7b7461ba471d6fac5e6b1b867ad462": "The empirical results are illustrated in Figure",
        "2211.03032_f02977a06bfab1cb02f394de116b7ec3": "We can find that although DPO may fall behind IPPO at the beginning of the training in some tasks, DPO learns a better policy in the end for all three tasks.",
        "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d": "Multi-Agent MuJoCo",
        "2211.03032_735b21ecfa0c64b8266722296c705abc": "\\begin{figure}[t]\n    \\vspace{-2mm}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{figures/mujoco_performance.pdf}\n    \\vspace{-5mm}\n    \\caption{Learning curve of DPO compared with IPPO in 3-agent Hopper, 3-agent HalfCheetah, 3-agent Walker2d,  4-agent Ant, and 17-agent Humanoid in multi-agent MuJoCo, where x-axis is environment steps. }\n    \\label{fig:mujoco}\n    \\vspace{-2mm}\n\\end{figure}",
        "2211.03032_6a1c3c09bb713d815c9133f99b3b717f": "Multi-agent MuJoCo is a robotic locomotion control environment for multi-agent settings, which is built upon single-agent MuJoCo",
        "2211.03032_22a6d9c7b805499fb3ed3e573775991a": "\\citep{todorov2012mujoco}",
        "2211.03032_46fb561d162022d818fc3bf0fb81be8b": "In multi-agent MuJoCo, each agent controls one part of a robot to carry out different tasks",
        "2211.03032_3cee2a7034e390c76861275cee1cfe6d": "We choose this environment for the reason of continuous state and action spaces",
        "2211.03032_dff0e673d301e76630e31a0a06ca2a8c": "We select 5 tasks for our experiments: 3-agent Hopper, 3-agent HalfCheetah, 3-agent Walker2d, 4-agent Ant and 17-agent Humanoid",
        "2211.03032_fbd4f0762b042bfa6ddd56bddcb69f4a": "$d_{{target}} = 0.001$",
        "2211.03032_9db171eb10c3103fcdd617a8551524ac": "for all multi-agent MuJoCo tasks",
        "2211.03032_189da5a587b2bcfb50a99d9ea9f495f5": "We can find that in all five tasks, DPO outperforms IPPO, though in 3-agent HalfCheetah DPO learns slower than IPPO at the beginning",
        "2211.03032_c3a6710a258fc922c11cc39b69cb74e3": "The results on multi-agent MuJoCo verify that DPO is also effective in facing continuous state and action spaces",
        "2211.03032_484d1ede1fa1c79dd00b51116e2f44a3": "Moreover, the better performance of DPO in the 17-agent Humanoid task could be evidence of the scalability of DPO.",
        "2211.03032_16f449313a0da4a7436138b759a3e7d6": "SMAC",
        "2211.03032_13dd73e8530507211ffd9eb8285b18cb": "SMAC is a partially observable and high-dimensional environment that has been used in many cooperative MARL studies",
        "2211.03032_cfe31d452e3f5b2517aca6e665b3300c": "We select five maps in SMAC, 2s3z, 8m, 3s5z, 27m",
        "2211.03032_f4842dcb685d490e2a43212b8072a6fe": "vs",
        "2211.03032_ac733609c7dc1d8d9ffce855a1bd3c33": "30m, and MMM2 for our experiments",
        "2211.03032_0e6feaa5266e6f32f12fcd21a60ddce4": "$d_{{target}} = 0.02$",
        "2211.03032_52767b78cb6531bae8f6042d9276ad3a": "for all SMAC tasks",
        "2211.03032_5f6b209f56ac9868a2c1016cf8200f4a": "The two super hard SMAC tasks (27m",
        "2211.03032_d42fb93c8246e62eb1362fb9d382da6d": "30m and MMM2) are too difficult for both DPO and IPPO to win, so we use episode reward as the metric to show their difference",
        "2211.03032_dba705f2372e3865140f9e14720245c8": "DPO performs better than IPPO in all five maps",
        "2211.03032_f6bfe797810540520a6bba4674d78434": "We need to argue that though we have controlled the network architectures of DPO and IPPO to be the same, in our experiments each agent has its individual parameters which increases the difficulty of training",
        "2211.03032_dfc51747057d37c312d33c2dde3a151f": "So our results in SMAC may be different from other works",
        "2211.03032_4d44d94f2e908780ec4b29b9be544557": "Although IPPO has been shown to perform well in SMAC",
        "2211.03032_bd3accc25517d69f02e91f0530d1841d": ", DPO can still outperform IPPO, which verifies the effectiveness of the practical algorithm of DPO in high-dimensional complex tasks and can also be evidence of our theoretical result",
        "2211.03032_c6f0a7518b6e2ecb29065fff80462eec": "Again, the better performance of DPO in 27m",
        "2211.03032_fd4157db664b42859852f6b4144d1243": "30m shows its good scalability in the task with many agents.",
        "2211.03032_d59e689431e928f9a7180e10fc6f403a": "\\begin{figure}[t]\n    \\centering\n    %\\vspace{-2mm}\n    \\includegraphics[width=1.0\\textwidth]{figures/SC2_performance.pdf}\n    \\vspace{-5mm}\n    \\caption{Learning curve of DPO compared with IPPO in 2s3z, 8m, 3s5z, 27m\\_vs\\_30m, and MMM2 in SMAC, where x-axis is environment steps.}\n    \\label{fig:smac}\n    \\vspace{-2mm}\n\\end{figure}",
        "2211.03032_6f8b794f3246b0c1e1780bb4d4d5dc53": "Conclusion",
        "2211.03032_60709e8ff79acb18d7ce31948e679d7a": "In this paper, we investigate fully decentralized learning in cooperative multi-agent reinforcement learning",
        "2211.03032_af7d068ac2494bbeed36f8c85a529be5": "We derive a novel decentralized lower bound for the joint policy improvement and we propose DPO, a fully decentralized actor-critic algorithm with convergence guarantee and monotonic improvement",
        "2211.03032_db1615214ee7d5b65889f44deab8f0f5": "Empirically, we test DPO compared with IPPO in a variety of environments including a cooperative stochastic game, MPE, multi-agent MuJoCo, and SMAC, covering both discrete and continuous action spaces, and fully and partially observable environments",
        "2211.03032_d950dc75296a6e7a6bfc83215e34856c": "The empirical results show the advantage of DPO over IPPO, which can be evidence for our theoretical results.",
        "2211.03032_aa66d95e83ccb800b2bfb705185b36ba": "Experimental Settings",
        "2211.03032_a61595da772f39aed1e1e0198a79a94c": "The three tasks are built on the origin MPE",
        "2211.03032_eba0fda1cd244642555224dcdb29b21e": "(MIT license) and are originally used in",
        "2211.03032_bb87024f3e950ec90a0f09926d56754f": "(MIT license)",
        "2211.03032_fd11616b5af85b4c79eb9db36628c030": "The objective in these three tasks are listed as follows:",
        "2211.03032_a8f65dd93445b90d34246cc201862695": "List (itemize)",
        "2211.03032_7d74f3b92b19da5e606d737d339a9679": "Item",
        "2211.03032_6357d3551190ec7e79371a8570121d3a": "There are",
        "2211.03032_eac502a200f6c48989c75fb0303d64af": "agents who need to occupy the locations of",
        "2211.03032_3cf4ed3986d53472a4db851c21a1a4fa": "landmarks.",
        "2211.03032_92b09bf4169f0d9c9ad311f7d947b87c": "agents who need to line up between 2 landmarks.",
        "2211.03032_cb93ee031b9af6c0f2411b1e40401791": "agents who need to form a circle around a landmark.",
        "2211.03032_f0a2bcbca6c273db3133be99807fc17b": "The reward in these tasks is the distance between all the agents and their target locations",
        "2211.03032_7f23c14e4efd3b2d70c3b3eff877231e": "We set the number of agents",
        "2211.03032_ba602ef36d499a51045a5e96776fd50f": "for these three tasks in our experiment.",
        "2211.03032_a7cc3a64b5b3565a39a1b6b85110640b": "Multi-agent MuJoCo",
        "2211.03032_41f0e0fb82ee5bdd8ebd50fddc9bc654": "(Apache-2.0 license) is a robotic locomotion task with continuous action space for multi-agent settings",
        "2211.03032_8975d2a34d73631d1524735ba5c2171a": "The robot could be divided into several parts and each part contains several joints",
        "2211.03032_f227ab4279f62637be997edb8cc59905": "Agents in this environment control a part of the robot which could be different varieties",
        "2211.03032_d87a719a8bb8591c178d9e8be8e40e1d": "So the type of the robot and the assignment of the joints decide a task",
        "2211.03032_09a91103eda7a1ab1337618315b5a916": "For example, the task `HalfCheetah-3",
        "2211.03032_bdbf342b57819773421273d508dba586": "$\\times$",
        "2211.03032_f63078530544733db3736c1fb71bf926": "2' means dividing the robot `HalfCheetah' into three parts for three agents and each part contains 2 joints",
        "2211.03032_a03d8b072851a1125958814463dff402": "The details about our experiment settings in multi-agent Mujoco are listed in Table",
        "2211.03032_12d150eb7565888fc91f98907e6367f8": "The configuration defines the number of agents and the joints of each agent",
        "2211.03032_5e1a7255ac13aca535bc45bb322591a6": "The `agent obsk' defines the number of nearest agents an agent can observe.",
        "2211.03032_4ff5626a9222a231dbd70e0f4e649c86": "\\begin{table}[h]\n    \\centering\n    \\caption{The task settings of multi-agent MuJoCo}\n    \\label{tab:mujoco}\n    %\\vspace{2mm}\n    \\begin{tabular}{c|c|c}\n    \\toprule\n         task & configuration & agent obsk \\\\\n         \\midrule\n         HalfCheetah & 3$\\times$2 & 2 \\\\\n         Hopper & 3$\\times$1 & 2 \\\\\n         Walker2d & 3$\\times$2 & 2 \\\\\n         Ant & 4$\\times$2 & 2 \\\\\n    \\bottomrule\n\\end{tabular}\n    \n\\end{table}",
        "2211.03032_37391eed5d0c918622ba5fdedccd0689": "Training Details",
        "2211.03032_fb4617e5f455a72e5dce67ad6db1dff5": "Our code is based on the open-source code",
        "2211.03032_f792bb6be4701c3c07e1fbd1fca6a643": "of MAPPO",
        "2211.03032_0aab3d2265dbc856db177ec80c8c6cf0": "We modify the code for individual parameters and ban the tricks used by MAPPO for SMAC",
        "2211.03032_4c32fb26637d55f3b805f9d4f6ea8293": "The network architectures and base hyperparameters of DPO and IPPO are the same for all the tasks in all the environments",
        "2211.03032_67996d2ca8b6d1ebd650053e600b1488": "We use 3-layer MLPs for the actor and the critic and use ReLU as non-linearities",
        "2211.03032_941b6e3c0d304dcf34a377e38c2d8179": "The number of the hidden units of the MLP is 128",
        "2211.03032_101b895ca0f1f6adf2b1f8464d78722e": "We train all the networks with an Adam optimizer",
        "2211.03032_404fd57eac9a58c20d7078af9ac5e294": "The learning rates of the actor and critic are both 5e-4",
        "2211.03032_33f0f4e346ca8d49a9b93cf276c7ae48": "The number of epochs for every batch of samples is 15 which is the recommended value in",
        "2211.03032_8d275f096604e0a511c9f71daffbd262": "\\citet{MAPPO}",
        "2211.03032_4077771f848cd406e1c49ade01d09cca": "For IPPO, the clip parameter is 0.2 which is the same as",
        "2211.03032_7da75571cdfc59cbc559f332757dd423": "\\citet{PPO}",
        "2211.03032_edee5f1bc3a447ce8fea79799b7045ad": "For DPO, the initial values of the coefficient",
        "2211.03032_f2c555e8a728c22182b4b8874891c323": "are 0.01",
        "2211.03032_920285c93c9aa9bdb393bb9dbfe9518a": "The value of",
        "2211.03032_a30dbf69970929f2c4dac046bd23ce97": "$d_{\\operatorname{target}}$",
        "2211.03032_c6e0b2a4c0af08a40a70dd704a429605": "is 0.1 for the cooperative stochastic game, 0.01 for MPE, 0.001 for multi-agent MuJoCo, and 0.02 for SMAC",
        "2211.03032_81a19be471d538ed7acbdd7d54585a2c": "The version of the game StarCraft2 in SMAC is 4.10 for our experiments in all the SMAC tasks",
        "2211.03032_9a0719ad2d097234fe51c8ccd9625274": "We set the episode length of all the multi-agent MuJoCo tasks as 1000 in all of our multi-agent MuJoCo experiments",
        "2211.03032_de43c46236be90c72e1f08bd061706a2": "We perform the whole experiment with a total of four NVIDIA A100 GPUs",
        "2211.03032_31a54bcd73442d5e25af8e06241eefce": "We have summarized the hyperparameters in Table",
        "2211.03032_5c623dda8641467b047c4058d15ab312": "\\begin{table}[h]\n    \\centering\n    \\caption{Hyperparameters for all the experiments}\n    \\label{tab:hyperparameter}\n    %\\vspace{2mm}\n    \\begin{tabular}{cc}\n    \\toprule\n         hyperparameter & value\\\\\n         \\midrule\n         MLP layers & 3 \\\\\n         hidden size & 128 \\\\\n         non-linear & ReLU \\\\\n         optimizer & Adam \\\\\n         actor\\_lr & 5e-4  \\\\\n         critic\\_lr & 5e-4 \\\\\n         numbers of epochs & 15 \\\\\n         initial $\\beta^i_1$ & 0.01 \\\\\n         initial $\\beta^i_2$ & 0.01 \\\\\n         $\\delta$ & 1.5  \\\\\n         $\\omega$ & 2   \\\\\n         $d_{\\on{target}}$ & different for environments as aforementioned  \\\\ \n         clip parameter for IPPO  & 0.2 \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}",
        "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd": "Additional Results",
        "2211.03032_daf171cb43eb2642707cbf198d399a45": "actually proposed two versions of PPO",
        "2211.03032_bdbfc5641607b7f2d6456d018c1df168": "The first version, which is also the most popular version, is with the clip trick",
        "2211.03032_36d15d9527f9edc025d9b69b79b14da6": "The second version is directly optimizing the penalty formula with adaptive coefficients and we refer to this algorithm as PPO-KL",
        "2211.03032_91523a185d5b3e1f0191817dc52f8510": "is actually extended from the first version, while the practical algorithm of DPO is similar to the second version",
        "2211.03032_f4e4b51667d414d83ec65a4190f024c8": "The main difference between DPO and PPO-KL is the term of the square root of the KL-divergence in the policy loss",
        "2211.03032_8f1a9ad8644a3197a456f4326021b1ee": "We modify IPPO by making each agent learn with PPO-KL to obtain IPPO-KL",
        "2211.03032_9854e2f244c14ffbafff2bb60bb348c5": "On the other hand, independent Q-learning (IQL)",
        "2211.03032_0a56bd4549d9882dd3d23ebeda1666df": "\\citep{IQL}",
        "2211.03032_3bf919545efae328a081ac1c6b75b360": "is a classic independent learning algorithm",
        "2211.03032_856054b335bea591f920ebc0bf1a0754": "IQL could obtain good performance in some tasks but it is not with any theoretical guarantee, to the best of our knowledge.",
        "2211.03032_437c6ae9f38c1a5d45a0f084a305d7bb": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.45\\textwidth]{figures/matrix_game_ablation.pdf}\n    \\caption{Learning curve of DPO compared with IPPO, IPPO-KL, IQL, and the global optimum in the cooperative stochastic game, where x-axis is environment steps. }\n    \\label{fig:matrix2}\n\\end{figure}",
        "2211.03032_e86a6c172dfa3ede533188f0a5786354": "For the completeness of our experiments, we test the performance of IPPO-KL and IQL in the cooperative stochastic game and the empirical result is illustrated in Figure",
        "2211.03032_3d1953a40e0e4c7dea47bfc79f591f15": "We find that the performance of IPPO-KL is close to DPO but a little bit lower and more unstable",
        "2211.03032_b6ad24bdd594a6169ee7191b0a4ce19f": "This can be explained as the policy loss of IPPO-KL is actually a biased approximation of DPO, which omits the square root term",
        "2211.03032_a11c1a95cb1989f3b62573f498246807": "The theoretical guarantee of DPO may also help IPPO-KL perform better than IPPO in this game, while the bias in the policy loss of IPPO-KL makes it perform worse than DPO",
        "2211.03032_83fc8be8e917532055f532ca5f49724f": "As for IQL, its performance is lower than IPPO though it converges faster",
        "2211.03032_c94c54155448d46512776b2413707bf5": "Since IQL performs worse than IPPO in such a didactic environment and IQL is a value-based algorithm and is out of the scope of our discussion, we do not take IQL as a baseline in other environments",
        "2211.03032_eafd28bf46e72f9413af396b32c8e7ef": "Moreover, we think the essential relations among IPPO, IPPO-KL, and DPO can be clearly witnessed from the empirical results in the cooperative stochastic game",
        "2211.03032_3a18bcf595663f61e9e81cb4b7cfc8b6": "For other tasks, we focus on the mainstream version of IPPO as in",
        "2211.03032_cb76ac3d889e8aa35e7166bd9cb0bb40": "\\citet{IPPO,MAPPO,benchmark}",
        "2211.03032_0f4e5d60e1e551ba10d2f88200ae10bb": "Besides our empirical results, we would like to share our views on the difference between DPO and IPPO and give some intuitive ideas",
        "2211.03032_540aec2809691cf23f822d87d05d7112": "KL regularization and ratio clipping are similar in the single-agent setting, but they are not supposed to be similar in multi-agent settings",
        "2211.03032_8c0ffd3f1ab0379b1d2f432f779c6958": "The `correct' ratio clipping in multi-agent setting according to the theory of PPO should clip over the joint policy ratio",
        "2211.03032_ab967eb4889e0b78b5946422a0b0823d": "$\\frac{ \\boldsymbol{\\pi_{\\operatorname{new}}} (\\boldsymbol{a}|s) }{\\boldsymbol{\\pi_{\\operatorname{old}} } (\\boldsymbol{a}|s)}$",
        "2211.03032_18c70615888a05c05003b480569dbad9": "IPPO just clips individual policy ratio",
        "2211.03032_9696142dd1bd1d15f2a9812681b0b69b": "$\\frac{\\pi^i_{\\operatorname{new}}(a_i|s)}{\\pi^i_{\\operatorname{old}}(a_i|s)}$",
        "2211.03032_1cba7b233d490225d29e84c47a3360d9": "which may not be enough to realize the `correct' ratio clipping",
        "2211.03032_e8716304ea53b38e224b5e21ebbaf0cf": "We could find more discussion about this in the CoPPO",
        "2211.03032_7e3f660480ab1640de8024c200b5a4d3": "paper",
        "2211.03032_bbd6d9b10753829889376b93d5bccb63": "So IPPO is not supposed to enjoy the theoretical results of DPO",
        "2211.03032_d8d28a708f82e0e6777934796c40b8e2": "We could rewrite the objective of IPPO for agent",
        "2211.03032_dc07d8944512f6d333d7601e33fa1e75": "with a similar formulation in HPO",
        "2211.03032_7cb65568ac6239a794c15ab88549a5ed": "\\citep{HPO}",
        "2211.03032_574bd3b0fd3ea20556b90498a2840b35": "as follows:",
        "2211.03032_d874cced19760bd4e55b837ff1e50d73": "\\begin{equation}$\n\\mathcal{L}^{i,\\operatorname{IPPO}}_{\\boldsymbol{\\pi}_{\\operatorname{old}}}(\\pi^i_{\\operatorname{new}}) = \\sum_{s} \\boldsymbol{\\rho}_{\\operatorname{old}}(s) \\sum_{a_i} \\pi^i_{\\operatorname{new}}(a_i|s) |A^i_{\\operatorname{old}}(s,a_i)|l\\left( \\operatorname{sign}(A^i_{\\operatorname{old}}(s,a_i)), u_i(s,a_i)- 1, \\epsilon \\right),\n$\\end{equation}",
        "2211.03032_4e88112f91cd1118c0737de657582625": "$l(y,x,\\epsilon) = \\operatorname{max}\\{0,\\epsilon - y\\times x\\}$",
        "2211.03032_5aba693c4e9e01af14743113e4c693fa": "is the hinge loss and",
        "2211.03032_4d889c0e55e2c441228a0196dd8ea2f8": "$u_i(s,a_i) = \\frac{\\pi^i_{\\operatorname{new}}(a_i|s)}{\\pi^i_{\\operatorname{old}}(a_i|s)}$",
        "2211.03032_062722c0dab879418814c0487e0346b6": "is the ratio",
        "2211.03032_76dd0afe8599c925f111fdcc1661e2b4": "If we follow the same idea as PPO, then IPPO is the `correct' ratio clipping version for the surrogate of DPO",
        "2211.03032_e805e7b0149866f8b0a0511fb62b7c29": "However, the effectiveness of this ratio clipping formulation in theory is still open in decentralized learning since there is not any convergence guarantee for IPPO, to the best of our knowledge",
        "2211.03032_9bf18f790e43b49bd1193fe2acbd2383": "Though the effectiveness of IPPO in theory is beyond the scope of our paper, we could provide an intuitive explanation for the fact that the performance of DPO can surpass IPPO from this formulation and the analysis in HPO",
        "2211.03032_969d031d2a0ed463d1645bfafbd22bc0": "In the proof of HPO, there is a critical assumption that the sign of the estimated advantage is the same as that of the true advantage (Assumption 4 in Section 2.3 in",
        "2211.03032_977c7c6f8e5ca8a9a8e920d15208934a": "\\citet{HPO}",
        "2211.03032_9371d7a2e3ae86a00aab4771e39d255d": ")",
        "2211.03032_a9badfbe15ff3a7c7391eb3666936804": "And HPO also shows that the sign of the advantage is more important than the value for this formulation of PPO-clip",
        "2211.03032_123413f147ceffad4c7307cdc97c7306": "In decentralized learning, both DPO and IPPO are facing the difficulty of learning the individual advantage function as there may be noise in the individual value function",
        "2211.03032_0344a5797e414bebdfc4d72f4717dc64": "However, the objective of DPO is continuous and the objective of IPPO is discrete for",
        "2211.03032_23c575eca6f9915e045576ae76e05df4": "$\\operatorname{sign}(A^i_{\\operatorname{old}}(s,a_i))$",
        "2211.03032_840c551f50b03b65dadc04a6e6b34290": "So the impact of the noise in the value function may be larger on IPPO than DPO.",
        "2211.03032_eed9474fd944d045ff056d20004acaa3": "Discussion",
        "2211.03032_8f63b33fcdf4ac70d20e39d8d0b3abdf": "In the paper, we derive a novel lower bound that can be naturally divided into independent surrogate",
        "2211.03032_d114a57fc5b445359332f97bbb538006": "By each agent optimizing this surrogate, the monotonic improvement of the joint policy can be guaranteed in fully decentralized settings",
        "2211.03032_8ab1b5b36aade9e62d64e368e74d08ef": "However, the practical algorithm of DPO takes the formula of",
        "2211.03032_700b90bb3f588eae57dd01cee81b082e": "with several approximations",
        "2211.03032_319a298979a4fac25a120fea4c12f99b": "How to solve the optimization of",
        "2211.03032_de24f2d2cb7973bed784530134d9148d": "more precisely is left as future work",
        "2211.03032_44b9d581316276c4ad4f29d0827a7e2f": "Moreover, we expect our work could provide some insights for future studies on fully decentralized multi-agent reinforcement learning, since current methods still have a gap from the optimum as shown in Figure"
    },
    "hierarchy": {
        "1": {
            "2211.03032_f91c96473601e4a43c22c64d3b9b87e6": "2211.03032_91712315ed3daaaaf7cfca5529a2da50",
            "2211.03032_104dc29c71f6583580826c6c186b9234": "2211.03032_91712315ed3daaaaf7cfca5529a2da50",
            "2211.03032_6bc5d8e5e737c4dd07872b2bcf3ae2dd": "2211.03032_91712315ed3daaaaf7cfca5529a2da50",
            "2211.03032_e353dbe42c8654f33588d4da0b517469": "2211.03032_91712315ed3daaaaf7cfca5529a2da50",
            "2211.03032_436833da1629f5577d16a32a22f0ea64": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_6dd0f57dc208839ba586ec8b85f2683f": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_e98e442015262c20b25163d13741f88a": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_5c1320498a2faf4a55cc23d5f89910b1": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_10c77ec2996d30c9dee297005dbdc54c": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_aa7b3cc6b73d98f655a077fb215a768a": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_b7c1e1eb7c6cad3fb4247cf502dcbfe3": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_24fd634675e30ec3c16c6593efb7340e": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_e0aa77551432ef65bd9153e6ce15b58a": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_5b04def1c6ab9153649f5146c7c046b7": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_0b79795d3efc95b9976c7c5b933afce2": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_e457bad44372dadc697b33597c695e93": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_c33dfb33eda12fa9a6063c9ea68b7163": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_d01189478ab4d0fca879e822897f2e00": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_6c5846e1683aa50629d0340c9b9be12a": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_bf40af7a2723b5d4b44e066938e178be": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_8c0795eeedc7eb3a81f17e172c327a19": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_c0e4b7502ffa7e281ac1cd1cda767884": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_1028f1399af1457283043081c0212cda": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_75e75e0204fad046c55965ce3d6c44f1": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_a61f5956d054fdee9875e61520dfd747": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_61ab0bf258da9555894fbf487d8fe704": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_a9229bdc2cab904ecc075b07bbb3d942": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_cbb0a0e3d88b455b0b8e6c3a032c3622": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_30f42dd9618548057535af7404dfede1": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_6643f152a3bda52316cc9b5afd1cdec0": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_e9019e42a1b58590ca39c2d1c916005a": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_7282cb56fdedf30e1aa5cfa30f8347f7": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_b3a33df7dd0367f6cf69aa34b3c0c9ec": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_4b99ad4f32b999c31dae0b336712e6d9": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_c2b92150d8194f87f92e795ce5a365f0": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_7edfda7b45b61b64c1e5a87587dba4bc": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_b87cb19a7aab05135766550cc41dcee2": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_8816d04b6ced742df833190d8663830f": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_0460728ab83ff8cba7b4171c06611c61": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_e96d86039d433a8482f53b1508e3eb8a": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_5da6a11149c2ff4eb721eb079a89cfc3": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_e93721a7b140f24f03d8c4d66a5c9a0c": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_bc596e09fc965d8a5b787569928a2b2c": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_b184c6cdaddff7c8ba6c1713db6264ef": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_73c43c1c0aacef650603a00e04125d99": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_84c40473414caf2ed4a7b1283e48bbf4": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_7be39d56a7e7a7c89759870337c9d624": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_30f614992a65bc3f1a15d6891c7d0e2e": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_aaaf519ed5223e75e3e922426acb64e6": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_dbca265ef9b44d705716a2f86cb5be05": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_e4a5ff7ffc4e06b32474d0cda6fc261f": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_747752d616466f0fe5ff3088f1b2fb11": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_281bef11f87b4453e64a3bd9d7278948": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_457ba6a0e4dc17629529d2efd9406640": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_558f1e6f61dbe543cbdbe10c37ae0247": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_2d254cad7f533d73a599ec41aa980e08": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_3b960e777acb3a6ef35113baa4f5a636": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_e9d4f1882b6beda93330cc45582ec07a": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_5cc3cec5164f5ec70820836be8465c5d": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_f5cf8856523c64fd5131060376754611": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_8b8bbd5728ee44be1050049581fd0a4f": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_42bdf8fe4241735368e2acfe9535be37": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_364b97ae7784b00b7fb411b34c42e4ca": "2211.03032_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03032_aed402c3112b4749a9a98a72cbe9093d": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_46bf109b62667b94f3818d1e03ee3650": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_c9f7f78998dcf1b5dc3c0383a01cf16c": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_73d14c9a206fa4f877b1c8e3eb4dc7c9": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_2956f006448fa5cb2a9070fb5831dfbf": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_1a447e08b7ee35ed200e5d052a2eaad7": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_67ee2f64abc93c9c7be2dc5a507ac88d": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_4f74e52b43944b6ce9b8ce8921c7b7a3": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_41b312409f529bb7f333c16f2e7144c7": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_8f7fc7c5fc1d7c540ef16a5927bee73f": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_bc0ebca39c67127903cc95bfeaa8fa10": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_0d93969c834e54ad8a5f302fb3bd6b4a": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_2597c418d416dd9ba720ef7ec88d8e6a": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_597b9c0193aa3cdb1ef9e22ee411f319": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_3caa2e055ad94a3110cd4be221a05399": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_906883b18812b49326ff2c95e67565fe": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_fc17636d92606d4173a8612a42ba8038": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_098b4db3a262d8205367b653b6f02c14": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_50374ba5b005f49a08a31122167ef4ab": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_73a8f940ccecf87f69e26fa5e016dcf5": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_ed023035184a5672cb3f5e9df5a471f8": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_bf3f6366a017b8c80d5b104a913b4f83": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_1901ee77c153adf18b9e7f8eb03c16a0": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_82120baba9940de761d4339cc8fa6193": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_60070b8ae3fe7777814f965dd5ce5ba9": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_62e01c193d50615fe355a59ca80b119e": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_edbe74f9ccdf1c4f92de622182cce209": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_6a8068d7f79ccbf1f79a2b71e612e4de": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_a4376d6be42e65b1f745a8db1aa1e316": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_879f5682dd8582771c39a9ee6b5503ee": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_d326bbf20615f6ed1dda7d4d0a27ab62": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_60bfb97b8487805e4b1e97d278c8ca92": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_81d8ff13e400696069dc3b4252d2f2f6": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_19703ccaab41c49f70e352399aa55788": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_e3a544caeb837629547b1e5415d10b7e": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_3e3defaaf3ba13b4b7c8c84315e0325d": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_b6060d33874e54910b586f3d6ee791e4": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_4f91f358b6b3b0f740c51fa5277aa1bc": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_4a1e563289fb75d466706d19becc5a23": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_aac34cdeedf9fc8aa86072c35c3b9cc0": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_0f0ecda859dc70c0e4e3675ad9d74beb": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_e434536bff9c3c2691aa2b1ccaad4ad0": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_db6f74f5d37661bbf4dba8ff7c0ee4f5": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_266f565b3e575cdd3d584553ab792479": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_82276e2d61988c42bc0aad2d8c04fa87": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_423ec130107b2b22fc20bf50c213e4ec": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_c6cc787923e0a5a231c405e63e50cd45": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_234bd832ef37c155663faebd4531d743": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_5d089676392d57300173872779938a6a": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_a04bcf47dfed729859dcabd3ae9769f6": "2211.03032_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03032_4c3880bb027f159e801041b1021e88e8": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_c547cc5d0ad23f5e1fd11403937a02fe": "2211.03032_4c3880bb027f159e801041b1021e88e8",
            "2211.03032_a20f40b2a90c6cd22a0d153a3c9bff28": "2211.03032_4c3880bb027f159e801041b1021e88e8",
            "2211.03032_1270f34fa86c0f69b21ba359237e878f": "2211.03032_4c3880bb027f159e801041b1021e88e8",
            "2211.03032_27847feb74ee52b9e7980e799440e0f1": "2211.03032_4c3880bb027f159e801041b1021e88e8",
            "2211.03032_7ad034002c27ec941420ccdc527cfb38": "2211.03032_4c3880bb027f159e801041b1021e88e8",
            "2211.03032_8a3546e2baef5f32e0ca0291171a65ec": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_2da515861492a86ca31b9ce498283068": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_e95cd28c6a3810e0365430c657c5f15d": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_5058f1af8388633f609cadb75a75dc9d": "2211.03032_eed9474fd944d045ff056d20004acaa3",
            "2211.03032_e257acd1ccbe7fcb654708f1a866bfe9": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_9b5ca9d71852a45f4390ac69dd1a3ed1": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_f9c4988898e7f532b9f826a75014ed3c": "2211.03032_7d74f3b92b19da5e606d737d339a9679",
            "2211.03032_0c0b555c01f9949b891241640f746fcd": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_2917c679d53b0db00e3ed70559018e51": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_059c6d69d675fa6682a8b46626599f2b": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_e8927b2b0b673f26d0224343f1641ede": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_8c4f3a79d451fb139b1b677b4188563b": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_a4dbffcf1a359a61c5bf4415b0ed107e": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_b3686abf2d8ca5e943b0a47e42a9ec0f": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_4ebf880807deff5796460f39aea46f80": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_097e7026d1853e85e5101af129c07fa7": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_77a3b857d53fb44e33b53e4c8b68351a": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_071a26c74380aa4b26ff47f336eed4df": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_0260498a254fc67961289fbeba078921": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_f81d8e4f4d5f00dee3c14f2671c027aa": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_bac4e2110b17ccf249cfcd768527d6cb": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_2d8cca33f0ee74986943da285a93a659": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_b1cbe6740a819f2d7fea85227b1fdc25": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_bb1b18782b54e8fbab53f4f2e6ad6eff": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_91aac9730317276af725abd8cef04ca9": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_ec5a92d657eba94fcd6d65f3ea32becf": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_479f598ce476926d00d024e05dd2e955": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_298a47385c99448a9ec046d955b04f59": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_e9d3178eefc7e430cc6608c9a30c6cff": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_7ce2f6919b55c0ab472dc0d56f92e7ce": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_d3ebc1984eb8c6cf0510de9a9a1fb86f": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_1570d9e11abaf2fd85128728500413ee": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_0816751dcf9279046c7e856d705e63bc": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_c802083e40cdda94f83e2570a7b57920": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_4c89860beda33c31f28e9ac9952dabe7": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_48a4861be2e8aca4e1edec1fe6c910c9": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_93437e830467a948ad72eae25822e4d6": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_840a24e03fb2455f822c106660677745": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_adc32a0d1e0220358ec233ea4df67aec": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_94c4a9d86e87771f4204890c81517454": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_7859e9f1b0faaea11a4a99c04a2738ab": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_720a34aaf7b1f313edab61fda6ab1c76": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_e89b71cbae7cfafdc3ce65ffe06082d1": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_787bd5bfdb8cb5b2f4367df05dca4b58": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_2f1a3e6c23505941bab9464c3a571800": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_6f9bad7347b91ceebebd3ad7e6f6f2d1": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_88ca02ce1276fd88685be8d3b3b82dc0": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_737129ba0e37846939bf10fe72c86d8e": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_72ef2b3c05cad09db2786841539d6f91": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_f0cf53c30791fded654d20b2b3b329f1": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_ff1ef9ddfb9118bdb4056111be81f998": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_dc9bc62a4014316c0f6b5b5506663f3c": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_4cb767b954f64a57670a5989b6333e2b": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_7d8aa5220ad25d1d0056dd7cf6397e85": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_33f1349eb4bcc83abbb57ce64217f50e": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_567904efe9e64d9faf3e41ef402cb568": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_bf534a7583f0a4288f3858f5c95d3e53": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_c0cb5f0fcf239ab3d9c1fcd31fff1efc": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_7f666b43383b2c8c31355ccea53e11d7": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_abeaa9e137ed6a908615f0c412f7c491": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_23b1f852095e157691b6003ca8973139": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_02d5df8138f662e6b332d60c629f1376": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_ae3e80f13e17a2ecd81eee0bedbb7d53": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_86738b8aee15c70d7bde6f1ba7dcc096": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_8a0e2b549d223aba55a866c38d1c9275": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_9b325b9e31e85137d1de765f43c0f8bc": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_fbc23d83994fb3f4e56f47869d7ee28c": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_a52acd5c6d5fa94b31bd4f1df870e872": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_5b5399dc6a69180549439f3f5add2513": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_13e97e9f4a55743a14e87ef5f48900d8": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_0de04c6e11b924c4ba933177d13d5db1": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_0682743df5648b7011b8585653ad7f54": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_ff8a18709286f805940ba085c0ae421b": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_5571c7375745e3db22eb1b6e4905e491": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_0f0cb3759d73f5a80429d8a8dd88ff51": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_37d2872de6a1962e5196cfa3dc6cb929": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_fc3be43301d8bc53651636a917c0732c": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_d6456e32e8a4ca1b706c1228d8c50f8a": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_e311856ed0cc3ef93a16ac615b684c96": "2211.03032_7ad034002c27ec941420ccdc527cfb38",
            "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44": "2211.03032_4c3880bb027f159e801041b1021e88e8",
            "2211.03032_c6a6fc28a5b911126979ce6c1c826a65": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_493c13e375dafc98bb8d24db985db8e7": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_648c8269b177a64d718ddd3a1517a3a5": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_d68fbdd5939055fd3357b5ee40f40629": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_7577abb1d43510396d998b03cdc055b1": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_ecc0e97074ec6d374b64ff40816aa9cd": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_be5d5d37542d75f93a87094459f76678": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_bc85e4fea70c0fadf7421cac0b051af2": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_3db35a4db6eafec06d0f6ed661a7e1ea": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_80ab841985ff20e34c7623bc6cd45ce6": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_aef485e36b6756f13ee47d3b3561dabb": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_54b9ea98fdc4a11de9a02675d69412c0": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_e8d339e34296980200497148da022b6c": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_9995f347aac1b769a3d393615cf66827": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_b0ddce9b08dd34deb66e48a4943e76d1": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_a430ecc501bb01d022a23a2c430eb435": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_42a3a2037c8227e9ff22f59093167dc3": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_707baf3a3f5cf3777fedcc61fa072dab": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_b457ace2bb5ce9b11aefa80a762e0cc6": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_799b61690872d6c887679c689395a146": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_2f5030b6b126a005be63d99add3b2147": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_3a20859426a38f45ed62200aa48b8b51": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_47fde7a0db9d9bafacdb190ce4747ee1": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_1eaed172f767d412aeaff3a01512cd1e": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_1fd92975660e16837445e2c119d6228f": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_11045e43379819e919d42e872934a0e2": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_396392595c5f68f79198d6a59d3e4c3b": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_b3e07525c64893f724160bd7046f85d7": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_d455ef8f60a853c27a0abd1e49898ea0": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_0f4ad8ed6f8abf5056019865b8b13f87": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_ddaa44618cf3a32ff9a3d1133d39848b": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_a07a7cf4670477e71b1908d9190d2c92": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_a5d431f3390b6c51d1a8a7d1851b3ef1": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_3882ff84d4940c1539ee37a9f1572cba": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_f43e37843fa011a80bf07f5f9e720276": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_6e66e7779d3c682e1b4fcc61624abcd9": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_f310232493ed7bad711ee30be8f2754c": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_052708a7a998a756ed6846968fd7587a": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_b30f95f29556d543afab549a1ffb487b": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_09d6630f15ebed1a68325026bef45812": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_ac3c78a4068b296a9859c1fdd475492b": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_8beb6b66ad3958a11fd4b712dfad31b6": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_5692c01e7909f7af05b5de728d25c01d": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_85b8127d62ecc66825d28fca5e384c9a": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_cf47ef61d4d199a8ecdaacc66cad39c1": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_da32c036a0afce487964d965a38bb373": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_519f207ec1d99060e9560686c626d1bb": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_3c13a810a0494ddfade56f97e21f2a9c": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_bd8b2e7146fe87a129042914b1ae7bc3": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_56dbb97fc91302bd8bcc7540436f8ea5": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_d051c9590d956f2c787ebdc0952a9f4d": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_dcc905487342811937e7467b4bf7faab": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_21844f38a52d3d39410cc0f724a9e3c1": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_23a58bf9274bedb19375e527a0744fa9": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_7361032a679e00bb41dc0cd2137918c8": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_73f815a0fbeb6c6311ab29c632bbfd3a": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_62e677139cea580ee831ffa38559ec4a": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_039d72fc2b5172230852f09464a059d1": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_f3345f1e510541414cdfa41d864facb4": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_a26d42ec640fe54a9cf4cb45846f41cf": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_c54cfe49fc0a021d13fcdc9bac86724b": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_bf58637d0875fe280bd9bfaef798acdc": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_508be29dacdf019c6fc430af4bbf6e2c": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_49df72e05810d70c8cc0833e46627cf2": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_ba7a442bd9dcf8d384851a79aa6c750e": "2211.03032_32b1cf7d8f5f96442df9a85eefdb7f44",
            "2211.03032_bdc8ac1715448e99be17fa11376a2028": "2211.03032_4c3880bb027f159e801041b1021e88e8",
            "2211.03032_4ed3f61a0ca12db5cebe9d7b1eff6956": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_b14608dbe80247d6ace70aadd234f634": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_800efb76c88ed758854b775e2a554cd3": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_9e6dd5dfdd88d81a8f81d882e21385d9": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_39abc0c3af755dc5d6ab8fd8bba6791d": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_f2e130e07814efa3754bc46e3f645583": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_1728f22cf0a81ce0cd5ff044aca30682": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_407bfeb24d1036f525398ffe999e4da8": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_222c05434b3c7aee774e3405b9e9fe7f": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_be9aa1aa618366bbe27962a10e57fc23": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_65dd75ca6665ccbba4d9984a25088718": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_88924c19517b83228a6cb51c3237718f": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_712282a57f058d1fffe7532b71ff6971": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_e746d83e9ad5ea3e662504c8e866d565": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_66d3ac9b51a69a057fb42ca8dece94e7": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_11c981f03cdb3c9a469bf36492b32004": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_e4699885c23ee52ca02ebd084babdbf9": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_27f9e4d8bb9f8a74fa1db7117eb465a0": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_03e2bd5112ed8d6f137440cdfd7851bf": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_aa67a51c7cf35b85cff62fb05db600c3": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_5da618e8e4b89c66fe86e32cdafde142": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_74cb4cdf246f86d424bc1ec6553c5cf5": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_b0fa5db4b1a32c8a67fc2e2884bff409": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_7e8a3d9412a480b23f68708d12a22f3f": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_88ed5dc6a0002c127ff717d2d7f83fe4": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_aac9135b48bda028b0c512263039cfbf": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_2fc2fdbac26fb3077dfcbe40c3c38ff0": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_b8ec7f4bae9dc26f466cc73ae7f151c4": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_f210eb4a396145ab3fb56a6e83e74e83": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_de1e07d2e73389bf0e7352f392dd2a16": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_516ac41c88be20e3926b09730b0baa17": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_309acd31e99e792891849124861665ed": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_c950bf568f73db830a6d5a28c99b691b": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_b36f83aff0f688e4d0e8dd2bb932aaaf": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_fc0c9a2e86ebbd82b35a22043099a00a": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_4c927f0757e6877db398f48f016f75b8": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_76d3f30508bb2c9b278434e11d92bc96": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_f970e2767d0cfe75876ea857f92e319b": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_d06db2b315675fe3e1c2cadf7af9d1a6": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_b7e4cd0e85ee785de1cd742bfbef0126": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_1062ec0701845810beddf3efba7c2987": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_26f947e26ecd1180afe973ca55981e31": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_ca4efa5ec17563c745100c851dd3cd01": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_47431ff9b236d30e23540f6f76240b21": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_1ff95a647834e801064b7182ad42818b": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_92740d4ac83595ad95a878c7b16609a3": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_263e1bfea9f1f1a006467f230dcb7b46": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_306f13a1f075f2012efb33d2e87ad7e5": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_70333d7078a9387f19c1c437e9e31034": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_4f4f4e395762a3af4575de74c019ebb5": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_e6251e20770f0f39e18a420376f2fe73": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_7002b245d017fe0595435a3c31cf57d7": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_e7cdf5013524d24e01bb7ecb5878d45f": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_91f2b4611b1bcdce55929d7fde5eedb0": "2211.03032_bdc8ac1715448e99be17fa11376a2028",
            "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a": "2211.03032_4c3880bb027f159e801041b1021e88e8",
            "2211.03032_1e01aec4d3ef2402d271dbb2eac34a94": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_c53bb3e8082bd04ca2adc30ce251f89c": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_d218b4d5df0580d68b6142a044223f26": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_4ab7d2f300bb22508b2bd96eaae093c2": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_fdca614a7186158fc68a97283dbe63d6": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_f58f0f04c7d8105c4764796e22178def": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_02c9e98e7d9ece3a1b2a41ce24c5d531": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_1ad63b19ba49298487a4eb18d5383643": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_d2319d40766dfd50fde1770ae96f721a": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_257937869572521fdd52716164057925": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_2c0cfefbd27f799756e0b064cddc4395": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_eb3131b4e1dafaf9559546feb9bf795f": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_204fe7ef0b41920bfe0382b396686c18": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_d8ba9e6953d566d49f4cbbcb404cac04": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_950f76abb7fa54caf7727fe22271766f": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_28caa999d0925744d7982ff1a818d131": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_112683a6ddc004027c84f6edfafe8d40": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_e8d6f4efc276fff3c7075c5e311acbf0": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_fc02139c3d2d67731eeaf2dd3019204f": "2211.03032_eed9474fd944d045ff056d20004acaa3",
            "2211.03032_786887572f6ef1c20f2d8177cb2f1639": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_1c8667bcfb412d6a3d70be4187e860a5": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_0be5e2220327ab8d0606f7dfe74366ba": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_d765248e8fdfa4eb91bc0b058eb1ef4e": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_484803f778f0161d1cb3838241da9662": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_ab1094af334394eaaaa5e115f3d9e664": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_66c793daa0ec252e59902bb55d244066": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_2c647b655fcc8e11c2a37b90a3e327d8": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_853ae90f0351324bd73ea615e6487517": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_66f384a0fe29fa3101361e5a0fb26f39": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_b28a6e240ff9c0b97f3777bc67aa39ff": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_57dffa21802cc3d2953e9ee63004de71": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_ba03f681d92b887e1a770eaedca4944e": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_d6bf618d614c766d86193ac0c83606fa": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_bcbb594c372346613098476f18b3ce6a": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_238cc7623559f1b9e0f91db4a90c5920": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_eae417273daa4f11bab257929fbffda6": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_8c834d30bbe046c99321eb71949382e5": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_35dba5d75538a9bbe0b4da4422759a0e": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_10927a67903afd162cc29267d434a2bb": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_fbb415fbf0c5be1c512757914e8a0b51": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_68a973ee70c70e9391459fe065fa3ff5": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_65ed4b231dcf18a70bae40e50d48c9c0": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_4d474f59f0c4d970629e4ad31088176e": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_89f2e0d2d24bcf44db73aab8fc03252c": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_3f09298d18a487faee631ddace1f0f9d": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_ac5fc6bb75d77e4b6a16e3a7946496b3": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_0788a6922bd5f9f130e7ed8980193bab": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_8e5460b266eeffcbc8765576aa540418": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_32c30587cb6b2fdf0201b2b0ff79501a": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_c6d01501684c50a5bdcb008b0bce0a09": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_06933067aafd48425d67bcb01bba5cb6": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_93156b3a9ec50122adcae63cff800cf4": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_736a063b5369f7c08dc04e85c1d4b681": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_440a3bbc91716125aef7de35fc6e49ae": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_6972391bc97af6f3885acaa10baaffd6": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_51545d33aa90316992b9c5465ae33cd6": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_097a4fa3b9a94508dfdd052653a8fd6f": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_a4b03bd947079c8ac3162311168c526a": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_d86066b0c6a7fd9664b769ae4b504d36": "2211.03032_4afa80e77a07f7488ce4d1bdd8c4977a",
            "2211.03032_4829262cecb9828817b33e0f9c907f91": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_932237b32f854a053c74b89207c2e273": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_f12c623007c875d4da97685b58baa3f5": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_9ac44d5ed136ebce1155daa80595fc3d": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_6162253436a471e3b480d07068d9888b": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_d55a7659447463fbaf7df5e350687f07": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_36c8d160c8583e7cbe1fa8f78785403c": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_7adeccc27721de7ab090900939496a27": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_f0bd75186528cacd658f796b8337e4b0": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_b8f78ecb847bc34d3dd6f4920501fc97": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_cd373ce12ecadec4ba0da47b8bae334a": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_aabd501209bb621299f9f1927ed1921a": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_514c5300d34b272ea166e89722a02e5b": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_af474c73deb478d2b9b5b5dd1a571d67": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_ee7b90e5b6a3772c5d33cded5f76d8d3": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_e8759403a34e23fc508eb2846b808fba": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_1986749af4b913cc785600804f134daa": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_51c007088db7726413d7e7ae82b19728": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_4d35e757b9552f721c6542ca766bb961": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_4ee39a2789d074c02f4512443e59f2b8": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_b51c5209b8eede99f6f98f931c23c111": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_4a92b6ae9d4ba8dc2fdb1225a2d91ef9": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_d70c37f51740a5b03e27653e6f7b961e": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_17f780e8668ab4a3e660c3bff1a2105e": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_bf0a330db46f07ada5eb6f73aaebb46b": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_133c4f787e87d708e777c2a01faa6cb6": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_18ebe717682630c89581d20a195374f2": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_23b868f41949ab56c790ffd6e86403e0": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_dab5902ebe39ffe2330e5265fe1c3306": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_251d4ffd0f3c57a1b350236d5d5880ea": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_2c7a7be8c53bf19e7d6949c12da5e366": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_079fe1dcb4caa12f29b14392d1354093": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_15ef3b23ef739e47090fa0825bf9d390": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_2cae3bbfffb6ab2858054ba28bfcba80": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_a582908ccc70f2cf7b3f6734058e8887": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_5bc4f929ef8135a715d3fa584e68d08a": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_e89907fe33fb5becd26ff5b133e0d1a4": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_30e0f9d4bbe45fb1cc62805b28482f86": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_eb96aab2148055ca3e2c533126477c5a": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_5df1581a5a30e698f431cd34d4ada81d": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_9e1cba125ff8b3813163826030bfb190": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_49ac13a76fc79da81f1606aba5176d2e": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_47fb50b771f731d801c88793368347ec": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_ebb202de48bb08f2f6cf62952dcb9bb0": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_10b37bc4b45c36e89abc5f8602729d92": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_172c7f5a9649309400f1095cb978bd1f": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_019adb4198575d6f89d9ccdd88bac2c5": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_8eea3e9f5e983760681f0d6cd8fe20cc": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_59df6fe776a939108b9f2a186ab3d988": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_24b0757de192585f7a74fb5f1bac2748": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_14c62b788db94b0b338de9d7688c8014": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_10e3868c1bf97282778005cadb2b2a6f": "2211.03032_cd373ce12ecadec4ba0da47b8bae334a",
            "2211.03032_93eda4801aa7098be93d51b921fc4c72": "2211.03032_aa66d95e83ccb800b2bfb705185b36ba",
            "2211.03032_dba755585539b1a852ed6d3b64bba072": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_f641029bf4cdd92681e2e7c16b21a445": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_28014961a866a8cf9860705b542ce7c7": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_f5887093866db214c5809cf87f97cd16": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_e997fbf8d4e245410b225a3907b8cf67": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_557fabc831fefd512f3bb2a47b0f9871": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_6407be9e0f56eae66428e90d960f9987": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_87dc5a86ccc79c48bcc85e0bf20d2ad4": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_e5e7cce7a4b22df5b88fdceae8564a04": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_83476fbc3bd5f99bdaabbadd899f3d4c": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_0b6dbf1bf9fb5c93829f007ee7e23f3c": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_55e75eebee71644699ce17673ff5ae78": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_63e5bc1038148c0ef71fa56598bc70a1": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_c3bf56131b9241bc4287f97b8ab2c631": "2211.03032_7d74f3b92b19da5e606d737d339a9679",
            "2211.03032_b56ecd7f3d749e393a972068beef03ec": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_5c7b7461ba471d6fac5e6b1b867ad462": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_f02977a06bfab1cb02f394de116b7ec3": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d": "2211.03032_aa66d95e83ccb800b2bfb705185b36ba",
            "2211.03032_735b21ecfa0c64b8266722296c705abc": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_6a1c3c09bb713d815c9133f99b3b717f": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_22a6d9c7b805499fb3ed3e573775991a": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_46fb561d162022d818fc3bf0fb81be8b": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_3cee2a7034e390c76861275cee1cfe6d": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_dff0e673d301e76630e31a0a06ca2a8c": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_fbd4f0762b042bfa6ddd56bddcb69f4a": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_9db171eb10c3103fcdd617a8551524ac": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_189da5a587b2bcfb50a99d9ea9f495f5": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_c3a6710a258fc922c11cc39b69cb74e3": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_484d1ede1fa1c79dd00b51116e2f44a3": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_16f449313a0da4a7436138b759a3e7d6": "2211.03032_4829262cecb9828817b33e0f9c907f91",
            "2211.03032_13dd73e8530507211ffd9eb8285b18cb": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_cfe31d452e3f5b2517aca6e665b3300c": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_f4842dcb685d490e2a43212b8072a6fe": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_ac733609c7dc1d8d9ffce855a1bd3c33": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_0e6feaa5266e6f32f12fcd21a60ddce4": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_52767b78cb6531bae8f6042d9276ad3a": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_5f6b209f56ac9868a2c1016cf8200f4a": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_d42fb93c8246e62eb1362fb9d382da6d": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_dba705f2372e3865140f9e14720245c8": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_f6bfe797810540520a6bba4674d78434": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_dfc51747057d37c312d33c2dde3a151f": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_4d44d94f2e908780ec4b29b9be544557": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_bd3accc25517d69f02e91f0530d1841d": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_c6f0a7518b6e2ecb29065fff80462eec": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_fd4157db664b42859852f6b4144d1243": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_d59e689431e928f9a7180e10fc6f403a": "2211.03032_16f449313a0da4a7436138b759a3e7d6",
            "2211.03032_6f8b794f3246b0c1e1780bb4d4d5dc53": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_60709e8ff79acb18d7ce31948e679d7a": "2211.03032_6f8b794f3246b0c1e1780bb4d4d5dc53",
            "2211.03032_af7d068ac2494bbeed36f8c85a529be5": "2211.03032_6f8b794f3246b0c1e1780bb4d4d5dc53",
            "2211.03032_db1615214ee7d5b65889f44deab8f0f5": "2211.03032_6f8b794f3246b0c1e1780bb4d4d5dc53",
            "2211.03032_d950dc75296a6e7a6bfc83215e34856c": "2211.03032_6f8b794f3246b0c1e1780bb4d4d5dc53",
            "2211.03032_aa66d95e83ccb800b2bfb705185b36ba": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_a61595da772f39aed1e1e0198a79a94c": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_eba0fda1cd244642555224dcdb29b21e": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_bb87024f3e950ec90a0f09926d56754f": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_fd11616b5af85b4c79eb9db36628c030": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_a8f65dd93445b90d34246cc201862695": "2211.03032_93eda4801aa7098be93d51b921fc4c72",
            "2211.03032_7d74f3b92b19da5e606d737d339a9679": "2211.03032_a8f65dd93445b90d34246cc201862695",
            "2211.03032_6357d3551190ec7e79371a8570121d3a": "2211.03032_7d74f3b92b19da5e606d737d339a9679",
            "2211.03032_eac502a200f6c48989c75fb0303d64af": "2211.03032_7d74f3b92b19da5e606d737d339a9679",
            "2211.03032_3cf4ed3986d53472a4db851c21a1a4fa": "2211.03032_7d74f3b92b19da5e606d737d339a9679",
            "2211.03032_92b09bf4169f0d9c9ad311f7d947b87c": "2211.03032_7d74f3b92b19da5e606d737d339a9679",
            "2211.03032_cb93ee031b9af6c0f2411b1e40401791": "2211.03032_7d74f3b92b19da5e606d737d339a9679",
            "2211.03032_f0a2bcbca6c273db3133be99807fc17b": "2211.03032_7d74f3b92b19da5e606d737d339a9679",
            "2211.03032_7f23c14e4efd3b2d70c3b3eff877231e": "2211.03032_7d74f3b92b19da5e606d737d339a9679",
            "2211.03032_ba602ef36d499a51045a5e96776fd50f": "2211.03032_7d74f3b92b19da5e606d737d339a9679",
            "2211.03032_a7cc3a64b5b3565a39a1b6b85110640b": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_41f0e0fb82ee5bdd8ebd50fddc9bc654": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_8975d2a34d73631d1524735ba5c2171a": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_f227ab4279f62637be997edb8cc59905": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_d87a719a8bb8591c178d9e8be8e40e1d": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_09a91103eda7a1ab1337618315b5a916": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_bdbf342b57819773421273d508dba586": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_f63078530544733db3736c1fb71bf926": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_a03d8b072851a1125958814463dff402": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_12d150eb7565888fc91f98907e6367f8": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_5e1a7255ac13aca535bc45bb322591a6": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_4ff5626a9222a231dbd70e0f4e649c86": "2211.03032_a2b71e35ce6ab7c7fe3841ed5ee1219d",
            "2211.03032_37391eed5d0c918622ba5fdedccd0689": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_fb4617e5f455a72e5dce67ad6db1dff5": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_f792bb6be4701c3c07e1fbd1fca6a643": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_0aab3d2265dbc856db177ec80c8c6cf0": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_4c32fb26637d55f3b805f9d4f6ea8293": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_67996d2ca8b6d1ebd650053e600b1488": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_941b6e3c0d304dcf34a377e38c2d8179": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_101b895ca0f1f6adf2b1f8464d78722e": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_404fd57eac9a58c20d7078af9ac5e294": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_33f0f4e346ca8d49a9b93cf276c7ae48": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_8d275f096604e0a511c9f71daffbd262": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_4077771f848cd406e1c49ade01d09cca": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_7da75571cdfc59cbc559f332757dd423": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_edee5f1bc3a447ce8fea79799b7045ad": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_f2c555e8a728c22182b4b8874891c323": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_920285c93c9aa9bdb393bb9dbfe9518a": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_a30dbf69970929f2c4dac046bd23ce97": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_c6e0b2a4c0af08a40a70dd704a429605": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_81a19be471d538ed7acbdd7d54585a2c": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_9a0719ad2d097234fe51c8ccd9625274": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_de43c46236be90c72e1f08bd061706a2": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_31a54bcd73442d5e25af8e06241eefce": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_5c623dda8641467b047c4058d15ab312": "2211.03032_37391eed5d0c918622ba5fdedccd0689",
            "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_daf171cb43eb2642707cbf198d399a45": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_bdbfc5641607b7f2d6456d018c1df168": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_36d15d9527f9edc025d9b69b79b14da6": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_91523a185d5b3e1f0191817dc52f8510": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_f4e4b51667d414d83ec65a4190f024c8": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_8f1a9ad8644a3197a456f4326021b1ee": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_9854e2f244c14ffbafff2bb60bb348c5": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_0a56bd4549d9882dd3d23ebeda1666df": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_3bf919545efae328a081ac1c6b75b360": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_856054b335bea591f920ebc0bf1a0754": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_437c6ae9f38c1a5d45a0f084a305d7bb": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_e86a6c172dfa3ede533188f0a5786354": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_3d1953a40e0e4c7dea47bfc79f591f15": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_b6ad24bdd594a6169ee7191b0a4ce19f": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_a11c1a95cb1989f3b62573f498246807": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_83fc8be8e917532055f532ca5f49724f": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_c94c54155448d46512776b2413707bf5": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_eafd28bf46e72f9413af396b32c8e7ef": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_3a18bcf595663f61e9e81cb4b7cfc8b6": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_cb76ac3d889e8aa35e7166bd9cb0bb40": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_0f4e5d60e1e551ba10d2f88200ae10bb": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_540aec2809691cf23f822d87d05d7112": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_8c0ffd3f1ab0379b1d2f432f779c6958": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_ab967eb4889e0b78b5946422a0b0823d": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_18c70615888a05c05003b480569dbad9": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_9696142dd1bd1d15f2a9812681b0b69b": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_1cba7b233d490225d29e84c47a3360d9": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_e8716304ea53b38e224b5e21ebbaf0cf": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_7e3f660480ab1640de8024c200b5a4d3": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_bbd6d9b10753829889376b93d5bccb63": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_d8d28a708f82e0e6777934796c40b8e2": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_dc07d8944512f6d333d7601e33fa1e75": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_7cb65568ac6239a794c15ab88549a5ed": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_574bd3b0fd3ea20556b90498a2840b35": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_d874cced19760bd4e55b837ff1e50d73": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_4e88112f91cd1118c0737de657582625": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_5aba693c4e9e01af14743113e4c693fa": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_4d889c0e55e2c441228a0196dd8ea2f8": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_062722c0dab879418814c0487e0346b6": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_76dd0afe8599c925f111fdcc1661e2b4": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_e805e7b0149866f8b0a0511fb62b7c29": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_9bf18f790e43b49bd1193fe2acbd2383": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_969d031d2a0ed463d1645bfafbd22bc0": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_977c7c6f8e5ca8a9a8e920d15208934a": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_9371d7a2e3ae86a00aab4771e39d255d": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_a9badfbe15ff3a7c7391eb3666936804": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_123413f147ceffad4c7307cdc97c7306": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_0344a5797e414bebdfc4d72f4717dc64": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_23c575eca6f9915e045576ae76e05df4": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_840c551f50b03b65dadc04a6e6b34290": "2211.03032_a5a514e81319abdbfa6dd6d50d7667bd",
            "2211.03032_eed9474fd944d045ff056d20004acaa3": "2211.03032_e353dbe42c8654f33588d4da0b517469",
            "2211.03032_8f63b33fcdf4ac70d20e39d8d0b3abdf": "2211.03032_eed9474fd944d045ff056d20004acaa3",
            "2211.03032_d114a57fc5b445359332f97bbb538006": "2211.03032_eed9474fd944d045ff056d20004acaa3",
            "2211.03032_8ab1b5b36aade9e62d64e368e74d08ef": "2211.03032_eed9474fd944d045ff056d20004acaa3",
            "2211.03032_700b90bb3f588eae57dd01cee81b082e": "2211.03032_eed9474fd944d045ff056d20004acaa3",
            "2211.03032_319a298979a4fac25a120fea4c12f99b": "2211.03032_eed9474fd944d045ff056d20004acaa3",
            "2211.03032_de24f2d2cb7973bed784530134d9148d": "2211.03032_eed9474fd944d045ff056d20004acaa3",
            "2211.03032_44b9d581316276c4ad4f29d0827a7e2f": "2211.03032_eed9474fd944d045ff056d20004acaa3"
        }
    }
}