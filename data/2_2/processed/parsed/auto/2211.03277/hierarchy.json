{
    "elements": {
        "2211.03277_490b44de255309ca7ed4dc8452242cdc": "Document Root 2211.03277",
        "2211.03277_0fbd1776e1ad22c59a7080d35c7fd4db": "]",
        "2211.03277_fa9497f5acccafcc3e6019657bdc5eb1": "[ ]",
        "2211.03277_e353dbe42c8654f33588d4da0b517469": "Abstract",
        "2211.03277_354179e73b445da2ab4865bddb7e3064": "Multi-hop reading comprehension requires not only the ability to reason over raw text but also the ability to combine multiple evidence",
        "2211.03277_92fb24fb473201aa26785337581244a9": "We propose a novel learning approach that helps language models better understand difficult multi-hop questions and perform",
        "2211.03277_a790c55306be09f3db229870079d7b49": "complex, compositional",
        "2211.03277_4db0062a5ec022e76d5f853ebfba540a": "reasoning",
        "2211.03277_7e03aa597bf3ac958dcf3fe0201c8b6e": "Our model first learns to decompose each multi-hop question into several sub-questions by a trainable",
        "2211.03277_6d9446714100d05956aacc9d613b151b": "Instead of answering these sub-questions, we directly concatenate them with the original question and context, and leverage a",
        "2211.03277_3fd30d9511b352b88cc88cd759922111": "model to predict the answer in a sequence-to-sequence manner",
        "2211.03277_331865bbcf608253d61861c2558e594d": "By using the same language model for these two components, our best",
        "2211.03277_6666cd76f96956469e7be39d750cc7d9": "/",
        "2211.03277_d77a8d1e6b8e41705dc9c101032b3543": "t5-base variants outperform the baseline by 7.2/6.1 absolute F1 points on a hard subset of DROP dataset.",
        "2211.03277_0b79795d3efc95b9976c7c5b933afce2": "Introduction",
        "2211.03277_4e23c9db5ab30b89fcb6cf7607798372": "Multi-hop Reading Comprehension (RC) is a challenging problem that requires compositional, symbolic and arithmetic reasoning capabilities",
        "2211.03277_3606d017d7f899424e7db801a6191ea0": "Facing a difficult question, humans tend to first decompose it into several sub-questions whose answers can be more easily identified",
        "2211.03277_17cbc133d4006eaec4cff84c0633bec8": "The final answer to the overall question can then be concluded from the aggregation of all sub-questions' answers",
        "2211.03277_5a152f37483b7602c7fc0e0ed6c14a88": "For instance, for the question in Table",
        "2211.03277_2c070cecfcf704dca62f0ead65178b97": ", we can naturally decompose it into three simpler sub-questions (1)",
        "2211.03277_2ca9b494eaabd6576eabe1ad5f64fcc0": "return the touchdown yards",
        "2211.03277_5d376baac2cf8691282536d6101682e4": ", (2)",
        "2211.03277_5a055992d4bfbb52dcad3e3d6f877248": "return the fewest of",
        "2211.03277_8c26e6655aab9ae92411073abc805918": "$\\#1$",
        "2211.03277_e3d2a08e4ee3994a46c249a414e241d2": ", and (3)",
        "2211.03277_d63748ff6bf643591b03d8366b40d494": "return who caught",
        "2211.03277_fb19066311f84c5909400aa479652a43": "$\\#2$",
        "2211.03277_426c2fd37aae968c8cb5669fa8395534": "The tokens",
        "2211.03277_be5d5d37542d75f93a87094459f76678": "and",
        "2211.03277_d72d9b95cca301977fc25307f3146fb6": "are the answers to the first and second sub-questions respectively",
        "2211.03277_153af6246ce72c12fd4d96b4eb6b1391": "Finally, the player with the touchdown of",
        "2211.03277_8632fe25ee7d71fc9d7f51135d4adc15": "is returned as the final answer.",
        "2211.03277_ec2d8f61724eb750982f5fef138dcf12": "\\begin{table}[!htb]\n    \\centering\n    \\begin{tabular}{ll}\n         \\hline\n         $\\mathbf{C}$\n         & First, Detroit's Calvin Johnson caught \\\\\n         & a 1-yard pass in the third quarter. The \\\\\n         & game's final points came when Mike \\\\\n         & Williams of Tampa Bay caught a 5-yard.  \\\\ \\hline\n         $\\mathbf{Q}$ & Who caught the touchdown for the \\\\\n         & fewest yards? \\\\ \\hline\n         $\\mathbf{Q_1}$ & return the touchdown yards \\\\\n         $\\mathbf{Q_2}$ & return the fewest of $\\#1$ \\\\ \n         $\\mathbf{Q_3}$ & return who caught $\\#2$ \\\\ \\hline\n         $\\mathbf{A}$ & Calvin Johnson \\\\ \\hline\n    \\end{tabular}\n    \\caption{An example for reading comprehension. $\\mathbf{C}$ is the context, $\\mathbf{Q}$ is a hard multi-hop question, and $\\mathbf{Q_1}$, $\\mathbf{Q_2}$, $\\mathbf{Q_3}$ are sub-questions annotated in \\textsc{Break} dataset. $\\mathbf{A}$ is the answer to $\\mathbf{Q}$.}\n    \\label{tab:example}\n\\end{table}",
        "2211.03277_5fa29b323228df32a080c0c9f64cc020": "State-of-the-art RC techniques employ large-scale pre-trained language models (LMs) such as GPT-3",
        "2211.03277_2fd3f02df168ce74e0c42220eba77437": "\\cite{lms-gpt-3}",
        "2211.03277_e610023717ffae6e4b7186bee02afb24": "for their superior representation and reasoning capablities",
        "2211.03277_062044ae774eddf61ed78cd45c18ba09": "Chain of thought prompting",
        "2211.03277_27187839470353501e07515a725580f0": "\\cite{chain-of-thought}",
        "2211.03277_8a9980a78cbb8fa1c1e0b20d0e655d50": "elicits strong reasoning capability of LMs by providing intermediate reasoning steps",
        "2211.03277_87b7c5f22f4c4d665b5f887feb6fb4e6": "Least-to-most prompting",
        "2211.03277_4d4e581eba47590f5e2950347590eeaa": "\\cite{least-to-most}",
        "2211.03277_63cbcf9ee5e83822600339262db2ef02": "further shows the feasibility of conducting decomposition and multi-hop reasoning, which happen on the decoder side together with the answer prediction procedure",
        "2211.03277_b720c53bb56b3d2d93a3547fb6235f69": "However, compared to supervised learning models, both of these methods rely on extremely large LMs with tens and hundreds of",
        "2211.03277_b37c1eac6cc92f5fa84aefecc62d3813": "of parameters to achieve competitive performance, thus requiring expensive hardware and incurring a large computation footprint",
        "2211.03277_3763f81c57d774edfee50cf4e9371463": "Despite significant research on RC",
        "2211.03277_9e93599fe6d6d916e73e30e3c44c5992": "\\cite{DROP:2019,Unsupervised:2020}",
        "2211.03277_b332c13fbf053911ad31ecb1e07d65b0": ", those questions that require strong compositional generalisability and numerical reasoning abilities are still challenging to even the state-of-the-art models",
        "2211.03277_8bdd9748663560c4dc11774f8c4f4eb8": "\\cite{ran-etal-2019-numnet,chen-etal-2020-qdgat,ChenLYZSL20-nerd,chain-of-thought,least-to-most}",
        "2211.03277_8cfa886b576ba40148388c46c5163738": "While decomposition is a natural approach to tackle this problem, the lack of sufficient ground-truth sub-questions limits our ability to train RC models based on large LMs",
        "2211.03277_e31c163a56dd08b41c7155242c8c2b1c": "In this paper, we propose a novel low-budget (only 1",
        "2211.03277_c2533e4ac80a068b94e26a7a56169c9b": "parameters of GPT-3) learning approach to improve LMs' performance on hard multi-hop RC such as the Break subset of DROP",
        "2211.03277_a11e1fc5e7122e1ecbc1ca9a9026d01e": "\\cite{DROP:2019}",
        "2211.03277_16536230b6fe77121e0552fe87063928": "Our model consists of two main modules: (1) an encoder-decoder LM as a",
        "2211.03277_dade5bf00b51aa093338f89be7b0341a": "and (2) another encoder-decoder LM as the",
        "2211.03277_20f35e630daf44dbfa4c3f68f5399d8c": "model",
        "2211.03277_740d574afebe5c39d5f88bdc5ecec95b": "First, we train the question decomposer to decompose a difficult multi-hop question to sub-questions from a limited amount of annotated data",
        "2211.03277_eb705fc8915dd3a0bbd3d4d0f0ba4768": "Next, instead of solving these sub-questions, we train the reading comprehension model to predict the final answer by directly concatenating the sub-questions with the original question",
        "2211.03277_ffcaa08788fcff4d52ef8ee5866e2c61": "We further propose a",
        "2211.03277_90e8d33410cfcc27bb93a73f581356b7": "model that utilizes the same LM for both question decomposition and reading comprehension with task-specific prompts",
        "2211.03277_d445f14930b1842ec6066e88a576d5a9": "With 9",
        "2211.03277_bdbf342b57819773421273d508dba586": "$\\times$",
        "2211.03277_fb4f24d435ddf2f62b633fef2e496f8a": "weakly supervised data, we design a Hard EM-style algorithm to iteratively optimise the",
        "2211.03277_4dfff143a9bbdc7ffcf330098e746594": "To prove the effectiveness of our approach, we leverage two different types of LMs: T5",
        "2211.03277_39da08aefb6c5b353a5d2f7ade63fdc9": "\\cite{lms-t5}",
        "2211.03277_60ec48a9de4502c2ae8597592f1a459b": "and Bart",
        "2211.03277_b7012658d546dac77191f51366d119f6": "\\cite{lms-bart}",
        "2211.03277_763d71cec4ca4b48263dd96ccd8670cd": "to build baselines and our variants",
        "2211.03277_e9c2e9817160c909e80b9c2755a7aca5": "The experimental results show that without changing the model structure, our proposed variant outperforms the end-to-end baseline",
        "2211.03277_fa9e324d2a768f020d0ac3b9ca396f19": "By adding ground-truth sub-questions, gains on the F1 metric are 1.7 and 0.7 using T5 and Bart separately",
        "2211.03277_c92fcf4fc05bc64d930c473f816946fe": "Introducing weakly supervised training data can help improve the performance of both",
        "2211.03277_4053af40a01fa4dc1e066e15d4ddb62d": "variants by at least 4.4 point on F1",
        "2211.03277_9e01be3faccece6993d973d9fd316e6c": "And our method beats the state-of-the-art model GPT-3 by a large margin.",
        "2211.03277_aed402c3112b4749a9a98a72cbe9093d": "Related Work",
        "2211.03277_a1b2ebec076d74acf17f8bf9d16051bf": "mentioned in this paper requires more than one reasoning or inference step to answer a question",
        "2211.03277_9254b6cbb24ffa59705da97de5d259ed": "For example, multi-hop RC in DROP",
        "2211.03277_2645b77065690f34e5a6094cad44d75e": "requires numerical reasoning such as addition, subtraction",
        "2211.03277_3f816854c8b74a9f949d011d66b308c0": "To address this problem,",
        "2211.03277_40ba26c829643c67a67defade2c9a6fe": "\\citeauthor{DROP:2019}",
        "2211.03277_68935f2b6d31e69bd854ca41b2fb4161": "proposed a number-aware model NAQANet that can deal with such questions for which the answer cannot be directly extracted",
        "2211.03277_370f45c4be9a1e0a6b2a11f949fc518c": "NumNet",
        "2211.03277_fc365e7088a9fababc51d7f2e29a0f56": "\\cite{ran-etal-2019-numnet}",
        "2211.03277_d994b495319c2f6507de031a8bde2124": "leveraged Graph Neural Network to design a number-aware deep learning model",
        "2211.03277_5da4dec80f667cc3dc392721f9a38f4a": "QDGAT",
        "2211.03277_4d45662826887ef8d629ee279a149888": "\\cite{chen-etal-2020-qdgat}",
        "2211.03277_974b3a011509b5fab9dd01850895d256": "distinguished number types more precisely by adding the connection with entities and obtained better performance",
        "2211.03277_dabdb387ade6712237057f681b38bd12": "Nerd",
        "2211.03277_ad81003d74216bc7c8205c31c731c133": "\\cite{ChenLYZSL20-nerd}",
        "2211.03277_81145250c1587b18c7f5fa5ed13c3898": "searched possible programs exhaustively based on the ground-truth and employed these programs as weak supervision to train the whole model.",
        "2211.03277_af5c78e037da034b5ebe870431cb1bdc": "is the approach that given a complex question, break it into several simple sub-questions",
        "2211.03277_a284ed43500931b47bc0be2711b92aa1": "These sub-questions can also be Question Decomposition Meaning Representation (QDMR)",
        "2211.03277_1f26871b799a23ad27dd7d6f0512d51f": "\\cite{break-it-down}",
        "2211.03277_3e5c439a3b891b2c68bfffffc9d7c419": "for complex questions",
        "2211.03277_06c441a02fc9ad73b112636c3d119ae3": "Many researchers",
        "2211.03277_0b0168372b1107e08191464d342611be": "\\cite{Unsupervised:2020,decomposition-break-perturb-build}",
        "2211.03277_c0c46671dc82af76a54179411b0c522b": "have been trying to solve the problem by incorporating decomposition procedures",
        "2211.03277_cabbc139b7f74771c47e3eb24d94c92b": "For example,",
        "2211.03277_38e7aa84d7becfbb3c53b8cc46422561": "\\citet{Unsupervised:2020}",
        "2211.03277_0c0c62c7000d281aa1669956e1f352d9": "propose a model that can break hard questions into easier sub-questions",
        "2211.03277_890c19e4f13928b2e02f223a9a12928d": "Then, simple QA systems provide answers of these sub-questions for downstream complex QA systems to produce the final answer corresponding to the original complex question.",
        "2211.03277_926e5261560d68333da76763b3075d3c": "\\citet{fu-etal-2021}",
        "2211.03277_7bf75c24569111c5a3bc3743a78f61ff": "propose a three-stage framework called Relation Extractor Reader and Comparator (RERC), based on complex question decomposition",
        "2211.03277_ba64c112420a9c3b8a6bd9a8d13827b6": "Different from these approaches, we aim to improve the multi-hop capability of current encoder-decoder models without dedicated pre-designing the architecture.",
        "2211.03277_e8daa762dccf1737afecf2b72608ca52": "like BERT",
        "2211.03277_d4f1365b2f67dcab3fda60f3c584248b": "\\cite{BERT:2019}",
        "2211.03277_62cb29028114e01e2724639864a66f22": ", GPT families",
        "2211.03277_967fafcf79adf5698f15f769e9bb1608": "\\cite{lms-gpt,lms-gpt-2,lms-gpt-3}",
        "2211.03277_44647ca88446fa7c38d188f99110fb02": ", BART",
        "2211.03277_ec0cda02ca353c14e976243b7f60263b": "and T5",
        "2211.03277_c602d659eec31b308ae3dab3329405bb": "are demonstrated to be effective on many NLP tasks, base on either fine-tuning or few-shot learning",
        "2211.03277_9f65da3b5330392b93f966c434f9a517": "\\cite{chain-of-thought,least-to-most}",
        "2211.03277_3d2c12477f5e039088efb6823785a182": ", even zero-shot learning",
        "2211.03277_20d1bedbde8aea7d708611a616ea94ef": "However, LMs suffer a lot from solving multi-hop questions and logic reasoning and numerical reasoning problems",
        "2211.03277_3cd60f63a89a88cf918b11ba046d66a0": "Although some research",
        "2211.03277_33d7ef2ae13444aa7318a00f18ac52a7": "\\cite{scratchpads,chain-of-thought}",
        "2211.03277_4e78b2854ee0b81cf76870709ce8c5c4": "has conducted experiments on either simple or synthetic datasets and shown the effectiveness,",
        "2211.03277_7287209614a444089c74230a021a2edd": "\\citet{impact-2022}",
        "2211.03277_ece208255886b96ccac6dbeecaefac0a": "indicates that the model reasoning is not robust enough",
        "2211.03277_fd616f527e088bc105cd0bfa096c1e99": "Recently,",
        "2211.03277_82d76e2d394407c4d28f0cfa1e7348ef": "\\citet{lmc-2022}",
        "2211.03277_5047da1f4d39332bd0c0b8c2185a8948": "points out that prompted models can be regarded as employing a unified framework a",
        "2211.03277_d27562ca5f264db64273a0cefc112b40": "From the perspective view of probabilistic programming, several recent literature",
        "2211.03277_03d55413478a34f019479b51a8cdb3eb": "are formalized",
        "2211.03277_e719001b8cba9cab6db2cd76a1e5bd4e": "In this paper, we also treat our whole process as a probabilistic model that is consistent to",
        "2211.03277_5058f1af8388633f609cadb75a75dc9d": ".",
        "2211.03277_8b921702e775c208929b7f66c2c5a543": "\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{Figures/structure.png}\n    \\caption{Our model structure on complex reading comprehension through question decomposition. Step 1: Question Decomposer generates a sequence of sub-questions; Step 2: RC component predicts the answer based on question, sub-questions and the given context. The context of this given example is truncated.}\n    \\label{fig:decomposer-rc}\n\\end{figure*}",
        "2211.03277_e30144b8015f54bb5a0c3ff49f90d5e2": "Complex Question Answering Through Decomposition",
        "2211.03277_871cc50ed718556264aee5c950917f9d": "Our focus in this work is on complex questions requiring multi-hop reasoning.",
        "2211.03277_0e0605e05027ff61c5f438f834de6c60": "As such, our approach consists of the following two steps:",
        "2211.03277_7bade40785d1f5c1dbec8c0414eaac9a": "List (enumerate)",
        "2211.03277_7d74f3b92b19da5e606d737d339a9679": "Item",
        "2211.03277_6ab498a42f8abbd6b1fbde93a6a2f1bb": "The complex question is decomposed to a sequence of sub-questions",
        "2211.03277_3027dc21efd035029a03d483a52e8a5e": "The decomposition of the question is performed by the",
        "2211.03277_814bcb2ffb3d22873648b579e0de1002": "component of our system.",
        "2211.03277_bb82a310ae95928b5a984266fad2a301": "The model produces the answer to the complex question leveraging the generated subquestions to provide guidance to the reasoning of the system",
        "2211.03277_e8ce6136e54a4f8604eaddeb5e8ede10": "This is performed by the",
        "2211.03277_1f8236f4f2978edcd761ee6240f17e5b": "component.",
        "2211.03277_49cd0f8bd10fdab7bb7125c00c26298e": "We use LMs such as T5 and Bart as the backbone",
        "2211.03277_d55669822f1a8cf72ec1911e462a54eb": "for",
        "2211.03277_e1d04488606109e7eb46c88634221d01": "question decomposer and the reading comprehension (Figure",
        "2211.03277_ce96ce64475d88003b81a20710c10f5e": ").",
        "2211.03277_109f4b406d0ef8c53067ec3bc52d99df": "We present several variants of our model, depending whether the models for the above two steps are either separate or unified using multitask learning.",
        "2211.03277_1ce1250f7c46bd51d4b1a4f171207877": "As we have the ground truth question decomposition for only a subset of the training data, we treat the missing decompositions as latent variables.",
        "2211.03277_738f68f084c693dbe477b437f69e56b8": "We then propose an algorithm based on Hard-EM",
        "2211.03277_14361ef74ad99165a277ae9a6f03d333": "\\cite{Neal98aview}",
        "2211.03277_af20ca244e854d5418df5f72ca6404db": "for learning the model",
        "2211.03277_5dbc9b0eb68253d08f12a82bb4a8cf92": "The rest of this section provides more details.",
        "2211.03277_14a0b4a24f0683353ffbe235d7657bdc": "Given a question",
        "2211.03277_1afcdb0f704394b16fe85fb40c45ca7a": "$Q$",
        "2211.03277_4805129561d8bf2d2467a98c38b46e52": "and a",
        "2211.03277_9b325b9e31e85137d1de765f43c0f8bc": "$C$",
        "2211.03277_dbc49cb1b86107e1ebbbf54ec055671f": "context pair, our system generates the answer",
        "2211.03277_53d147e7f3fe6e47ee05b88b166bd3f6": "$A$",
        "2211.03277_737cce81ab1a956e04e0519de6b06a9d": "according to the following probabilistic model:",
        "2211.03277_567904efe9e64d9faf3e41ef402cb568": "where",
        "2211.03277_5b51bd2e6f329245d425b8002d7cf942": "$Z$",
        "2211.03277_6167b443811d3b2f05a171bbaa0e2925": "denotes the unobserved decomposition of the question,",
        "2211.03277_01a443d8fda57ca72fe242b04f8ac018": "$P^{\\text{dc}}_{\\text{LM}}(Z|Q)$",
        "2211.03277_93233862c95c966b4e70eb4fd4c83aae": "denotes the question decomposer (operationalised based on one specific LM), and",
        "2211.03277_56d682243a37f25f0eaf4a3df692d67c": "$P^{\\text{rc}}_{\\text{LM}}(A|Q,C,Z)$",
        "2211.03277_1e50c0aa0150691147567e1e9778041a": "denotes the reading comprehension component.",
        "2211.03277_c97f2a2e4b83cffee06344fcc99257f2": "In principle, the",
        "2211.03277_928f23e5bf20d3f25a8b32b2b921fd4e": "$P^{\\text{dc}}_{\\text{LM}}$",
        "2211.03277_874f4b69feb2bc09f2c16611c7391949": "$P^{\\text{rc}}_{\\text{LM}}$",
        "2211.03277_f7d571365f6be112ed2ecacc4655eed6": "components can be constructed using different models, so the parameters",
        "2211.03277_27e556cf3caa0673ac49a8f0de3c73ca": "$\\theta$",
        "2211.03277_59ad1af4938fb25daa10db31aca60ed4": "of the whole probabilistic model consists of those for these two models",
        "2211.03277_7f7c57d1e10dd7e1a2c4722e144693d1": "This is denoted by the",
        "2211.03277_aa7ac8cd5e0a6993a177746279cc00d2": "variant",
        "2211.03277_a46a1179ee7614e48a50ab4ef9d744af": "We further investigate using the same LM for both the question decomposer and reading comprehension component, which we denote by the",
        "2211.03277_346fa62042aaa5d59b0bb3aeabed2d4e": "variant in the experiments",
        "2211.03277_cf7966dfbd476267d060494bd5350a51": "In this case, the probabilistic model parameter",
        "2211.03277_398b7824ff08eb158ce9441b8a2f9c35": "consists of only one set of parameters corresponding to the underlying model.",
        "2211.03277_936f36542e7dabef7e8b22fc3d5e4663": "To obtain high-quality sub-questions, we first train a question decomposer",
        "2211.03277_304c2ce6eef3ed8f38528cafb6e1f503": "to break down difficult multi-hop questions, i.e., the first term in Equation",
        "2211.03277_5a4a7c670eb7c71eb2d93f8bff7bd972": "It learns the decomposition based on",
        "2211.03277_11737adab02e11da609e035f0727a9c2": "We only use the specific partition on the DROP dataset",
        "2211.03277_c293728f7bf4095f5f37797877292a79": "and treat QDMRs as sub-questions.",
        "2211.03277_467011b757110b378f3ef994359bc545": "These sub-questions only cover around 10",
        "2211.03277_ecfdb1eb758442453e853438d7a688f9": "QA pairs in DROP",
        "2211.03277_cddc8d84dee6cc0220c804deb336277c": "Therefore, we need to predict decompositions for the rest of the dataset",
        "2211.03277_279045037a58a12dc389269d7a60cd39": "More details will be revealed in Section",
        "2211.03277_d1831323eb5bdda49c845920a30d06c3": "Formally, given a multi-hop question",
        "2211.03277_c36dd91721de6906ab5e329a175f7d93": ", the question decomposer",
        "2211.03277_945ff45452989de6ffa52538341da2a0": "generates the sub-questions",
        "2211.03277_385a56a84b0cd4762156e38e37d38ed8": "$Z := \\{Q^1, Q^2, ..., Q^s\\}$",
        "2211.03277_65542fbb11208c7eb97b02ae155637c6": "Intuitively, We treat it as a seq2seq learning problem:",
        "2211.03277_83f833c7956779ef1e9d9130567ee171": "our input to the encoder is",
        "2211.03277_cd17637111d023af22ebfcea33c0714a": "$\\texttt{<PARSE>} Q $",
        "2211.03277_a29f17caa3e965b909d1aef183a202e4": ", where",
        "2211.03277_44964e4efc0a19fcd09719f13abd2abc": "is a special token",
        "2211.03277_936e281e8b887884061749f37a567f0e": "The decoder then generates tokens of the sub-questions in auto-regressive way",
        "2211.03277_79b39406d0b67c84e1518f28ba04ee15": "$\\texttt{<subQ>} Q^1 \\texttt{<subQ>} Q^2 \\texttt{<subQ>} \\ldots Q^s$",
        "2211.03277_1e3677bf29d4ef5f476050352a98c8cb": "To further obtain answers based on the question and generated sub-questions, the reading comprehension component",
        "2211.03277_9ab54051d35513e485e015a16361b59b": "generates the answer",
        "2211.03277_e538edc73cfdd22f4d2a8da94609ef2e": ", i.e., the second term in Equation",
        "2211.03277_653b1f70c93da21813eb8d603913dfcc": "In stead of directly answering all the sub-questions given by the trained question decomposer, we train our RC component to predict the final answer in a sequence-to-sequence way",
        "2211.03277_ecfc6d6f115694f31bef67f0a152313a": "Formally, given a multi-hop complex question",
        "2211.03277_ed80b3f5a128d95be8d9c6914f89a5a8": "and the corresponding sub-questions",
        "2211.03277_359372669d9774dad2b3ad8a7c415dd0": "$Z := \\{Q1, Q2, ..., Q^s\\}$",
        "2211.03277_913b0624ddc3133517206fe20651d3c5": "generated by a trained question decomposer, our input to the RC encoder is",
        "2211.03277_280f67dbab96421dffa3df3932ae59a6": "are special tokens",
        "2211.03277_15c99bd29b73bde135f225ca2f8539a7": "In other words, we concatenate the multi-hop question and all the sub-questions, together with the context as the input to our RC component",
        "2211.03277_63bc38d43247a8278ba778cf92d3cbb0": "The decoder then generates the tokens of the answer autoregressively.",
        "2211.03277_8c834d30bbe046c99321eb71949382e5": "[t]",
        "2211.03277_35dba5d75538a9bbe0b4da4422759a0e": "[1]",
        "2211.03277_9ee51e0ca6ab7ca19aa873091f25c7ac": "an initial pre-trained LM",
        "2211.03277_fb97d38bcc19230b0acd442e17db879c": "$M$",
        "2211.03277_6f705e84571784cf0b5cb60adbf1e4d0": "; the full reading comprehesion dataset",
        "2211.03277_27be2bc45eaa115101d1e90b3de3c381": "$\\mathcal{D}_1$",
        "2211.03277_fc9477219af263c2ae7464816be5ad6c": "; the subset with sub-question annotations",
        "2211.03277_f432b3358e5ab24106bca3b445316f5c": "$\\mathcal{D}_2$",
        "2211.03277_e41273f43af504714d85465294f1f369": "Train",
        "2211.03277_ed2b5c0139cec8ad2873829dc1117d50": "on",
        "2211.03277_7c9b95fd64ce596113f8fda8bd217c70": "to get",
        "2211.03277_94988e03aba2574e2fae88ea64970d8e": "$M^0$",
        "2211.03277_b273a613b8596eea4fd3d7723565399e": "For all",
        "2211.03277_0d55bd2cc9e3c5ba2fde111282c75e00": "$\\mathcal{D}=\\mathcal{D}_1\\setminus\\mathcal{D}_2$",
        "2211.03277_cd7edc81293debefa5f08a55b544c45c": "employ",
        "2211.03277_ffb6c2624e0d394982f92f837c0848e9": "$M^{iter-1}$",
        "2211.03277_889b4d8857821ce1c2730544a95b8c89": "to predict sub-questions and get",
        "2211.03277_5983bd6d614c8704c716498193702283": "$\\mathcal{D}^{iter}$",
        "2211.03277_99077aa1753fc4f769c402fb096914b8": "Retrain",
        "2211.03277_a51de93e24f260b06f192f60f82e0f01": "on all examples:",
        "2211.03277_72dfb105963f1538c5b739180fd6f712": "$\\mathcal{D}_2\\cup\\mathcal{D}^{iter}$",
        "2211.03277_72d1d84a02ecfa2713790572f9e067ac": ", get updated model",
        "2211.03277_c721fb953e4dadf3d341cba6aa949c3d": "$M^{iter}$",
        "2211.03277_ca2979b8912b819d17ada24347019490": "The training objective of our model is",
        "2211.03277_46de177b7fd7a541d82db4dab35f5493": "\\begin{equation}\n\\begin{split}\n    \\mathcal{L} = &\\sum_{(Q,C,A) \\in \\mathcal{D}_1 \\setminus \\mathcal{D}_2} \\log P_{\\theta}(A|Q,C) + \\\\ &\\sum_{(Q,C,Z^*,A)\\in \\mathcal{D}_2} \\log P_{\\theta}(A,Z^*|Q,C),\n\\end{split}\n\\end{equation}",
        "2211.03277_fb41a074abb51810d0b13686f648f0f4": "$Z^*$",
        "2211.03277_568461bda901703834f814067ec235df": "denotes the ground truth decomposition available only for the subset of the training data referred to by",
        "2211.03277_ea2caa3f8b0ddca884cd9e99e6439bbc": "The first term of the training objective involves enumerating over all possible latent decompositions, which is computationally intractable",
        "2211.03277_79bcb8399b99736c6b912501be2631cf": "Therefore, we resort to Hard-EM for learning the parameters of our model (see Algorithm",
        "2211.03277_97122726ee38968dab28c3d5c36b7310": ") for the unified variant.",
        "2211.03277_667bd26f2f4bd03f5c598f131d48e4a8": "We found taking 10 iterations of the Hard-EM algorithm to be mostly sufficient for learning model parameters in our experiments",
        "2211.03277_156abf7624df9cb729c2aa1d007e51d2": "For the separate variant, i.e., using two different LMs for",
        "2211.03277_e470dc2f15b00c7d00653d9b7b759599": ", we train the question decomposer on",
        "2211.03277_e8ca4a714dcb64ed1ce2a9ec7e92e46b": ", and then train the reading comprehension component on",
        "2211.03277_756fc496c55892dc274ded063136d1c3": "as well as",
        "2211.03277_52a9ba72befc2013c3374f28e6b96aca": "$\\mathcal{D}_1 \\setminus \\mathcal{D}_2$",
        "2211.03277_fd6cf67915016f3bdc64ae3d06628573": "augmented with the generated decomposition",
        "2211.03277_71511cdbdae5e009d9b78158d6fe36bf": "We also compare with training the reading comprehension component on",
        "2211.03277_dbe53b89bec489fac49f06bdaae8484f": "only, in the experiments",
        "2211.03277_fee6e7dd3cc4f79cbab8483ad9c1eb0f": "During inference time, we first generate the question decomposition",
        "2211.03277_e94e7074a31ec487efddda54efbe579a": "$\\tilde{Z}$",
        "2211.03277_440a3bbc91716125aef7de35fc6e49ae": "according to",
        "2211.03277_2f967d2f0af96fd6e208952860ce6181": ", and then use",
        "2211.03277_13b5bfe96f3e2fe411c9f66f4a582adf": "in",
        "2211.03277_e6d82c5aaf61cdcfbef0e27a6afcdb0d": "to generate the answer.",
        "2211.03277_4829262cecb9828817b33e0f9c907f91": "Experiments",
        "2211.03277_a092d919b1ba8b6e4e14c75eb90b56a9": "\\begin{table}[t]%{0.49\\textwidth}\n    \\centering\n    \\resizebox{.48\\textwidth}{!}{\n    \\begin{tabular}{c|c||c|c|c|c|c} \\hline\n       \\multicolumn{2}{c||}{Proportions} & 1\\% & 5\\% & 10\\% & 50\\% & 100\\% \\\\ \\hline \\hline\n       \\multicolumn{2}{c||}{BLEU} & 39.08 & 44.76 & 47.74 & 50.12 & \\textbf{54.69}  \\\\\n       \\multicolumn{2}{c||}{Rouge-1} & 77.49 & 81.75 & 83.12 & 84.76 & \\textbf{85.67} \\\\\n       \\multicolumn{2}{c||}{Rouge-2} & 57.00 & 62.83 & 64.97 & 66.94 & \\textbf{68.61} \\\\\n       \\multicolumn{2}{c||}{RougeL} & 67.78 & 72.65 & 74.37 & 76.55 & \\textbf{77.43} \\\\ \\hline\n       \\multirow{2}{*}{RC} & EM & 26.0 & 26.5 & 27.0 & \\textbf{27.8} & 27.2 \\\\ \n       & F1 & 31.3 & 31.3 & 31.6 & \\textbf{32.2} & 32.0 \\\\ \\hline\n    \\end{tabular}}\n    \\caption{Experimental results of the Bart based question decomposer: (1) Row 1-4 show intrinsic metrics for the question decomposition by using different proportions of training instances. (2) Row 5-6 show extrinsic metrics of the RC model by using the corresponding decomposer generated sub-questions.}\n    \\label{tab:bart decomposer}\n\\end{table}",
        "2211.03277_197da3bc28a85d4149927dff350f3be7": "\\begin{table*}[t]\n    \\centering\n    \\begin{tabular}{c|c||c|c|c|c|c|c|c|c|c|c} \\hline\n       \\multicolumn{2}{c||}{LMs} & \\multicolumn{5}{c|}{t5-small} & \\multicolumn{5}{c}{t5-base} \\\\ \\hline\n       \\multicolumn{2}{c||}{Proportions} & 1\\%~\\footnote{Trained as a question decomposer, the t5-small model and cannot be further evaluated on downstream RC task, as the generated sub-questions are poor-quality.} & 5\\% & 10\\% & 50\\% & 100\\% & 1\\% & 5\\% & 10\\% & 50\\% & 100\\% \\\\ \\hline \\hline\n       \\multicolumn{2}{c||}{BLEU} & 11.21 & 44.50 & 50.44 & 60.15 & \\underline{62.73} & 34.86 & 52.98 & 57.3 & 62.18 & \\textbf{64.40}  \\\\\n       \\multicolumn{2}{c||}{Rouge-1} & 43.00 & 76.93 & 81.53 & 87.25 & \\underline{88.59} & 70.66 & 84.16 & 85.77 & 88.50 & \\textbf{89.27} \\\\\n       \\multicolumn{2}{c||}{Rouge-2} & 28.18 & 59.13 & 64.33 & 72.60 & \\underline{74.76} & 50.57 & 66.86 & 70.24 & 74.24 & \\textbf{75.72} \\\\\n       \\multicolumn{2}{c||}{RougeL} & 39.22 & 68.92 & 73.66 & 79.99 & \\underline{81.57} & 62.10 & 75.49 & 78.07 & 81.20 & \\textbf{82.53} \\\\ \\hline\n       \\multirow{2}{*}{RC} & EM & - & 28.9 & \\underline{29.9} & 29.0 & 29.0 & 33.7 & 34.3 & 34.3 & 34.6 & \\textbf{34.8} \\\\ \n       & F1 & - & 33.0 & \\underline{34.0} & 33.2 & 33.1 & 37.8 & 38.4 & 38.5 & 38.5 & \\textbf{38.6} \\\\ \\hline\n    \\end{tabular}\n    \\caption{Results of the T5 based question decomposer (left-half: t5-small, right-half: t5-base): (1) Row 1-4 show all intrinsic metrics to evaluate the question decomposer by using different proportions of training instances. (2) Row 5-6 show extrinsic metrics of the RC component by using the corresponding decomposer generated sub-questions.}\n    \\label{tab:t5 decomposer}\n\\end{table*}",
        "2211.03277_239658e016e3d5d06ae719d280a79fec": "Dataset",
        "2211.03277_2adfde7231396173b4ca0e0fa317cc59": "We consistently use the same notations as in Algorithm",
        "2211.03277_a8f65dd93445b90d34246cc201862695": "List (itemize)",
        "2211.03277_594c2513c07dcda9a32d85917dab0d5d": ": the",
        "2211.03277_3c4d09e4ef50b370ae0efacdb43ec2dd": "dataset",
        "2211.03277_0cc011acdc7dfb28285371b288377955": "that contains 77,400/9,536 question (",
        "2211.03277_06726ea77ceaca2c5441e947eba9b8c1": ") answer (",
        "2211.03277_aaee9fd23e4d8743eb0830a00e19bf48": ") training/testing pairs for the reading comprehension component.",
        "2211.03277_d217c410b42679ed278381092ca023fb": "that contains 7,683/1,268 question (",
        "2211.03277_9d09980920f228e18b6618ac92aa326f": ") decomposition (",
        "2211.03277_4ebc412fb49fb0a4476f4c8ffbd9348e": ") training/testing pairs for the question decomposer",
        "2211.03277_48e5432b5e652716d56d0876897673c7": "$\\mathcal{D} = \\mathcal{D}_1 \\setminus \\mathcal{D}_2$",
        "2211.03277_f5b577a1129ac6cee674d0d8ec71b71a": ": the difference set between",
        "2211.03277_6987a7a4f2e86758ad44f8867482e431": "that contains only question answer pairs without ground-truth decomposition.",
        "2211.03277_853ae90f0351324bd73ea615e6487517": ":",
        "2211.03277_eaf85f2b753a4c7585def4cc7ecade43": "$\\mathcal{D}$",
        "2211.03277_fa1fd542ccc1fbe5b4bc7ed429d5e4c5": "with decomposition (",
        "2211.03277_12554475d176528d3b89fe9adcecbe44": ") generated by the trained question decomposer.",
        "2211.03277_a1fef22aae603e478eb464736cd3bae6": "Note that every question (",
        "2211.03277_fab1712162b0b7924960496af4d43252": ") is associated with a specific context (",
        "2211.03277_9371d7a2e3ae86a00aab4771e39d255d": ")",
        "2211.03277_5075d30b23fbe3450d319993a8b3bd8d": "With all question decomposition labelled,",
        "2211.03277_73652ac487be7a267d99767a22545b03": "is actually a subset of",
        "2211.03277_2117f84682321b24e72aacaf4bfc0431": "and is more challenging.",
        "2211.03277_a82437031a32b1da38218b1ac6f9ca67": "Backbone and Evaluation Metric",
        "2211.03277_3d87a6da5dfbed5a618bb52ba3988584": "There are three LMs of different types and sizes we employ as backbones in this paper: (1) t5-small (60M parameters), (2) t5-base (220M parameters), (3) bart-base (140M parameters)",
        "2211.03277_426c6cf041e0bbe2a6c5db178f658725": "We also employ GPT-3 (175B parameters) as it is the current state-of-the-art language model in a various of natural language processing tasks.",
        "2211.03277_5b61fd210b48229b9dd8ba1d532b9c51": "We train and evaluate our question decomposer using",
        "2211.03277_615c0cec8dfba380af481f263da01a88": ", which was proposed to better understand difficult multi-hop questions",
        "2211.03277_9cce0993608ce2519c03733928550a0a": "We report BLEU",
        "2211.03277_fde585210a12ce464e2568f6f14828e6": "\\cite{Papineni02bleu:a}",
        "2211.03277_a74da44697ee4cb750625153c2472132": "and Rouge",
        "2211.03277_20c2575bd366455560bcb67fe2466a86": "\\cite{lin-2004-rouge}",
        "2211.03277_afc76d99cdc92e51d456d643fe442848": "scores to show the intrinsic performance of the decomposer.",
        "2211.03277_3f03b7e57b6337668bab8ac1f1557f4a": "We evaluate our RC model on",
        "2211.03277_f7b6af2a3e53aadcad704f78758a1390": "For the Hard-EM approach, we have",
        "2211.03277_61d7a2454ed31953faf1e368c66c97bb": "as weakly supervised data",
        "2211.03277_bddbda7e9f1f9ee7f581612dbe9b145d": "We report F1 and Exact Match(EM)",
        "2211.03277_c95b787e6feeb91938a5f932a898d5ae": "scores in the following experiments.",
        "2211.03277_e5781e48d431154723cfabcbdcc158b3": "\\begin{table*}[t]%{0.5\\textwidth}\n    \\centering\n    \\begin{tabular}{l|c|c||c|c} \\hline\n       Backbone & Variant & Training Set & F1 & EM\\\\ \\hline \\hline\n       baselines & & & &\\\\\n       bart-base~\\cite{lms-bart} & - & $\\mathcal{D}_2$ & 30.9 & 27.1 \\\\\n       t5-base~\\cite{lms-t5} & - & $\\mathcal{D}_2$ & 37.9 & 33.9 \\\\ \\hline \\hline\n       our bart-base variants & & & &\\\\\n       w/ predicted sub-questions & \\emph{separate} & $\\mathcal{D}_2$ & 32.0 & 27.2 \\\\\n       w/ ground-truth sub-questions & \\emph{separate} & $\\mathcal{D}_2$ & 33.2 & 29.0 \\\\\n       w/ ground-truth sub-questions & \\emph{separate} & $\\mathcal{D}_2, \\mathcal{D}^{1}$ & \\textbf{45.0} & \\textbf{40.5} \\\\\n       w/o Hard-EM & \\emph{unified} & $\\mathcal{D}_2, \\mathcal{D}^{1}$ & 44.2 & 39.9 \\\\\n       w/ Hard-EM & \\emph{unified} & $\\mathcal{D}_2, \\mathcal{D}^{iter}$ & \\underline{44.3} & \\underline{40.0} \\\\ \\hline \\hline\n       our t5-base variants & & & &\\\\\n       w/ predicted sub-questions & \\emph{separate} & $\\mathcal{D}_2$ & 38.6 & 34.8 \\\\\n       w/ ground-truth sub-questions & \\emph{separate} & $\\mathcal{D}_2$ & 39.6 & 35.6 \\\\\n       w/ ground-truth sub-questions & \\emph{separate} & $\\mathcal{D}_2, \\mathcal{D}^{1}$ & \\textbf{45.1} & \\textbf{40.8} \\\\ %49.8, 45.9\n       w/o Hard-EM & \\emph{unified} & $\\mathcal{D}_2, \\mathcal{D}^{1}$ & 38.8 & 34.9 \\\\\n       w/ Hard-EM & \\emph{unified} & $\\mathcal{D}_2, \\mathcal{D}^{iter}$ & \\underline{44.0} & \\underline{40.1} \\\\ \\hline \\hline\n       GPT-3 (zero-shot) & - & - & 15.7 & 4.6 \\\\ \n       GPT-3 (few-shot) & - & - & 34.9 & 27.0 \\\\ \\hline\n    \\end{tabular}\n    \\caption{Overall results for baselines, our separate and unified variants. All models are evaluated on the same test set from $\\mathcal{D}_2$.}\n    \\label{tab:models}\n\\end{table*}",
        "2211.03277_510256e896bff115b965cad513311620": "Results on Decomposition",
        "2211.03277_7cb5a1b9371fcfa0949638f60ebd574e": "Based on Bart and T5, Table",
        "2211.03277_05fbb6792748c449bd8177660c851bd3": "and Table",
        "2211.03277_97fdce2ef556f57ac0ceef65e31a12da": "respectively show the experimental results of the question decomposers",
        "2211.03277_1d001f3e33bb870b78b01fdddc30562c": "To comprehensively show their performance, we conducted two aspects of experiments including intrinsic decomposition evaluation and extrinsic RC evaluation.",
        "2211.03277_59e387e8f9afb4868793fef533255cb6": "We first evaluate the quality of sub-questions generated by different question decomposers",
        "2211.03277_f0384883c3d0b1f4af159db4d1f6ff7a": "In this part, intrinsic metrics, BLEU and Rouge scores, are shown in the first four rows of Table",
        "2211.03277_ae1711311d060d0ae99e80750b890d29": "And also we show the results of five decomposers trained on different proportions (1",
        "2211.03277_217bb768154d817307473a47fe166c53": ", 5",
        "2211.03277_a08c0789c5bd0c5d2e3ee48a3f490346": ", 10",
        "2211.03277_8d8a3cb6edd99205162807709244df24": ", 50",
        "2211.03277_2a19a64deb4aae1ada72a5017d6cdbdb": ", 100",
        "2211.03277_b3fe0487ab3aef422024ae2ebb542446": ") of the",
        "2211.03277_94fcda9c354881a2fb12a92f6e2c23af": "'s training data",
        "2211.03277_481d76a31c3c0e5d6c364c9b6d5f0f23": "All these evaluations are conducted on the same validation set of",
        "2211.03277_1e3faf0e27b0a02101b4150b2ad1c2a4": "Comparing column-by-column, we find that with more training data, both question decomposers achieve a better performance for both BLEU and Rouge",
        "2211.03277_39f0f282048b4d68a86449ce9ede679a": "We also note that the rate of improvement of these metrics becomes slower when more data is added (e.g.",
        "2211.03277_c4ca4238a0b923820dcc509a6f75849b": "1",
        "2211.03277_219bd344ff3928edd4643b105c1fb4b6": "to 5",
        "2211.03277_e1314e9eb517591af59538c62aca534e": "and 10",
        "2211.03277_b85a170b4bfe2f6d0c3305e8a358399a": "to 50",
        "2211.03277_9aa0d14da3db564b4afd93c4eb2f7601": "Therefore, we posit that with more training data, the performance of the decomposer will not improve due to the capability of the LM model.",
        "2211.03277_ff481847ff67066fbde0f235c908c6e7": "Since the eventual usage of the generated sub-questions is to improve the RC component, we conduct a RC performance comparison experiments to see how can the quality of these sub-questions influence the downstream RC task",
        "2211.03277_9bb0cec5145b2eabb4e82c11092025a8": "Also like the intrinsic evaluation, we show the results based on decomposers trained on different proportions of",
        "2211.03277_d0e597ee8e225ae47c4ce404178f2eb4": "by using two extrinsic metrics: EM and F1",
        "2211.03277_79b50d241b2e5dc18b80c08291a03414": "All the evaluations are conducted on the same validation set of",
        "2211.03277_2a78dc22f1702b7f2fec0db7b7b94aea": "To clarify our settings in this part, we don't employ the ground-truth sub-questions from",
        "2211.03277_aeafd46ac1f6450f212f2c9e88bf6a6e": "Instead, we employ the sub-questions generated by five question decomposers for the RC component to predict answers",
        "2211.03277_620445eba1172d5e8e3c62c769d4ec68": "As the last two rows of both Table",
        "2211.03277_3d474227b85f2c565e68e6d3893b13ca": "show, both EM and F1 scores show a gradually increasing trend when more training instances are used to train the question decomposer",
        "2211.03277_9c445f6c2e219be7ce60a2ac7200bdca": "With more parameters, t5-base tends to have a better performance than t5-small.",
        "2211.03277_0d6c7f9e503f179d0b118217583f0c8d": "Results on Reading Comprehension",
        "2211.03277_51c45b795d5d18a3e4e0c37e8b20a141": "Table",
        "2211.03277_d7d845dbd7581da4e454cc4a92dc3bf4": "shows the experimental results for the downstream RC task",
        "2211.03277_733804907e375cda5bd03d916200db56": "We show two baselines in the first place:",
        "2211.03277_0e9a6545e6ca782413665469d6adf773": "bart-base",
        "2211.03277_73de5ed3384d04a5c62a689ded6aa086": "t5-base",
        "2211.03277_25ad0164f9750f602f235ac162535abd": "Without taking sub-questions as input, both are trained on the",
        "2211.03277_151e0b98f6e9cdb16aef8b4b0f978b6d": "Based on these vanilla models, we show our",
        "2211.03277_6e1a0a0909ce1acbb1a16ce1e469f82d": "approaches that use",
        "2211.03277_ec936611bad8c20896029f9c28d7b22b": "as backbones separately in Table",
        "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56": "Separate Variant",
        "2211.03277_f41378dc2401a066a07547accdc467d0": "Our",
        "2211.03277_7d7727f2ca22610bd137d9400ff98104": "variants are base on the architecture in Figure",
        "2211.03277_84d8298209171837bf0e668735a9ba23": "In Table",
        "2211.03277_b9867355fd3227b83d4aad7449e7796c": ", we have three",
        "2211.03277_831855e5e1ad2c68998023962f4321ae": "variants based on each backbone for comparison",
        "2211.03277_7ac96855ed978ee4f1127550d560c84d": "Taking t5-base as one example, comparing to the t5-base, using predicted sub-questions achieves a 0.7-point gain of F1 score",
        "2211.03277_2fbaae84dad5dcd9eb09342a81a778d6": "Meanwhile using ground-truth sub-questions, our model outperforms the t5-base by 1.7 points of F1 score",
        "2211.03277_08a85228389e455b14164ef2e1211640": "The same improvement can be also concluded from the bart-base model",
        "2211.03277_1c5b52ca31ed23054c9495237d4be5d9": "They employ",
        "2211.03277_1cb92407de28bdf19d782f89345ca17b": "for training but their testing sets are different: predicted one use generated sub-questions while ground-truth one use sub-questions from",
        "2211.03277_c796c1c7712dee1f3069fd7376c68ca5": "$\\emph{D}_2$",
        "2211.03277_7092bfce241abfcfdae0533c45abc91d": "The reason why our approach is more effective than the baseline model is that concatenating sub-questions can give LMs hints on the reasoning procedure, which helps LMs produce step-by-step thoughts implicitly",
        "2211.03277_2ad6218af6d2b49974b696940948cd57": "Furthermore, we add",
        "2211.03277_9d56d742982a3d188df203814f814c90": "$\\mathcal{D}^1$",
        "2211.03277_9d084d21b14bd896ff888fbc453bb644": "as the training set to train our seperate model",
        "2211.03277_f46a65365156738f14482c03fd283dc2": "As it shows in Table",
        "2211.03277_64ea55fe19c97fc0525a647ee61e55cf": ", this kind of",
        "2211.03277_6e6f2497b6a2a80e05644807fb796228": "variants show the overall best performance since we have two sets of parameters separately learning question decomposition and reading comprehension",
        "2211.03277_d0474cc26cc51c9ee5393f68dd586e0c": "Compared to t5-base, the bart-base variant shows a higher performance gain that proves the effectiveness of our method.",
        "2211.03277_f4c6ed20aa143995745ff84775e8db7d": "\\begin{table*}[t]\n    \\centering\n    \\begin{tabular}{p{0.41\\linewidth}|p{0.18\\linewidth}|p{0.1\\linewidth}|p{0.1\\linewidth}|p{0.09\\linewidth}}\n    \\hline\n        Context & Question & GPT-3 (few-shot) & bart-base \\emph{separate} (best) & ground-truth answer\\\\ \\hline \n        ... notably striking out \\textcolor{cyan}{Julio Franco}, at the time the \\textcolor{cyan}{oldest player} in the MLB at \\textcolor{cyan}{47 years old}; \\textcolor{cyan}{Clemens} was himself \\textcolor{cyan}{43}. In the bottom of the eighteenth inning, Clemens came to bat again... & Which player playing in the 2005 National League Division Series was older, Julio Franco or Roger Clemens? & Julio Franco (\\cmark) & Julio Franco (\\cmark) & Julio Franco \\\\ \n        \\hline \n        ... Nyaungyan then systematically reacquired nearer Shan states. He \\textcolor{cyan}{captured Nyaungshwe in February 1601, and the large strategic Shan state of Mone in July 1603}, bringing his realm to the border of Siamese Lan Na. In response, Naresuan of Siam marched in early 1605 to ... & How many years after capturing Nyaungshwe did Nyaungyan capture the large strategic Shan state of Mone? & 3 years (\\xmark) & 2 (\\cmark) & 2 \\\\\n        \\hline \n        Kannada language is the official language of Karnataka and spoken as a native language by about \\textcolor{red}{66.54\\%} of the people as of 2011. Other linguistic minorities in the state were Urdu (10.83\\%), \\textcolor{cyan}{Telugu language (5.84\\%)}, Tamil language (3.45\\%), ... & How many in percent of people for Karnataka don't speak Telugu? & 66.54\\% (\\xmark) & 94.04\\% (\\xmark) & 94.16\\% \\\\\n        \\hline \n        A 2013 analysis of the National Assessment of Educational Progress found that \\textcolor{cyan}{from 1971 to 2008, the size of the black-white IQ gap in the United States decreased from 16.33 to 9.94 IQ points}. It has also concluded however that, ... & How many IQ points did the black-white IQ gap decrease between 1971 and 2008? & 16.33 (\\xmark) & 0.9 (\\xmark) & 6.39\\\\ \n        \\hline\n    \\end{tabular}\n    \\caption{Correct and incorrect outputs from GPT-3 and our \\emph{separate} variant. \\textcolor{cyan}{Correct} and \\textcolor{red}{Wrong} supporting facts are annotated in the context using the corresponding color. Correct and wrong answer predictions are also marked with \\cmark and \\xmark \\ (the table is best seen in colours).}\n    \\label{tab:error}\n\\end{table*}",
        "2211.03277_778116e52bdb779b7909a708875f3018": "\\begin{table*}[t]\n    \\centering\n    \\begin{tabular}{l|l||c|c|c|c}\n    \\hline\n        \\multicolumn{2}{c||}{overlaps} & $0\\sim25\\%$ & $25\\%\\sim50\\%$ & $50\\%\\sim75\\%$ & $75\\%\\sim100\\%$ \\\\ \\hline\n        \\multirow{4}{*}{uni-grams} & bart-base & - & 0 & 25.7 & 27.4 \\\\\n        & \\emph{unified} & - & 0 & 32.9 & \\underline{40.2} \\\\\n        & \\emph{separate} & - & 0 & \\textbf{35.7} & \\textbf{41.3} \\\\\n        & GPT-3 & - & \\textbf{100.0} & \\textbf{35.7} & 26.4 \\\\\n        \\hline \\hline\n        \\multirow{4}{*}{bi-grams} & bart-base & - & 16.7 & 23.6 & 28.2 \\\\\n        & \\emph{unified} & - & 33.3 & \\textbf{29.1} & \\underline{41.9} \\\\\n        & \\emph{separate} & - & \\textbf{50.0} & 28.6 & \\textbf{43.2} \\\\\n        & GPT-3 & - & \\underline{44.4} & \\textbf{29.1} & 26.2 \\\\\n        \\hline \\hline\n        \\multirow{4}{*}{tri-grams} & bart-base & 22.2 & 20.5 & 25.5 & 29.3 \\\\\n        & \\emph{unified} & 38.9 & 26.2 & \\underline{32.3} & \\underline{45.1} \\\\\n        & \\emph{separate} & \\textbf{50.0} & \\textbf{30.0} & \\textbf{33.4} & \\textbf{45.9} \\\\\n        & GPT-3 & \\textbf{50.0} & \\underline{28.0} & 25.8 & 26.8 \\\\\n        \\hline\n        \n        \\hline\n    \\end{tabular}\n    \\caption{EM scores separately computed based on overlaps of sub-questions n-grams between training set and testing set on $\\mathcal{D}_2$. Four models listed in this table are: the bart-base baseline, the best performed \\emph{separate} model, the best performed \\emph{unified} model}\n    \\label{tab:n-grams}\n\\end{table*}",
        "2211.03277_a5524984347c586c9c004b70d8c9ea90": "Unified Variant",
        "2211.03277_b64541c9afebb6867724faf9ebb3c4b5": "and one single model is used to train on both steps",
        "2211.03277_3ba5f178c65f0189efe5b9031375eb14": ", the last two rows of each variants show the performance of our",
        "2211.03277_b808fa58cd05f24536649751b9ce9e9d": "Without the Hard-EM algorithm, performing multi-task learning achieves a 0.9 point improve over the T5 baseline",
        "2211.03277_c91bb09611c93cd65b5f0e83a47f33d6": "However, it shows a performance drop when compared to the",
        "2211.03277_c8bdde3e512f7823208c78cf591e73b9": "variant with ground-truth sub-questions",
        "2211.03277_2fca00fb889580a665490b891d1f3b4d": "This can be caused by the enlarged dataset and the additional decomposition work the",
        "2211.03277_093bb0f659d1b687c9e7d32480fc5db4": "variant need to handle",
        "2211.03277_b0a145f6be444e7186989eb3822afe48": "When more training data is provided (i.e.",
        "2211.03277_f818e2f79ac892a6b1cd39d9aef6a4d8": "), though without ground-truth sub-questions, the",
        "2211.03277_45bdbcec874f8b34d56ad2fada005149": "variants substantially outperforms the baselines by 10.1 and 6.1 points over bart-base and t5-base model",
        "2211.03277_06a945180de124d3e044364474f3da19": "Furthermore, when compared with the best",
        "2211.03277_56a2958c78fb0dd2ff40237ad8d959b0": "variants, our",
        "2211.03277_b5aee532b982a1d6f56895f201ebe87e": "models also show comparable performance on both F1 and EM metrics",
        "2211.03277_01a5fd0cfd7245968e2895cc93a51dc2": "Based on the observations of the last three rows of each backbone, it can be concluded that introducing more weakly-supervised training data can significantly help our model address the original difficult multi-hop RC task",
        "2211.03277_4353a8d44172b608b2bb3d2f45d128af": "We also include another evaluation of employing GPT-3, which is the state-of-the-art language model on many tasks and also in a large parameter scale (175B)",
        "2211.03277_f90c01189acf24cac5196da77ddd26a8": "The results are shown by last two rows in Table",
        "2211.03277_a2e3330a6397ebbf27eaa9fab8b791e4": "Based on the experimental results, GPT-3 cannot even beat two baseline models under the zero-shot learning paradigm, which again shows the complexity and challenging of the task",
        "2211.03277_7af92c12a484bda9e4e59e96796be13e": "When provided with several exemplars, it can easily outperform the bart-base model by 2.4 points on F1 score",
        "2211.03277_7f415a009670a1a1dca9546342fc27c7": "However, even with",
        "2211.03277_176bb539c91849e35d552b98159767b2": "$\\times 1000$",
        "2211.03277_9d8d91820d515fd6e0a27b168579e739": "parameters, GPT-3 is still far behind to our best variants by 10.2 F1 points.",
        "2211.03277_74799250ff5585a24184a79f12bf080a": "Analysis and Discussions",
        "2211.03277_007dcc215740ecc7be1eea157e40f504": "Qualitative Analysis",
        "2211.03277_810a7ad815764cc3e3baede4417f4804": "In this section, we will further discuss some real-life cases generated by our proposed variants from the dataset",
        "2211.03277_70be3361c625be3dfa44efc081d69c9f": ", the first row shows a comparison question and both GPT-3 and our bart-base",
        "2211.03277_ddb2819e03067f9573edd94cc9f51034": "model can produce the correct answer",
        "2211.03277_0a71397b235c031f33ba129a6d9df152": "However, when the question requires some arithmetic operations, such as addition or subtraction, the GPT-3 model would fail to answer correctly",
        "2211.03277_5be809ece1d2e0c5e07c8f7512e6bb06": "Our model can handle this as shown by the second row",
        "2211.03277_f0e5e01a67a2b5704ec5b26e0e8552b3": "There are two types of failures from our variants: one is that our model cannot handle unseen numbers, and the other is arithmetic between float numbers",
        "2211.03277_c0ff19b36bf67ce53d2e773c61faf931": "The unseen number case happens in the third row of Table",
        "2211.03277_12828d228d4dc700d81140ad548d9844": "Asking for the number of a complement set, though the number 94.04",
        "2211.03277_0a8631a299c09f9f7bb6337b20f482d1": "is wrongly predicted by our model, it is more close to the ground-truth (94.16",
        "2211.03277_468a63f02961d4774f93bd4324f18ffb": ") when compared to the GPT-3, which directly predict an wrong evidence annotated with red color",
        "2211.03277_588f4221b1b2448eac5783078fd99421": "Furthermore, the last row shows a subtraction question between two float numbers",
        "2211.03277_e2f96952cd10a33dddec519e96b047e4": "Different from integer number subtraction in the second row, it is much harder to compute this arithmetic for language models",
        "2211.03277_9f9eab64930954640b1fcf86febdcaa7": "Traditionally, some symbolic methods can handle this problem very well",
        "2211.03277_4c8d99223325c485d536ff611567a2de": "Tackling these problems can be interesting future work directions.",
        "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979": "Quantitative Analysis",
        "2211.03277_9ba020bcf77737bec46aae3df60caddf": "We look into details of",
        "2211.03277_1f579a679fff54d8936a80f3573b6cee": "from the perspective of sub-question n-grams for both training and testing data",
        "2211.03277_bd462465abaf81d4b620327a36c94fe6": "Intuitively, given one instance from the test set, more n-grams overlap it shows with the training set, higher the EM and F1 scores",
        "2211.03277_5975ae121d279002828b36cfdf949d7f": "Therefore, we further conducted the analysis and list all the statistics in Table",
        "2211.03277_cc978df04c4f160fc936d9a48ada43e2": "We calculate for uni-grams, bi-grams and tri-grams for four models: bart-base baseline, the best-performed",
        "2211.03277_4a4c4ab9cc85136ec98497a5c5e9c677": "variants proposed in Section",
        "2211.03277_68b00eff25cc53b2e1940a272cbd2c40": "and GPT-3 with few-shot learning",
        "2211.03277_84937c0866effefa446b0ec51d16a48a": "The overlaps we choose is four intervals using percentages to represent",
        "2211.03277_64ebc0f9810c81d8ab690740209c2538": "$0\\sim25\\%$",
        "2211.03277_c3ad332c279d8a23d83c63028347e175": "overlapping on bi-grams means that the test instance have this proportion of bi-grams overlaps with all the training instances",
        "2211.03277_18dcba2c80006fd8290be7a85983dfa2": "Note that there is no overlapping for uni-grams and bi-grams in",
        "2211.03277_04b8947e99ed00775a3fbb62313ae121": ", we report the EM score (F1 score shows the similar results)",
        "2211.03277_0d5222d9b62acc623d3e904916a752ae": "The bart-base model show a tendency that with more overlaps across all n-grams, the performance will increase, which is consistent with our assumption",
        "2211.03277_8129442325c18fe41c8a0ad9c9a57b1e": "However, on the contrary, GPT-3 model show a reverse tendency that is probably due to the pre-trained corpus that shares far less n-grams with the test set",
        "2211.03277_5a8ba7c5c7c715ffc39bb5f7abb6da82": "This characteristic improves the compositional generalisation ability as it outperforms the baseline model on the low-overlapping part of test set",
        "2211.03277_e3800a9b79bdfd6bfd99dd4cff996df3": "Both of our",
        "2211.03277_943b7024247560e30ca2ee5596868304": "variants show overall improvements over the bart-base baseline",
        "2211.03277_cddaf805d0070e0eaa5a847d1cfa71ab": "In particular, the first and second columns also show our model can better handle the low-overlapping questions, even without performance drop on the high-overlapping questions (",
        "2211.03277_3ee4c0098a9bcc36be1fa1e9133bcec2": "$50\\%\\sim100\\%$",
        "2211.03277_449a218d97353d345dfc3f3311bc040d": "This experiment can further prove the compositional generalisation of our method is comparable to GPT-3.",
        "2211.03277_6f8b794f3246b0c1e1780bb4d4d5dc53": "Conclusion",
        "2211.03277_bd0fd529ee09874109168ff3dfe89135": "We propose a two-step process for multi-hop reading comprehension task",
        "2211.03277_a0c67423f269fae5c41b1bbe5b72c94c": "The first step involves a question decomposer that maps a difficult multi-hop question into several sub-questions",
        "2211.03277_131d8f7b10b4cc29b7bfc7d56395b6d9": "The second step is to train a reading comprehension model based on (question, sub-questions, paragraph, answer) tuples",
        "2211.03277_0f3a57545d74bc7da988e49ae1952061": "With the addition of sub-questions, our bart-/t5-base variants outperform the baseline model by 2.3/1.7 using ground truth sub-questions and 1.1/0.7 using generated ones on F1 score",
        "2211.03277_6e84b3f90940222c0d6e7f686695b570": "Based on the hard-EM paradigm, large positive gains of another 11.1/4.4 point on F1 by the unified multi-task learning bart-/t5-base models shows the effectiveness of introducing weakly supervised training data",
        "2211.03277_e660a4448089b9fa1c1f10fbac1ac8fa": "By further analysing the predicted examples and dataset, we also found our model can make a more comprehensive improvement compared with the SOTA GPT-3 model",
        "2211.03277_f892edbb69eb55dfe5c7eb49bb08cbbc": "But some problems like handling unseen numbers still exist and will be our future research directions."
    },
    "hierarchy": {
        "1": {
            "2211.03277_0fbd1776e1ad22c59a7080d35c7fd4db": "2211.03277_490b44de255309ca7ed4dc8452242cdc",
            "2211.03277_fa9497f5acccafcc3e6019657bdc5eb1": "2211.03277_490b44de255309ca7ed4dc8452242cdc",
            "2211.03277_e353dbe42c8654f33588d4da0b517469": "2211.03277_490b44de255309ca7ed4dc8452242cdc",
            "2211.03277_354179e73b445da2ab4865bddb7e3064": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_92fb24fb473201aa26785337581244a9": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_a790c55306be09f3db229870079d7b49": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_4db0062a5ec022e76d5f853ebfba540a": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_7e03aa597bf3ac958dcf3fe0201c8b6e": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_6d9446714100d05956aacc9d613b151b": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_3fd30d9511b352b88cc88cd759922111": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_331865bbcf608253d61861c2558e594d": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_6666cd76f96956469e7be39d750cc7d9": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_d77a8d1e6b8e41705dc9c101032b3543": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_0b79795d3efc95b9976c7c5b933afce2": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_4e23c9db5ab30b89fcb6cf7607798372": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_3606d017d7f899424e7db801a6191ea0": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_17cbc133d4006eaec4cff84c0633bec8": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_5a152f37483b7602c7fc0e0ed6c14a88": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_2c070cecfcf704dca62f0ead65178b97": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_2ca9b494eaabd6576eabe1ad5f64fcc0": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_5d376baac2cf8691282536d6101682e4": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_5a055992d4bfbb52dcad3e3d6f877248": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_8c26e6655aab9ae92411073abc805918": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_e3d2a08e4ee3994a46c249a414e241d2": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_d63748ff6bf643591b03d8366b40d494": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_fb19066311f84c5909400aa479652a43": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_426c2fd37aae968c8cb5669fa8395534": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_be5d5d37542d75f93a87094459f76678": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_d72d9b95cca301977fc25307f3146fb6": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_153af6246ce72c12fd4d96b4eb6b1391": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_8632fe25ee7d71fc9d7f51135d4adc15": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_ec2d8f61724eb750982f5fef138dcf12": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_5fa29b323228df32a080c0c9f64cc020": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_2fd3f02df168ce74e0c42220eba77437": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_e610023717ffae6e4b7186bee02afb24": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_062044ae774eddf61ed78cd45c18ba09": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_27187839470353501e07515a725580f0": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_8a9980a78cbb8fa1c1e0b20d0e655d50": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_87b7c5f22f4c4d665b5f887feb6fb4e6": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_4d4e581eba47590f5e2950347590eeaa": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_63cbcf9ee5e83822600339262db2ef02": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_b720c53bb56b3d2d93a3547fb6235f69": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_b37c1eac6cc92f5fa84aefecc62d3813": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_3763f81c57d774edfee50cf4e9371463": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_9e93599fe6d6d916e73e30e3c44c5992": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_b332c13fbf053911ad31ecb1e07d65b0": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_8bdd9748663560c4dc11774f8c4f4eb8": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_8cfa886b576ba40148388c46c5163738": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_e31c163a56dd08b41c7155242c8c2b1c": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_c2533e4ac80a068b94e26a7a56169c9b": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_a11e1fc5e7122e1ecbc1ca9a9026d01e": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_16536230b6fe77121e0552fe87063928": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_dade5bf00b51aa093338f89be7b0341a": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_20f35e630daf44dbfa4c3f68f5399d8c": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_740d574afebe5c39d5f88bdc5ecec95b": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_eb705fc8915dd3a0bbd3d4d0f0ba4768": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_ffcaa08788fcff4d52ef8ee5866e2c61": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_90e8d33410cfcc27bb93a73f581356b7": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_d445f14930b1842ec6066e88a576d5a9": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_bdbf342b57819773421273d508dba586": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_fb4f24d435ddf2f62b633fef2e496f8a": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_4dfff143a9bbdc7ffcf330098e746594": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_39da08aefb6c5b353a5d2f7ade63fdc9": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_60ec48a9de4502c2ae8597592f1a459b": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_b7012658d546dac77191f51366d119f6": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_763d71cec4ca4b48263dd96ccd8670cd": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_e9c2e9817160c909e80b9c2755a7aca5": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_fa9e324d2a768f020d0ac3b9ca396f19": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_c92fcf4fc05bc64d930c473f816946fe": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_4053af40a01fa4dc1e066e15d4ddb62d": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_9e01be3faccece6993d973d9fd316e6c": "2211.03277_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03277_aed402c3112b4749a9a98a72cbe9093d": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_a1b2ebec076d74acf17f8bf9d16051bf": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_9254b6cbb24ffa59705da97de5d259ed": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_2645b77065690f34e5a6094cad44d75e": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_3f816854c8b74a9f949d011d66b308c0": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_40ba26c829643c67a67defade2c9a6fe": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_68935f2b6d31e69bd854ca41b2fb4161": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_370f45c4be9a1e0a6b2a11f949fc518c": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_fc365e7088a9fababc51d7f2e29a0f56": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_d994b495319c2f6507de031a8bde2124": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_5da4dec80f667cc3dc392721f9a38f4a": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_4d45662826887ef8d629ee279a149888": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_974b3a011509b5fab9dd01850895d256": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_dabdb387ade6712237057f681b38bd12": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_ad81003d74216bc7c8205c31c731c133": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_81145250c1587b18c7f5fa5ed13c3898": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_af5c78e037da034b5ebe870431cb1bdc": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_a284ed43500931b47bc0be2711b92aa1": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_1f26871b799a23ad27dd7d6f0512d51f": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_3e5c439a3b891b2c68bfffffc9d7c419": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_06c441a02fc9ad73b112636c3d119ae3": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_0b0168372b1107e08191464d342611be": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_c0c46671dc82af76a54179411b0c522b": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_cabbc139b7f74771c47e3eb24d94c92b": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_38e7aa84d7becfbb3c53b8cc46422561": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_0c0c62c7000d281aa1669956e1f352d9": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_890c19e4f13928b2e02f223a9a12928d": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_926e5261560d68333da76763b3075d3c": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_7bf75c24569111c5a3bc3743a78f61ff": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_ba64c112420a9c3b8a6bd9a8d13827b6": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_e8daa762dccf1737afecf2b72608ca52": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_d4f1365b2f67dcab3fda60f3c584248b": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_62cb29028114e01e2724639864a66f22": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_967fafcf79adf5698f15f769e9bb1608": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_44647ca88446fa7c38d188f99110fb02": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_ec0cda02ca353c14e976243b7f60263b": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_c602d659eec31b308ae3dab3329405bb": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_9f65da3b5330392b93f966c434f9a517": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_3d2c12477f5e039088efb6823785a182": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_20d1bedbde8aea7d708611a616ea94ef": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_3cd60f63a89a88cf918b11ba046d66a0": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_33d7ef2ae13444aa7318a00f18ac52a7": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_4e78b2854ee0b81cf76870709ce8c5c4": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_7287209614a444089c74230a021a2edd": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_ece208255886b96ccac6dbeecaefac0a": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_fd616f527e088bc105cd0bfa096c1e99": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_82d76e2d394407c4d28f0cfa1e7348ef": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_5047da1f4d39332bd0c0b8c2185a8948": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_d27562ca5f264db64273a0cefc112b40": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_03d55413478a34f019479b51a8cdb3eb": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_e719001b8cba9cab6db2cd76a1e5bd4e": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_5058f1af8388633f609cadb75a75dc9d": "2211.03277_0d6c7f9e503f179d0b118217583f0c8d",
            "2211.03277_8b921702e775c208929b7f66c2c5a543": "2211.03277_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03277_e30144b8015f54bb5a0c3ff49f90d5e2": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_871cc50ed718556264aee5c950917f9d": "2211.03277_e30144b8015f54bb5a0c3ff49f90d5e2",
            "2211.03277_0e0605e05027ff61c5f438f834de6c60": "2211.03277_e30144b8015f54bb5a0c3ff49f90d5e2",
            "2211.03277_7bade40785d1f5c1dbec8c0414eaac9a": "2211.03277_e30144b8015f54bb5a0c3ff49f90d5e2",
            "2211.03277_7d74f3b92b19da5e606d737d339a9679": "2211.03277_a8f65dd93445b90d34246cc201862695",
            "2211.03277_6ab498a42f8abbd6b1fbde93a6a2f1bb": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_3027dc21efd035029a03d483a52e8a5e": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_814bcb2ffb3d22873648b579e0de1002": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_bb82a310ae95928b5a984266fad2a301": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_e8ce6136e54a4f8604eaddeb5e8ede10": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_1f8236f4f2978edcd761ee6240f17e5b": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_49cd0f8bd10fdab7bb7125c00c26298e": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_d55669822f1a8cf72ec1911e462a54eb": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_e1d04488606109e7eb46c88634221d01": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_ce96ce64475d88003b81a20710c10f5e": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_109f4b406d0ef8c53067ec3bc52d99df": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_1ce1250f7c46bd51d4b1a4f171207877": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_738f68f084c693dbe477b437f69e56b8": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_14361ef74ad99165a277ae9a6f03d333": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_af20ca244e854d5418df5f72ca6404db": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_5dbc9b0eb68253d08f12a82bb4a8cf92": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_14a0b4a24f0683353ffbe235d7657bdc": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_1afcdb0f704394b16fe85fb40c45ca7a": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_4805129561d8bf2d2467a98c38b46e52": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_9b325b9e31e85137d1de765f43c0f8bc": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_dbc49cb1b86107e1ebbbf54ec055671f": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_53d147e7f3fe6e47ee05b88b166bd3f6": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_737cce81ab1a956e04e0519de6b06a9d": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_567904efe9e64d9faf3e41ef402cb568": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_5b51bd2e6f329245d425b8002d7cf942": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_6167b443811d3b2f05a171bbaa0e2925": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_01a443d8fda57ca72fe242b04f8ac018": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_93233862c95c966b4e70eb4fd4c83aae": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_56d682243a37f25f0eaf4a3df692d67c": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_1e50c0aa0150691147567e1e9778041a": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_c97f2a2e4b83cffee06344fcc99257f2": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_928f23e5bf20d3f25a8b32b2b921fd4e": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_874f4b69feb2bc09f2c16611c7391949": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_f7d571365f6be112ed2ecacc4655eed6": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_27e556cf3caa0673ac49a8f0de3c73ca": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_59ad1af4938fb25daa10db31aca60ed4": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_7f7c57d1e10dd7e1a2c4722e144693d1": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_aa7ac8cd5e0a6993a177746279cc00d2": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_a46a1179ee7614e48a50ab4ef9d744af": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_346fa62042aaa5d59b0bb3aeabed2d4e": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_cf7966dfbd476267d060494bd5350a51": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_398b7824ff08eb158ce9441b8a2f9c35": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_936f36542e7dabef7e8b22fc3d5e4663": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_304c2ce6eef3ed8f38528cafb6e1f503": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_5a4a7c670eb7c71eb2d93f8bff7bd972": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_11737adab02e11da609e035f0727a9c2": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_c293728f7bf4095f5f37797877292a79": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_467011b757110b378f3ef994359bc545": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_ecfdb1eb758442453e853438d7a688f9": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_cddc8d84dee6cc0220c804deb336277c": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_279045037a58a12dc389269d7a60cd39": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_d1831323eb5bdda49c845920a30d06c3": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_c36dd91721de6906ab5e329a175f7d93": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_945ff45452989de6ffa52538341da2a0": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_385a56a84b0cd4762156e38e37d38ed8": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_65542fbb11208c7eb97b02ae155637c6": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_83f833c7956779ef1e9d9130567ee171": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_cd17637111d023af22ebfcea33c0714a": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_a29f17caa3e965b909d1aef183a202e4": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_44964e4efc0a19fcd09719f13abd2abc": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_936e281e8b887884061749f37a567f0e": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_79b39406d0b67c84e1518f28ba04ee15": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_1e3677bf29d4ef5f476050352a98c8cb": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_9ab54051d35513e485e015a16361b59b": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_e538edc73cfdd22f4d2a8da94609ef2e": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_653b1f70c93da21813eb8d603913dfcc": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_ecfc6d6f115694f31bef67f0a152313a": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_ed80b3f5a128d95be8d9c6914f89a5a8": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_359372669d9774dad2b3ad8a7c415dd0": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_913b0624ddc3133517206fe20651d3c5": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_280f67dbab96421dffa3df3932ae59a6": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_15c99bd29b73bde135f225ca2f8539a7": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_63bc38d43247a8278ba778cf92d3cbb0": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_8c834d30bbe046c99321eb71949382e5": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_35dba5d75538a9bbe0b4da4422759a0e": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_9ee51e0ca6ab7ca19aa873091f25c7ac": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_fb97d38bcc19230b0acd442e17db879c": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_6f705e84571784cf0b5cb60adbf1e4d0": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_27be2bc45eaa115101d1e90b3de3c381": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_fc9477219af263c2ae7464816be5ad6c": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_f432b3358e5ab24106bca3b445316f5c": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_e41273f43af504714d85465294f1f369": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_ed2b5c0139cec8ad2873829dc1117d50": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_7c9b95fd64ce596113f8fda8bd217c70": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_94988e03aba2574e2fae88ea64970d8e": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_b273a613b8596eea4fd3d7723565399e": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_0d55bd2cc9e3c5ba2fde111282c75e00": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_cd7edc81293debefa5f08a55b544c45c": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_ffb6c2624e0d394982f92f837c0848e9": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_889b4d8857821ce1c2730544a95b8c89": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_5983bd6d614c8704c716498193702283": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_99077aa1753fc4f769c402fb096914b8": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_a51de93e24f260b06f192f60f82e0f01": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_72dfb105963f1538c5b739180fd6f712": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_72d1d84a02ecfa2713790572f9e067ac": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_c721fb953e4dadf3d341cba6aa949c3d": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_ca2979b8912b819d17ada24347019490": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_46de177b7fd7a541d82db4dab35f5493": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_fb41a074abb51810d0b13686f648f0f4": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_568461bda901703834f814067ec235df": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_ea2caa3f8b0ddca884cd9e99e6439bbc": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_79bcb8399b99736c6b912501be2631cf": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_97122726ee38968dab28c3d5c36b7310": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_667bd26f2f4bd03f5c598f131d48e4a8": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_156abf7624df9cb729c2aa1d007e51d2": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_e470dc2f15b00c7d00653d9b7b759599": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_e8ca4a714dcb64ed1ce2a9ec7e92e46b": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_756fc496c55892dc274ded063136d1c3": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_52a9ba72befc2013c3374f28e6b96aca": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_fd6cf67915016f3bdc64ae3d06628573": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_71511cdbdae5e009d9b78158d6fe36bf": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_dbe53b89bec489fac49f06bdaae8484f": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_fee6e7dd3cc4f79cbab8483ad9c1eb0f": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_e94e7074a31ec487efddda54efbe579a": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_440a3bbc91716125aef7de35fc6e49ae": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_2f967d2f0af96fd6e208952860ce6181": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_13b5bfe96f3e2fe411c9f66f4a582adf": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_e6d82c5aaf61cdcfbef0e27a6afcdb0d": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_4829262cecb9828817b33e0f9c907f91": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_a092d919b1ba8b6e4e14c75eb90b56a9": "2211.03277_4829262cecb9828817b33e0f9c907f91",
            "2211.03277_197da3bc28a85d4149927dff350f3be7": "2211.03277_4829262cecb9828817b33e0f9c907f91",
            "2211.03277_239658e016e3d5d06ae719d280a79fec": "2211.03277_4829262cecb9828817b33e0f9c907f91",
            "2211.03277_2adfde7231396173b4ca0e0fa317cc59": "2211.03277_239658e016e3d5d06ae719d280a79fec",
            "2211.03277_a8f65dd93445b90d34246cc201862695": "2211.03277_239658e016e3d5d06ae719d280a79fec",
            "2211.03277_594c2513c07dcda9a32d85917dab0d5d": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_3c4d09e4ef50b370ae0efacdb43ec2dd": "2211.03277_0d6c7f9e503f179d0b118217583f0c8d",
            "2211.03277_0cc011acdc7dfb28285371b288377955": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_06726ea77ceaca2c5441e947eba9b8c1": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_aaee9fd23e4d8743eb0830a00e19bf48": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_d217c410b42679ed278381092ca023fb": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_9d09980920f228e18b6618ac92aa326f": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_4ebc412fb49fb0a4476f4c8ffbd9348e": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_48e5432b5e652716d56d0876897673c7": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_f5b577a1129ac6cee674d0d8ec71b71a": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_6987a7a4f2e86758ad44f8867482e431": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_853ae90f0351324bd73ea615e6487517": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_eaf85f2b753a4c7585def4cc7ecade43": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_fa1fd542ccc1fbe5b4bc7ed429d5e4c5": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_12554475d176528d3b89fe9adcecbe44": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_a1fef22aae603e478eb464736cd3bae6": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_fab1712162b0b7924960496af4d43252": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_9371d7a2e3ae86a00aab4771e39d255d": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_5075d30b23fbe3450d319993a8b3bd8d": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_73652ac487be7a267d99767a22545b03": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_2117f84682321b24e72aacaf4bfc0431": "2211.03277_7d74f3b92b19da5e606d737d339a9679",
            "2211.03277_a82437031a32b1da38218b1ac6f9ca67": "2211.03277_4829262cecb9828817b33e0f9c907f91",
            "2211.03277_3d87a6da5dfbed5a618bb52ba3988584": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_426c6cf041e0bbe2a6c5db178f658725": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_5b61fd210b48229b9dd8ba1d532b9c51": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_615c0cec8dfba380af481f263da01a88": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_9cce0993608ce2519c03733928550a0a": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_fde585210a12ce464e2568f6f14828e6": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_a74da44697ee4cb750625153c2472132": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_20c2575bd366455560bcb67fe2466a86": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_afc76d99cdc92e51d456d643fe442848": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_3f03b7e57b6337668bab8ac1f1557f4a": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_f7b6af2a3e53aadcad704f78758a1390": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_61d7a2454ed31953faf1e368c66c97bb": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_bddbda7e9f1f9ee7f581612dbe9b145d": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_c95b787e6feeb91938a5f932a898d5ae": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_e5781e48d431154723cfabcbdcc158b3": "2211.03277_a82437031a32b1da38218b1ac6f9ca67",
            "2211.03277_510256e896bff115b965cad513311620": "2211.03277_4829262cecb9828817b33e0f9c907f91",
            "2211.03277_7cb5a1b9371fcfa0949638f60ebd574e": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_05fbb6792748c449bd8177660c851bd3": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_97fdce2ef556f57ac0ceef65e31a12da": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_1d001f3e33bb870b78b01fdddc30562c": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_59e387e8f9afb4868793fef533255cb6": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_f0384883c3d0b1f4af159db4d1f6ff7a": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_ae1711311d060d0ae99e80750b890d29": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_217bb768154d817307473a47fe166c53": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_a08c0789c5bd0c5d2e3ee48a3f490346": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_8d8a3cb6edd99205162807709244df24": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_2a19a64deb4aae1ada72a5017d6cdbdb": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_b3fe0487ab3aef422024ae2ebb542446": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_94fcda9c354881a2fb12a92f6e2c23af": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_481d76a31c3c0e5d6c364c9b6d5f0f23": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_1e3faf0e27b0a02101b4150b2ad1c2a4": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_39f0f282048b4d68a86449ce9ede679a": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_c4ca4238a0b923820dcc509a6f75849b": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_219bd344ff3928edd4643b105c1fb4b6": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_e1314e9eb517591af59538c62aca534e": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_b85a170b4bfe2f6d0c3305e8a358399a": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_9aa0d14da3db564b4afd93c4eb2f7601": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_ff481847ff67066fbde0f235c908c6e7": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_9bb0cec5145b2eabb4e82c11092025a8": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_d0e597ee8e225ae47c4ce404178f2eb4": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_79b50d241b2e5dc18b80c08291a03414": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_2a78dc22f1702b7f2fec0db7b7b94aea": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_aeafd46ac1f6450f212f2c9e88bf6a6e": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_620445eba1172d5e8e3c62c769d4ec68": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_3d474227b85f2c565e68e6d3893b13ca": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_9c445f6c2e219be7ce60a2ac7200bdca": "2211.03277_510256e896bff115b965cad513311620",
            "2211.03277_0d6c7f9e503f179d0b118217583f0c8d": "2211.03277_4829262cecb9828817b33e0f9c907f91",
            "2211.03277_51c45b795d5d18a3e4e0c37e8b20a141": "2211.03277_0d6c7f9e503f179d0b118217583f0c8d",
            "2211.03277_d7d845dbd7581da4e454cc4a92dc3bf4": "2211.03277_0d6c7f9e503f179d0b118217583f0c8d",
            "2211.03277_733804907e375cda5bd03d916200db56": "2211.03277_0d6c7f9e503f179d0b118217583f0c8d",
            "2211.03277_0e9a6545e6ca782413665469d6adf773": "2211.03277_0d6c7f9e503f179d0b118217583f0c8d",
            "2211.03277_73de5ed3384d04a5c62a689ded6aa086": "2211.03277_0d6c7f9e503f179d0b118217583f0c8d",
            "2211.03277_25ad0164f9750f602f235ac162535abd": "2211.03277_0d6c7f9e503f179d0b118217583f0c8d",
            "2211.03277_151e0b98f6e9cdb16aef8b4b0f978b6d": "2211.03277_0d6c7f9e503f179d0b118217583f0c8d",
            "2211.03277_6e1a0a0909ce1acbb1a16ce1e469f82d": "2211.03277_0d6c7f9e503f179d0b118217583f0c8d",
            "2211.03277_ec936611bad8c20896029f9c28d7b22b": "2211.03277_0d6c7f9e503f179d0b118217583f0c8d",
            "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56": "2211.03277_0d6c7f9e503f179d0b118217583f0c8d",
            "2211.03277_f41378dc2401a066a07547accdc467d0": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_7d7727f2ca22610bd137d9400ff98104": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_84d8298209171837bf0e668735a9ba23": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_b9867355fd3227b83d4aad7449e7796c": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_831855e5e1ad2c68998023962f4321ae": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_7ac96855ed978ee4f1127550d560c84d": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_2fbaae84dad5dcd9eb09342a81a778d6": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_08a85228389e455b14164ef2e1211640": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_1c5b52ca31ed23054c9495237d4be5d9": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_1cb92407de28bdf19d782f89345ca17b": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_c796c1c7712dee1f3069fd7376c68ca5": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_7092bfce241abfcfdae0533c45abc91d": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_2ad6218af6d2b49974b696940948cd57": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_9d56d742982a3d188df203814f814c90": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_9d084d21b14bd896ff888fbc453bb644": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_f46a65365156738f14482c03fd283dc2": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_64ea55fe19c97fc0525a647ee61e55cf": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_6e6f2497b6a2a80e05644807fb796228": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_d0474cc26cc51c9ee5393f68dd586e0c": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_f4c6ed20aa143995745ff84775e8db7d": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_778116e52bdb779b7909a708875f3018": "2211.03277_917ebf194b1cb9440cd9fa2c84e4be56",
            "2211.03277_a5524984347c586c9c004b70d8c9ea90": "2211.03277_0d6c7f9e503f179d0b118217583f0c8d",
            "2211.03277_b64541c9afebb6867724faf9ebb3c4b5": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_3ba5f178c65f0189efe5b9031375eb14": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_b808fa58cd05f24536649751b9ce9e9d": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_c91bb09611c93cd65b5f0e83a47f33d6": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_c8bdde3e512f7823208c78cf591e73b9": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_2fca00fb889580a665490b891d1f3b4d": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_093bb0f659d1b687c9e7d32480fc5db4": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_b0a145f6be444e7186989eb3822afe48": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_f818e2f79ac892a6b1cd39d9aef6a4d8": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_45bdbcec874f8b34d56ad2fada005149": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_06a945180de124d3e044364474f3da19": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_56a2958c78fb0dd2ff40237ad8d959b0": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_b5aee532b982a1d6f56895f201ebe87e": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_01a5fd0cfd7245968e2895cc93a51dc2": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_4353a8d44172b608b2bb3d2f45d128af": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_f90c01189acf24cac5196da77ddd26a8": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_a2e3330a6397ebbf27eaa9fab8b791e4": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_7af92c12a484bda9e4e59e96796be13e": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_7f415a009670a1a1dca9546342fc27c7": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_176bb539c91849e35d552b98159767b2": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_9d8d91820d515fd6e0a27b168579e739": "2211.03277_a5524984347c586c9c004b70d8c9ea90",
            "2211.03277_74799250ff5585a24184a79f12bf080a": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_007dcc215740ecc7be1eea157e40f504": "2211.03277_74799250ff5585a24184a79f12bf080a",
            "2211.03277_810a7ad815764cc3e3baede4417f4804": "2211.03277_007dcc215740ecc7be1eea157e40f504",
            "2211.03277_70be3361c625be3dfa44efc081d69c9f": "2211.03277_007dcc215740ecc7be1eea157e40f504",
            "2211.03277_ddb2819e03067f9573edd94cc9f51034": "2211.03277_007dcc215740ecc7be1eea157e40f504",
            "2211.03277_0a71397b235c031f33ba129a6d9df152": "2211.03277_007dcc215740ecc7be1eea157e40f504",
            "2211.03277_5be809ece1d2e0c5e07c8f7512e6bb06": "2211.03277_007dcc215740ecc7be1eea157e40f504",
            "2211.03277_f0e5e01a67a2b5704ec5b26e0e8552b3": "2211.03277_007dcc215740ecc7be1eea157e40f504",
            "2211.03277_c0ff19b36bf67ce53d2e773c61faf931": "2211.03277_007dcc215740ecc7be1eea157e40f504",
            "2211.03277_12828d228d4dc700d81140ad548d9844": "2211.03277_007dcc215740ecc7be1eea157e40f504",
            "2211.03277_0a8631a299c09f9f7bb6337b20f482d1": "2211.03277_007dcc215740ecc7be1eea157e40f504",
            "2211.03277_468a63f02961d4774f93bd4324f18ffb": "2211.03277_007dcc215740ecc7be1eea157e40f504",
            "2211.03277_588f4221b1b2448eac5783078fd99421": "2211.03277_007dcc215740ecc7be1eea157e40f504",
            "2211.03277_e2f96952cd10a33dddec519e96b047e4": "2211.03277_007dcc215740ecc7be1eea157e40f504",
            "2211.03277_9f9eab64930954640b1fcf86febdcaa7": "2211.03277_007dcc215740ecc7be1eea157e40f504",
            "2211.03277_4c8d99223325c485d536ff611567a2de": "2211.03277_007dcc215740ecc7be1eea157e40f504",
            "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979": "2211.03277_74799250ff5585a24184a79f12bf080a",
            "2211.03277_9ba020bcf77737bec46aae3df60caddf": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_1f579a679fff54d8936a80f3573b6cee": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_bd462465abaf81d4b620327a36c94fe6": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_5975ae121d279002828b36cfdf949d7f": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_cc978df04c4f160fc936d9a48ada43e2": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_4a4c4ab9cc85136ec98497a5c5e9c677": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_68b00eff25cc53b2e1940a272cbd2c40": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_84937c0866effefa446b0ec51d16a48a": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_64ebc0f9810c81d8ab690740209c2538": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_c3ad332c279d8a23d83c63028347e175": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_18dcba2c80006fd8290be7a85983dfa2": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_04b8947e99ed00775a3fbb62313ae121": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_0d5222d9b62acc623d3e904916a752ae": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_8129442325c18fe41c8a0ad9c9a57b1e": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_5a8ba7c5c7c715ffc39bb5f7abb6da82": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_e3800a9b79bdfd6bfd99dd4cff996df3": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_943b7024247560e30ca2ee5596868304": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_cddaf805d0070e0eaa5a847d1cfa71ab": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_3ee4c0098a9bcc36be1fa1e9133bcec2": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_449a218d97353d345dfc3f3311bc040d": "2211.03277_5c6bd6f3e488e695d79d9b31eaadc979",
            "2211.03277_6f8b794f3246b0c1e1780bb4d4d5dc53": "2211.03277_e353dbe42c8654f33588d4da0b517469",
            "2211.03277_bd0fd529ee09874109168ff3dfe89135": "2211.03277_6f8b794f3246b0c1e1780bb4d4d5dc53",
            "2211.03277_a0c67423f269fae5c41b1bbe5b72c94c": "2211.03277_6f8b794f3246b0c1e1780bb4d4d5dc53",
            "2211.03277_131d8f7b10b4cc29b7bfc7d56395b6d9": "2211.03277_6f8b794f3246b0c1e1780bb4d4d5dc53",
            "2211.03277_0f3a57545d74bc7da988e49ae1952061": "2211.03277_6f8b794f3246b0c1e1780bb4d4d5dc53",
            "2211.03277_6e84b3f90940222c0d6e7f686695b570": "2211.03277_6f8b794f3246b0c1e1780bb4d4d5dc53",
            "2211.03277_e660a4448089b9fa1c1f10fbac1ac8fa": "2211.03277_6f8b794f3246b0c1e1780bb4d4d5dc53",
            "2211.03277_f892edbb69eb55dfe5c7eb49bb08cbbc": "2211.03277_6f8b794f3246b0c1e1780bb4d4d5dc53"
        }
    }
}