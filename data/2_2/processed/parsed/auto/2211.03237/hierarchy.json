{
    "elements": {
        "2211.03237_f6cd9a7a24fe51353f5aa75b23f78879": "Document Root 2211.03237",
        "2211.03237_708aede88adbc288358e60e7a474bb44": "=1",
        "2211.03237_e353dbe42c8654f33588d4da0b517469": "Abstract",
        "2211.03237_3241f27239eef5f6d6db4f504b105cca": "Attention-based autoregressive models have achieved state-of-the-art performance in various sequence-to-sequence tasks, including Text-To-Speech (TTS) and Neural Machine Translation (NMT), but can be difficult to train",
        "2211.03237_8e86672508b516b6604c3db80f8ff40e": "The standard training approach, teacher forcing, guides a model with the reference back-history",
        "2211.03237_9fc20df0f4ee1dd08e974d43f560206e": "During inference, the generated back-history must be used",
        "2211.03237_ca6d8b3c86460dc12a760af48eccb20c": "This mismatch limits the evaluation performance",
        "2211.03237_62e9887f9136c6b7f1cb8614a8889a46": "Attention forcing has been introduced to address the mismatch, guiding the model with the generated back-history and reference attention",
        "2211.03237_16196f5fbd2ba6c3492306449db180b4": "While successful in tasks with continuous outputs like TTS, attention forcing faces additional challenges in tasks with discrete outputs like NMT",
        "2211.03237_72dd38f7baf9875c1c0b89ca4b845a99": "This paper introduces the two extensions of attention forcing to tackle these challenges. (1) Scheduled attention forcing automatically turns attention forcing on and off, which is essential for tasks with discrete outputs. (2) Parallel attention forcing makes training parallel, and is applicable to Transformer-based models",
        "2211.03237_15d7fe1c5c8161e56d18cf133e8f341f": "The experiments show that the proposed approaches improve the performance of models based on RNNs and Transformers.",
        "2211.03237_0b79795d3efc95b9976c7c5b933afce2": "Introduction",
        "2211.03237_01d5ab6bc12e63f1b292ac1b89a3998a": "Attention-based models are good at connecting sequences of different length, and have achieved state-of-the-art performance in various sequence-to-sequence (seq2seq) tasks",
        "2211.03237_d7dc853cd8cb39098559e64628d15189": "\\cite{vaswani2017attention, tay2020efficient}",
        "2211.03237_2d5a453a758bb3a21c3064b748d26ffa": "Here the term performance refers to the overall quality of the output sequences, e.g. word error rate in Automatic Speech Recognition (ASR)",
        "2211.03237_67638a16078751387ac15daeb140632a": "On the other hand, these models can be difficult to train",
        "2211.03237_22fee921c75058ca2fec455b125ed9d1": "\\cite{bengio2015scheduled}",
        "2211.03237_03e94cd028bb5212f2db3a0dcd179c3e": "From a probabilistic perspective, seq2seq models estimate the probability of the output sequence conditioned on the input sequence",
        "2211.03237_11f19af116b87495e3a0c5814798ff19": "To achieve more accurate estimation, the models are often autoregressive",
        "2211.03237_64091d73ee714d72068ce8fec4251d94": "\\cite{chen2018best}",
        "2211.03237_bf85162521103c8f6c2df000ae3a60d1": "The standard training approach, teacher forcing, guides a model with reference back-history during training",
        "2211.03237_6055ef10a36bb16a03901454b9b2b563": "This makes the model unlikely to recover from its mistakes during inference, where the model operates in free running mode, and the reference output is replaced by the generated output",
        "2211.03237_b5c1acc169f101031007064c4886401c": "This problem is referred to as exposure bias",
        "2211.03237_1e8f90d848b9da7b908fd637dbb55e04": "\\cite{ranzato2015sequence}",
        "2211.03237_fb0f3979fbed80049355fe4075f71a54": "Many approaches have been introduced to address exposure bias, and will be described in section",
        "2211.03237_2e69b3b9a2d6be9f90a5708c6c3ebe18": "Attention forcing is a simple and effective option",
        "2211.03237_c61caa1c934aebfea649e4ada8958d66": "\\cite{dou2020attention}",
        "2211.03237_18c36b857d96959aebc9de049a18f51f": "The idea is to guide the model with the generated output history and reference attention",
        "2211.03237_3edd333405a512d5f33659101b84eeca": "While successful in TTS, attention forcing faces additional challenges when it comes to tasks with discrete outputs",
        "2211.03237_166e1990f11be36d115eb58fd54ba22a": "\\cite{dou2019attention}",
        "2211.03237_d71ba0b53b33d4a8dbf93a8d6286e647": ", and models such as Transformers",
        "2211.03237_2fdb4d0205b168a4fc84398a3e37cd72": "\\cite{vaswani2017attention}",
        "2211.03237_cb228d402723c608530950de36efd3b1": "To tackle these challenges, this paper introduces scheduled attention forcing in section",
        "2211.03237_25228685733f4e1b46af3e00bded04c2": ", and parallel attention forcing in section",
        "2211.03237_0a7adb44bb57b55f7408f935a55a7210": "The experiments in section",
        "2211.03237_63e39b5ff68853943e1dc24c87f24356": "show that these approaches improve strong NMT models based on RNNs and Transformers.",
        "2211.03237_66396a60f38aa383916a543795cd64f3": "Attention-based sequence-to-sequence generation",
        "2211.03237_a8f99d30a5d8298c5cf25ce7c4274f50": "Sequence-to-sequence generation can be defined as the task of mapping an input sequence",
        "2211.03237_6e99ad71ffe0ad88cc3b9f6c0f04acd6": "$\\bm{x}_{1:L}$",
        "2211.03237_6f590e615b75efd9a0c2382b8eb4b9e0": "to an output sequence",
        "2211.03237_33ec7fb8ac34096caab6864e6d59b368": "$\\bm{y}_{1:T}$",
        "2211.03237_5058f1af8388633f609cadb75a75dc9d": ".",
        "2211.03237_e4be693f65518ed2163a88d73790db2e": "From a probabilistic perspective, a model",
        "2211.03237_0d3b3ca33334697a4e51cec71663e64d": "$\\bm{\\theta}$",
        "2211.03237_6b85c501ea4b4dbf14f5e0f7c2e7909f": "estimates the distribution of",
        "2211.03237_b9f4c1cc743af7b09673ba380390d2f1": "given",
        "2211.03237_9431aa15332f1042bebddb1aa9c08b8f": ", which can be factorized into token distributions:",
        "2211.03237_1cd4f9dd1a43b226bee040f5d88def5a": "$p(\\bm{y}_{1:T}|\\bm{x}_{1:L}; \\bm{\\theta}) = \\prod_{t=1}^{T} p(\\bm{y}_{t} | \\bm{y}_{1:t-1}, \\bm{x}_{1:L}; \\bm{\\theta})$",
        "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16": "Encoder-attention-decoder architecture",
        "2211.03237_8a1fa078ab2ec4c143ca552c5f21c5b9": "Attention-based seq2seq models usually have the encoder-attention-decoder architecture",
        "2211.03237_46a94433f185c5aa4c914721b41a28ed": "\\cite{lewis2020bart, tay2020efficient}",
        "2211.03237_666df2f48cd16f38212c093999248769": "Figure",
        "2211.03237_dab640951ec26735330cca5b97f8106f": "shows the architecture",
        "2211.03237_6cdca64ac0011e6fca70d65cf9f21f86": "The distribution of a token is conditioned on the back-history",
        "2211.03237_d8baf8d57c3de7a15243f027ac07bb40": "$\\bm{y}_{1:t-1}$",
        "2211.03237_bc8da6433819c46eabcdba987b294264": ", input sequence",
        "2211.03237_ccdd68c3a34b5a8d868df287b462aa02": "and an attention map",
        "2211.03237_f91652aa0e855c9ac052d8159410edf2": "$\\bm{\\alpha}_{1:T}$",
        "2211.03237_853ae90f0351324bd73ea615e6487517": ":",
        "2211.03237_83189372d03fa78432cfe2b53fb75ab0": "\\begin{align}\np(\\bm{y}_{t}|\\bm{y}_{1:t-1}, \\bm{x}_{1:L}; \\bm{\\theta}) &\\approx p(\\bm{y}_{t}|\\bm{y}_{1:t-1}, \\bm{\\alpha}_{t}, \\bm{x}_{1:L}; \\bm{\\theta}) \\nonumber \\\\\n&\\approx p(\\bm{y}_{t}|\\bm{s}_{t}, \\bm{c}_{t}; \\bm{\\theta}_{y}) \\label{eq:py_general_p} \n\\end{align}",
        "2211.03237_567904efe9e64d9faf3e41ef402cb568": "where",
        "2211.03237_1c5db0dfefabb38977fb3d06d24e16fd": "$\\bm{\\theta} = \\{\\bm{\\theta}_{y}, \\bm{\\theta}_{s}, \\bm{\\theta}_{\\alpha}, \\bm{\\theta}_{h}\\}$",
        "2211.03237_9eecb7db59d16c80417c72d1e1f4fbf1": ";",
        "2211.03237_a06451f378b7162f45e402dd9cee52bf": "$\\bm{s}_{t}$",
        "2211.03237_7a02aa022b690e7166ad763b440fb763": "is a state vector representing",
        "2211.03237_8a0e2b549d223aba55a866c38d1c9275": ", and",
        "2211.03237_b7f8f7fa95d6859a2ddc9e4406d6e7d5": "$\\bm{c}_{t}$",
        "2211.03237_0852a31655e9da6e4295d5d00d18ecde": "is a context vector summarizing",
        "2211.03237_11023aa32abfe6132432d7ee8b07fce7": "for time step",
        "2211.03237_4f4f4e395762a3af4575de74c019ebb5": "$t$",
        "2211.03237_338a7cff4d1143ffe90b346fc674c1d8": "The discussions in this paper are agnostic to the form of attention",
        "2211.03237_f7a0d8a09223195ad85a5ba1fe08a5bc": "The following equations give a detailed example about how",
        "2211.03237_d7f0de3f762c11c90c71962455d7d3f7": "$\\bm{\\alpha}_{t}$",
        "2211.03237_c0cb5f0fcf239ab3d9c1fcd31fff1efc": ",",
        "2211.03237_be5d5d37542d75f93a87094459f76678": "and",
        "2211.03237_b836826b1c868296f337377b2b61e1ce": "can be computed:",
        "2211.03237_b14a7b8059d9c055954c92674ce60032": "_",
        "2211.03237_f64f79ad17c83193d7b252e16f9cdc02": "= f(",
        "2211.03237_9371d7a2e3ae86a00aab4771e39d255d": ")",
        "2211.03237_43ec3e5dee6e706af7766fffea512721": "=",
        "2211.03237_7e6a2afe551e067a75fafacf47a6d981": "^",
        "2211.03237_49406e834d03adedb812f1c1ac1091fe": "p(",
        "2211.03237_b99834bc19bbad24580b3adfa04fb947": "|",
        "2211.03237_02976acc6c3c7e0c690d5424c0b43d93": "The encoder maps",
        "2211.03237_01b6e20344b68835c5ed1ddedf20d531": "to",
        "2211.03237_e3c884d6dc1c82cb4fe4c8b2b2aac54d": "$\\bm{h}_{1:L}$",
        "2211.03237_6ea8bee408f5049beb06c22a73991f17": ", considering the entire input sequence;",
        "2211.03237_bc45988429e68600175585ade55e3353": "summarizes",
        "2211.03237_ffd9a42674409f2a92e35632a71f4fba": ", considering only the past",
        "2211.03237_370964eec07cba8c52acb1ce1b35ab6a": "With",
        "2211.03237_55c752a5bd71596bc0aff31531053f03": ", the attention mechanism computes",
        "2211.03237_6fcc1b8aaffec3bc3b754c3a227d052e": ", and then",
        "2211.03237_78b3dce30fd587853f3f25d209969c7a": "The decoder estimates a distribution based on",
        "2211.03237_beb70c9b5a6b5d3907385f5f67e9cf87": ", and optionally generates an output token",
        "2211.03237_c699133b4b8bf5882968ea5c3e8cde17": "$\\hat{\\bm{y}}_{t}$",
        "2211.03237_f9ff7bbdbb85132cc4136998f62a4079": "\\begin{figure}\n\\centering\n\\includegraphics[width=7.5cm]{Figs/model_ead.png}\n\\caption{An attention-based encoder-decoder model, in teacher forcing mode, at decoding step $t$; a circle depicts a token, and a rounded square a distribution.}\\label{fig:EAD}\n\\end{figure}",
        "2211.03237_d5bf96ce448b27486e749926c12bf073": "Inference and training",
        "2211.03237_0393ef102cbbaa8400333c3d210c8f4e": "During inference, given an input",
        "2211.03237_79b4a1afcb3cee6f6acd1b00a8726504": ", the output",
        "2211.03237_6653fe39aabcc3c69d5d6ce236c86c62": "$\\hat{\\bm{y}}_{1:T}$",
        "2211.03237_9480c86853f1a6ad27284aad8ca9c4fb": "can be obtained from the distribution estimated by the model",
        "2211.03237_eac6fedc4e717ecaac78f60be5299720": "$\\hat{\\bm{y}}_{1:T} = \\underset{\\bm{y}_{1:T}}{\\mathrm{argmax}} \\, p( \\bm{y}_{1:T} | \\bm{x}_{1:L}; \\bm{\\theta})$",
        "2211.03237_40a45b154f0e0e2c18d037d5182e7477": "The exact search is often too expensive and approximated by greedy search for continuous output, or beam search for discrete output",
        "2211.03237_6672a80bab8b49d021baa7a9d3cebbab": "Conceptually, the model is trained to learn the natural distribution, e.g. through minimizing the KL-divergence between the natural distribution",
        "2211.03237_a3bfac1a31379f869e8886dcdae41eff": "$p(\\bm{y}_{1:T}|\\bm{x}_{1:L})$",
        "2211.03237_e52cb29c369bb4353f0de3b1ef035528": "and the estimated distribution",
        "2211.03237_6003bfb79a3176ab654015f82020cfd7": "$p(\\bm{y}_{1:T} | \\bm{x}_{1:L}; \\bm{\\theta})$",
        "2211.03237_86c08392411de17249256c37cd4ecd7a": "In practice, this can be approximated by minimizing the Negative Log-Likelihood (NLL) over some training data",
        "2211.03237_d3ac4d923f9a8a9293aaffea0e41c2e0": "$\\{\\bm{y}^{(n)}_{1:T}, \\bm{x}^{(n)}_{1:L}\\}_{1}^{N}$",
        "2211.03237_750af08bcae0e43cd6658e1e824e659f": ", sampled from the natural distribution:",
        "2211.03237_1d90bc695b9ea4faf24dec4329241f1d": "\\begin{align}\n\\mathcal{L}(\\bm{\\theta}) &= \\E_{\\scalebox{0.8}{$\\bm{x}_{1:L}$}} \\mathrm{KL} \\big(p(\\bm{y}_{1:T}|\\bm{x}_{1:L}) || p(\\bm{y}_{1:T}|\\bm{x}_{1:L}; \\bm{\\theta}) \\big) \\nonumber \\\\\n&\\propto - \\textstyle \\sum_{n=1}^{N} \\log p(\\bm{y}^{(n)}_{1:T}|\\bm{x}^{(n)}_{1:L}; \\bm{\\theta}) \\label{eq:pb_train}\n\\end{align}",
        "2211.03237_eb0e1f947572817a62cbeaf85f90c630": "$\\mathcal{L}(\\bm{\\theta})$",
        "2211.03237_00ba8ba9b9376e5c9780798dd79d10ff": "denotes the loss;",
        "2211.03237_f9c4988898e7f532b9f826a75014ed3c": "$N$",
        "2211.03237_f0de468c61904ab040a628221ffc2979": "denotes the size of the training dataset;",
        "2211.03237_55a049b8f161ae7cfeb0197d75aff967": "$n$",
        "2211.03237_f9623005bc53f64036ca95afa3c345b2": "denotes the data index",
        "2211.03237_22a473b274a614f54b4f8593e2a70218": "To simplify the notation,",
        "2211.03237_07af1386a32cb1bb5028273e14af418d": "is omitted for the length of the sequences, although they also vary with",
        "2211.03237_703c41b12d7c079491ed89f4a299ad03": "In the following sections, the sum over the training set will also be omitted",
        "2211.03237_cfca3c24ea3ae7f3b484082ff13a5ed3": "A key question here is how to compute the token distribution",
        "2211.03237_1fde9f7dafa6e21d5ea510823fb786b7": "$p(\\bm{y}_{t}|\\bm{y}_{1:t-1}, \\bm{x}_{1:L}; \\bm{\\theta})$",
        "2211.03237_3373f5609b6acf68efe87b8d25a6bfc4": "For the most standard training approach, teacher forcing, the token distribution is computed with the reference output history",
        "2211.03237_6d213c0b980e809712eb087963a37d5e": "at each time step",
        "2211.03237_4d288677725268e0014d9865d1ff3bbc": "The loss can be written as:",
        "2211.03237_431fc985bbe53efb695b2f19cc88c14c": "\\begin{equation} \\label{eq:loss_y_iclr}\n    \\mathcal{L}_{y}(\\bm{\\theta}) \n    = - \\textstyle \\sum_{t=1}^{T} \\log p(\\bm{y}_{t}| \\bm{y}_{1:t-1}, \\bm{x}_{1:L}; \\bm{\\theta})\n\\end{equation}",
        "2211.03237_f1cd8e0c8a143ef40d6e62b2b8f046ae": "Despite its advantages such as parallel training",
        "2211.03237_92f3b7cfc13afef13490c2690f4b0496": "\\cite{dou2022improving}",
        "2211.03237_731ea7d8303734bf57ca113183dbf0cb": ", teacher forcing suffers from exposure bias: during training, the model is guided by the reference output history; during inference, however, the model runs in free running mode, where the generated output history is used",
        "2211.03237_ccd909a0789143de6eb610060b13fe41": "This mismatch leads to errors that can accumulate along the inference process",
        "2211.03237_88128fec49c5d6ac46e92de51554bed1": "There are mainly two lines of research addressing exposure bias",
        "2211.03237_74a18ec85d73194af602e241793aab1f": "Scheduled sampling",
        "2211.03237_5f9d6b8ae6b216c84b0e6c06ae769f40": "and professor forcing",
        "2211.03237_699903aac1ea4b2d1eb70fb7294d39b6": "\\cite{lamb2016professor}",
        "2211.03237_3082492e724c3157ce17ccd6bb349068": "are prominent examples along the first line",
        "2211.03237_f825393c3553171cfd15645cbdd09a8a": "These approaches guide a model with both the reference and the generated output history, and the goal is to learn the data distribution",
        "2211.03237_29bf412631dde69f0f02d670efd91bed": "To facilitate convergence, they often depend on a heuristic schedule or an auxiliary classifier, which can be difficult to design and tune",
        "2211.03237_f69263e6b61cc3a0bbb409a29dcb2283": "\\cite{guo2019new}",
        "2211.03237_d4d7423d7dcaf93da91675229e64f611": "The second line is a series of sequence-level training approaches, leveraging reinforcement learning",
        "2211.03237_94fb103d2ba768b1e16ef21db0e575fb": ", minimum risk training",
        "2211.03237_99af422c6072c0778f00dd45a86b17d3": "\\cite{shen2016minimum}",
        "2211.03237_bc46982331ec624563254dca0f419d31": "or generative adversarial training",
        "2211.03237_c070da04683cf07cceb271c6bb61a1a5": "\\cite{yu2017seqgan}",
        "2211.03237_b10bb21c07aa31df1aa86e1a10ca0560": "Theses approaches guide a model with the generated output history",
        "2211.03237_424d29f00c30d23b5dac6d32c39f986d": "During training, the model operates in free running mode, and the goal is not to generate the reference output, but to optimize a sequence-level loss",
        "2211.03237_e0685fd43e7989acc88a9c07e78445dc": "However, many tasks do not have well established sequence-level objective metrics",
        "2211.03237_ddb367a44cb8b6803222ba8ee72a0b10": "Examples include voice conversion, machine translation and text summarization",
        "2211.03237_7f738ec54989b7b7190b1a3356173539": "\\cite{tay2020efficient}",
        "2211.03237_28d2e9ad717f001327044b49fbd5fd50": "Both lines of research require sequentially generating output sequences during training",
        "2211.03237_116a94f691be516e15db92ef16dfbd9e": "In recent years, Transformers",
        "2211.03237_c0bd028b228d5042d917103067cde39c": "have been widely used, and parallel training has been essential",
        "2211.03237_f08e83425de0977de174774ebb80c75f": "To efficiently generate output sequences from Transformer-based models, an approximation scheme",
        "2211.03237_5b46bfc1ea33ca2d10fe06c883316aa0": "\\cite{duckworth2019parallel}",
        "2211.03237_41f8ca2d768e4739d0861d753766331d": "has been proposed to parallelize scheduled sampling.",
        "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0": "Attention forcing 2",
        "2211.03237_8d6abb9ea04626038fc88f4cb5d5a9ca": "\\begin{figure}\n\\centering\n\\includegraphics[width=7.5cm]{Figs/model_TF_AF.png}\n\\caption{Attention forcing; the solid blocks are the attention forcing model $\\hat{\\bm{\\theta}}$; the dashed blocks the teacher forcing model $\\bm{\\theta}$. During inference, $\\hat{\\bm{\\theta}}$ runs in free running mode without $\\bm{\\theta}$.} \\label{fig:AF_coarse}\n\\end{figure}",
        "2211.03237_e0510a8b5359e21a3d81a4b974205884": "This section will revisit the general framework of attention forcing, and then analyze its challenges and introduce extensions to tackle them",
        "2211.03237_996b8b7c1ec55dd743396d6e7d05f6cf": "The basic idea of attention forcing",
        "2211.03237_291c38084b2bb6256f846d6a977cdecf": "is to train the model with the generated output and reference attention",
        "2211.03237_66a8b53e0c3f158793c8865bb53252da": "The generated output helps addressing the exposure bias, and reference attention helps with convergence",
        "2211.03237_cae6404c4aecf46684930fe2a86676a6": "Let",
        "2211.03237_68f6d1e2a5af193b053aec5877fbaf11": "denote a standard attention-based model, trained with teacher forcing",
        "2211.03237_cf14d225b7a2bb7cada4004da229e118": "$\\hat{\\bm{\\theta}}$",
        "2211.03237_e34d147f09f2c192d5f99705c1bcebb4": "denote a model with the same structure, but trained with attention forcing, and later used for inference",
        "2211.03237_5c7ea5e82198880f2e4f16b99d412932": "illustrates attention forcing",
        "2211.03237_0671de4fb80046863460e326e7098f6f": "In attention forcing mode, equation",
        "2211.03237_aa5114a8e2cfaa500f251a93accb98e0": "becomes:",
        "2211.03237_aeba388a64a6db929038580e30a58f4f": "\\begin{align} \\label{eq:py_A}\np(\\bm{y}_{t}|\\bm{y}_{1:t-1}, \\bm{x}_{1:L}; \\hat{\\bm{\\theta}}) &\\approx p(\\bm{y}_{t}| \\hat{\\bm{y}}_{1:t-1}, \\bm{\\alpha}_{t}, \\bm{x}_{1:L}; \\hat{\\bm{\\theta}}) \\nonumber \\\\\n&\\approx p(\\bm{y}_{t} | \\hat{\\bm{s}}_{t}, \\hat{\\bm{c}}_{t}; \\hat{\\bm{\\theta}}_{y})\n\\end{align}",
        "2211.03237_e37b7710659e462207e7a33bb5e31cb0": "$\\hat{\\bm{s}}_{t}$",
        "2211.03237_a55164aad956cfd0137faca10edca6ab": "$\\hat{\\bm{c}}_{t}$",
        "2211.03237_c62b4db3dc93b187c799f97bae7a68ed": "denote the state vector and context vector generated by",
        "2211.03237_be2efcae3a3ae8b6d0866500fb4be872": "Details of attention forcing are in the following equations:",
        "2211.03237_4069a4ae5ea2121af744af6cfb8a0c32": "The right side of the equations",
        "2211.03237_6fb73b2a3f93a9dbbf42e58025bde35d": ", as well as equations",
        "2211.03237_168c3b2a24a9bbdba43074daa99105c0": ", show how the attention forcing model",
        "2211.03237_ec95cb6e492bb4874f966d05f9255868": "operates",
        "2211.03237_3fce714888c89e7d09799dcc83f4e880": "The decoder state",
        "2211.03237_21c332361d9660c7f7f24d4ae87b745a": "is computed with",
        "2211.03237_58ed3f084ab3ca6fe5b0c84afa67df7d": "$\\hat{\\bm{y}}_{1:t-1}$",
        "2211.03237_f4f89ab557423d1ba7fc934127a0352a": "While an alignment",
        "2211.03237_84c6e4a5f7953b496bb34868c19256f0": "$\\hat{\\bm{\\alpha}}_{t}$",
        "2211.03237_fcec949e7d2d437851fc0307077b35df": "is generated by",
        "2211.03237_044e8ec1269926086e9adae1a387a1a8": ", it is not used by the decoder, because the context",
        "2211.03237_16405fbb0d7b1099e7cce3aa6d702b5f": "is computed with the reference alignment",
        "2211.03237_f80026cfa1fa85fcff5557b3a389d369": "One option of obtaining",
        "2211.03237_c62e957b0238892123faf9589bf3c81e": "is shown by the left side of equations",
        "2211.03237_1697bc01f30a143846822c42b005ffbe": ": to generate",
        "2211.03237_026bc1214b443cd38910c79b3e9ac5c2": "from a teacher forcing model",
        "2211.03237_6128884cad8f8db27c1273a5183f9046": "is trained in teacher forcing mode, and generates",
        "2211.03237_32afe1aee01096a8e566b32c4da1fbab": "in the same mode",
        "2211.03237_ff4d1898a6d2b0822f0c52e4d46e3a8b": "Although the reference output is used to compute the reference attention, it is not directly fed into the model, hence the model cannot rely too much on the back-history",
        "2211.03237_94cbdb2a2222db6554ef82488bf13704": "At the inference stage, the attention forcing model operates in free running mode, and equation",
        "2211.03237_962fdda7283421af0fd680e8972b4af7": "becomes",
        "2211.03237_c08e6c61f935ef98f82d80ac6bc16459": "$\\hat{\\bm{c}}_{t} = \\textstyle \\sum_{l=1}^{L} \\hat{\\alpha}_{t,l} \\hat{\\bm{h}}_{l}$",
        "2211.03237_543255c3c152606a6221ed8e57d518d6": "The decoder is guided by",
        "2211.03237_bd16210b1c55682486487dc3003ee0eb": ", instead of",
        "2211.03237_ba4a16823206b62a57632a7afd6d50b7": "During training, there are two objectives: to infer the reference output and to imitate the reference alignment",
        "2211.03237_261c1da936217d03c804d367e653a5e0": "This can be formulated as:",
        "2211.03237_c3b83358b13d7a35e301afa65b2be14f": "\\begin{align}\n\\mathcal{L}_{y,\\alpha} (\\hat{\\bm{\\theta}}) &= \\mathcal{L}_{y} (\\hat{\\bm{\\theta}}) + \\gamma \\mathcal{L}_{\\alpha} (\\hat{\\bm{\\theta}}) \\label{eq:loss_joint_AF}\\\\\n\\mathcal{L}_{y}(\\hat{\\bm{\\theta}}) &= -\\scalebox{1.0}{$\\sum_{t=1}^{T}$} \\log p(\\bm{y}{\\scriptscriptstyle _{t}}|\\hat{\\bm{y}}{\\scriptscriptstyle _{1:t-1}}, \\bm{\\alpha}{\\scriptscriptstyle _{t}}, \\bm{x}{\\scriptscriptstyle _{1:L}}; \\hat{\\bm{\\theta}}) \\nonumber\\\\ \n\\mathcal{L}_{\\alpha} (\\hat{\\bm{\\theta}}) &= \\scalebox{1.0}{$\\sum_{t=1}^{T}$} \\mathrm{KL}(\\bm{\\alpha}_{t}||\\hat{\\bm{\\alpha}}_{t}) \\nonumber\\\\\n&= \\textstyle \\sum_{t=1}^{T} \\sum_{l=1}^{L} \\alpha_{t,l} \\log \\frac{\\alpha_{t,l}} {\\hat{\\alpha}_{t,l}} \\nonumber\n\\end{align}",
        "2211.03237_a19bf05a563da2e92a5edc1b7af72422": "$\\mathcal{L}_{y}$",
        "2211.03237_264e9417c4a2a7cb05aee0152ef44ba5": "$\\mathcal{L}_{\\alpha}$",
        "2211.03237_3dbfda516eb9b4694662b192a52cb833": "respectively denote the loss over the output and the attention;",
        "2211.03237_11c596de17c342edeed29f489aa4b274": "$\\gamma$",
        "2211.03237_92f56902657db5adbfc5df9094d25296": "is a scaling factor",
        "2211.03237_a2f9496e2808fa7972186434d074c6c8": "As an alignment corresponds to a categorical distribution, KL-divergence is a natural difference metric",
        "2211.03237_e41b38e1c09e9ed3723f4de2e8bce085": "By default, the two models are trained separately.",
        "2211.03237_4baff9ce34b2f9be5a26615d8246a7d6": "is trained in teacher forcing mode, and then fixed to generate the reference attention.",
        "2211.03237_10b9b070d1deaa5ba66bfa405dcae725": "is trained with the joint loss",
        "2211.03237_caf254ed629b39dbb47b78abfeda0d0b": "$\\mathcal{L}_{y,\\alpha}$",
        "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1": "Scheduled attention forcing",
        "2211.03237_53cd0f1129ad097eacb779099855abf9": "When applying attention forcing, it is important to consider the nature of the attention connecting the input and output",
        "2211.03237_fb657c737cd3d95e75a82a98a8b78370": "For some tasks, the attention is expected to be monotonic, and there is only one type of valid attention maps, where the important positions roughly form a diagonal line",
        "2211.03237_3ed145b64a4d2750c626c73d8060146e": "Examples include ASR and TTS",
        "2211.03237_09831db76237540795cf532129d4d153": "For other tasks, there may be multiple valid modes of attention: the ordering of tokens can be changed while the output sequence remains correct",
        "2211.03237_0fb2e4dc7b704ee782e4143a68530f4e": "Examples include NMT and text summarization",
        "2211.03237_fc23a3f4b881bd96a1b46942b6961749": "If the model takes an ordering that is different from the reference output, the token-level losses will be misleading",
        "2211.03237_a78a6d8e2df67024b5753e07eac74ef0": "illustrates the problem with an NMT example",
        "2211.03237_63fad2880be947e5a98713377d1dc212": "Furthermore, there might be other issues such as grave mistakes",
        "2211.03237_18ec7a4a104cca2191f82d1fb74c5a45": "For tasks where the output is continuous,",
        "2211.03237_784f564a2ee656bb2cccae3dc072ec5b": "such as TTS and voice conversion, a small deviation from the reference output is usually not a serious problem",
        "2211.03237_b321a7ad3af2977966524b4a152f8758": "However, this is more serious for tasks where the output is discrete, such as NMT and text summarization",
        "2211.03237_1e2d7d9957163d571e2aa19f1c22b6eb": "During training, errors in the output history can be so serious that the token-level target is not appropriate, often due to misalignment between the generated output and the reference output",
        "2211.03237_30852bd40ab98fffb9069b44d5c59483": "To illustrate the problem, suppose the reference output is",
        "2211.03237_816e43212243ad04c3e56e658035f84f": ", and the model predicts",
        "2211.03237_0270020c992a2f7da14e0da445f71b79": "at the first time step",
        "2211.03237_538689e2ba2991dd90dd6d7a78b02644": "In this case, the next output should not be",
        "2211.03237_5bceb7dc10358b3dc7fa85abff06575f": "would be a more sensible target.",
        "2211.03237_dfa693049b5a87f66caa54ac66187647": "\\begin{figure}%\n    \\centering\n    \\includegraphics[width=5cm]{Figs/align_nmt.png}%\n    \\caption{Alignments: up) $\\bm{\\alpha}_{1:T}$ between the input and the reference output; down) $\\hat{\\bm{\\alpha}}_{1:T}$ between the input and the generated output. For the input \\emph{``je suis rentrÃ©e chez moi hier''}, the reference output is \\emph{``I went home yesterday''}. When using attention forcing, the model is guided by the generated back-history and outputs \\emph{``yesterday I went home''}. Here the alignment $\\bm{\\alpha}_{1:T}$ is not a sensible target for $\\hat{\\bm{\\alpha}}_{1:T}$.} \\label{align-TF-AF}\n\\end{figure}",
        "2211.03237_ded7df61a820c4972f1216f8b33013d6": "Scheduled attention forcing is proposed for applications where attention forcing, also referred to as",
        "2211.03237_8b7b05f7fa771bee94cb717141885a98": "vanilla attention forcing",
        "2211.03237_3e049feb2f67a1c94683fa169e2b69e3": ", may result in an inappropriate loss",
        "2211.03237_d416f2e1eb207e52d4c7175b6e4d9c89": "The basic idea is to automatically decide, for each input-output pair in the training data, whether vanilla attention forcing will be used",
        "2211.03237_5f9d319c7ff79a7a8e9683ff531e2de5": "This is realized by tracking the alignment between the reference and the generated output",
        "2211.03237_4d62fb3e0e46b4eb3681930a7a624b09": "If they are relatively well-aligned, vanilla attention forcing will be used, otherwise a more stable training mode will be used.",
        "2211.03237_df38dc0d0d3a10895aaf77aa674b6b3e": "\\begin{figure}\n\\centering\n\\includegraphics[width=7.5cm]{Figs/model_TF_AAF.png}\n\\caption{Scheduled attention forcing. Passes A and B share the same model parameters; only one of them will be used in back-propagation, depending on the data.} \\label{fig:AAF}\n\\end{figure}",
        "2211.03237_ace63f7576ba80f4166508122f0d53b3": "illustrates scheduled attention forcing",
        "2211.03237_d60e6e7e41e62a054ac81a8cb450f17d": "For each input sequence, the attention forcing model",
        "2211.03237_13f119b889a7d289fbe4e515a235508c": "takes two forward passes",
        "2211.03237_fb5a827d9694ede970b94fd907f48ddc": "Pass A is guided by the generated output history",
        "2211.03237_1b3572e4769f88b804e56ad615d96c53": ", which is the same as vanilla attention forcing",
        "2211.03237_5e3d27bfddb0faf90ac9dc1a1da25b74": "Pass B is guided by the reference output history",
        "2211.03237_2f692a1b42d97887a10cdc4fd439ee4b": "The reference attention is always used, so the context vector",
        "2211.03237_0bf5806281375d529ef0143e191b635b": "is the same in both passes",
        "2211.03237_6adc37fa8bee10b315fcc10d4fdda00f": "If memory permits, the two forward passes can be completed in parallel, resulting in no extra time",
        "2211.03237_466f7daabb4534ae7342c6a9a5a04669": "This can be formulated as follows",
        "2211.03237_13004378fa302b2165711e6ca149257a": "For vectors produced in pass A, the notation has the hat",
        "2211.03237_dd471a51ba375d64965d6bec1939bab6": "$\\hat{}$",
        "2211.03237_77fc4677b75316306583e68f22ffe9cb": "accent; the equivalent for pass B is the check",
        "2211.03237_ef6efc3d25d8730ddcf919e25806b1d7": "$\\check{}$",
        "2211.03237_ce9ead88dd6e34ccf1de0b64604c2b2f": "accent.",
        "2211.03237_b66aa7852804bc5cb725d176c23653d0": "Next, the choice of training mode is made at the sequence level",
        "2211.03237_786887572f6ef1c20f2d8177cb2f1639": "If",
        "2211.03237_4ae39003a757454bae50e5ebd2f1f04b": "$\\textstyle \\sum_{t=1}^{T} \\mathrm{KL}( \\bm{\\alpha}_{t} || \\hat{\\bm{\\alpha}}_{t}; \\hat{\\bm{\\theta}}) < \\lambda \\sum_{t=1}^{T} \\mathrm{KL}( \\bm{\\alpha}_{t} || \\check{\\bm{\\alpha}}_{t}; \\hat{\\bm{\\theta}})$",
        "2211.03237_51a404744077e3984ba1d1a22ab51023": ", meaning that",
        "2211.03237_def50838bc8154bb67a98ea31e591acf": "is well aligned with",
        "2211.03237_c15a3bc471b078f7f7ba95ba5812c2a5": ", pass A will be used in the back-propagation",
        "2211.03237_4940308f97c391541eb2913151c83d4b": "The loss is the same as in vanilla attention forcing, shown in equation",
        "2211.03237_d2619d2336c6d0cdfb95499c28b40af2": "Otherwise pass B will be used:",
        "2211.03237_790d3622a1a69e84bf0f9e87a266991c": "\\begin{align}\n\\mathcal{L}_{y,\\alpha}(\\hat{\\bm{\\theta}}) &= \\textstyle \\sum_{t=1}^{T} \\log p(\\bm{y}_{t} | \\bm{x}_{1:L}, \\bm{y}_{1:t-1}, \\bm{\\alpha}_{t}; \\hat{\\bm{\\theta}}) \\nonumber \\\\\n&+ \\textstyle \\gamma \\sum_{t=1}^{T} \\mathrm{KL}( \\bm{\\alpha}_{t} || \\check{\\bm{\\alpha}}_{t}; \\hat{\\bm{\\theta}} )\n\\end{align}",
        "2211.03237_d5ad55aa8ce283a36730085c4a4d2bc0": "The KL attention loss is used to determine if the alignment is good enough between the reference output",
        "2211.03237_3bc5f99ae0a43414555a3e0a72da2a33": "and the generated output",
        "2211.03237_6e91d7f8d8294cabcfaaf143ab724082": "As both",
        "2211.03237_69562572b4697895ac6d1328e6a90ef2": "$\\check{\\bm{\\alpha}}_{t}$",
        "2211.03237_9d9597aa60d2013edd9447d91493925a": "are computed using",
        "2211.03237_9ffe5f84589e3e2e166c75abf6b523e6": ", they are expected to be similar, yielding a relatively small",
        "2211.03237_6ce8dd9441580d04b73ea2e98b15161f": "$\\mathrm{KL}( \\bm{\\alpha}_{t} || \\check{\\bm{\\alpha}}_{t}; \\hat{\\bm{\\theta}})$",
        "2211.03237_38c0f1319473c9726f3824232f932222": "In contrast,",
        "2211.03237_8e1d39aa4a72eaf86b704caa99811539": "is computed using",
        "2211.03237_d270707e4d30a1f470baf56cce35575f": "$\\mathrm{KL}( \\bm{\\alpha}_{t} || \\hat{\\bm{\\alpha}}_{t}; \\hat{\\bm{\\theta}})$",
        "2211.03237_fcf7472f91639d8074b424d6329f19d4": "is expected to be larger.",
        "2211.03237_fd8be73b54f5436a5cd2e73ba9b6bfa9": "$\\lambda$",
        "2211.03237_e3194c938ea4305e3c24c0d7b04fe7a2": "is a hyper-parameter controlling how much out-of-alignment",
        "2211.03237_7bf1054bf2055f6f8f5a941edc183e35": "can be",
        "2211.03237_c3eb2607643e40f5f07d0891de74770b": "$\\lambda \\to +\\infty$",
        "2211.03237_592b0a4c21a45bf0edde58822bccaf46": ", scheduled attention forcing will be the same as vanilla attention forcing",
        "2211.03237_b6f9da811716861f2aa5d9555f8fa056": "For each pair of training data, scheduled attention forcing makes a choice whether to guide the model with the reference output history or the generated output history",
        "2211.03237_dc3d54eb8b26b00380b76e0ce18c8b46": "This approach is named",
        "2211.03237_6a8fa6a808df0142a1f3660af09c9458": "scheduled attention forcing",
        "2211.03237_3f8a2a6347913054471a67d4693b48dd": ", because scheduled sampling also selectively uses the generated output history",
        "2211.03237_dab3738c917fc55493be33d006734c0e": "For scheduled sampling, the selection is random",
        "2211.03237_3ab628fbf316645730b045430991b5d2": "For scheduled attention forcing, the selection depends on the data.",
        "2211.03237_66b2f3b882205744398df9e03fc76338": "Parallel attention forcing",
        "2211.03237_ed2d4c1ec16890c3e3befdd9e48561c2": "Transformer-style models have achieved state-of-the-art performance in various tasks including NMT and TTS",
        "2211.03237_f361ca4a0e082e29c31c6296eae1cb0e": "For such models with a large number of parameters, parallel training is essential",
        "2211.03237_a66514b9cc3c37f97e45c6a1e94dbef7": "When teacher forcing is used, there are no recurrent connections in the model, and training can be done in parallel across the length",
        "2211.03237_2f118ee06d05f3c2d98361d9c30e38ce": "$T$",
        "2211.03237_7d0fde0c6afcdf1f43dbed8c3fa85e5f": "of the output",
        "2211.03237_cc1351fcb5dcafde51d0782dca14ac91": "This is more obvious when teacher forcing is rewritten as",
        "2211.03237_d6625c1d4178e2b9f77698bdebd50e52": "$\\hat{\\bm{y}}_{t} \\sim p(\\cdot | \\bm{y}_{1:t-1}, \\bm{\\alpha}_{t}, \\bm{x}_{1:L}; \\bm{\\theta})$",
        "2211.03237_a29f17caa3e965b909d1aef183a202e4": ", where",
        "2211.03237_39f67f551406b50bacba898efd606cad": "$\\bm{\\alpha}_{t} = f(\\bm{y}_{1:t-1}, \\bm{x}_{1:L}; \\bm{\\theta})$",
        "2211.03237_b51e85518bdc7b71fd59b8df3fa90dbb": "The reference output history",
        "2211.03237_f62f8e4f637d80bb059fbd2e038c4ccc": "is available for any",
        "2211.03237_ee8268b2d1242fea2acd1903bba5399f": ", so",
        "2211.03237_13c77b6975af2651451e16bcd049640b": "can be computed in parallel",
        "2211.03237_f3f8be025dc0acf3f78196fdac9b5ffd": "Attention forcing can be rewritten in a similar fashion:",
        "2211.03237_99859dbf880d25c8f2c0f1ed21d027d1": "$\\hat{\\bm{y}}_{t} \\sim p(\\cdot | \\hat{\\bm{y}}_{1:t-1}, \\bm{\\alpha}_{t}, \\bm{x}_{1:L}; \\hat{\\bm{\\theta}})$",
        "2211.03237_cda5b64a7fc5b6f7e55d4e7b340c33c9": "$\\hat{\\bm{\\alpha}}_{t} = f(\\hat{\\bm{y}}_{1:t-1}, \\bm{x}_{1:L}; \\hat{\\bm{\\theta}})$",
        "2211.03237_d86d6921d7ec7c32d7c1e612aafc9b1c": "The model is guided with generated output history",
        "2211.03237_347c490a2a25192d11b5f7922c3ef120": "is not available beforehand, and is generated sequentially",
        "2211.03237_a059d0d09e97bf9d6f842d4f70342c7e": "So when applying attention forcing to Transformer-based models, training is no longer parallel.",
        "2211.03237_02145ec1217155319b628714585fa080": "For parallel attention forcing, the core idea is to approximate the sequential generation of",
        "2211.03237_00beae5cb262f9de2b2a71f91537ee9f": "with a parallelizable process",
        "2211.03237_68b1b6f337ec4fd6ff05b71f18617ec8": "Here the output",
        "2211.03237_0041f9146b0961034baf13d0ace6a294": "$\\hat{\\bm{y}}_{1:T}^{K}$",
        "2211.03237_b5f196050225b26c6190fe21cbc39cd5": "is generated iteratively in",
        "2211.03237_d6328eaebbcd5c358f426dbea4bdbf70": "$K$",
        "2211.03237_b99c3b80f46ac1ccae7520a8b046a111": "forward passes",
        "2211.03237_62fdb10cfb4df634adfd045e0d40f0ed": "For each pass, the complete output history is available beforehand, so training can be run in parallel across time",
        "2211.03237_502edc9729409363fa2f1000139926db": ", as illustrated by figure",
        "2211.03237_aba35a115d4856c3f939c064ee7b3e2e": "\\begin{figure}\n\\centering\n\\includegraphics[width=7.5cm]{Figs/model_TF_PAF.png}\n\\caption{Parallel attention forcing; at pass $k$, $\\hat{\\bm{y}}_{1:T}^{k-1}$ is available, so $\\hat{\\bm{y}}_{1:T}^{k}$ can be computed in parallel.} \\label{fig:PAF}\n\\end{figure}",
        "2211.03237_a940f71611d3575d4e1809a76ba17592": "For the first pass, the output history is the reference",
        "2211.03237_e900eb45fc0df2048619b5c25bb21ae5": "For the following passes, the output history is the output of the previous pass",
        "2211.03237_692287ec96d0b2ab5c2334cbbcba404c": "$\\hat{\\bm{y}}_{1:T}^{k-1}$",
        "2211.03237_747158ad3a63299ee30d320ca4cdf2e8": "t<k",
        "2211.03237_e358efa489f58062f10dd7316b65649e": "t",
        "2211.03237_8ce4b16b22b58894aa86c421e8759df3": "k",
        "2211.03237_284f6ae814b2ef5cdca445798823b34b": "It can be proved that when",
        "2211.03237_3aa8917c1ab600d5891c721729303509": "$K=T$",
        "2211.03237_0af62e9b08249d6ce07750a6ce97fcc6": "is independent of the reference back-history, and is equivalent to an output sequentially generated",
        "2211.03237_a60f5d76d8e66f43b9f9abdb7ff307af": "In appendix",
        "2211.03237_9a572e469e76eb6d1568e06cf3b7f850": ", figure",
        "2211.03237_bb74153bd347061fd5cc3131b40fc551": "illustrates how the iterative parallel generation approximates sequential generation",
        "2211.03237_b2fb07066a53d19796bed6817f40b78a": "Empirically,",
        "2211.03237_e32d21d78f2e48d3f6661fc11e5f7f05": "could be much smaller than",
        "2211.03237_47eab8e3038ca060222db197a7050d34": ", while still addressing the exposure bias",
        "2211.03237_2206d93a86b1c2c297be6dd1527fed18": "So although parallel attention forcing requires more computation than vanilla attention forcing, it is more efficient thanks to parallel training",
        "2211.03237_2f3a097e3d7db8ba13cdd6efd1853045": "Attention forcing has a regularizing effect",
        "2211.03237_315b7e4f8354020fa37f5e28193bf261": "During training, the attention mechanism(s) of the attention forcing model is encouraged to mimic the teacher forcing model",
        "2211.03237_c59ad9f6bbd20662fbb2d4e041f0dd64": "Hence there is the risk of over regularization, in which case the attention forcing model converges to the teacher forcing model",
        "2211.03237_f482e7e8e2f8868c060e48758bf75707": "When applying attention forcing to Transformer-based models, our default option is to force all the cross-attention connecting the encoder and the decoder, while leaving the self-attention alone",
        "2211.03237_8de3444c0bd7f39154a2aacc1226b7f9": "However, in a Transformer-based model, there are usually dozens of such attention maps",
        "2211.03237_1d6964ecb70e3deaeaa5161eff69deac": "The exact number is equal to the number of decoder layers times the number of attention heads in each layer",
        "2211.03237_fc26cb222807deaeac73a308d55f4eda": "In contrast, in a model based on RNN or CNN, there is only one attention map",
        "2211.03237_ade7b474252c483b26dd15855ce61af9": "Forcing all the attention heads tends to over regularize Transformer-based models",
        "2211.03237_8d698b19dfb0cac360bd6abd169d7a2c": "This problem can be addressed by forcing selected attention heads only",
        "2211.03237_a6a1447fc011c67560e55aa7f30831ad": "For the selected attention heads, the reference attention is given to the following layer, and an alignment loss is computed between the reference and the predicted attention",
        "2211.03237_8674305876eda3dcc8f735853f25e7f9": "For the other attention heads, the predicted attention is given to the following layer as usual",
        "2211.03237_4e8c8e5c1107e5b4bba39c33084ae536": "illustrates the idea of forcing selected attention heads",
        "2211.03237_3fb86fe794faf6f1c6d10edad601d08c": "For Transformer-based models, different attention heads have different functions",
        "2211.03237_031e4fe6b82d72238113e90d21dd8104": "\\cite{voita2019analyzing, vig2019analyzing}",
        "2211.03237_3b88c33649ac0189e9ed2cd2fc51dc34": "For example, attention heads in the deepest layers of the model capture the most distant relationships",
        "2211.03237_30e4a8e65f627253af9b1c789407ba44": "\\cite{vig2019analyzing}",
        "2211.03237_50db597c2429a1b40c225ef8bce7541f": "In this paper, the selection is mainly based on the layer.",
        "2211.03237_92e1f3949b053f63865cd5024bf136be": "Related work",
        "2211.03237_d438366799aa6034a31e35591bd4d8d1": "Attention forcing follows the first line of approaches addressing exposure bias, descried in section",
        "2211.03237_8db76ef49b5be1d7d78fdf86e4eb71ce": "Similar to scheduled sampling and professor forcing, it is between teacher forcing and free running",
        "2211.03237_f668f111ef03c82e996c3fdd90d4a10c": "An advantage of attention forcing is that it does not require a heuristic schedule or a discriminator, which can be difficult to tune.",
        "2211.03237_9d43404795c0a1f3dc7a46997ec06260": "reported negative results on TTS",
        "2211.03237_263230bacd81adf932a631b4e7ebbc6e": "Variations of scheduled sampling were applied to NMT, resulting in both positive",
        "2211.03237_1e6acc41aa32cbc2631edca27c233358": "\\cite{zhang2019bridging}",
        "2211.03237_1fec803dea9bc06e2085d33e0243c08b": "and negative",
        "2211.03237_79e66e915fabb7dd11fe01d14c079a71": "results.",
        "2211.03237_484ba1fc7794eae7a09803e1138e6935": "Compared with sequence-level training approaches, attention forcing is more efficient, in the sense that it does not require generating multiple output sequences during training",
        "2211.03237_63d5049791d9d79d86e9a108b0a999ca": "Reference",
        "2211.03237_1656f2c7200872f21b6420f2e5ef3d32": "applied Minimum Bayes Risk (MBR) training to NMT, and approximates the expectation of the risk by sampling and renormalizing the probability of the samples",
        "2211.03237_d95867deadfe690e40f42068d6b59df8": "References",
        "2211.03237_012643f5928d809589b09c7e12bfb993": "\\cite{ranzato2015sequence, bahdanau2016actor}",
        "2211.03237_0d89e715c5e24469f63deb69f1ee4d90": "approximate the same loss with Monte Carlo sampling, and optimizes the loss using Reinforcement Learning (RL).",
        "2211.03237_1b6dc195533e55b3c3e4f599b19cddf3": "For sequence-level training, another general concern is the choice of the distance metric, i.e. the risk",
        "2211.03237_afbaf99bea54ca3b3074dc20fc29ac9d": "Many tasks, including NMT and TTS, do not have a gold-standard objective metric",
        "2211.03237_676851aed24ba5e133a54b1bec5da540": "Empirically, models trained with one metric may not perform equally well when assessed using another metric",
        "2211.03237_80d3e9eabed32088591420fe5a3c2cd0": "To tackle this issue, adversarial training",
        "2211.03237_b484b8402463767614616e1ea24fbbf1": "\\cite{yu2017seqgan, wu2018adversarial}",
        "2211.03237_3d7dd8c39860ea92cba3fae766384492": "can be used: a discriminator learns a loss function, which is potentially better than standard metrics",
        "2211.03237_d4a1d4c397105cdf5e0338c96bab3b6a": "The difficulty here is that the discriminator itself can be difficult to train",
        "2211.03237_04a87841d232a938576582bd5c30035c": "\\cite{zhang2018bidirectional}",
        "2211.03237_32105bf81670986762aed8c24d68839d": "While attention forcing does not directly optimize a sequence-level loss, it can indirectly reduce the loss by training the model to recover from errors",
        "2211.03237_161d7388390c20539c8aff4fcc1349ad": "This is because sequence-level metrics are usually computed by comparing units of the sequences",
        "2211.03237_fc885fc90f50de23e711a53a9818082a": "Examples include word error rate for ASR, and BLEU",
        "2211.03237_8b579ff256d8c486c0d8bd186a16ba15": "\\cite{papineni2002bleu}",
        "2211.03237_ebe069f8a3da6b22040cba2a6f746e14": "and ROUGE",
        "2211.03237_615158364857ca839978f45b7f0dc952": "\\cite{lin2003automatic}",
        "2211.03237_94ae3caa63f7856d889955860bb19129": "for NMT",
        "2211.03237_39558e297d6cd9378551dfda106622ca": "If models can recover from its errors, the sequence-level loss can be reduced",
        "2211.03237_65c0dcdd86cb1b38bde0035660174cdd": "It is challenging to apply attention forcing to models without an attention mechanism",
        "2211.03237_1c0b56063ceb209cbf8d8b189a8c8dcd": "However, the concept of attention forcing can be applied to such models, where it is essential to find something analogous to attention",
        "2211.03237_a1c925b27bc89e36bdfbea6755bc9b61": "For convolutional neural networks, for example, attention maps can be defined based on the activation or gradient",
        "2211.03237_ef269b8c1241f872d04cef63bb36510e": "\\cite{zagoruyko2016paying}",
        "2211.03237_d05bcca54b9013f28f19ba7dd5388bda": "On a historical note, attention forcing was first proven successful in TTS",
        "2211.03237_2747ca1d0edc868e1b950ba30e9c271b": "\\cite{dou2019attention, dou2020attention}",
        "2211.03237_92fe28e3877f479c7ed100908f58369a": "Then scheduled attention forcing was introduced for machine translation in our preprint paper",
        "2211.03237_93ba9c4aff7979d75d74ea6be2a45f3f": "\\cite{dou2021attention}",
        "2211.03237_cec13676abcba9a76572b7cea2141f09": "At the time it was named \"automatic attention forcing\"",
        "2211.03237_7cd1f92add5235575b06d6f131ad933a": "We find it best to introduce both scheduled attention forcing and parallel attention forcing in a single paper as they are applied together for Transformer-style NMT models.",
        "2211.03237_4829262cecb9828817b33e0f9c907f91": "Experiments",
        "2211.03237_c62157c00326f83c7c8ad111cf73cbc5": "There are two sources of data: IWSLT'15",
        "2211.03237_cac0f1ddd1caf90a41ce4a4eb06fa819": "\\cite{cettolo2012wit3, cettolo2015iwslt}",
        "2211.03237_b0ccaa3b3dd6f1083a1b8f7d1a1c4417": "and WMT'16",
        "2211.03237_17e2c72ac9d61ea4f7c15a7116075d8d": "\\cite{bojar2016findings}",
        "2211.03237_51c45b795d5d18a3e4e0c37e8b20a141": "Table",
        "2211.03237_e6d818988b5b41ce97619350496cc2d0": "shows the data split",
        "2211.03237_19ba5bd79ad2ecd90821f84d5c34557f": "Detailed descriptions are in appendix",
        "2211.03237_8a3312a9d58c501d361adc915e60ef20": "The sentences in IWSLT are from TED talks",
        "2211.03237_2d86160c81d365162838bdd09024e4e9": "Two translation directions are investigated: English-to-French (EnFr) and English-to-Vietnamese (EnVi)",
        "2211.03237_006c5457305ce3a4f4937a06a02b4dd9": "The data preprocessing follows",
        "2211.03237_10454456bf61483e87d87b28d6fc0846": "\\cite{luong2015stanford}",
        "2211.03237_eb56bf0bd94a4717a6f0fd0929dd1540": "The sentences in WMT are from newspaper articles",
        "2211.03237_348c6f46bcd1776e83d3e1417f1b7a9d": "Here English-to-German (EnDe) translation is investigated",
        "2211.03237_5446e34754a7715298b13f9f6db9f113": "\\cite{ott2018scaling}",
        "2211.03237_eb49329d6319d9bc08deb624c3ee0e8e": "For all the translation directions, the Moses tokenizer",
        "2211.03237_1647deb3ec47eb07ff208cc10889b242": "\\cite{koehn2007moses}",
        "2211.03237_a39c4b6a13cc1cf9b55e228ef1d315d0": "is adopted, and the translations are detokenized before evaluation",
        "2211.03237_5af4d5b0ee35f5b6c74f29d8f77790fe": "The checkpoints are selected based on the validation set, and the results are compared on the test set.",
        "2211.03237_873681762ff53b5029e3aacb940e18ff": "\\begin{table}\n\\footnotesize\n\\centering\n\\caption{\\label{tab:chNMT_exp_data} Data used in the experiments.}\n\\begin{tabular}{@{}l|l|l}\n\\toprule\n        &             & \\# sentence pairs \\\\\n        & Languages & Training-valid-test       \\\\ \\midrule\nIWSLT'15 & En$\\to$Fr   & 208K-1026-1305       \\\\\n        & En$\\to$Vi   & 133K-1553-1268       \\\\ \\midrule\nWMT'16   & En$\\to$De   & 4.5M-3000-3003       \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}",
        "2211.03237_670bf7e000cd36d76c014728e7f4094b": "The overall translation quality is measured by BLEU",
        "2211.03237_ec5e18432ee22614f569a1698cb63d2d": "The average of 1-to-4 gram BLEU scores are computed and a 0.6 brevity penalty is applied",
        "2211.03237_c0eec38d7bd8cef3c4f5fe97de08eb11": "For IWSLT and WMT data, the BLEU score is computed using the Moses toolkit",
        "2211.03237_d9c550097b549378ef126d16215eb1e3": "and the SacreBLEU toolkit",
        "2211.03237_4ae8feebe2ec2c66be45a600d09e13d8": "\\cite{post2018call}",
        "2211.03237_53ad699b31123921b09f0a7845118ce1": ", respectively",
        "2211.03237_49dae9f4896aa5e9b1eee785b2d66953": "In NMT, there can be multiple valid output sequences for a single input sequence",
        "2211.03237_f214a36f74e10d55dbbfc7ba8c06ec6b": "Given that the overall translation quality is the same, it is desirable for an NMT model output to be diverse translations for the same input",
        "2211.03237_baba4995a53fc9729f15f22dd061dbba": "This work measures the diversity of the candidate translations by pairwise BLEU",
        "2211.03237_38e85ad61643a8fdc395f415c69b6711": "\\cite{shen2019mixture}",
        "2211.03237_cd1b12747087873bd70c15248af5e951": "and entropy",
        "2211.03237_354358e27bafe01366dae6fcaf524d05": "For a translation model",
        "2211.03237_879e0d521c2ac9f35fe20788fab143d7": ", we use sampling search",
        "2211.03237_fb97d38bcc19230b0acd442e17db879c": "$M$",
        "2211.03237_4cffc6488758193baa7482b981d107a3": "times with different random seeds, obtaining a group of translations",
        "2211.03237_58a9b69a8e58c47fbe66cbd1855cdf16": "$\\{ \\hat{\\bm{y}}^{(m)} \\}_{m=1}^{M}$",
        "2211.03237_ee989d9aa368bef635f53ba67dbee9ba": "$\\hat{\\bm{y}}^{(m)}$",
        "2211.03237_5159ae7d461788ce38cefd5aebdd3db3": "denotes all the output sentences in the dev or test set",
        "2211.03237_d4f8a8da766004eee31c021418c53b95": "Then we compute the average BLEU between all pairs:",
        "2211.03237_e5dff2a9138b8fcc2b3af09af888ffcd": "$\\frac{1}{M(M-1)} \\sum_{n=1}^{M} \\sum_{m=1}^{M} {\\tt BLEU} (\\hat{\\bm{y}}^{(n)}, \\hat{\\bm{y}}^{(m)})_{n \\neq m}$",
        "2211.03237_6c1267e9bad2d2ccde8347062e48bef2": "In our experiments,",
        "2211.03237_41c69616de1f6f5985f6796e08cf8645": "is set to 5",
        "2211.03237_1e7b4ec28645b83607e1cdc9e546db20": "The more diverse the translations, the lower the pairwise BLEU",
        "2211.03237_01d0005c80042a97d215bf60abe93d38": "In addition to pairwise BLEU, we use greedy search and save the entropy",
        "2211.03237_6e42c9a31a013f46d5ec333ad1b77e89": "$e_{t}$",
        "2211.03237_261e493d74c6ab360974264370cf3b22": "of the output token's distribution at each time step",
        "2211.03237_dd6265dd2cc73f14b934d9d6d606216a": "$e_{1:T}$",
        "2211.03237_7f159679c89bf23156350141a963e109": "cover all the model output steps, we compute the average value:",
        "2211.03237_8d30d188a673f6bc8424b8e6edc03cb8": "$e = \\frac{1}{T} \\sum_{t=1}^{T} e_{t}$",
        "2211.03237_3224651ce366326aac37aee901ee5aa1": "Higher entropy means that the model is less certain, and thus more likely to produce diverse outputs",
        "2211.03237_90f76c417e2895d53101393bcb663476": "This process is deterministic, and is not repeated with different random seeds.",
        "2211.03237_4470066f77a448cc5058718fccf33ef8": "The experiments are conducted with EnFr and EnVi data in IWSLT'15",
        "2211.03237_260e6206980e26ba8901c85e50f8436a": "The model is based on GNMT",
        "2211.03237_37341b3fcd8c1f41c2a2f0fcf90fdba3": "\\cite{wu2016google}",
        "2211.03237_510842838905f62dfcf9d66e5660687e": "The details of the model and training are described in appendix",
        "2211.03237_690828b812b64e779aad91ef364e573b": "By default, the baseline models are trained with Teacher Forcing (TF) for fewer than 60 epochs",
        "2211.03237_8ac920cfe5fb8dc369976bf10cab5fcb": "Starting from the baseline, models are finetuned with Attention Forcing (AF) for fewer than 30 epochs",
        "2211.03237_e7a80d4afd2ab8336797f96638e9eebc": "For AF, the scale",
        "2211.03237_ae0ed33e917b09336cd02b67bf995d0f": "of the attention loss is 10",
        "2211.03237_889043c91d7ffddd7db4a85f2a263759": "The default inference approach is greedy search",
        "2211.03237_70da0cbcdef582399311f1cfb7f94fb4": "When investigating diversity, sampling search is also adopted",
        "2211.03237_5588fdd15e0ae41e2ac57e1da6750a69": "The checkpoints are selected based on the validation BLEU",
        "2211.03237_d30d3b938216a48ac09d70c67766e288": "For all the training approaches, the effective number of epochs is smaller than the maximum, i.e. training goes on until convergence",
        "2211.03237_551b11ca8bd077db603eb10f3f2943df": "The computational budget to train a model is 96 hours on a single Nvidia Tesla P100.",
        "2211.03237_2a6efa583e7d54391278e925ca94b284": "First, we compare TF and vanilla AF",
        "2211.03237_0160c89755ea44da9477a6c09a774717": "The preliminary experiments show that when pretraining with TF is adopted, the BLEU of AF increases from 21.77 to 22.93 for EnFr, and from 13.92 to 18.27 for EnVi",
        "2211.03237_4f352a4ad2b4c699bca5860e19b41d2a": "However, it does not outperform TF, as shown by the first two rows in each section of table",
        "2211.03237_316013651692eadd87822b4dd4a85adb": "This result is expected, considering the discrete and multi-modal nature of the NMT output space, analyzed in section",
        "2211.03237_e8776f9e8a0ab84ab29838c91b457ed9": "\\begin{table} \\footnotesize\n\\centering\n\\caption{BLEU of Teacher Forcing (TF), Attention Forcing (AF) and Scheduled Attention Forcing (SAF) with different values of $\\lambda$; the higher $\\lambda$ is, the more likely the generated output history is used; the models are based on GNMT, and trained with data from IWSLT'15.}\n\\begin{tabular}{l|l|l|ll}\n\\toprule\nTask  & Training  & $\\lambda$     & BLEU $\\uparrow$ &              \\\\ \\midrule\nEnFr  & TF  & -           & 30.70        \\\\\n    %   & SS  & -           & 31.57        \\\\\n      & AF  & -           & 22.93        \\\\\n      & SAF  & 2.5     & 31.44        \\\\\n      & SAF  & 3.0     & \\textbf{31.66}        \\\\\n      & SAF  & 3.5     & 31.34        \\\\ \\midrule\nEnVi  & TF  & -           & 25.57        \\\\\n    %   & SS  & -           & 26.46        \\\\\n      & AF  & -           & 18.27        \\\\\n      & SAF  & 2.5   & 26.02        \\\\ \n      & SAF  & 3.0   & 25.71        \\\\ \n      & SAF  & 3.5   & \\textbf{26.72}        \\\\ \\bottomrule\n\\end{tabular}\n\\label{tab:BLEU_TF_AAF_tune}\n\\end{table}",
        "2211.03237_c2e7ce9df1a1e4690da086763369e302": "Next, TF is compared with Scheduled Attention Forcing (SAF)",
        "2211.03237_d6b174fdfe70c9b7e98c70bc2d074a3f": "The hyper parameter",
        "2211.03237_2c57f9c6722cf63adc22dd3e5562a881": ", introduced in section",
        "2211.03237_282858a3bcaa3c015cec10204709fe90": ", controls the tendency to use generated outputs",
        "2211.03237_958f62d64d5c72f815ef6a58f1ca3b0e": "As shown in table",
        "2211.03237_6df2924942a3000babf07a6a5e35cf98": ", with very limited tuning, SAF outperforms TF",
        "2211.03237_c2aadd5d8e79b481f778dc06c0268484": "The performance is robust in a certain range of",
        "2211.03237_31aee56c71153820af225bc08cf34afb": "In the following experiments,",
        "2211.03237_95c1d019403c25b5d2cce79933e124dc": "is set to 3.0 for EnFr and 3.5 for EnVi",
        "2211.03237_b0a50ace6c86a9fdd41321b07329b8b8": "To reduce the randomness of the experiments, both TF and SAF are run",
        "2211.03237_3a0d4bf5e23712ac93e1df660fd57dfe": "$R=5$",
        "2211.03237_16d2508760f8065482da7a14ecb50fad": "times with different random seeds",
        "2211.03237_8e969febe9350c1eb405718833ecf4ef": "$\\{ \\bm{\\theta}^{(r)} \\}_{r=1}^{R}$",
        "2211.03237_124fda37737e8fbcfd3763f04d89cd88": "denote the group of TF models, and",
        "2211.03237_b10c06e1b266578dc3545d9fbe3760a1": "$\\{ \\hat{\\bm{\\theta}}^{(r)} \\}_{r=1}^{R}$",
        "2211.03237_17bce7c7e8f2de6c70e046073e0ef13a": "the SAF models",
        "2211.03237_d84b70b0c1d65c4afa1b9dafb9b8176a": "For both groups, the BLEU's mean",
        "2211.03237_f62db12f95e34116f1f1e827b2c64ce5": "$\\pm$",
        "2211.03237_b80b59610bc3c27c236827155a1d8d0e": "standard deviation is computed",
        "2211.03237_fd403e299c50d01150a9cff5f96c3afa": "shows the results",
        "2211.03237_9a9becd399be8ddccd3fd6d875d68089": "In terms of mean BLEU, SAF yields a 0.44 gain for EnFr, and a 0.55 gain for EnVi.",
        "2211.03237_72f2f3835101c1e41a5ffe85e9ee4227": "\\begin{table}\\footnotesize\n\\setlength{\\tabcolsep}{4pt}\n\\centering\n\\caption{BLEU, Entropy and pairwise (P.) BLEU of TF and SAF; each approach is run 5 times, and the mean $\\pm$ std is shown; the models are based on GNMT, and trained with data from IWSLT'15.}\n\\begin{tabular}{l|l|l|ll}\n\\toprule\nTask  & Training     & BLEU$\\uparrow$  & Entropy$\\uparrow$ & P. BLEU$\\downarrow$              \\\\ \\midrule\nEnFr  & TF           & 31.10 $\\scriptstyle \\pm 0.27$  & 1.060 $\\scriptstyle \\pm 0.047$           & 27.43 $\\scriptstyle \\pm 0.75$        \\\\\n    %   & SS     & 31.45 $\\scriptstyle \\pm$ 0.45        \\\\\n      & SAF     & \\textbf{31.54} $\\scriptstyle \\pm 0.14$  & 1.034 $\\scriptstyle \\pm 0.013$           & 27.82 $\\scriptstyle \\pm 0.67$        \\\\ \\midrule\nEnVi  & TF           & 25.86 $\\scriptstyle \\pm 0.44$  & 1.508 $\\scriptstyle \\pm 0.012$           & 22.11 $\\scriptstyle \\pm 0.34$        \\\\\n    %   & SS     & 26.29 $\\scriptstyle \\pm$ 0.19        \\\\\n      & SAF   & \\textbf{26.41} $\\scriptstyle \\pm 0.33$  & \\textbf{1.582} $\\scriptstyle \\pm 0.017$              & \\textbf{20.75} $\\scriptstyle \\pm 0.29$        \\\\ \\bottomrule\n\\end{tabular}\n\\label{tab:BLEU_TF_AAF}\n\\end{table}",
        "2211.03237_2446771b2e50fbf05b763a872271b618": "To measure the diversity of the translations, the entropy and pairwise BLEU are computed for",
        "2211.03237_9da16bfc4cc879f9dc40721c3275fd2d": ", as shown in the last two columns of table",
        "2211.03237_dacfd91c7596b86604eb0b426a41e84e": "Focusing on the entropy column, SAF leads to higher entropy for EnVi, which indicates higher diversity",
        "2211.03237_4479e011c6001bd0c8b0d468db33998a": "For EnFr, SAF and TF lead to similar levels of diversity, especially when the standard deviation is considered",
        "2211.03237_d5782915ded1ead8dd8bc881ff3f4ac7": "We believe that the difference is due to the nature of the tasks",
        "2211.03237_cea38dec744d5c961d3b81965ac4999c": "While English and French have similar syntax and lexicon, English and Vietnamese are more different",
        "2211.03237_0d3a9033bbd27b8c02cccb0c6596876f": "When trained with SAF, the EnVi model benefits more from using generated back-history, which is more likely to be different from the reference back-history",
        "2211.03237_db3540b319f86242f7867ce609ba7006": "The pairwise BLEU shows similar trends",
        "2211.03237_98603b39b900538773ae6aee29093fe3": "For EnVi, SAF leads to lower pairwise BLEU, i.e. higher diversity",
        "2211.03237_6c338ce698031189f0b341492e55e9a7": "For EnFr, the difference between SAF and TF is negligible",
        "2211.03237_8d1c05b72aada80a3ec37e2a593fe5c0": "So in the following experiments, only pairwise BLEU will be reported.",
        "2211.03237_8ba9ff75a04453dcf926e3d287072ff9": "The experiments in this section are conducted with WMT'16 EnDe data",
        "2211.03237_d3acf4766becbf4ebf47476cfe5b1d04": "Compared with IWSLT, WMT is more suitable for Transformer models in terms of the amount of data.",
        "2211.03237_f943520daa007c1c248918b48330d025": "The Translation models have the same structure as the âbigâ Transformer in",
        "2211.03237_8a62dfb06d59161af590690b385f1959": ", and the training follows",
        "2211.03237_ede87580b3109c33a7a1c605f1130deb": "The details are in appendix",
        "2211.03237_bd372d8dcc9717eac0477d0c040fe6d0": "The baseline models are trained with Teacher Forcing (TF)",
        "2211.03237_9acc7ef13b9ed3966a5946e44d853159": "Starting from the baseline, other models are finetuned respectively with sequence-level Scheduled Sampling (SS) and Attention Forcing (AF)",
        "2211.03237_7649a1305012163ca68123a3732be498": "To keep the benefit of parallel training, AF and SS are approximated by their parallel version, as described in section",
        "2211.03237_d973bc1c738e6b052bdb4c9c916d350c": "and reference",
        "2211.03237_c340e9e61466cec421a81680f3dc4826": "The number of iterations is two",
        "2211.03237_d8cb1fde563cd7ceb0cd5974f9a3021c": "For SS, the probability of using the reference output decreases linearly from 1 to 0.7; more aggressive schedules are found to degrade the performance",
        "2211.03237_2fae8e38070b382242f0fa09e14d2602": "of the attention loss is 1000",
        "2211.03237_65d3a10e44576441452172bf8f1a3908": "The default inference approach is beam search with beam size 4",
        "2211.03237_71d5873c82a00c96c94796df263c433d": "The validation BLEU is monitored to select checkpoints and to stop training when no performance gain is observed after 10 epochs",
        "2211.03237_b92070aba5459b042beaf85c40f22ff8": "The computational budget to train a model is 144 hours on a single Nvidia Tesla V100.",
        "2211.03237_7799abfd14cfb468d2f54df7fc252458": "The first two sections of table",
        "2211.03237_71a7a819a5c7dd0afeb535a90f457218": "lists the preliminary results of TF, parallel SS and parallel AF",
        "2211.03237_0c4c7e3dfb41b2a613fbee21cfd3980e": "Here all the attention heads are constantly forced, regardless of the alignment between the reference and generated output history",
        "2211.03237_21e4c6fd0ec560fa608dd3195f71d1c0": "Compared with TF, parallel SS yields lower BLEU as well as pairwise BLEU",
        "2211.03237_4437685fe845cde4919fb88f5036c921": "It is difficult to conclude whether the decrease in pairwise BLEU results from higher diversity or lower translation quality",
        "2211.03237_daa970c5ed792f8089392e2e98a9ffec": "In its parallel version, AF performs similarly to TF",
        "2211.03237_29c4cbd7b4b12de4893172af93180043": "This is probably because the back-history is generated in TF mode.",
        "2211.03237_13019063bc6a3fbedbb8abf221242045": "As analyzed in section",
        "2211.03237_81f4380bfc60485d55473c9e2f47fd15": ", when applying AF to NMT, it is important to turn AF on and off based on the alignment between the reference and the generated outputs",
        "2211.03237_6cc43fc15ac52e9cf5562ca8b5a26191": "Hence unless otherwise mentioned, a schedule is added to parallel AF in the following experiments",
        "2211.03237_6c5fe8898c2b17c9c7205e677c8b2d05": "The last section of table",
        "2211.03237_0e8997fd84c3ef264f6d92bde1d1ddbe": "lists the performance of parallel AF, where the hyperparameter",
        "2211.03237_099bd047108401e3efad2f192d037b17": "of the schedule is tuned",
        "2211.03237_b55d665dc0e25bc5845c07feca7363c1": "While the BLEU remains at the same level, the pairwise BLEU decreases when the percentage of AF decreases, signaling that AF regularizes the translation model to operate in a safe zone.",
        "2211.03237_476ca69b40c8bf85106a0d051f4e3e80": "\\begin{table}\\footnotesize\n\\centering\n\\caption{\\label{tab:chNMT_exp_paf_sched} BLEU and Pairwise BLEU of Teacher Forcing (TF), Parallel Scheduled Sampling (PSS) and Parallel Attention Forcing (PAF); higher $\\lambda$ means higher tendency to use AF; the models Transformer-based, trained with WMT'16 EnDe, tested on newstest14; the bold numbers correspond to the $\\lambda$ for further experiments.}\n\\begin{tabular}{lc|c|c}\n\\toprule\n & $\\lambda$  & BLEU$\\uparrow$   & Pairwise BLEU$\\downarrow$  \\\\ \\midrule\nTF      & -    & 28.68   & 31.12    \\\\\nPSS      & -    & 28.19   & 30.17    \\\\ \\midrule\nPAF      & $+\\infty$    & 28.74   & 31.90    \\\\ \\midrule\nPAF      &1.1    & 28.75    & 31.60     \\\\\nPAF      &1.2    & \\textbf{28.57}   & \\textbf{30.62}    \\\\\nPAF      &1.3    & 28.48   & 31.89    \\\\\nPAF      &1.4    & 28.56   & 31.99    \\\\\nPAF      &1.5    & 28.47   & 32.06    \\\\\\bottomrule\n\\end{tabular}\n\\end{table}",
        "2211.03237_bfb763682cd6246df6f975e747756534": ", the encoder and decoder are connected by multiple attention mechanisms",
        "2211.03237_a1d67eb1e6e99dbf37ea0d1f6a9aa36b": "It is likely that too much information is passed from the TF baseline to the AF model",
        "2211.03237_9f10f5b07d483e5ab325ca70f71a72e2": "To reduce this information, we only force selected attention heads",
        "2211.03237_5da63a807597364ec69082fcd34819a2": "The first two decoder layers are selected, and the reason is discussed in section",
        "2211.03237_e6aef7738163e3c224a3db6d01b6ba24": "in the appendix",
        "2211.03237_d5410b49d352dcda5260161902b2dc08": "In each layer, the number of heads forced are 8, 12 or 16 out of 16",
        "2211.03237_e29d5dd0df34cb0f7a98af01e7c961c7": "lists the results",
        "2211.03237_043ceebd8391c77b0b9152c10f70e6c4": "When only two layers are forced, the performance of parallel AF surpasses TF in both BLEU and pairwise BLEU",
        "2211.03237_4b263c7d011971a6898e0f7d960822c4": "The best performance (first row) is achieved when 8 heads are forced in each layer",
        "2211.03237_7df0e62090163c76c876b2e811554074": "To reduce the randomness of hyperparameter tuning, we run another experiment where the other 8 heads are forced, and the result (second row) is comparable to the best performance.",
        "2211.03237_fc291f16b7648cbfe831909cc4f46031": "\\begin{table}\\footnotesize\n\\setlength{\\tabcolsep}{4pt}\n\\centering\n\\caption{\\label{tab:chNMT_exp_paf_sched_select} BLEU and Pairwise BLEU of TF, PSS and PAF, where only selected attention heads are forced; the models are based on Transformer, trained with WMT'16 EnDe, tested on newstest14.}\n\\begin{tabular}{lc|cc|c|c}\n\\toprule\n & $\\lambda$  & Layers  & Heads  & BLEU$\\uparrow$   & Pairwise BLEU$\\downarrow$  \\\\ \\midrule\nTF      & -    & -      & -      & 28.68   & 31.12    \\\\\nPSS      & -    & -      & -      & 28.19   & 30.17    \\\\ \\midrule\nPAF      &1.2    & 1-2      & 1-8      & \\textbf{29.04}   & \\textbf{30.47}    \\\\\nPAF      &1.2    & 1-2      & 9-16      & \\textbf{28.91}   & \\textbf{30.05}    \\\\\nPAF      &1.2    & 1-2      & 1-12      & 28.86   & 30.87    \\\\\nPAF      &1.2    & 1-2      & 1-16      & 28.64   & 30.79     \\\\\\bottomrule\n\\end{tabular}\n\\end{table}",
        "2211.03237_6f8b794f3246b0c1e1780bb4d4d5dc53": "Conclusion",
        "2211.03237_476ae2cb2944d3d692a3185a93fab093": "This paper introduced two extensions to attention forcing, a training approach addressing exposure bias in attention-based seq2seq models",
        "2211.03237_fee020ec6a053a1c893d94c881698311": "We recommend the basic form of attention forcing in tasks with continuous outputs like TTS",
        "2211.03237_21682eeaa568cb6316f5d1612c6b1f90": "For tasks with discrete outputs like NMT, we recommend scheduled attention forcing",
        "2211.03237_717f00b7cfaf9f690f052e8e963493f9": "For models based on Transformers, it is essential to use parallel attention forcing, and to not force all the attention heads.",
        "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9": "Broader impacts",
        "2211.03237_e89ef62242bbb9ca7a9d610a658225b6": "The training approaches introduced in this work can be applied to a range of attention-based seq2seq models",
        "2211.03237_0b97e857b3d92c771f35f125b577ba4d": "NMT is used as a representative task, where the output is discrete",
        "2211.03237_7f12adccb54d24d72fb836d9390a226f": "The baselines",
        "2211.03237_8c0693780382f3bf162ec87d71988ffe": "\\cite{wu2016google, vaswani2017attention}",
        "2211.03237_8fc67e9009749e2e65fdca5242dc0cb2": "in the experiments are prominent models based on RNNs or Transformers, which are widely used in various tasks",
        "2211.03237_a6c836c477252976b2ba04f0c33571e0": "\\cite{tay2020efficient, dou2022improving}",
        "2211.03237_17a780f195c22772c556ed4f59a6efa3": "While this work focuses on autoregressive models, it can also benefit non-autoregressive models",
        "2211.03237_7337a2bda2f2fc5ad84a4feca716a941": "For example, autoregressive models can act as teachers in teacher-student training of non-autoregressive models",
        "2211.03237_4ba600c5a92f72cf7e3a3ddc06bd96cb": "They can also generate data for semi-supervised training",
        "2211.03237_be5a952c162a5f9f528f5078d1e972d6": "The datasets",
        "2211.03237_1f16c27a5972b6d507f77f2c04c62d62": "\\cite{cettolo2012wit3, cettolo2015iwslt, bojar2016findings}",
        "2211.03237_fac08ae536482677f5069d8763fc055e": "used in the experiments are public dataset repeatedly used in the NMT community",
        "2211.03237_f2d06f0e7ed30aa8b88ee51727195e44": "\\cite{luong2015stanford, ott2018scaling}",
        "2211.03237_2d1bfe07587496192995d84cd75dc442": "Sentences in IWSLT are from TED talks, and sentences in WMT are from newspapers",
        "2211.03237_a4829dba72c981b09367733364b83938": "While we did not observe any sensitive information in the data, please contact us if any potential risks are spotted in the data, such as privacy issues",
        "2211.03237_b2c39a0d7a4d3317e94668317296115e": "The tools",
        "2211.03237_97cd6debaf5e1085d9ff4494e3d2cdb9": "\\cite{koehn2007moses, post2018call}",
        "2211.03237_cae5b03cf47eb200a6540dd8b734f162": "used to compute BLEU scores are also public",
        "2211.03237_406ca04349293a98cd532acd5396cfba": "The use of the data and tools is consistent with their intended use",
        "2211.03237_bd2b6e2f298bcb9262c240292623d9b1": "Detailed documentation of the data and code is available in their references.",
        "2211.03237_68b87943d564c12fea83bb07cc7db5ec": "Details of parallel attention forcing",
        "2211.03237_d2c24d59e0baff4d0155fbdf62590867": "Section",
        "2211.03237_90cfb408aef429c33e2c52d4b7d6e726": "introduced parallel attention forcing, and explained the reasons to not force all the attention heads",
        "2211.03237_181d88ebee0bfabc1c182c2f6c9ac08f": "Parallel attention forcing is applied to a Transformer with two decoder layers, but only forcing the attention heads in the second layer",
        "2211.03237_86dd11ff3ccddc5855e408455fcfcaff": "Here the Transformer model is simplified; the detailed model structure is shown in figure",
        "2211.03237_13d3aede1e0a4aec0535056933888fad": "\\begin{figure}\n\\centering\n\\includegraphics[width=8cm]{Figs/paf_masked.png}\n% \\vspace{-0.2cm}\n\\caption{Illustration of parallel attention forcing, applied to a Transformer with two encoder layers and two decoder layers; the attention heads in the second decoder layer are forced.}\\label{fig:chAF_paf_mask}\n\\end{figure}",
        "2211.03237_ebdbae7d6a12328e6efcedec86eee39f": "A Transformer block",
        "2211.03237_a4b6a9cbf4853aec792bd188b8c295a9": ", also referred to as a Transformer layer, is a combination of many basic modules",
        "2211.03237_9b237a2fef6c954ddfb977d63960550a": "There are two types of Transformer blocks: Transformer encoder blocks, which encode a sequence, and Transformer decoder blocks, which connect two sequences",
        "2211.03237_5d652883c2b121b4d3690b5d05dce9e3": "illustrates both types of Transformer blocks, as well as how they are combined to form an encoder-decoder model.",
        "2211.03237_6c69ed8c15a3bd8d8f91ad3dc6b264d3": "\\begin{figure}\n\\centering\n\\includegraphics[width=7.5cm]{Figs/transformer_block.png}\n% \\vspace{-0.2cm}\n\\caption{Illustration of Transformer blocks \\cite{vaswani2017attention}; the dashed rectangle in the left is a Transformer encoder block; the dashed rectangle in the right is a Transformer decoder block. }\\label{fig:ch2_transformer_block}\n\\end{figure}",
        "2211.03237_ddfce10583189e947b577406f7cefb19": "Similar to scheduled attention forcing, multiple forward passes can be taken for each pair of training data in parallel attention forcing",
        "2211.03237_a7d402adb3dffb44de93f98f8f154ce9": "The loss function is selected based on the alignment between the reference and generated output sequences",
        "2211.03237_5d6b8a585b1a6686516a97ab8a421ca1": "$ \\sum_{t=1}^{T} \\mathrm{KL} (\\bm{\\alpha}_{t} || \\hat{\\bm{\\alpha}}_{t}^{K}; \\hat{\\bm{\\theta}} ) < \\lambda \\sum_{t=1}^{T} \\mathrm{KL} (\\bm{\\alpha}_{t} || \\hat{\\bm{\\alpha}}_{t}^{1}; \\hat{\\bm{\\theta}} )$",
        "2211.03237_16e06047245c4ff36cd4889d77afc762": ", it will be assumed that",
        "2211.03237_7461dfd21e75d77395e7da7fe67dcb34": ", and the",
        "2211.03237_1175b2842e6d602c6962763add91899e": "-th forward pass will be used in the back-propagation:",
        "2211.03237_5fef6f43c06acde493115c690130c20b": "\\begin{align}\n\\mathcal{L}_{y,\\alpha} (\\hat{\\bm{\\theta}}) &=  -\\sum_{t=1}^{T} \\log p(\\bm{y}_{t} | \\hat{\\bm{y}}_{1:t-1}^{K}, \\bm{\\alpha}_{t}, \\bm{x}_{1:L}; \\hat{\\bm{\\theta}}) \\nonumber \\\\\n&+ \\gamma \\sum_{t=1}^{T} \\mathrm{KL} (\\bm{\\alpha}_{t} || \\hat{\\bm{\\alpha}}_{t}^{K}; \\hat{\\bm{\\theta}} )\n\\end{align}",
        "2211.03237_2bcfa24c2f46b55022faa64370932898": "Otherwise the first pass will be used:",
        "2211.03237_52df3b63d926ec22e9e0f1ae4f486baa": "\\begin{align}\n\\mathcal{L}_{y,\\alpha} (\\hat{\\bm{\\theta}}) &=  -\\sum_{t=1}^{T} \\log p(\\bm{y}_{t} | \\hat{\\bm{y}}_{1:t-1}^{1}, \\bm{\\alpha}_{t}, \\bm{x}_{1:L}; \\hat{\\bm{\\theta}}) \\nonumber \\\\\n&+\\gamma \\sum_{t=1}^{T} \\mathrm{KL} (\\bm{\\alpha}_{t} || \\hat{\\bm{\\alpha}}_{t}^{1}; \\hat{\\bm{\\theta}} )\n\\end{align}",
        "2211.03237_afe969dbaff31a33889f927436c5a65c": "So although parallel attention forcing requires more computation than vanilla attention forcing, it is more efficient thanks to parallel training.",
        "2211.03237_ef3fe19e74a7bfd1e75b6f802ed4377c": "\\begin{figure}\n\\centering\n\\includegraphics[width=6cm]{Figs/parallel_af.png}\n\\caption{Illustration of iterative parallel generation; the dark blue circles are the reference tokens, and the rest are generated tokens; the light blue circles are influenced by the reference, and the white circles are not; the solid arrows represent copying, the dashed arrows represent dependency. \\label{fig:parallel}}\n\\end{figure}",
        "2211.03237_2fbd0b716c43d6dd46a524a716324edb": "Details of experimental setup",
        "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee": "Data",
        "2211.03237_a2fddced585ad126645abe2e0231e7c3": ", there are two sources of data: IWSLT'15",
        "2211.03237_04b2e1590db5dd0c59dccc20972b4c19": "One reference output is provided for each input",
        "2211.03237_cde67776ad6f9c7e47f8c773ca692654": "The IWSLT datasets correspond to subtitle translation tasks, where the sentences are from TED talks",
        "2211.03237_f2cd639c6fd7807caa8895e9fc2095a1": "These datasets are relatively small, and are used to train RNN-based models",
        "2211.03237_8546d714be6f700d52bcf89ad2152917": "For EnFr, the training set contains 208K sentence pairs",
        "2211.03237_8c147a45b35974bf4c8da6c0833b1b18": "The validation set (tst2013) and test set (tst2014) respectively contain 1026 and 1305 sentence pairs",
        "2211.03237_44fba20f2ba0f6b409fbdd7153246195": "For EnVi, the training set contains 133K sentence pairs",
        "2211.03237_dc32ba231d0471e2c29354378624fde5": "The validation set (tst2012) and test set (tst2013) respectively contain 1553 and 1268 sentence pairs",
        "2211.03237_ad2befe34810472101413a96e42fb280": "The vocabularies are at the word-level, i.e. the units are words",
        "2211.03237_a0631c5e2f989d18cd392626027c1de3": "For EnFr, both English and French vocabularies are limited to 50K",
        "2211.03237_7bcbd2f3d4e8a794f5d9fe25977cbf60": "For EnVi, the vocabulary sizes are 17K and 7.7K for English and Vietnamese",
        "2211.03237_4f7e8763bfc7585e63fc582a54e0ec45": "The WMT datasets correspond to news translation tasks, where the sentences are from newspaper articles",
        "2211.03237_d74a707969d2d6a639a1ef87ca9c6ceb": "The dataset is considerably bigger, and is used to train Transformer-based models",
        "2211.03237_2405407c6eed068e115aa8457dc98ff2": "The training set contains 4.5M sentence pairs",
        "2211.03237_62168861a2ea35cf829c04ce2e9154c5": "The validation set (newstest13) and test set (newstest14) respectively contain 3000 and 3003 sentence pairs",
        "2211.03237_95e27fe09625d861a2b615f3f19a4582": "The data preprocessing follows reference",
        "2211.03237_47cca55d4d42338c8c6c8ae53c08ea9e": "A joint source and target sub-word vocabulary is built using byte pair encoding",
        "2211.03237_9507023777f50a5cd25b2594bbc67f2c": "The vocabulary is 32K BPE tokens",
        "2211.03237_ad2376beebecdcf7846ba973fa1a005b": "Setup",
        "2211.03237_cf53196623798c92c76793f88f50f654": "are conducted with EnFr and EnVi data in IWSLT'15",
        "2211.03237_9d660150b7c98e93025f751f5407e528": "The differences are as follows",
        "2211.03237_7c3f09489ad9c1bee4fd03a4da9821c1": "The model is simplified with a smaller number of LSTM layers due to the small scale of data: the encoder has 2 layers of bidirectional LSTM and the decoder has 4 layers of unidirectional LSTM; the attention mechanism is the general form of dot-product attention",
        "2211.03237_bf4e50ebffc967cae8000dbe34aad268": "\\cite{luong2015effective}",
        "2211.03237_153817e7debc92d14346fa353b9996d6": "; both English and Vietnamese word embeddings have 200 dimensions and are randomly initialized",
        "2211.03237_85e9ac5c72d8c2b8afc787697ba7db66": "summarizes the hyperparameters.",
        "2211.03237_80aaac78fe47f7d2209fcf0ff29cf54f": "\\begin{table}\\footnotesize\n\\setlength{\\tabcolsep}{4pt}\n\\centering\n\\caption{\\label{tab:chNMT_exp_gnmt} Hyperparameters of the RNN-based translation model \\cite{wu2016google}.}\n\\begin{tabular}{@{}l|l@{}}\n\\toprule\nWord embedding & 200D                                      \\\\ \\midrule\nEncoder        & 2-layer bidirectional LSTM (200D)    \\\\ \\midrule\nAttention      & General dot-product \\cite{luong2015effective}             \\\\ \\midrule\nDecoder        & 4-layer unibidirectional LSTM (200D) \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}",
        "2211.03237_1166118d89407a15ae7e3da7c13165d9": "The Adam optimiser is used with a learning rate of 0.002;",
        "2211.03237_24256e638091dbdf4720709c855d5a6f": "$\\beta_{1}=0.9, \\beta_{2}=0.999, \\epsilon=10^{-8}$",
        "2211.03237_d398577e46ff3cdf87e1f5327e1e66b6": "The maximum gradient norm is set to be 1",
        "2211.03237_11df3c5fbcff723dc44bf5cc65a2781c": "If there is a finetuning phase, the learning rate will be halved",
        "2211.03237_9fa52959af94e0ae469ba8dc98450fc9": "The batch size is 50",
        "2211.03237_87000c4a3ea665b6400e1b07b5e34fde": "Dropout is used with a probability of 0.2",
        "2211.03237_d191523a54f192b78d460d53179250ee": "When investigating diversity, sampling search is also adopted, which replaces the argmax operation by sampling",
        "2211.03237_d324fbc8eecf2e30f1a743069b4312d9": "are conducted with WMT'16 EnDe data",
        "2211.03237_f6a2294f2e4aa365e7e9b8cb1468b213": "shows the hyperparameters",
        "2211.03237_69bc7d7c81bdf2b9771b475446a400fc": "The models are optimized with Adam using",
        "2211.03237_b2bc0684de20b8a04f363cfd40716f8f": "$\\beta_{1}= 0.9$",
        "2211.03237_8c54e7feade9c6482b80dff95b1601ad": "$\\beta_{2}= 0.98$",
        "2211.03237_08486f6bb9800d814d6ebe5dd9092b79": "$\\epsilon=1e^{-8}$",
        "2211.03237_067d3c157bc90afac083f6e055e40ff1": "Following reference",
        "2211.03237_a5dd77c0bd50dba46573944e50ce1e8c": ", large batches are built to have a maximum of 3584 tokens",
        "2211.03237_36a9ddc46590934b00c9c1dc49175068": "The learning rate increases linearly for 4,000 steps to",
        "2211.03237_4264dba0094f54a7416f44dfb72ab063": "$5e^{-4}$",
        "2211.03237_40e1534902581393e9d7a8e30bddb223": ", after which it is decayed proportionally to the inverse square root of the number of steps",
        "2211.03237_8f90a1262246eb5d31bce05235e5e163": "Label smoothing",
        "2211.03237_4446de0215949fd50489d14f3b2aed9b": "\\cite{Pereyra2017Regularizing}",
        "2211.03237_c32a0c10e0173c81918903cc7209b9c6": "is applied with 0.1 weight for the uniform prior distribution over the vocabulary",
        "2211.03237_0af2a1d99cfb737052b743432f4787b0": "Dropout is applied with probability 0.3 after each attention or feedforward module",
        "2211.03237_7c348c2241e47f1ecde291ae0c7cdb2f": "Half precision optimization techniques",
        "2211.03237_d3d07424856e5c2af046de90ce9baed2": ", are adopted to speed up training",
        "2211.03237_1341501b0936b0bb59707a138baca218": "\\begin{table}\\footnotesize\n\\setlength{\\tabcolsep}{4pt}\n\\centering\n\\caption{\\label{tab:chNMT_exp_transformer} Hyperparameters of the Transformer-based translation model \\cite{vaswani2017attention}; ``FC'' stands for ``fully connected''.}\n\\begin{tabular}{@{}l|l@{}}\n\\toprule\nSub-word embedding   & 1024D                                                                                                                             \\\\ \\midrule\nEncoder              & \\begin{tabular}[c]{@{}l@{}}(Multi-head self-attention â \\\\ Feedforward) $\\times$ 6\\end{tabular}                                   \\\\ \\midrule\nDecoder              & \\begin{tabular}[c]{@{}l@{}}(Multi-head self-attention â \\\\ Multi-head cross attention â \\\\ Feedforward) $\\times$ 6\\end{tabular}   \\\\ \\midrule\nMulti-head attention & \\begin{tabular}[c]{@{}l@{}}16 heads $\\times$ (64D query / key / value)\\\\ â 1024D output \\\\Scaled dot-product attention\\\\ \\cite{vaswani2017attention} \\end{tabular} \\\\ \\midrule\nFeedforward          & FC-4096-ReLU â FC-1024-Linear                                                                                                     \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}",
        "2211.03237_4bf54f2384f9cc370e9d1c98c3066f25": "Ablation studies",
        "2211.03237_4cbe20747c69ba3c3d6692010fca0869": "\\begin{table}\\footnotesize\n\\setlength{\\tabcolsep}{4pt}\n% \\renewcommand{\\arraystretch}{0.8}\n\\centering\n\\caption{\\label{tab:chNMT_exp_paf_select} Ablation study on forcing selected attention heads; BLEU and Pairwise BLEU of Teacher Forcing (TF), Parallel Scheduled Sampling (PSS) and Parallel Attention Forcing (PAF) without a schedule; the models are based on Transformer, trained with WMT'16 EnDe, tested on newstest14.}\n\\begin{tabular}{lc|cc|c|c}\n\\toprule\n      & $\\lambda$    & Layers  & Heads    & BLEU$\\uparrow$   & Pairwise BLEU$\\downarrow$  \\\\ \\midrule\nTF      & -    & -      & -  & 28.68   & 31.12    \\\\\nPSS      & -    & -      & -  & 28.19   & 30.17    \\\\ \\midrule\nPAF      & $+\\infty$    & 1-2      & 1-4  & 28.38   & 31.43    \\\\\nPAF      & $+\\infty$    & 1-2      & 1-8  & 28.53   & 31.28    \\\\\nPAF      & $+\\infty$    & 1-2      & 1-12  & 28.80   & 31.85    \\\\\nPAF      & $+\\infty$    & 1-2      & 1-16  & 28.74   & 31.31    \\\\\nPAF      & $+\\infty$    & 3-4      & 1-16   & 27.94   & 31.61    \\\\\nPAF      & $+\\infty$    & 5-6      & 1-16  & 27.46   & 32.29    \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}",
        "2211.03237_67cf50f41dad1a45a8adf128343dd61f": "has shown that adding a schedule itself is not enough for parallel AF to surpass TF",
        "2211.03237_6b3f0b249533a9fc7fba89e92015f37f": "Another series of experiments show that limiting the information passed from the TF baseline is also not enough",
        "2211.03237_d5c387bac485c3ea02de7a149ef74883": "In other words, the two techniques must be combined",
        "2211.03237_a7f76765f90d6e945b12d974f092d290": "shows the results of forcing selected heads, without using a schedule",
        "2211.03237_ebce3d4746bc183f472eb7db13005868": ", different layers in a Transformer model perform different roles",
        "2211.03237_877973887f66fd60874bd006b134d8d3": "The last three rows show that forcing layers 1 and 2 yields the best performance",
        "2211.03237_37cd6a3cc30360dd1635bef252c98896": "The first four rows show that once the layers are selected, forcing more than four heads generally leads to better performance in BLEU and pairwise BLEU",
        "2211.03237_c6fedbaf1cb91ac28107163a2cfb6c55": "This is the motivation behind the setup of the experiments described in section"
    },
    "hierarchy": {
        "1": {
            "2211.03237_708aede88adbc288358e60e7a474bb44": "2211.03237_f6cd9a7a24fe51353f5aa75b23f78879",
            "2211.03237_e353dbe42c8654f33588d4da0b517469": "2211.03237_f6cd9a7a24fe51353f5aa75b23f78879",
            "2211.03237_3241f27239eef5f6d6db4f504b105cca": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_8e86672508b516b6604c3db80f8ff40e": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_9fc20df0f4ee1dd08e974d43f560206e": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_ca6d8b3c86460dc12a760af48eccb20c": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_62e9887f9136c6b7f1cb8614a8889a46": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_16196f5fbd2ba6c3492306449db180b4": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_72dd38f7baf9875c1c0b89ca4b845a99": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_15d7fe1c5c8161e56d18cf133e8f341f": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_0b79795d3efc95b9976c7c5b933afce2": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_01d5ab6bc12e63f1b292ac1b89a3998a": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_d7dc853cd8cb39098559e64628d15189": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_2d5a453a758bb3a21c3064b748d26ffa": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_67638a16078751387ac15daeb140632a": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_22fee921c75058ca2fec455b125ed9d1": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_03e94cd028bb5212f2db3a0dcd179c3e": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_11f19af116b87495e3a0c5814798ff19": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_64091d73ee714d72068ce8fec4251d94": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_bf85162521103c8f6c2df000ae3a60d1": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_6055ef10a36bb16a03901454b9b2b563": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_b5c1acc169f101031007064c4886401c": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_1e8f90d848b9da7b908fd637dbb55e04": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_fb0f3979fbed80049355fe4075f71a54": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_2e69b3b9a2d6be9f90a5708c6c3ebe18": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_c61caa1c934aebfea649e4ada8958d66": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_18c36b857d96959aebc9de049a18f51f": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_3edd333405a512d5f33659101b84eeca": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_166e1990f11be36d115eb58fd54ba22a": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_d71ba0b53b33d4a8dbf93a8d6286e647": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_2fdb4d0205b168a4fc84398a3e37cd72": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_cb228d402723c608530950de36efd3b1": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_25228685733f4e1b46af3e00bded04c2": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_0a7adb44bb57b55f7408f935a55a7210": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_63e39b5ff68853943e1dc24c87f24356": "2211.03237_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03237_66396a60f38aa383916a543795cd64f3": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_a8f99d30a5d8298c5cf25ce7c4274f50": "2211.03237_66396a60f38aa383916a543795cd64f3",
            "2211.03237_6e99ad71ffe0ad88cc3b9f6c0f04acd6": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_6f590e615b75efd9a0c2382b8eb4b9e0": "2211.03237_66396a60f38aa383916a543795cd64f3",
            "2211.03237_33ec7fb8ac34096caab6864e6d59b368": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_5058f1af8388633f609cadb75a75dc9d": "2211.03237_4bf54f2384f9cc370e9d1c98c3066f25",
            "2211.03237_e4be693f65518ed2163a88d73790db2e": "2211.03237_66396a60f38aa383916a543795cd64f3",
            "2211.03237_0d3b3ca33334697a4e51cec71663e64d": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_6b85c501ea4b4dbf14f5e0f7c2e7909f": "2211.03237_66396a60f38aa383916a543795cd64f3",
            "2211.03237_b9f4c1cc743af7b09673ba380390d2f1": "2211.03237_66396a60f38aa383916a543795cd64f3",
            "2211.03237_9431aa15332f1042bebddb1aa9c08b8f": "2211.03237_66396a60f38aa383916a543795cd64f3",
            "2211.03237_1cd4f9dd1a43b226bee040f5d88def5a": "2211.03237_66396a60f38aa383916a543795cd64f3",
            "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16": "2211.03237_66396a60f38aa383916a543795cd64f3",
            "2211.03237_8a1fa078ab2ec4c143ca552c5f21c5b9": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_46a94433f185c5aa4c914721b41a28ed": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_666df2f48cd16f38212c093999248769": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_dab640951ec26735330cca5b97f8106f": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_6cdca64ac0011e6fca70d65cf9f21f86": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_d8baf8d57c3de7a15243f027ac07bb40": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_bc8da6433819c46eabcdba987b294264": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_ccdd68c3a34b5a8d868df287b462aa02": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_f91652aa0e855c9ac052d8159410edf2": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_853ae90f0351324bd73ea615e6487517": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_83189372d03fa78432cfe2b53fb75ab0": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_567904efe9e64d9faf3e41ef402cb568": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_1c5db0dfefabb38977fb3d06d24e16fd": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_9eecb7db59d16c80417c72d1e1f4fbf1": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_a06451f378b7162f45e402dd9cee52bf": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_7a02aa022b690e7166ad763b440fb763": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_8a0e2b549d223aba55a866c38d1c9275": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_b7f8f7fa95d6859a2ddc9e4406d6e7d5": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_0852a31655e9da6e4295d5d00d18ecde": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_11023aa32abfe6132432d7ee8b07fce7": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_4f4f4e395762a3af4575de74c019ebb5": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_338a7cff4d1143ffe90b346fc674c1d8": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_f7a0d8a09223195ad85a5ba1fe08a5bc": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_d7f0de3f762c11c90c71962455d7d3f7": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_c0cb5f0fcf239ab3d9c1fcd31fff1efc": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_be5d5d37542d75f93a87094459f76678": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_b836826b1c868296f337377b2b61e1ce": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_b14a7b8059d9c055954c92674ce60032": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_f64f79ad17c83193d7b252e16f9cdc02": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_9371d7a2e3ae86a00aab4771e39d255d": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_43ec3e5dee6e706af7766fffea512721": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_7e6a2afe551e067a75fafacf47a6d981": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_49406e834d03adedb812f1c1ac1091fe": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_b99834bc19bbad24580b3adfa04fb947": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_02976acc6c3c7e0c690d5424c0b43d93": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_01b6e20344b68835c5ed1ddedf20d531": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_e3c884d6dc1c82cb4fe4c8b2b2aac54d": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_6ea8bee408f5049beb06c22a73991f17": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_bc45988429e68600175585ade55e3353": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_ffd9a42674409f2a92e35632a71f4fba": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_370964eec07cba8c52acb1ce1b35ab6a": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_55c752a5bd71596bc0aff31531053f03": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_6fcc1b8aaffec3bc3b754c3a227d052e": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_78b3dce30fd587853f3f25d209969c7a": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_beb70c9b5a6b5d3907385f5f67e9cf87": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_c699133b4b8bf5882968ea5c3e8cde17": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_f9ff7bbdbb85132cc4136998f62a4079": "2211.03237_2f31ef94bc40bfcf0db269a7a792dd16",
            "2211.03237_d5bf96ce448b27486e749926c12bf073": "2211.03237_66396a60f38aa383916a543795cd64f3",
            "2211.03237_0393ef102cbbaa8400333c3d210c8f4e": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_79b4a1afcb3cee6f6acd1b00a8726504": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_6653fe39aabcc3c69d5d6ce236c86c62": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_9480c86853f1a6ad27284aad8ca9c4fb": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_eac6fedc4e717ecaac78f60be5299720": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_40a45b154f0e0e2c18d037d5182e7477": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_6672a80bab8b49d021baa7a9d3cebbab": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_a3bfac1a31379f869e8886dcdae41eff": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_e52cb29c369bb4353f0de3b1ef035528": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_6003bfb79a3176ab654015f82020cfd7": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_86c08392411de17249256c37cd4ecd7a": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_d3ac4d923f9a8a9293aaffea0e41c2e0": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_750af08bcae0e43cd6658e1e824e659f": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_1d90bc695b9ea4faf24dec4329241f1d": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_eb0e1f947572817a62cbeaf85f90c630": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_00ba8ba9b9376e5c9780798dd79d10ff": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_f9c4988898e7f532b9f826a75014ed3c": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_f0de468c61904ab040a628221ffc2979": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_55a049b8f161ae7cfeb0197d75aff967": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_f9623005bc53f64036ca95afa3c345b2": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_22a473b274a614f54b4f8593e2a70218": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_07af1386a32cb1bb5028273e14af418d": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_703c41b12d7c079491ed89f4a299ad03": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_cfca3c24ea3ae7f3b484082ff13a5ed3": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_1fde9f7dafa6e21d5ea510823fb786b7": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_3373f5609b6acf68efe87b8d25a6bfc4": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_6d213c0b980e809712eb087963a37d5e": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_4d288677725268e0014d9865d1ff3bbc": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_431fc985bbe53efb695b2f19cc88c14c": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_f1cd8e0c8a143ef40d6e62b2b8f046ae": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_92f3b7cfc13afef13490c2690f4b0496": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_731ea7d8303734bf57ca113183dbf0cb": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_ccd909a0789143de6eb610060b13fe41": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_88128fec49c5d6ac46e92de51554bed1": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_74a18ec85d73194af602e241793aab1f": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_5f9d6b8ae6b216c84b0e6c06ae769f40": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_699903aac1ea4b2d1eb70fb7294d39b6": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_3082492e724c3157ce17ccd6bb349068": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_f825393c3553171cfd15645cbdd09a8a": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_29bf412631dde69f0f02d670efd91bed": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_f69263e6b61cc3a0bbb409a29dcb2283": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_d4d7423d7dcaf93da91675229e64f611": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_94fb103d2ba768b1e16ef21db0e575fb": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_99af422c6072c0778f00dd45a86b17d3": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_bc46982331ec624563254dca0f419d31": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_c070da04683cf07cceb271c6bb61a1a5": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_b10bb21c07aa31df1aa86e1a10ca0560": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_424d29f00c30d23b5dac6d32c39f986d": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_e0685fd43e7989acc88a9c07e78445dc": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_ddb367a44cb8b6803222ba8ee72a0b10": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_7f738ec54989b7b7190b1a3356173539": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_28d2e9ad717f001327044b49fbd5fd50": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_116a94f691be516e15db92ef16dfbd9e": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_c0bd028b228d5042d917103067cde39c": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_f08e83425de0977de174774ebb80c75f": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_5b46bfc1ea33ca2d10fe06c883316aa0": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_41f8ca2d768e4739d0861d753766331d": "2211.03237_d5bf96ce448b27486e749926c12bf073",
            "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_8d6abb9ea04626038fc88f4cb5d5a9ca": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_e0510a8b5359e21a3d81a4b974205884": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_996b8b7c1ec55dd743396d6e7d05f6cf": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_291c38084b2bb6256f846d6a977cdecf": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_66a8b53e0c3f158793c8865bb53252da": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_cae6404c4aecf46684930fe2a86676a6": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_68f6d1e2a5af193b053aec5877fbaf11": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_cf14d225b7a2bb7cada4004da229e118": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_e34d147f09f2c192d5f99705c1bcebb4": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_5c7ea5e82198880f2e4f16b99d412932": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_0671de4fb80046863460e326e7098f6f": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_aa5114a8e2cfaa500f251a93accb98e0": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_aeba388a64a6db929038580e30a58f4f": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_e37b7710659e462207e7a33bb5e31cb0": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_a55164aad956cfd0137faca10edca6ab": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_c62b4db3dc93b187c799f97bae7a68ed": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_be2efcae3a3ae8b6d0866500fb4be872": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_4069a4ae5ea2121af744af6cfb8a0c32": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_6fb73b2a3f93a9dbbf42e58025bde35d": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_168c3b2a24a9bbdba43074daa99105c0": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_ec95cb6e492bb4874f966d05f9255868": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_3fce714888c89e7d09799dcc83f4e880": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_21c332361d9660c7f7f24d4ae87b745a": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_58ed3f084ab3ca6fe5b0c84afa67df7d": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_f4f89ab557423d1ba7fc934127a0352a": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_84c6e4a5f7953b496bb34868c19256f0": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_fcec949e7d2d437851fc0307077b35df": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_044e8ec1269926086e9adae1a387a1a8": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_16405fbb0d7b1099e7cce3aa6d702b5f": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_f80026cfa1fa85fcff5557b3a389d369": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_c62e957b0238892123faf9589bf3c81e": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_1697bc01f30a143846822c42b005ffbe": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_026bc1214b443cd38910c79b3e9ac5c2": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_6128884cad8f8db27c1273a5183f9046": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_32afe1aee01096a8e566b32c4da1fbab": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_ff4d1898a6d2b0822f0c52e4d46e3a8b": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_94cbdb2a2222db6554ef82488bf13704": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_962fdda7283421af0fd680e8972b4af7": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_c08e6c61f935ef98f82d80ac6bc16459": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_543255c3c152606a6221ed8e57d518d6": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_bd16210b1c55682486487dc3003ee0eb": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_ba4a16823206b62a57632a7afd6d50b7": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_261c1da936217d03c804d367e653a5e0": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_c3b83358b13d7a35e301afa65b2be14f": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_a19bf05a563da2e92a5edc1b7af72422": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_264e9417c4a2a7cb05aee0152ef44ba5": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_3dbfda516eb9b4694662b192a52cb833": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_11c596de17c342edeed29f489aa4b274": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_92f56902657db5adbfc5df9094d25296": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_a2f9496e2808fa7972186434d074c6c8": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_e41b38e1c09e9ed3723f4de2e8bce085": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_4baff9ce34b2f9be5a26615d8246a7d6": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_10b9b070d1deaa5ba66bfa405dcae725": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_caf254ed629b39dbb47b78abfeda0d0b": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1": "2211.03237_2fbd0b716c43d6dd46a524a716324edb",
            "2211.03237_53cd0f1129ad097eacb779099855abf9": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_fb657c737cd3d95e75a82a98a8b78370": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_3ed145b64a4d2750c626c73d8060146e": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_09831db76237540795cf532129d4d153": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_0fb2e4dc7b704ee782e4143a68530f4e": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_fc23a3f4b881bd96a1b46942b6961749": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_a78a6d8e2df67024b5753e07eac74ef0": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_63fad2880be947e5a98713377d1dc212": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_18ec7a4a104cca2191f82d1fb74c5a45": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_784f564a2ee656bb2cccae3dc072ec5b": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_b321a7ad3af2977966524b4a152f8758": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_1e2d7d9957163d571e2aa19f1c22b6eb": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_30852bd40ab98fffb9069b44d5c59483": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_816e43212243ad04c3e56e658035f84f": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_0270020c992a2f7da14e0da445f71b79": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_538689e2ba2991dd90dd6d7a78b02644": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_5bceb7dc10358b3dc7fa85abff06575f": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_dfa693049b5a87f66caa54ac66187647": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_ded7df61a820c4972f1216f8b33013d6": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_8b7b05f7fa771bee94cb717141885a98": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_3e049feb2f67a1c94683fa169e2b69e3": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_d416f2e1eb207e52d4c7175b6e4d9c89": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_5f9d319c7ff79a7a8e9683ff531e2de5": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_4d62fb3e0e46b4eb3681930a7a624b09": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_df38dc0d0d3a10895aaf77aa674b6b3e": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_ace63f7576ba80f4166508122f0d53b3": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_d60e6e7e41e62a054ac81a8cb450f17d": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_13f119b889a7d289fbe4e515a235508c": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_fb5a827d9694ede970b94fd907f48ddc": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_1b3572e4769f88b804e56ad615d96c53": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_5e3d27bfddb0faf90ac9dc1a1da25b74": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_2f692a1b42d97887a10cdc4fd439ee4b": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_0bf5806281375d529ef0143e191b635b": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_6adc37fa8bee10b315fcc10d4fdda00f": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_466f7daabb4534ae7342c6a9a5a04669": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_13004378fa302b2165711e6ca149257a": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_dd471a51ba375d64965d6bec1939bab6": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_77fc4677b75316306583e68f22ffe9cb": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_ef6efc3d25d8730ddcf919e25806b1d7": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_ce9ead88dd6e34ccf1de0b64604c2b2f": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_b66aa7852804bc5cb725d176c23653d0": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_786887572f6ef1c20f2d8177cb2f1639": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_4ae39003a757454bae50e5ebd2f1f04b": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_51a404744077e3984ba1d1a22ab51023": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_def50838bc8154bb67a98ea31e591acf": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_c15a3bc471b078f7f7ba95ba5812c2a5": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_4940308f97c391541eb2913151c83d4b": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_d2619d2336c6d0cdfb95499c28b40af2": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_790d3622a1a69e84bf0f9e87a266991c": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_d5ad55aa8ce283a36730085c4a4d2bc0": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_3bc5f99ae0a43414555a3e0a72da2a33": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_6e91d7f8d8294cabcfaaf143ab724082": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_69562572b4697895ac6d1328e6a90ef2": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_9d9597aa60d2013edd9447d91493925a": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_9ffe5f84589e3e2e166c75abf6b523e6": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_6ce8dd9441580d04b73ea2e98b15161f": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_38c0f1319473c9726f3824232f932222": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_8e1d39aa4a72eaf86b704caa99811539": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_d270707e4d30a1f470baf56cce35575f": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_fcf7472f91639d8074b424d6329f19d4": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_fd8be73b54f5436a5cd2e73ba9b6bfa9": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_e3194c938ea4305e3c24c0d7b04fe7a2": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_7bf1054bf2055f6f8f5a941edc183e35": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_c3eb2607643e40f5f07d0891de74770b": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_592b0a4c21a45bf0edde58822bccaf46": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_b6f9da811716861f2aa5d9555f8fa056": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_dc3d54eb8b26b00380b76e0ce18c8b46": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_6a8fa6a808df0142a1f3660af09c9458": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_3f8a2a6347913054471a67d4693b48dd": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_dab3738c917fc55493be33d006734c0e": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_3ab628fbf316645730b045430991b5d2": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_66b2f3b882205744398df9e03fc76338": "2211.03237_2fbd0b716c43d6dd46a524a716324edb",
            "2211.03237_ed2d4c1ec16890c3e3befdd9e48561c2": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_f361ca4a0e082e29c31c6296eae1cb0e": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_a66514b9cc3c37f97e45c6a1e94dbef7": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_2f118ee06d05f3c2d98361d9c30e38ce": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_7d0fde0c6afcdf1f43dbed8c3fa85e5f": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_cc1351fcb5dcafde51d0782dca14ac91": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_d6625c1d4178e2b9f77698bdebd50e52": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_a29f17caa3e965b909d1aef183a202e4": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_39f67f551406b50bacba898efd606cad": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_b51e85518bdc7b71fd59b8df3fa90dbb": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_f62f8e4f637d80bb059fbd2e038c4ccc": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_ee8268b2d1242fea2acd1903bba5399f": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_13c77b6975af2651451e16bcd049640b": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_f3f8be025dc0acf3f78196fdac9b5ffd": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_99859dbf880d25c8f2c0f1ed21d027d1": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_cda5b64a7fc5b6f7e55d4e7b340c33c9": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_d86d6921d7ec7c32d7c1e612aafc9b1c": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_347c490a2a25192d11b5f7922c3ef120": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_a059d0d09e97bf9d6f842d4f70342c7e": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_02145ec1217155319b628714585fa080": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_00beae5cb262f9de2b2a71f91537ee9f": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_68b1b6f337ec4fd6ff05b71f18617ec8": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_0041f9146b0961034baf13d0ace6a294": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_b5f196050225b26c6190fe21cbc39cd5": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_d6328eaebbcd5c358f426dbea4bdbf70": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_b99c3b80f46ac1ccae7520a8b046a111": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_62fdb10cfb4df634adfd045e0d40f0ed": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_502edc9729409363fa2f1000139926db": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_aba35a115d4856c3f939c064ee7b3e2e": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_a940f71611d3575d4e1809a76ba17592": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_e900eb45fc0df2048619b5c25bb21ae5": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_692287ec96d0b2ab5c2334cbbcba404c": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_747158ad3a63299ee30d320ca4cdf2e8": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_e358efa489f58062f10dd7316b65649e": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_8ce4b16b22b58894aa86c421e8759df3": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_284f6ae814b2ef5cdca445798823b34b": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_3aa8917c1ab600d5891c721729303509": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_0af62e9b08249d6ce07750a6ce97fcc6": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_a60f5d76d8e66f43b9f9abdb7ff307af": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_9a572e469e76eb6d1568e06cf3b7f850": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_bb74153bd347061fd5cc3131b40fc551": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_b2fb07066a53d19796bed6817f40b78a": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_e32d21d78f2e48d3f6661fc11e5f7f05": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_47eab8e3038ca060222db197a7050d34": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_2206d93a86b1c2c297be6dd1527fed18": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_2f3a097e3d7db8ba13cdd6efd1853045": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_315b7e4f8354020fa37f5e28193bf261": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_c59ad9f6bbd20662fbb2d4e041f0dd64": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_f482e7e8e2f8868c060e48758bf75707": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_8de3444c0bd7f39154a2aacc1226b7f9": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_1d6964ecb70e3deaeaa5161eff69deac": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_fc26cb222807deaeac73a308d55f4eda": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_ade7b474252c483b26dd15855ce61af9": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_8d698b19dfb0cac360bd6abd169d7a2c": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_a6a1447fc011c67560e55aa7f30831ad": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_8674305876eda3dcc8f735853f25e7f9": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_4e8c8e5c1107e5b4bba39c33084ae536": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_3fb86fe794faf6f1c6d10edad601d08c": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_031e4fe6b82d72238113e90d21dd8104": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_3b88c33649ac0189e9ed2cd2fc51dc34": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_30e4a8e65f627253af9b1c789407ba44": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_50db597c2429a1b40c225ef8bce7541f": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_92e1f3949b053f63865cd5024bf136be": "2211.03237_2d00983bd50ef45d3b2ea61150c9cdb0",
            "2211.03237_d438366799aa6034a31e35591bd4d8d1": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_8db76ef49b5be1d7d78fdf86e4eb71ce": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_f668f111ef03c82e996c3fdd90d4a10c": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_9d43404795c0a1f3dc7a46997ec06260": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_263230bacd81adf932a631b4e7ebbc6e": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_1e6acc41aa32cbc2631edca27c233358": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_1fec803dea9bc06e2085d33e0243c08b": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_79e66e915fabb7dd11fe01d14c079a71": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_484ba1fc7794eae7a09803e1138e6935": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_63d5049791d9d79d86e9a108b0a999ca": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_1656f2c7200872f21b6420f2e5ef3d32": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_d95867deadfe690e40f42068d6b59df8": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_012643f5928d809589b09c7e12bfb993": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_0d89e715c5e24469f63deb69f1ee4d90": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_1b6dc195533e55b3c3e4f599b19cddf3": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_afbaf99bea54ca3b3074dc20fc29ac9d": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_676851aed24ba5e133a54b1bec5da540": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_80d3e9eabed32088591420fe5a3c2cd0": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_b484b8402463767614616e1ea24fbbf1": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_3d7dd8c39860ea92cba3fae766384492": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_d4a1d4c397105cdf5e0338c96bab3b6a": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_04a87841d232a938576582bd5c30035c": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_32105bf81670986762aed8c24d68839d": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_161d7388390c20539c8aff4fcc1349ad": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_fc885fc90f50de23e711a53a9818082a": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_8b579ff256d8c486c0d8bd186a16ba15": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_ebe069f8a3da6b22040cba2a6f746e14": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_615158364857ca839978f45b7f0dc952": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_94ae3caa63f7856d889955860bb19129": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_39558e297d6cd9378551dfda106622ca": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_65c0dcdd86cb1b38bde0035660174cdd": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_1c0b56063ceb209cbf8d8b189a8c8dcd": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_a1c925b27bc89e36bdfbea6755bc9b61": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_ef269b8c1241f872d04cef63bb36510e": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_d05bcca54b9013f28f19ba7dd5388bda": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_2747ca1d0edc868e1b950ba30e9c271b": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_92fe28e3877f479c7ed100908f58369a": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_93ba9c4aff7979d75d74ea6be2a45f3f": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_cec13676abcba9a76572b7cea2141f09": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_7cd1f92add5235575b06d6f131ad933a": "2211.03237_92e1f3949b053f63865cd5024bf136be",
            "2211.03237_4829262cecb9828817b33e0f9c907f91": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_c62157c00326f83c7c8ad111cf73cbc5": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_cac0f1ddd1caf90a41ce4a4eb06fa819": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_b0ccaa3b3dd6f1083a1b8f7d1a1c4417": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_17e2c72ac9d61ea4f7c15a7116075d8d": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_51c45b795d5d18a3e4e0c37e8b20a141": "2211.03237_4bf54f2384f9cc370e9d1c98c3066f25",
            "2211.03237_e6d818988b5b41ce97619350496cc2d0": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_19ba5bd79ad2ecd90821f84d5c34557f": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_8a3312a9d58c501d361adc915e60ef20": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_2d86160c81d365162838bdd09024e4e9": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_006c5457305ce3a4f4937a06a02b4dd9": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_10454456bf61483e87d87b28d6fc0846": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_eb56bf0bd94a4717a6f0fd0929dd1540": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_348c6f46bcd1776e83d3e1417f1b7a9d": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_5446e34754a7715298b13f9f6db9f113": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_eb49329d6319d9bc08deb624c3ee0e8e": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_1647deb3ec47eb07ff208cc10889b242": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_a39c4b6a13cc1cf9b55e228ef1d315d0": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_5af4d5b0ee35f5b6c74f29d8f77790fe": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_873681762ff53b5029e3aacb940e18ff": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_670bf7e000cd36d76c014728e7f4094b": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_ec5e18432ee22614f569a1698cb63d2d": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_c0eec38d7bd8cef3c4f5fe97de08eb11": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_d9c550097b549378ef126d16215eb1e3": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_4ae8feebe2ec2c66be45a600d09e13d8": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_53ad699b31123921b09f0a7845118ce1": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_49dae9f4896aa5e9b1eee785b2d66953": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_f214a36f74e10d55dbbfc7ba8c06ec6b": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_baba4995a53fc9729f15f22dd061dbba": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_38e85ad61643a8fdc395f415c69b6711": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_cd1b12747087873bd70c15248af5e951": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_354358e27bafe01366dae6fcaf524d05": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_879e0d521c2ac9f35fe20788fab143d7": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_fb97d38bcc19230b0acd442e17db879c": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_4cffc6488758193baa7482b981d107a3": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_58a9b69a8e58c47fbe66cbd1855cdf16": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_ee989d9aa368bef635f53ba67dbee9ba": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_5159ae7d461788ce38cefd5aebdd3db3": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_d4f8a8da766004eee31c021418c53b95": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_e5dff2a9138b8fcc2b3af09af888ffcd": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_6c1267e9bad2d2ccde8347062e48bef2": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_41c69616de1f6f5985f6796e08cf8645": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_1e7b4ec28645b83607e1cdc9e546db20": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_01d0005c80042a97d215bf60abe93d38": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_6e42c9a31a013f46d5ec333ad1b77e89": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_261e493d74c6ab360974264370cf3b22": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_dd6265dd2cc73f14b934d9d6d606216a": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_7f159679c89bf23156350141a963e109": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_8d30d188a673f6bc8424b8e6edc03cb8": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_3224651ce366326aac37aee901ee5aa1": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_90f76c417e2895d53101393bcb663476": "2211.03237_4829262cecb9828817b33e0f9c907f91",
            "2211.03237_4470066f77a448cc5058718fccf33ef8": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_260e6206980e26ba8901c85e50f8436a": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_37341b3fcd8c1f41c2a2f0fcf90fdba3": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_510842838905f62dfcf9d66e5660687e": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_690828b812b64e779aad91ef364e573b": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_8ac920cfe5fb8dc369976bf10cab5fcb": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_e7a80d4afd2ab8336797f96638e9eebc": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_ae0ed33e917b09336cd02b67bf995d0f": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_889043c91d7ffddd7db4a85f2a263759": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_70da0cbcdef582399311f1cfb7f94fb4": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_5588fdd15e0ae41e2ac57e1da6750a69": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_d30d3b938216a48ac09d70c67766e288": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_551b11ca8bd077db603eb10f3f2943df": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_2a6efa583e7d54391278e925ca94b284": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_0160c89755ea44da9477a6c09a774717": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_4f352a4ad2b4c699bca5860e19b41d2a": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_316013651692eadd87822b4dd4a85adb": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_e8776f9e8a0ab84ab29838c91b457ed9": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_c2e7ce9df1a1e4690da086763369e302": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_d6b174fdfe70c9b7e98c70bc2d074a3f": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_2c57f9c6722cf63adc22dd3e5562a881": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_282858a3bcaa3c015cec10204709fe90": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_958f62d64d5c72f815ef6a58f1ca3b0e": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_6df2924942a3000babf07a6a5e35cf98": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_c2aadd5d8e79b481f778dc06c0268484": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_31aee56c71153820af225bc08cf34afb": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_95c1d019403c25b5d2cce79933e124dc": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_b0a50ace6c86a9fdd41321b07329b8b8": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_3a0d4bf5e23712ac93e1df660fd57dfe": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_16d2508760f8065482da7a14ecb50fad": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_8e969febe9350c1eb405718833ecf4ef": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_124fda37737e8fbcfd3763f04d89cd88": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_b10c06e1b266578dc3545d9fbe3760a1": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_17bce7c7e8f2de6c70e046073e0ef13a": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_d84b70b0c1d65c4afa1b9dafb9b8176a": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_f62db12f95e34116f1f1e827b2c64ce5": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_b80b59610bc3c27c236827155a1d8d0e": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_fd403e299c50d01150a9cff5f96c3afa": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_9a9becd399be8ddccd3fd6d875d68089": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_72f2f3835101c1e41a5ffe85e9ee4227": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_2446771b2e50fbf05b763a872271b618": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_9da16bfc4cc879f9dc40721c3275fd2d": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_dacfd91c7596b86604eb0b426a41e84e": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_4479e011c6001bd0c8b0d468db33998a": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_d5782915ded1ead8dd8bc881ff3f4ac7": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_cea38dec744d5c961d3b81965ac4999c": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_0d3a9033bbd27b8c02cccb0c6596876f": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_db3540b319f86242f7867ce609ba7006": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_98603b39b900538773ae6aee29093fe3": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_6c338ce698031189f0b341492e55e9a7": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_8d1c05b72aada80a3ec37e2a593fe5c0": "2211.03237_cd1fe9cd9bd6fe0a1d3e41cb22fdbeb1",
            "2211.03237_8ba9ff75a04453dcf926e3d287072ff9": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_d3acf4766becbf4ebf47476cfe5b1d04": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_f943520daa007c1c248918b48330d025": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_8a62dfb06d59161af590690b385f1959": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_ede87580b3109c33a7a1c605f1130deb": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_bd372d8dcc9717eac0477d0c040fe6d0": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_9acc7ef13b9ed3966a5946e44d853159": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_7649a1305012163ca68123a3732be498": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_d973bc1c738e6b052bdb4c9c916d350c": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_c340e9e61466cec421a81680f3dc4826": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_d8cb1fde563cd7ceb0cd5974f9a3021c": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_2fae8e38070b382242f0fa09e14d2602": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_65d3a10e44576441452172bf8f1a3908": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_71d5873c82a00c96c94796df263c433d": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_b92070aba5459b042beaf85c40f22ff8": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_7799abfd14cfb468d2f54df7fc252458": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_71a7a819a5c7dd0afeb535a90f457218": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_0c4c7e3dfb41b2a613fbee21cfd3980e": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_21e4c6fd0ec560fa608dd3195f71d1c0": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_4437685fe845cde4919fb88f5036c921": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_daa970c5ed792f8089392e2e98a9ffec": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_29c4cbd7b4b12de4893172af93180043": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_13019063bc6a3fbedbb8abf221242045": "2211.03237_4bf54f2384f9cc370e9d1c98c3066f25",
            "2211.03237_81f4380bfc60485d55473c9e2f47fd15": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_6cc43fc15ac52e9cf5562ca8b5a26191": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_6c5fe8898c2b17c9c7205e677c8b2d05": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_0e8997fd84c3ef264f6d92bde1d1ddbe": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_099bd047108401e3efad2f192d037b17": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_b55d665dc0e25bc5845c07feca7363c1": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_476ca69b40c8bf85106a0d051f4e3e80": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_bfb763682cd6246df6f975e747756534": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_a1d67eb1e6e99dbf37ea0d1f6a9aa36b": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_9f10f5b07d483e5ab325ca70f71a72e2": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_5da63a807597364ec69082fcd34819a2": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_e6aef7738163e3c224a3db6d01b6ba24": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_d5410b49d352dcda5260161902b2dc08": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_e29d5dd0df34cb0f7a98af01e7c961c7": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_043ceebd8391c77b0b9152c10f70e6c4": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_4b263c7d011971a6898e0f7d960822c4": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_7df0e62090163c76c876b2e811554074": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_fc291f16b7648cbfe831909cc4f46031": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_6f8b794f3246b0c1e1780bb4d4d5dc53": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_476ae2cb2944d3d692a3185a93fab093": "2211.03237_6f8b794f3246b0c1e1780bb4d4d5dc53",
            "2211.03237_fee020ec6a053a1c893d94c881698311": "2211.03237_6f8b794f3246b0c1e1780bb4d4d5dc53",
            "2211.03237_21682eeaa568cb6316f5d1612c6b1f90": "2211.03237_6f8b794f3246b0c1e1780bb4d4d5dc53",
            "2211.03237_717f00b7cfaf9f690f052e8e963493f9": "2211.03237_6f8b794f3246b0c1e1780bb4d4d5dc53",
            "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_e89ef62242bbb9ca7a9d610a658225b6": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_0b97e857b3d92c771f35f125b577ba4d": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_7f12adccb54d24d72fb836d9390a226f": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_8c0693780382f3bf162ec87d71988ffe": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_8fc67e9009749e2e65fdca5242dc0cb2": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_a6c836c477252976b2ba04f0c33571e0": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_17a780f195c22772c556ed4f59a6efa3": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_7337a2bda2f2fc5ad84a4feca716a941": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_4ba600c5a92f72cf7e3a3ddc06bd96cb": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_be5a952c162a5f9f528f5078d1e972d6": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_1f16c27a5972b6d507f77f2c04c62d62": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_fac08ae536482677f5069d8763fc055e": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_f2d06f0e7ed30aa8b88ee51727195e44": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_2d1bfe07587496192995d84cd75dc442": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_a4829dba72c981b09367733364b83938": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_b2c39a0d7a4d3317e94668317296115e": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_97cd6debaf5e1085d9ff4494e3d2cdb9": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_cae5b03cf47eb200a6540dd8b734f162": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_406ca04349293a98cd532acd5396cfba": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_bd2b6e2f298bcb9262c240292623d9b1": "2211.03237_5456d1d278421b996ca2ede2e0dbb6a9",
            "2211.03237_68b87943d564c12fea83bb07cc7db5ec": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_d2c24d59e0baff4d0155fbdf62590867": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_90cfb408aef429c33e2c52d4b7d6e726": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_181d88ebee0bfabc1c182c2f6c9ac08f": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_86dd11ff3ccddc5855e408455fcfcaff": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_13d3aede1e0a4aec0535056933888fad": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_ebdbae7d6a12328e6efcedec86eee39f": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_a4b6a9cbf4853aec792bd188b8c295a9": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_9b237a2fef6c954ddfb977d63960550a": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_5d652883c2b121b4d3690b5d05dce9e3": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_6c69ed8c15a3bd8d8f91ad3dc6b264d3": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_ddfce10583189e947b577406f7cefb19": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_a7d402adb3dffb44de93f98f8f154ce9": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_5d6b8a585b1a6686516a97ab8a421ca1": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_16e06047245c4ff36cd4889d77afc762": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_7461dfd21e75d77395e7da7fe67dcb34": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_1175b2842e6d602c6962763add91899e": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_5fef6f43c06acde493115c690130c20b": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_2bcfa24c2f46b55022faa64370932898": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_52df3b63d926ec22e9e0f1ae4f486baa": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_afe969dbaff31a33889f927436c5a65c": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_ef3fe19e74a7bfd1e75b6f802ed4377c": "2211.03237_68b87943d564c12fea83bb07cc7db5ec",
            "2211.03237_2fbd0b716c43d6dd46a524a716324edb": "2211.03237_e353dbe42c8654f33588d4da0b517469",
            "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee": "2211.03237_2fbd0b716c43d6dd46a524a716324edb",
            "2211.03237_a2fddced585ad126645abe2e0231e7c3": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_04b2e1590db5dd0c59dccc20972b4c19": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_cde67776ad6f9c7e47f8c773ca692654": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_f2cd639c6fd7807caa8895e9fc2095a1": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_8546d714be6f700d52bcf89ad2152917": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_8c147a45b35974bf4c8da6c0833b1b18": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_44fba20f2ba0f6b409fbdd7153246195": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_dc32ba231d0471e2c29354378624fde5": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_ad2befe34810472101413a96e42fb280": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_a0631c5e2f989d18cd392626027c1de3": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_7bcbd2f3d4e8a794f5d9fe25977cbf60": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_4f7e8763bfc7585e63fc582a54e0ec45": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_d74a707969d2d6a639a1ef87ca9c6ceb": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_2405407c6eed068e115aa8457dc98ff2": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_62168861a2ea35cf829c04ce2e9154c5": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_95e27fe09625d861a2b615f3f19a4582": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_47cca55d4d42338c8c6c8ae53c08ea9e": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_9507023777f50a5cd25b2594bbc67f2c": "2211.03237_f6068daa29dbb05a7ead1e3b5a48bbee",
            "2211.03237_ad2376beebecdcf7846ba973fa1a005b": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_cf53196623798c92c76793f88f50f654": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_9d660150b7c98e93025f751f5407e528": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_7c3f09489ad9c1bee4fd03a4da9821c1": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_bf4e50ebffc967cae8000dbe34aad268": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_153817e7debc92d14346fa353b9996d6": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_85e9ac5c72d8c2b8afc787697ba7db66": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_80aaac78fe47f7d2209fcf0ff29cf54f": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_1166118d89407a15ae7e3da7c13165d9": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_24256e638091dbdf4720709c855d5a6f": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_d398577e46ff3cdf87e1f5327e1e66b6": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_11df3c5fbcff723dc44bf5cc65a2781c": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_9fa52959af94e0ae469ba8dc98450fc9": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_87000c4a3ea665b6400e1b07b5e34fde": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_d191523a54f192b78d460d53179250ee": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_d324fbc8eecf2e30f1a743069b4312d9": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_f6a2294f2e4aa365e7e9b8cb1468b213": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_69bc7d7c81bdf2b9771b475446a400fc": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_b2bc0684de20b8a04f363cfd40716f8f": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_8c54e7feade9c6482b80dff95b1601ad": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_08486f6bb9800d814d6ebe5dd9092b79": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_067d3c157bc90afac083f6e055e40ff1": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_a5dd77c0bd50dba46573944e50ce1e8c": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_36a9ddc46590934b00c9c1dc49175068": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_4264dba0094f54a7416f44dfb72ab063": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_40e1534902581393e9d7a8e30bddb223": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_8f90a1262246eb5d31bce05235e5e163": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_4446de0215949fd50489d14f3b2aed9b": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_c32a0c10e0173c81918903cc7209b9c6": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_0af2a1d99cfb737052b743432f4787b0": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_7c348c2241e47f1ecde291ae0c7cdb2f": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_d3d07424856e5c2af046de90ce9baed2": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_1341501b0936b0bb59707a138baca218": "2211.03237_ad2376beebecdcf7846ba973fa1a005b",
            "2211.03237_4bf54f2384f9cc370e9d1c98c3066f25": "2211.03237_66b2f3b882205744398df9e03fc76338",
            "2211.03237_4cbe20747c69ba3c3d6692010fca0869": "2211.03237_4bf54f2384f9cc370e9d1c98c3066f25",
            "2211.03237_67cf50f41dad1a45a8adf128343dd61f": "2211.03237_4bf54f2384f9cc370e9d1c98c3066f25",
            "2211.03237_6b3f0b249533a9fc7fba89e92015f37f": "2211.03237_4bf54f2384f9cc370e9d1c98c3066f25",
            "2211.03237_d5c387bac485c3ea02de7a149ef74883": "2211.03237_4bf54f2384f9cc370e9d1c98c3066f25",
            "2211.03237_a7f76765f90d6e945b12d974f092d290": "2211.03237_4bf54f2384f9cc370e9d1c98c3066f25",
            "2211.03237_ebce3d4746bc183f472eb7db13005868": "2211.03237_4bf54f2384f9cc370e9d1c98c3066f25",
            "2211.03237_877973887f66fd60874bd006b134d8d3": "2211.03237_4bf54f2384f9cc370e9d1c98c3066f25",
            "2211.03237_37cd6a3cc30360dd1635bef252c98896": "2211.03237_4bf54f2384f9cc370e9d1c98c3066f25",
            "2211.03237_c6fedbaf1cb91ac28107163a2cfb6c55": "2211.03237_4bf54f2384f9cc370e9d1c98c3066f25"
        }
    }
}