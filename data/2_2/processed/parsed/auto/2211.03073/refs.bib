@misc{1,
  title = {Philip Bachman, R Devon Hjelm, and William Buchwalter.Learning representations by maximizing mutual information across views.Advances in Neural Information Processing Systems, 32:15535–15545, 2019.},
}
@misc{2,
  title = {X. Zhang, Y. Sugano, and A. Bulling, “Evaluation of appearance-based methods and implications for gaze-based applications,” in Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, ser. CHI ’19. New York, NY, SA: Association for Computing Machinery, 2019.},
}
@misc{3,
  title = {Alisa Burova, John Makela, Jaakko Hakulinen, Tuuli Keski-nen, Hanna Heinonen, Sanni Siltanen, and Markku Turunen.Utilizing vr and gaze tracking to develop ar solutions for in-dustrial maintenance. In Proceedings of the 2020 CHI Con-ference on Human Factors in Computing Systems, pages 1–13, 2020.},
}
@misc{4,
  title = {Nora Castner, Thomas C Kuebler, Katharina Scheiter, Ju-liane Richter, Therese Eder, Fabian Huttig, Constanze Keu-tel, and Enkelejda Kasneci. Deep semantic gaze embedding and scan path comparison for expertise classification duringopt viewing. InACM Symposium on Eye Tracking Research and Applications, pages 1–10, 2020},
}
@misc{5,
  title = {Krafka, Kyle, et al. “Eye tracking for everyone.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.},
}
@misc{6,
  title = {Cheng, Yihua, et al. “Appearance-based gaze estimation with deep learning: A review and benchmark.” arXiv preprint arXiv:2104.12668 (2021).},
  year = {2021},
}
@misc{7,
  title = {Zhang, Xucong, et al. “Mpiigaze: Real-world dataset and deep appearance-based gaze estimation.” IEEE transactions on pattern analysis and machine intelligence 41.1 (2017): 162-175.},
  year = {2017},
}
@misc{8,
  title = {Zhang, Xucong, et al. “Eth-xgaze: A large scale dataset for gaze estimation under extreme head pose and gaze variation.” European Conference on Computer Vision. Springer, Cham, 2020.},
}
@misc{9,
  title = {Chen, Ting, et al. “A simple framework for contrastive learning of visual representations.” International conference on machine learning. PMLR, 2020.},
}
@misc{10,
  title = {Y. Wang et al., “Contrastive Regression for Domain Adaptation on Gaze Estimation,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 19354-19363, doi: 10.1109/CVPR52688.2022.01877.},
}
@misc{11,
  title = {Xucong Zhang, Yusuke Sugano, and Andreas Bulling. Revisiting data normalization for appearance-based gaze estimation. In Proceedings of the 2018 ACM Symposium on EyeTracking Research and Applications, pages 1–9, 2018},
}
@misc{12,
  title = {X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Appearance-based gaze estimation in the wild,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4511–4520, 2015.},
}
@misc{13,
  title = {X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “It’s written all over your face: Full-face appearance-based gaze estimation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 51–60, 2017.},
}
@misc{14,
  title = {K. Krafka, A. Khosla, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik, and A. Torralba, “Eyetracking for everyone,” in Proceedings of the IEEE conference on computer vision and pattern recognition,pp. 2176–2184, 2016.},
}
@misc{15,
  title = {Song. “Eye Movement Video Data” AIHub. 2022. Web1. 1 Sep 2022. https://aihub.or.kr/aihubdata/data/view.do?currMenu=115},
}
@misc{16,
  title = {S. Ghosh, A. Dhall, M. Hayat, J. Knibbe, and Q. Ji, “Automatic gaze analysis: A survey of deep learningbased approaches,”arXiv preprint arXiv:2108.05479, 2021.},
}
@misc{17,
  title = {K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for unsupervised visual representationlearning,” inProceedings of the IEEE/CVF conference on computer vision and pattern recognition,pp. 9729–9738, 2020.},
}
@misc{18,
  title = {T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visualrepresentations,” inInternational conference on machine learning, pp. 1597–1607, PMLR, 2020.},
}
@misc{19,
  title = {J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires,Z. Guo, M. Gheshlaghi Azar,et al., “Bootstrap your own latent-a new approach to self-supervised learning,”Advances in Neural Information Processing Systems, vol. 33, pp. 21271–21284, 2020.},
}
@misc{20,
  title = {Jindal, Swati, and Roberto Manduchi. “Contrastive Representation Learning for Gaze Estimation.” arXiv preprint arXiv:2210.13404 (2022).},
  year = {2022},
}
@misc{21,
  title = {Liu, Ruicong, et al. “Jitter Does Matter: Adapting Gaze Estimation to New Domains.” arXiv preprint arXiv:2210.02082 (2022).},
  year = {2022},
}
@misc{22,
  title = {Goodfellow, I. J.; Shlens, J.; and Szegedy, C. 2014. Explain-ing and harnessing adversarial examples.arXiv preprintarXiv:1412.6572.},
}
@misc{23,
  title = {Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; andVladu, A. 2017a. Towards deep learning models resistantto adversarial attacks.arXiv preprint arXiv:1706.06083.},
}
@misc{24,
  title = {D. W. Hansen and Q. Ji, “In the eye of the beholder: A survey of models for eyes and gaze,”IEEEtransactions on pattern analysis and machine intelligence, vol. 32, no. 3, pp. 478–500, 2009.},
}
@misc{25,
  title = {Yunfei Liu, Ruicong Liu, Haofei Wang, and Feng Lu. Gen-eralizing gaze estimation with outlier-guided collaborativeadaptation. InProceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 3835–3844, 2021.},
}
@misc{26,
  title = {Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. InProceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770–778, 2016},
}
@misc{27,
  title = {Xucong Zhang, Seonwook Park, Thabo Beeler, DerekBradley, Siyu Tang, and Otmar Hilliges. Eth-xgaze: A largescale dataset for gaze estimation under extreme head poseand gaze variation. InEuropean Conference on ComputerVision, pages 365–381. Springer, 2020.},
}
@misc{28,
  title = {Petr Kellnhofer, Adria Recasens, Simon Stent, Wojciech Ma-tusik, and Antonio Torralba. Gaze360: Physically uncon-strained gaze estimation in the wild. InProceedings of theIEEE/CVF International Conference on Computer Vision,pages 6912–6921, 2019.},
}
@misc{29,
  title = {Kang Wang, Rui Zhao, Hui Su, and Qiang Ji. Generalizingeye tracking with bayesian adversarial learning. InProceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 11907–11916, 2019.},
}
@misc{31,
  title = {Yihua Cheng, Yiwei Bao, and Feng Lu. Puregaze: Puri-fying gaze feature for generalizable gaze estimation.arXivpreprint arXiv:2103.13173, 2021.},
}
@misc{32,
  title = {Yihua Cheng, Haofei Wang, Yiwei Bao, and Feng Lu.Appearance-based gaze estimation with deep learning: Areview and benchmark.arXiv preprint arXiv:2104.12668,2021.},
}
