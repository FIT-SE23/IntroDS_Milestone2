{
    "elements": {
        "2211.03011_69c686a1e3ae00d774ff1ed33d0e844b": "Document Root 2211.03011",
        "2211.03011_a6164cb182c9c63cdade753c99db465d": "[theorem]",
        "2211.03011_e353dbe42c8654f33588d4da0b517469": "Abstract",
        "2211.03011_309dbf8458328e395c671cae07c6faec": "Reinforcement learning (RL) folklore suggests that history-based function approximation methods, such as recurrent neural nets or history-based state abstraction, perform better than their memory-less counterparts, due to the fact that function approximation in Markov decision processes (MDP) can be viewed as inducing a Partially observable MDP",
        "2211.03011_0c9fad2047a0144d8a5a7ddb84a53cc1": "However, there has been little formal analysis of such history-based algorithms, as most existing frameworks focus exclusively on memory-less features",
        "2211.03011_41b3f373fef3861bb0a21ba80dd9ca30": "In this paper, we introduce a theoretical framework for studying the behaviour of RL algorithms that learn to control an MDP using history-based feature abstraction mappings",
        "2211.03011_e62ac68e0e6f632a9559ba36337b32d8": "Furthermore, we use this framework to design a practical RL algorithm and we numerically evaluate its effectiveness on a set of continuous control tasks.",
        "2211.03011_0b79795d3efc95b9976c7c5b933afce2": "Introduction",
        "2211.03011_c48ed3abb0b442a8a0b00712244bd4b2": "State abstraction and function approximation are vital components used by reinforcement learning (RL) algorithms to efficiently solve complex control problems when exact computations are intractable due to large state and action spaces",
        "2211.03011_23d6f0f417d35bd3726ccbaec51d18db": "Over the past few decades, state abstraction in RL has evolved from the use of pre-determined and problem-specific features",
        "2211.03011_115206c0b4fca0ad7b71a2bed3cdf021": "\\cite{CritesB95, TsitsiklisR96,ndp,Sutton+Barto:1998,SinghLKW02,activesensing, ProperT06}",
        "2211.03011_c7231e55199e87dcf992f08907f0d61c": "to the use of adaptive basis functions learnt by solving an isolated regression problem",
        "2211.03011_c400beb29df46b5f1761d7a7d23afc82": "\\citep{kbrl,autobasis-MenacheMS05,6-keller,Petrik07}",
        "2211.03011_1e097e51e848d784b7ffde3ffe400f7a": ", and more recently to the use of neural network-based Deep-RL algorithms that embed state abstraction in successive layers of a neural network",
        "2211.03011_03594021c0847b4e978fd76a543f9d05": "\\citep{Barto2004SynthesisON,BellemareDDTCRS19}",
        "2211.03011_9686d6292766d1578881b636553ed415": "Feature abstraction results in information loss, and the resulting state features might not satisfy the controlled Markov property, even if this property is satisfied by the corresponding state",
        "2211.03011_e181aefaeb9c81936d245d31ed94aff5": "\\citep{Sutton+Barto:2018}",
        "2211.03011_857bbeec4a3c27b158f8f1be2467e856": "One approach to counteract the loss of the Markov property is to generate the features using the history of state-action pairs, and empirical evidence suggests that using such history-based features are beneficial in practice",
        "2211.03011_133363f4b1207280c00979b53ee2853f": "\\citep{openai2019learning}",
        "2211.03011_2aab0c594c5501bf23fc166a31ee0182": "However, a theoretical characterisation of history-based Deep-RL algorithms for fully observed Markov Decision Processes (MDPs) is largely absent form the literature.",
        "2211.03011_c0a798e82236a6762399268e1cf49226": "In this paper, we bridge this gap between theory and practise by providing a theoretical analysis of history-based RL agents acting in a MDP.",
        "2211.03011_ad688e4fb07f43fcf3af44dc0356c228": "Our approach adapts the notion of approximate information state (AIS) for POMDPs proposed in",
        "2211.03011_611ef6d6b926987a67ee0000b8824acc": "\\cite{ais-1,ais-2}",
        "2211.03011_f16115e11f3fec0df0bba0188997604a": "to feature abstraction in MDPs, and we develop a theoretically grounded policy search algorithm for history-based feature abstractions and policies.",
        "2211.03011_4fd4634702bde2a84ecc10dd483f39ed": "The rest of the paper is organised as follows: In",
        "2211.03011_250fb37be40d8c4ee899534c7ab2939d": ", following a brief review of feature-based abstraction, we motivate the need for using history-based feature abstractions",
        "2211.03011_efeb369cccbd560588a756610865664c": "In",
        "2211.03011_5b855c0e62aa57999cebffc1bab7fefe": ", we present a formal model for the co-design of the feature abstraction and control policy, derive a dynamic program using the AIS",
        "2211.03011_94921a6af06e2c002df8b6773e67d78a": "We also derive bounds on the quality of approximate solutions to this dynamic program",
        "2211.03011_e164aacf273e30e3eb7babf410f64a4e": "we build on these approximation bounds to develop an RL algorithm for learning a history-based state representation and control policy",
        "2211.03011_726cf58b3134bb1639a1fbf7a2bfb338": ", we present an empirical evaluation of our proposed algorithm on continuous control tasks",
        "2211.03011_78b53c8af37d83d5b641399c9d6d2d69": "Finally, we discuss related work in",
        "2211.03011_649c029cd1e36da8c4411b700a49ae82": "and conclude with future research directions in",
        "2211.03011_5058f1af8388633f609cadb75a75dc9d": ".",
        "2211.03011_90b9329bb5fae8bf53157c208cfd0f41": "Background and Motivation",
        "2211.03011_2a4a1702cc16a9af03b0fefdc904bf1c": "Consider an MDP",
        "2211.03011_f5c11bbc96a07bd58f1453788494ad38": "$ \\mdp = \\langle \\statespace, \\actionspace, \\transition, \\cost, \\discount \\rangle$",
        "2211.03011_567904efe9e64d9faf3e41ef402cb568": "where",
        "2211.03011_7d615cff00854eb008f6935851a8dc40": "$\\statespace$",
        "2211.03011_13c5e6d37a12c103d101f33792657813": "denotes the state space,",
        "2211.03011_a77ea1c78604675d469ea4c00f3d8d7a": "$\\actionspace$",
        "2211.03011_5d633873b74b3d2fc8219f7b6fed16ca": "denotes the action space,",
        "2211.03011_9d0db5860cea7c0f877d88f2b7cb89f3": "$\\transition$",
        "2211.03011_c19d342cdd06314295ae448ce0788ae7": "denotes the controlled transition matrix,",
        "2211.03011_3d150ce9a23a96fbcc39157621a7aa8c": "$\\cost \\colon \\statespace \\times \\actionspace \\to \\real$",
        "2211.03011_0409ee49c2b2992eaf178e32001cd941": "denotes the per-step reward, and",
        "2211.03011_17020815ddb68bbaf27882c1e1c090b5": "$\\discount \\in (0,1)$",
        "2211.03011_52cfab0882f919785a5e6d78d107c56f": "denotes the discount factor",
        "2211.03011_129fbe35c8b59dc7cb48fabd25dfa8dd": "The performance of a randomised (and possibly history-dependent) policy",
        "2211.03011_b70b0410af3bacd1a2650ce40d764af3": "$\\policy$",
        "2211.03011_7113382170194148d3b8f2bb94904f82": "starting from a start state",
        "2211.03011_ad2733f7bf355eb5e6910be61cc52765": "$\\sts_{0}$",
        "2211.03011_3879179cf3369405c240fb9cf8fa487a": "is measured by the value function, defined as:",
        "2211.03011_d1fcf32671e73130ad006b801e9e2de3": "\\begin{equation}\n          \\valuefunction^{\\policy}(\\sts_0) = \\expecun{}^{\\policy}\\bigg[\\sum_{\\timestep=1}^{\\infty}\\discount^{\\timestep-1}\\cost(\\State_{\\timestep},\\Action_{\\timestep}) \\bigg| \\State_{0} = \\sts_{0}\\bigg].\n        \\end{equation}",
        "2211.03011_75b1566874df6e167895b97e9d5cc6ce": "A policy maximising",
        "2211.03011_25c96da87fff397bb6b670de6872851e": "$\\valuefunction^{\\policy}(\\sts_0)$",
        "2211.03011_182077805b4455ee5e248ffc2c484a84": "over all (randomised and possibly history dependent) policies is called the optimal policy with respect to initial state",
        "2211.03011_ac3148a5746b81298cb0c456b661f197": "$s_0$",
        "2211.03011_91b5743fdd53120f6c436c6024926707": "and is denoted by",
        "2211.03011_a4fd57cf9b7e271e6f7f2f6e9080b858": "$\\policy^{\\star}$",
        "2211.03011_e02031955d01cb0fa61a4134633fbc26": "In many applications,",
        "2211.03011_be5d5d37542d75f93a87094459f76678": "and",
        "2211.03011_663ac504054da2f05b16d57c8619acee": "are combinatorially large or uncountable, which makes it intractable to compute the optimal policy",
        "2211.03011_553e7f860b7472c2f2bc05c01ec43462": "Most practical RL algorithms overcome this hurdle by using function approximation where the state is mapped to a feature space",
        "2211.03011_9b8a6316da8572e95ee2a8893ceb7a89": "$\\featurespace$",
        "2211.03011_916d023e6698347703907ac981760857": "using a state abstraction function",
        "2211.03011_d7ccf09047620d0ccbb8acc42a973163": "$\\basis:\\statespace \\to \\featurespace$",
        "2211.03011_0456dd8f50aa0f8130656d2a9578765f": "In Deep-RL algorithms, the last layer of the network is often viewed as a feature vector",
        "2211.03011_a7304ccb43dc45cca018f063a85efa85": "These feature vectors are then used as an approximate state for approximating the value function",
        "2211.03011_06bc7f8e9ec46f50a4114bf4774c855b": "$\\hat\\valuefunction: \\featurespace \\to \\real$",
        "2211.03011_3b3366a82828c93815199234712018f6": "and/or computing an approximately optimal policy",
        "2211.03011_32d7acc03bad6dc7366ea9f30fa2391b": "$\\mu:\\featurespace \\to \\Delta(\\actionspace)$",
        "2211.03011_8116c304f9fd2c642b7ddfa73d1252b6": "\\citep{Sutton+Barto:1998}",
        "2211.03011_2028d4bf2377b0e049fbf01f2ab44805": "(where",
        "2211.03011_558d4c50d0aa62bc5e711ecedfe47a76": "$\\Delta(\\actionspace)$",
        "2211.03011_b246202a262be48b42d8d3b2c8b9e96c": "denotes the set of probability distribution over actions)",
        "2211.03011_c48233c6260f0fa184932bf7d0a57506": "Therefore, the mapping from state to distribution of actions is given by the",
        "2211.03011_1d5d9fedd9ab44a19d9f30a0b81f79a8": "flattened",
        "2211.03011_f4af8b5789576c000ce9105b25609bd6": "policy",
        "2211.03011_61a0362e2ef15a5393c1e2ef3c17ca89": "$\\tilde{\\mu} = \\mu \\circ \\basis$",
        "2211.03011_c0cb5f0fcf239ab3d9c1fcd31fff1efc": ",",
        "2211.03011_42081e04497930d1590b479463780d25": "$\\tilde{\\mu} = \\mu(\\phi(\\cdot))$",
        "2211.03011_b6e12ce80d7402ec3522658fe951b47e": "A well known fact about function approximation is that the features that are used as an approximate state may not satisfy the controlled Markov property",
        "2211.03011_023913f46506fb2019b7b5ae199db7f9": ", in general,",
        "2211.03011_d02d0bf2da68e0b45b9026104009fb19": "\\begin{equation}\n            \\prob(\\Feature_{\\timestep+1} \\mid \\Feature_{1:\\timestep}, \\Action_{1:\\timestep}) \\neq\n            \\prob(\\Feature_{\\timestep+1} \\mid \\Feature_\\timestep, \\Action_\\timestep).\n        \\end{equation}",
        "2211.03011_b84c7ddc5de69e9d368d34b993fa7e33": "\\begin{figure}[!h]\n          \\centering\n          \\begin{minipage}{\\linewidth}\n          \\subfloat[$P(0)$]{\n            \\begin{tikzpicture}[thick,scale=0.9]\n              \\node [agent] at (0, 0) (0) {$0$};\n              \\node [agent] at (1, 0) (1) {$1$}; \n              \\node [agent] at (0, 1) (3) {$3$};\n              \\node [agent] at (1, 1) (2) {$2$};\n              \\path[->]\n                    (0) edge node[below] {$0.5$} (1)\n                    (1) edge node[right] {$0.5$} (2)\n                    (2) edge node[above] {$0.5$} (3)\n                    (3) edge node[left]  {$0.5$} (0)\n                    (0) edge[loop left] node {$0.5$} (0)\n                    (1) edge[loop right] node {$0.5$} (1)\n                    (2) edge[loop right] node {$0.5$} (2)\n                    (3) edge[loop left] node {$0.5$} (3)\n                    ;\n            \\end{tikzpicture}\n            \\label{fig:P(1)}}\n           \\hfill\n          \\subfloat[$P(1)$]{\n            % \\centering\n            \\begin{tikzpicture}[thick,scale=0.9]\n              \\node [agent] at (0, 0) (0) {$0$};\n              \\node [agent] at (1, 0) (1) {$1$}; \n              \\node [agent] at (0, 1) (3) {$3$};\n              \\node [agent] at (1, 1) (2) {$2$};\n              \\path[<-]\n                    (0) edge node[below] {$0.5$} (1)\n                    (1) edge node[right] {$0.5$} (2)\n                    (2) edge node[above] {$0.5$} (3)\n                    (3) edge node[left]  {$0.5$} (0)\n                    (0) edge[loop left] node {$0.5$} (0)\n                    (1) edge[loop right] node {$0.5$} (1)\n                    (2) edge[loop right] node {$0.5$} (2)\n                    (3) edge[loop left] node {$0.5$} (3)\n                    ;\n            \\end{tikzpicture}\n            \\label{fig:P(2)}}\n            \\hfill\n            \\subfloat[$P(2)$]{%\n                  \\begin{tikzpicture}[thick,scale=0.9]\n                  \\node [agent] at (0, 0) (0) {$0$};\n                  \\node [agent] at (1, 0) (1) {$1$}; \n                  \\node [agent] at (0, 1) (3) {$3$};\n                  \\node [agent] at (1, 1) (2) {$2$};\n                  \\path[->]\n                        (0) edge [bend right] node[below] {$0.5$} (1)\n                        (1) edge [bend right] node[right] {$0.5$} (2)\n                        (2) edge [bend right] node[above] {$0.5$} (3)\n                        (3) edge [bend right] node[left]  {$0.5$} (0)\n                        (0) edge [bend right] node[right] {} (3)\n                        (3) edge [bend right] node[below] {} (2)\n                        (2) edge [bend right] node[left]  {} (1)\n                        (1) edge [bend right] node[above]  {} (0);\n                \\end{tikzpicture}\n                \\label{fig:P(3)}}\n            \\hfill\n            \\subfloat[$P_{\\policy{}}$]{\n            \\begin{tikzpicture}[thick,scale=0.9]\n              \\node [agent] at (0, 0) (0) {$0$};\n              \\node [agent] at (1, 0) (1) {$1$}; \n              \\node [agent] at (0, 1) (3) {$3$};\n              \\node [agent] at (1, 1) (2) {$2$};\n              \\path[->]\n                    (0) edge node[below] {$0.5$} (1)\n                    (1) edge[bend right] node[right] {$0.5$} (2)\n                    (2) edge[bend right]  node[left] {$0.5$} (1)\n                    (3) edge node[left]  {$0.5$} (0)\n                    (3) edge node[above] {$0.5$} (2)\n                    (0) edge[loop left] node {$0.5$} (0)\n                    (1) edge[loop right] node {$0.5$} (1)\n                    (2) edge[loop right] node {$0.5$} (2)\n                    ;\n            \\end{tikzpicture}\n            \\label{fig:optimal}\n            }\n          \\end{minipage}%\n          \\caption{The transition probability for an example MDP}\n        \\end{figure}",
        "2211.03011_0564adc1219854e49dc38bcca131968f": "To see the implications of this fact, consider the toy MDP depicted in",
        "2211.03011_90de545e518547f74725a635ebec2dd8": ", with",
        "2211.03011_b4a1d6ce4f047fe1d7014db1adf79e85": "$ \\statespace = \\{0, 1, 2, 3\\}$",
        "2211.03011_88f9ef04de08b686edfdad1239813e25": "$\\actionspace =\n        \\{0, 1, 2\\}$",
        "2211.03011_a0d587998e0d897ece232d64ccbe43c3": "$\\{\\transition_{\\sts, \\sts'}(\\action)\\}_{\\action \\in \\actionspace}$",
        "2211.03011_8a0e2b549d223aba55a866c38d1c9275": ", and",
        "2211.03011_ee257ffc352abf7ad3b733f83bed702d": "$r(0) = r(1) = -1$",
        "2211.03011_f7cbd707910b7e6593b9f1bff795cb4f": "$r(2) = 1$",
        "2211.03011_a6aacf7d8cb19e2a518e34700f9a1ff0": "$r(3) = -K$",
        "2211.03011_a29f17caa3e965b909d1aef183a202e4": ", where",
        "2211.03011_d6328eaebbcd5c358f426dbea4bdbf70": "$K$",
        "2211.03011_73d7111aa2ae526fe9a3bbd506581205": "is a large positive number",
        "2211.03011_968245e65658f9d3e05d53982cadc1e2": "Given the reward structure the objective of the policy is to try to avoid state",
        "2211.03011_5dc642f297e291cfdde8982599601d7e": "$3$",
        "2211.03011_3754a68e6e8664f579833c2aa0674ebf": "and keep the agent at state",
        "2211.03011_76c5792347bb90ef71cfbace628572cf": "$2$",
        "2211.03011_52b3705250c5dc8eab0832260e994d88": "as much as possible",
        "2211.03011_d843094204e89edfe33c45294236051a": "It is easy to see that the optimal policy is",
        "2211.03011_42818217a9b0df80c535a410f960d34d": "\\begin{equation}\n              \\pi^\\star(0) = 0, \\quad\n              \\pi^\\star(1) = 0, \\quad\n              \\pi^\\star(2) = 1,\n              \\text{ and}\\quad\n              \\pi^\\star(3) = 2.\n            \\end{equation}",
        "2211.03011_23696bb68b987bce23cc98449556961d": "Note that if the initial state is not state",
        "2211.03011_b0316aaeba84883d381280f13ee0faf0": "then an agent will never visit that state under the optimal policy",
        "2211.03011_a551e9bd38d0e342200ac07a78d0e766": "Furthermore, any policy which cannot prevent the agent from visiting state",
        "2211.03011_df3c5c096fb5ea6306d3aef228df303f": "will have a large negative value and, therefore, cannot be optimal",
        "2211.03011_01efb56f09b9fae19d23eea5e3a1afdf": "Now suppose the feature space",
        "2211.03011_ca5c354431b9a7c3aecb72c34335b5de": "$\\featurespace = \\{0, 1\\}$",
        "2211.03011_b60daaf917fe7189c930f3897a8badec": "It is easy to see that for any Markovian feature-abstraction",
        "2211.03011_ae5e735a1d01159500fc22b615c993dc": "$\\policyencoder{} \\colon \\statespace \\to \\featurespace$",
        "2211.03011_a0efe538bc60d7ea680e07ee447bce0f": ", no policy",
        "2211.03011_e72173254f9184557dc20ed431dd5239": "$\\hat \\pi \\colon \\featurespace \\to \\actionspace$",
        "2211.03011_30676fc4ffafdba645e3c9a3e350345a": "can prevent the agent from visiting state",
        "2211.03011_e19f6ce00803d7973fb3d333cc0245d7": "Thus, the best policy when using Markovian feature abstraction will perform significantly worse than the optimal policy (which has direct access to the state)",
        "2211.03011_dd426d100a1f1018b013ef22fdaf0e6c": "However, it is possible to construct a history-based feature-abstraction",
        "2211.03011_d867a7dd8f9e1a969b8d5d955bffe7a2": "$\\policyencoder$",
        "2211.03011_080d640843c41cad228d403d485c75aa": "and a history-based control policy",
        "2211.03011_d3bb8c8185a90a90b00a6ba3d5ea6d36": "$\\hat \\pi$",
        "2211.03011_72ce25fac81027cf9c291e00caf6701b": "that works with",
        "2211.03011_f50853d41be7d55874e952eb0d80c53e": "$\\phi$",
        "2211.03011_fddd4e2b3810c8754e7cbc17a0924a3b": "and is of the same quality as",
        "2211.03011_4b9e818fb2bd9e00a29de1201046856b": "$\\pi^\\star$",
        "2211.03011_b02e8d6380570e7c0efe76c50c742079": "For this, consider the following",
        "2211.03011_48ae4dc4b4d97fca35e813b1145f23be": "(where the entries denoted by a dot do not matter):",
        "2211.03011_0aec6b1a0d42f1b68747d84dec7ea11e": "The Markov chain induced by the optimal policy is shown in",
        "2211.03011_cc7817a435708b499a822f10d683815f": "Now define",
        "2211.03011_45792bf40eb282f17d94ae6706f58ed5": "\\begin{align*}\n          F(1) &= \\begin{bmatrix}\n            0 & 1 & \\cdot & \\cdot \\\\\n            \\cdot & 0 & 1 & \\cdot \\\\\n            \\cdot & \\cdot & 0 & 1 \\\\\n            1 & \\cdot & \\cdot & 0\n          \\end{bmatrix}, \n          &\n          F(2) &= \\begin{bmatrix}\n            1 & \\cdot & \\cdot & 0 \\\\\n            0 & 1 & \\cdot & \\cdot \\\\\n            \\cdot & 0 & 1 & \\cdot \\\\\n            \\cdot & \\cdot & 0 & 1\n          \\end{bmatrix} ,\n          &\n          F(3) &= \\begin{bmatrix}\n            \\cdot & 0 & \\cdot & 1 \\\\\n            0 & \\cdot & 1 & \\cdot \\\\\n            \\cdot & 0 & \\cdot & 1 \\\\\n            0 & \\cdot & 1 & \\cdot\n          \\end{bmatrix},\n          \\\\\n          D(0) &= \\begin{bmatrix}\n            0 & 1 \\\\\n            1 & 2 \\\\\n            2 & 3 \\\\\n            3 & 0 \n          \\end{bmatrix} ,\n          &\n          D(1) &= \\begin{bmatrix}\n            3 & 0 \\\\\n            0 & 1 \\\\\n            1 & 2 \\\\\n            2 & 3 \n          \\end{bmatrix} ,\n          &\n          D(2) &= \\begin{bmatrix}\n            1 & 3 \\\\\n            0 & 2 \\\\\n            1 & 3 \\\\\n            0 & 2 \n          \\end{bmatrix} .\n        \\end{align*}",
        "2211.03011_a4c5f57a05898c2553c4f9d013ab5872": "and consider the feature-abstraction policy",
        "2211.03011_a457efa02bc2946e2ed0d285d3cdee39": "$\n          Z_t =  F_{S_{t-1}, S_t}(A_{t-1})\n        $",
        "2211.03011_68e43b5e8afd5bd4c4c7af8f90d20055": "and a control policy",
        "2211.03011_07617f9d8fe48b4a7b3f523d6730eef0": "$\\mu$",
        "2211.03011_d6d59c3bbd4d584005553e21a67da915": "which is a finite state machine with memory, where the memory",
        "2211.03011_94f19ee28014814bfdb82451d32d2f42": "$M_t$",
        "2211.03011_fcdec8ba599ba9dfe201e0429fddbf15": "that is updated as",
        "2211.03011_5f4fe13d269f7b4ac974f259a69df6e7": "$\n          M_t = D_{M_{t-1}, Z_t}(A_{t-1})\n        $",
        "2211.03011_d4a00b1d35fd099d00a5cc8c375493f1": "and the action",
        "2211.03011_df02e7666c632d22547b9c75b98c49bf": "$A_t$",
        "2211.03011_13ff0587ded1b3fbee98b34bfbc95c17": "is chosen as",
        "2211.03011_9af86d2fb9ab92c17a27eb048efa059f": "$\n          A_t = \\pi(M_t),\n        $",
        "2211.03011_a5b810f11c231d03bc9b97562ea31d37": "$\\pi \\colon \\statespace \\to \\Delta(\\actionspace)$",
        "2211.03011_0ee5ab8da7e9ac050cecd67156094799": "is any pre-specified reference policy",
        "2211.03011_bc3b923190e79ef890240ac314a257f9": "It can be verified that if the system starts from a known initial state then",
        "2211.03011_8be2b487eaf0803a532a2261cc333d39": "$\\mu \\circ \\basis = \\pi$",
        "2211.03011_9c03e5dc4ebd91a76554c8dd7caa3067": "Thus, if we choose the reference policy",
        "2211.03011_da6ee250684be292f9d142c973800ebf": "$\\pi=\\pi^\\star$",
        "2211.03011_ea84c470b3d33b1c19c076d7b79277cd": ", then the agent will never visit state",
        "2211.03011_388fa2509025674d7dbf4715a4a9783e": "under",
        "2211.03011_f6d372fb0fa29ea70ccc0a6e9e11a2af": "$\\mu \\circ \\basis$",
        "2211.03011_5f9cdc7c0f48165b1d9b0fee2f66a6ce": ", in contrast to Markovian feature-abstraction policies where (as we argued before) state",
        "2211.03011_d66a45e0022537aed886b2499dd0bb1f": "is always visited",
        "2211.03011_2a4da47a84f6723d6613e55a12bebe29": "In the above example, we used the properties of the system dynamics and the reward function to design a history-based feature abstraction which outperforms memoryless feature abstractions",
        "2211.03011_6a93aa1aaa01ce4e27009e2ea0d54022": "We are interested in developing such history-based feature abstractions using a learning framework when the system model is not known",
        "2211.03011_0beef5f09952c04d06b5e9d05056c4de": "We present such a construction in the next section.",
        "2211.03011_7e5ad6c44bdd82b92e2a85e02f7ecc8c": "Approximation bounds for history-based feature abstraction",
        "2211.03011_06052e031fcec39412abab363aae1858": "The approximation results of our framework depend on the properties of metrics on probability spaces",
        "2211.03011_d05487b62ecaa382f68c6cba959464f2": "We start with a brief overview of a general class of metrics known as Integral Probability Measures (IPMs)",
        "2211.03011_9ce854c86fdbe257d83b04d555a5f8be": "\\citep{ipm}",
        "2211.03011_f39292ba4bcbe105d3d0f7b621d751d6": "; many of the commonly used metrics on probability spaces such as total variation (TV) distance, Wasserstein distance, and maximum-mean discrepency (MMD) are instances of IPMs",
        "2211.03011_f78791212eacc17b98cda7fb89b8ee66": "We then derive a general approximation bound that holds for general IPMs, and then specialize the bound to specific instances (TV, Wassserstein, and MMD).",
        "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c": "Integral probability metrics (IPM)",
        "2211.03011_cae6404c4aecf46684930fe2a86676a6": "Let",
        "2211.03011_d2261afb4f5870a503af555cfd555f82": "$(\\mathcal{E},\\mathcal{G})$",
        "2211.03011_be000cebcfa2819d0babf38547d4d282": "be a measurable space and",
        "2211.03011_3ca8a5bc0cb9795f4ad5d27a7115a724": "$\\mathfrak{F}$",
        "2211.03011_943cd266063e8c7e8be27e11ec6c1acd": "denote a class of uniformly bounded measurable functions on",
        "2211.03011_d2c40e49b4bf1485686005f0a508feb9": "The integral probability metric between two probability distributions",
        "2211.03011_5e02bba5687e9e8a407ff781d9a1f220": "$\\nu_1, \\nu_2 \\in \\mathcal{P}(\\mathcal{E})$",
        "2211.03011_550d39a8b64a62afab5c9d93110e4437": "with respect to the function class",
        "2211.03011_a5e5171912cb444cdfd0c4a51c5eb32d": "is defined as:",
        "2211.03011_5422695d6e310d32f83a899e61ff6b6a": "\\begin{align}\n                        \\ipm(\\nu_1, \\nu_2) &= \\sup_{f \\in \\mathfrak{F}}\\bigg| \\int_{\\mathcal{E}}f d\\nu_1  - \\int_{\\mathcal{E}}f d\\nu_2\\bigg|.\\label{eq:def-ipm}\n                    \\end{align}",
        "2211.03011_0ce69584400d17167fe3e8473096d814": "For any function",
        "2211.03011_190083ef7a1625fbc75f243cffb9c96d": "$f$",
        "2211.03011_0e64568b6ef4b66acd2a78f70be08c6e": "(not necessarily in",
        "2211.03011_aa050808fa16b2f412c2311a0dfc2f74": "), the Minkowski functional",
        "2211.03011_fcc4599bb28793b7c46200b26b57eb1a": "$\\rho_{\\mathfrak{F}}$",
        "2211.03011_e43699be9d3f7ce528d1d65b30c31be7": "associated with the metric",
        "2211.03011_e8ca9d5653a2bcf11929ad07319ba346": "$\\ipm$",
        "2211.03011_a04cd6274f661fddda9a98418e6bd2c4": "\\begin{align}\n                        \\rho_{\\mathfrak{F}}(f)&\\define \\inf\\{\\rho \\in \\real_{\\geq 0}: \\rho^{-1}f \\in \\mathfrak{F}\\}.\\label{eq:minkowski-functional}\n                    \\end{align}",
        "2211.03011_8921bd7af5f9b2a5d1f7a2ebc8465744": "Eq.",
        "2211.03011_b45319a6ea1b7bfcf4e8ffd4cf1e77e8": ", implies that that for any function",
        "2211.03011_853ae90f0351324bd73ea615e6487517": ":",
        "2211.03011_21af775c9ed3aa7b482106b6ae258a5c": "\\begin{align}\n                         \\bigg|\\int_{\\mathcal{E}}fd\\nu_1 - \\int_{\\mathcal{E}}fd\\nu_2 \\bigg|\\leq \\rho_{\\mathfrak{F}}(f)\\ipm(\\nu_1,\\nu_2). \\label{eq:ipm-implication}\n                     \\end{align}",
        "2211.03011_8a02376b6f01a3fb1454923f077894bf": "In this paper, we use the following IPMs:",
        "2211.03011_7d74f3b92b19da5e606d737d339a9679": "Item",
        "2211.03011_82352ddd1e83a90141bb3e886e37bd21": ": If",
        "2211.03011_f516594d92a2e17ef62a163402708713": "$\\mathfrak{F}^{\\text{TV}} \\define \\{\\frac{1}{2}\\spn(f)$",
        "2211.03011_43ec3e5dee6e706af7766fffea512721": "=",
        "2211.03011_331d387eaa82aaec976361033ca4b142": "$\\frac{1}{2}(\\max(f)- \\min(f))\\}$",
        "2211.03011_fb4f3b79910c6656439b97f695d3f658": ", then",
        "2211.03011_ec9acb8d24391abfe49520372e152d95": "is the total variation distance, and its Minkowski functional is",
        "2211.03011_ef7544b2a30b56b41e08158b0851e1a3": "$\\rho_{\\mathfrak{F}^{\\text{TV}}}(f) = \\frac{1}{2}\\spn(f)$",
        "2211.03011_7114e8b70a29f3808a4b0ac1fc360fba": "$\\mathcal{E}$",
        "2211.03011_46dd09e3809614efce3edce6320ee3b8": "is a metric space and",
        "2211.03011_9e1e035b868fe89dd4993bd73d8a6f74": "$\\mathfrak{F}^{W} \\define \\{f: L_f \\leq 1 \\}$",
        "2211.03011_a37302359718eac43a8aec1b5310a2be": "$L_f$",
        "2211.03011_0504bf39f9472c4390ca69cee15073be": "denotes the Lipschitz constant of",
        "2211.03011_b476478a059076bd064cd484e55bd3e8": "with respect to the metric on",
        "2211.03011_55b540d6c3a545cda9559d252c21bc20": "), then",
        "2211.03011_1161abc180af83c1b4834b02611f99e2": "is the Wasserstein or the Kantorovich distance",
        "2211.03011_acf1fb1972262e375f7bad3701f5fa20": "The Minkowski function for the Wasserstein distance is",
        "2211.03011_a9ee20bb24959381e80f233187ff8777": "$\\rho_{\\mathfrak{F}^W}(f) = L_f$",
        "2211.03011_010e18380f5127e518be1e9d671cf408": ": Let",
        "2211.03011_e06ba62f2bfed5cf8a0fae61c45d4ac8": "$\\mathcal{U}$",
        "2211.03011_973ca979fddfba1db6f2774ff446c0fc": "be a reproducing kernel Hilbert space (RKHS) of real-valued functions on",
        "2211.03011_cd67b575db4ed2b9a0f5719d2aef7411": "is choosen as",
        "2211.03011_d58a0703ef6dd2b7ac6da1692fdd1f6d": "$\\mathfrak{F}^{MMD} \\define \\{f\\in \\mathcal{U}: \\Vert f \\Vert _{\\mathcal{U}} \\leq 1 \\}$",
        "2211.03011_ed5660e33d3f46f0905d6f49bfe917c8": ", (where",
        "2211.03011_981d477f5229e728371e1b646520a85b": "$\\Vert \\cdot \\Vert_{\\mathcal{U}}$",
        "2211.03011_d2e2a17faf26552552438906e4d97300": "denotes the RKHS norm), then",
        "2211.03011_0a2e541fccb9ace28f513ad172e9c378": "is the Maximum Mean Discrepancy (MMD) distance and its Minkowski functional is",
        "2211.03011_507377f5cf3fe310cd1151d38a0bfce6": "$\\rho_{\\mathfrak{F}^{\\text{MMD}}}(f) = \\Vert f\\Vert _{\\mathcal{U}}$",
        "2211.03011_7e2d84b1ce02eea574681dad45d216c4": "Approximate information state",
        "2211.03011_94936736293272f345248e3e9f1918e8": "Given an MDP",
        "2211.03011_98e10b206ba5c38ed19851d91569adf7": "$\\mdp$",
        "2211.03011_a437cbf5bde9cd9b6bac6d3d707e4c0d": "and a feature space",
        "2211.03011_0623934d2e2051c6ead0528a661c4313": "$\\aisspace$",
        "2211.03011_0b551d05d8f005791675d74ca69d5a73": ", let",
        "2211.03011_f2194a2def0670a6e2847f4d5c73a3b7": "$\\historyspace_\\timestep = \\statespace \\times  \\actionspace $",
        "2211.03011_9fab0f1ebc8b22cb954b711d5be4210c": "denote the space of all histories",
        "2211.03011_86ddd25d2aad69191512313db911ea1d": "$(\\State_{1:\\timestep}, \\Action_{1:\\timestep-1})$",
        "2211.03011_154cb8d316ec563a2e66124100ceda6b": "up to time",
        "2211.03011_4f4f4e395762a3af4575de74c019ebb5": "$t$",
        "2211.03011_993ac1f820cba3f1950d647e1902e3b8": "$\\State_{1:\\timestep}$",
        "2211.03011_3722e629d21484eb98395ce6fe1ec3cd": "is a shorthand notation for the history of states",
        "2211.03011_3e93a51b47befb7fc9a3eed5987b6188": "$(\\State_1,\\ldots, \\State_\\timestep)$",
        "2211.03011_0e7260f478363af724aaeaab04f00264": ", and similar interpretation holds for",
        "2211.03011_aae7ea259cda5d1b4f6faa85909060f3": "$\\Action_{1:\\timestep}$",
        "2211.03011_63eaac49b782ffcdc3d196ede3d84daf": "We are interested in learning history-based feature abstraction functions",
        "2211.03011_255c484618de115b366dcf63e8d07d96": "$\\{ \\aisfunction_\\timestep \\colon \\historyspace_\\timestep \\to \\aisspace \\}_{\\timestep \\ge 1}$",
        "2211.03011_e49d39947a7721d621e78d117e8b0b98": "and a time homogenous policy",
        "2211.03011_f01bcf651e471dc7a0be43aa70bf370f": "$\\mu \\colon \\aisspace \\to \\Delta(\\actionspace)$",
        "2211.03011_6da074617278fa6887c1bbea881ac6a2": "such that the flattened policy",
        "2211.03011_a62dd5e9bb41e370575bc9cdfd7beb19": "$\\policy{} = \\{\\policy{}_\\timestep\\}_{\\timestep \\ge 1}$",
        "2211.03011_d828a642e2875ea6e1dcb1c9ac405c16": "$\\policy{}_\\timestep = \\mu \\circ \\aisfunction_\\timestep$",
        "2211.03011_d9acee9e485ccc1a2fc8666788180979": ", is approximately optimal.",
        "2211.03011_38e7ce80eb790ab23fe30624874ca027": "Since the feature abstraction approximates the state, its quality depends on how well it can be used to approximate the per step reward and predict the next state",
        "2211.03011_cbd26d8a2e0dd465562853e9a6d0b1b9": "We formalise this intuition in definition below.",
        "2211.03011_353fb0bc1e64e5e8ff0b91f12a84e19f": "A family of history-based feature abstraction functions",
        "2211.03011_df917871e6e0a195c3af28139415826a": "$\\{\\aisfunction_\\timestep: \\historyspace_{\\timestep} \\to \\featurespace\\}_{\\timestep \\geq 1}$",
        "2211.03011_3f3e9703de90cdb1716d333e9e41f74c": "are said to be",
        "2211.03011_0dba8d672be3e549f965eee412c53b00": "if there exists an update function",
        "2211.03011_7a3e7da148fc31147168eac5bba0bc7a": "$\\hat f: \\featurespace \\times \\statespace \\times \\actionspace \\to \\featurespace $",
        "2211.03011_3426a75cfa4681e9db4a5072c1d0ca3e": "such that the process",
        "2211.03011_a2d3b68aa6e7c3ea60302ce7ed17c727": "$\\{\\Feature_\\timestep\\}_{\\timestep\\geq 1}$",
        "2211.03011_d270f59d2e4eb02af7c441259600b3e7": "$\\Feature_\\timestep = \\aisfunction_\\timestep(\\State_{1:\\timestep}, \\Action_{1:\\timestep-1})$",
        "2211.03011_ee1140d9db0c0b3cf1b25e624d18b984": ", satisfies:",
        "2211.03011_feab7d6a29c0a324bffb00555a6f6b00": "\\begin{equation}\n                    \\Feature_{\\timestep +1} = \\hat f(\\Feature_\\timestep, \\State_{\\timestep+1}, \\Action_\\timestep).\\quad \\timestep \\geq 1\n                \\end{equation}",
        "2211.03011_1cd3069d934e1d8d87343f04d2c74a23": "Given a family of history based recursively updatable feature abstraction functions",
        "2211.03011_d8aeb8fc900f9df2e2cfc8eff9cddaec": ", the features",
        "2211.03011_30f13dbcbbf37c30f11c95546b51e34a": "(AIS) with respect to a function space",
        "2211.03011_cb5d947b1298828ef4f2a044865d448f": "if there exist: (i)",
        "2211.03011_eac332f2e2ca0ca5c060d49f8224ed8d": "a reward approximation function",
        "2211.03011_84bf347f1aca4e9ce6474a71766c50b6": "$\\hat r: \\featurespace \\times \\actionspace \\to \\real$",
        "2211.03011_6eee077716ccf603a3d82e7abaaa9602": ", and (ii)",
        "2211.03011_1ce2becc7cf407aa96a91bf6fbb1a409": "an approximate transition kernel",
        "2211.03011_209490dfc8a074f8872110ef5181b9ef": "$\\hat\\transition: \\featurespace \\times \\actionspace \\to \\Delta(\\statespace)$",
        "2211.03011_67dcdcd2ff7dc889aba14d086a1df24f": "such that",
        "2211.03011_f511d79370a09218cddeb4a5c211f3da": "$\\Ais$",
        "2211.03011_f6b770da20531eb1d40e0ebaf980a4da": "satisfies the following properties:",
        "2211.03011_261e15505a1b57e5b4be78e5da1dcddb": "Sufficient for approximate performance evaluation: for all",
        "2211.03011_9aaa3b585b9a624b8bf1bc0d3e610f02": "$\\timestep$",
        "2211.03011_21634175583d382933cfab688766e011": "\\begin{equation}\n                     | \\cost(\\State_{\\timestep}, \\Action_{\\timestep})\n                                            - \\hat{\\cost}(\\Feature_\\timestep, \\Action_\\timestep)| \\leq \\epsilon.\n                 \\label{def:p1}\n                \\end{equation}",
        "2211.03011_125dcf0f5c8334292df8199cc9e89ea4": "Sufficient for predicting future states approximately: for all",
        "2211.03011_ecab5095068222371d477f91210d408b": "\\begin{equation}\n                 d_{\\mathfrak{F}}(\\transition(\\cdot \\vert \\State_\\timestep, \\Action_\\timestep), \\hat \\transition(\\cdot\\vert \\Feature_\\timestep, \\Action_\\timestep)) \\leq \\delta.\n                % \\text{with }\\kappa_{\\ipm}(\\hat{f}_\\timestep) &\\define \\sup_{\\history_\\timestep, \\action_\\timestep} \\bigg[ \\kappa_{\\ipm}(\\hat{f}_\\timestep (\\aisfunction_\\timestep (\\history_\\timestep), \\cdot, \\action_\\timestep))\\bigg].\n                \\end{equation}",
        "2211.03011_ef146718ccf9bbccaabada4e11c30dbc": "We call the tuple",
        "2211.03011_f75a6ebb7d62ba04bc8436b0cd00dc38": "$(\\hat r ,\\hat\\transition)$",
        "2211.03011_95fe648d8ef5b9f5e6a3994d5b02b84a": "as an",
        "2211.03011_d0a00029da440e685800066a561c1fc9": "$(\\epsilon, \\delta)$",
        "2211.03011_b9a404206d6325b011ab48e48d773dd6": "-AIS approximator",
        "2211.03011_553e3bbf8c8e52b44f20006a2b842d7b": "Note that similar definitions have appeared in other works",
        "2211.03011_054bae8b1788c0e689593e295073e0cf": ", latent state",
        "2211.03011_c8688043cf931f9c7dbc0b16cafce959": "\\citep{deepmdp}",
        "2211.03011_4d469e1fdc5c758ee9f00cb173a0e422": ", and approximate information state for for POMDPs",
        "2211.03011_37fcda0a5906bf1fa06532e46426996e": "\\citep{ais-1,ais-2}",
        "2211.03011_c79a3536ee84a4f0eeb16e5e512e47c4": "However, in",
        "2211.03011_be77643eceb3ba0dbb4c9537d25223ee": "it is assumed that the feature abstractions are memory-less and the discussion is restricted to Wasserstein distance",
        "2211.03011_2d474ccb5029bc3ffae00e5749f11b35": "The key difference from the POMDP model in",
        "2211.03011_039cda6a8e9c320eba9f4ff4e47f3fce": "is that the in POMDPs the observation",
        "2211.03011_fa5852d9aeb8e1b90fb7c3b243a06b3f": "$\\Feature_\\timestep$",
        "2211.03011_bd4e8467f6eec50d4e9925fc9898e71f": "is a pre-specified function of the state while in the proposed model",
        "2211.03011_238cee5f86f23f9555aa7a234e17e7f4": "depends on our choice of feature abstraction",
        "2211.03011_fbaf8464a019b9e45099f724e3f00347": "As such, our key insight is that an AIS-approximator of a recursively updatable history-based feature abstraction can be used to define a dynamic program",
        "2211.03011_7ad23a7d9ac23635efd21b62518d85f0": "In particular, given a history-based abstraction function",
        "2211.03011_302aea2d3b31f04bd1d05493219b0d54": "$\\{\\aisfunction_\\timestep: \\historyspace_\\timestep \\to \\featurespace\\}_{\\timestep \\geq 1}$",
        "2211.03011_a79f2ffb7acbdbd5c2284cbe7a1d1694": "which is recursively updatable using",
        "2211.03011_b9f85bb6936a624e5e7a704877a71bfd": "$\\hat f$",
        "2211.03011_79170fd81965ed64f415720cbafe70ed": "and an",
        "2211.03011_564537e0001836961460a712b29e9663": "AIS-approximator",
        "2211.03011_54e0854b89560b1abd88c023e66f22fa": "$(\\hat \\transition, \\hat \\cost)$",
        "2211.03011_041db6f5daba6dbd91357c47d95c766b": ", we can define the following dynamic programming decomposition: For any",
        "2211.03011_1a402055656256eb35e7c20e801793a3": "$\\feature_\\timestep \\in \\featurespace, \\ \\action_\\timestep \\in \\actionspace$",
        "2211.03011_fc0e4a6be1e579a5fef887ba9f3d9923": "\\begin{align}\n                   \\hat Q(\\ais_\\timestep, \\action_\\timestep) = \\hat \\cost(\\ais_\\timestep, \\action_\\timestep) + \\discount \\sum_{\\sts_{\\timestep+1} \\in \\statespace}\n                    \\hat \\transition(\\sts_{\\timestep+1}|\\ais_\\timestep,\\action_\\timestep) \\hat \\valuefunction(\\hat{f}(\\ais_\\timestep,\\sts_{\\timestep+1},\\action_\\timestep));&&\n                    \\hat \\valuefunction(\\ais_\\timestep) = \\max_{\\action_\\timestep \\in \\actionspace} \\hat Q(\\ais_\\timestep,\\action_\\timestep), \\quad\\forall \\feature_\\timestep \\in \\featurespace \n            \\end{align}",
        "2211.03011_9312cc2f02f42764c70c3713f80fe219": "Define",
        "2211.03011_2290a08ae763688f9bba3b655014288f": "be any policy such that for any",
        "2211.03011_98b9573060feb65ec13ee45e58de3a6b": "$\\ais \\in \\aisspace$",
        "2211.03011_3a91b239db7ab1491dbb3d8fa230a1d0": "\\begin{align}\n                \\support(\\mu(\\ais)) \\subseteq \n                \\arg\\max_{\\action \\in \\actionspace} \\hat Q(\\ais,\\action).\\label{eq:policy}\n            \\end{align}",
        "2211.03011_38c50b731f70abc42c8baa3e7399b413": "Since",
        "2211.03011_4c657fe19c72fc65b04316256e6a12b4": "is a policy from the feature space to actions, we can use it to define a policy from the history of the state action pairs to actions as:",
        "2211.03011_7a35d1fa95c69c610bcab0b249be58c4": "\\begin{align}\n             \\policy{}_{\\timestep}(\\sts_{1:\\timestep}, \\action_{1:\\timestep-1}) \\define \\mu(\\aisfunction_{\\timestep}(\\sts_{1:\\timestep}, \\action_{1:\\timestep-1})) \\label{eq:policy-definition}\n         \\end{align}",
        "2211.03011_74d39a1573fbbbcb294d2d1c501d3330": "Therefore, the dynamic program defined in",
        "2211.03011_1d69035ba0a7b66abf73d33831d447cb": "indirectly defines a history-based policy",
        "2211.03011_279915c30453c198bd50e15933483959": "$\\policy{}$",
        "2211.03011_1f07d90c713d8527d15006a8b597e7ce": "The performance of any such history-based policy is given by the following dynamic program: For any",
        "2211.03011_0e5a6d3cbc4ff3e000f672a3e01300b3": "$\\feature \\in \\featurespace, \\ \\action \\in \\actionspace$",
        "2211.03011_1197d9626a1b0e3281bb2efca3904bc1": "\\begin{align}\n                     Q^{\\policy{}}_{\\timestep}(\\history_\\timestep, \\action_\\timestep) = \\cost(\\sts_\\timestep, \\action_\\timestep) + \\discount \\sum_{\\sts_{\\timestep+1} \\in \\statespace}\n                    \\transition(\\sts_{\\timestep+1}|\\sts_\\timestep,\\action_\\timestep) \\valuefunction_{\\timestep+1}^{\\policy{}}(\\history_{\\timestep+1}); && \n                     \\valuefunction_{\\timestep}^{\\policy{}}(\\history_\\timestep) = \\max_{\\action \\in \\actionspace}  Q_{\\timestep}^{\\policy{}}(\\history_\\timestep,\\action_\\timestep), \\label{eq:hist-dp}\n            \\end{align}",
        "2211.03011_52b4477b11df35c367d2d120b455026e": "We want to quantify the loss in performance when using the history based policy",
        "2211.03011_ac57fe7ea9f1d98a76f25879c7d59013": "Note that since",
        "2211.03011_c2974f727435bd2c19ec136a88d9596e": "$\\valuefunction_\\timestep^\\policy$",
        "2211.03011_272bc0a063b6e9e2c707e5b0502b63ea": "is not time-homogeneous, we need to compute the worst-case difference between",
        "2211.03011_03c7f1f643b33a6d66d37a8912115497": "$\\valuefunction^{\\star}$",
        "2211.03011_bc64324d237711542f964071370112a2": ", which is given by:",
        "2211.03011_c203b52408cf1e6bffbc24e359fa538e": "\\begin{equation}\n            \\Delta \\define\\sup_{\\timestep \\geq 0}\\sup_{\\history_\\timestep = (\\sts_{1:\\timestep}, \\action_{1:\\timestep}) \\in \\historyspace_\\timestep} \\vert \\valuefunction^{\\star}(\\sts_\\timestep) - \\valuefunction^{\\policy{}}_{\\timestep}(\\history_\\timestep)\\vert, \\label{eq:sup-v}\n        \\end{equation}",
        "2211.03011_f855a9b5e6cbb183788a1d10e3d7c6b6": "Our main approximation result is the following:",
        "2211.03011_383b8f0cc76e5fe96b4aa1a6c7145be8": "The worst case difference between",
        "2211.03011_3268b2e3bf243528a48b49c2ae6d2de6": "$\\valuefunction^{\\policy{}}_{\\timestep}$",
        "2211.03011_54d44a61c5a51a5bc85f4da72eb49a0a": "is bounded by",
        "2211.03011_b3940a5c778684336ddd47fc77f59e3e": "\\begin{equation}\n                \\Delta\n                \\le 2 \\frac{\\varepsilon + \\discount\\delta \\kappa_{\\mathfrak{F}}(\\hat \\valuefunction^{\\mu}, \\hat{f})}{1 - \\discount},\\label{eq:ais-bound}\n            \\end{equation}",
        "2211.03011_4dc022b61f6390353ed66650618a4d0f": "$\\kappa_{\\mathfrak{F}}(\\hat \\valuefunction^{\\mu}, \\hat f)$",
        "2211.03011_b66ba1272e68cba980c14a2ae282c1fe": "$ \\sup_{\\feature, \\action}\\rho_{\\mathfrak{F}}(\\hat \\valuefunction^{\\mu}(\\hat f(\\cdot, \\feature, \\action)))$",
        "2211.03011_80b5a17a7721b02954b2b4cab4435772": "$\\rho_{\\mathfrak{F}}(\\cdot)$",
        "2211.03011_edfb19ec72ce4dceb1ad9373e4f005a1": "is the Minkowski functional associated with the IPM",
        "2211.03011_e5fcd666784e5b4114c35e63cbad39a8": "as defined in",
        "2211.03011_7a16c6cb5fc758239fb71d6e042e4a24": "Proof in",
        "2211.03011_3f60d645c75bd0702e9a0823975fa8e8": "Some salient features of the bound are as follows: First, the bound depends on the choice of metric on probability spaces",
        "2211.03011_8e1a3770649d8f6cf5de38bd1b72bee3": "Different IPMs will result in a different value of",
        "2211.03011_38f1e2a089e53d5c990a82f284948953": "$\\delta$",
        "2211.03011_8ba57dc1cba1a007e8322893a2560734": "and also a different value of",
        "2211.03011_82c7448328656e4dda7c3f7a2e61e736": "$\\kappa_{\\mathfrak F}(\\hat{\\valuefunction}^{\\mu}, \\hat f)$",
        "2211.03011_626549a5a258e01c9934431b3b0314c6": "Second, the bound depends on the properties of",
        "2211.03011_d28adb314e6a80173dce44cd0349b945": "$\\hat{\\valuefunction}^{\\mu}$",
        "2211.03011_2a346a4ac761168c11c82c6eb4abb447": "For this reason we call it an instance dependent bound",
        "2211.03011_55b6594a2cd55065bce98f7391553d20": "Sometimes, it is desirable to have bounds which do not require solving the dynamic program in",
        "2211.03011_eaad832ea9586c9dae667826d11b3bfc": "We present such bounds as below, note that these",
        "2211.03011_ce48df8cf209c244d6932b1a6dd59648": "instance independent",
        "2211.03011_808671871e2c98e867f7a585d494d54d": "bounds are the derived by upper bounding",
        "2211.03011_3fb7c4a26dd24c6aeb24aa6a4791cf8c": "$\\kappa_{\\mathfrak{F}}(\\hat{\\valuefunction}^{\\mu}, \\hat f)$",
        "2211.03011_5888411c8c645faa2939a83f56fa61b6": "Therefore, these are looser than the upper bound in",
        "2211.03011_b325ebe03975476152e00dde21c3590f": "If the function class",
        "2211.03011_a2a551a6458a8de22446cc76d639a9e9": "is",
        "2211.03011_a7def38322784e91536f66bc462c20a7": "$\\mathfrak{F}^{\\text{TV}}$",
        "2211.03011_7e9fe18dc67705c858c077c5ee292ab4": "$\\Delta$",
        "2211.03011_80853d56d52219996de2b6d480b5ba1b": "is upper bounded as:",
        "2211.03011_56a67126096b69d4b7ef1fb9a898b239": "\\begin{align}\n              \\Delta\n                \\le  \\frac{2\\epsilon}{(1-\\discount)} +  \\frac{\\discount\\delta \\spn(\\hat\\cost)} {(1-\\discount)^2}.\n            \\end{align}",
        "2211.03011_5f1a3ba5270897a6dcbcab46bbdc99ed": "Proof in Appendix",
        "2211.03011_8e832b62b7f4ade29bb75dad80ef556d": "$L_{\\hat \\cost}$",
        "2211.03011_e3c3774c048954142b7df64c642da3c1": "$L_{\\hat{\\transition}}$",
        "2211.03011_ff06d36da55702702c927a9c0bbfcde5": "denote the Lipschitz constants of the approximate reward function",
        "2211.03011_5721336c579cf4af612914063bead318": "$\\hat \\cost$",
        "2211.03011_c77d742d712aa220ddc49c56a3396a0b": "and approximate transition function",
        "2211.03011_597e3fc63df677f2a118a2acdcdef73e": "$\\hat \\transition$",
        "2211.03011_b1d5e6dec7426978d6ccd2ccfd42328b": "respectively, and",
        "2211.03011_162d61d860f7dca965c813f623a40bf7": "$L_{\\hat f}$",
        "2211.03011_87eed07edd7f3955b7a2419c88ed9bd1": "is the uniform bound on the Lipschitz constant of",
        "2211.03011_8389d291126551be45600c4e4abbdd64": "with respect to the state",
        "2211.03011_e37493765bac92d7dc767dc402c3425a": "$\\State_\\timestep$",
        "2211.03011_786887572f6ef1c20f2d8177cb2f1639": "If",
        "2211.03011_fae7d06d34c04831a201a8cafff26811": "$\\discount L_{\\hat \\transition}L_{\\hat f} \\leq 1$",
        "2211.03011_390cd41b15655a50c7ebbb6509e95c6f": "and the function class",
        "2211.03011_1838ee43e599ed9f21e6f0278031c52b": "$\\mathfrak{F}^{\\text{W}}$",
        "2211.03011_587037d7cf426d56a4240e03919348ca": "\\begin{align}\n                \\Delta\n                \\le  \\frac{2\\epsilon}{(1-\\discount)} + \\frac{2\\discount\\delta L_{\\hat \\cost} }{(1- \\discount)(1-\\discount L_{\\hat f}L_{\\hat\\transition})}.\n            \\end{align}",
        "2211.03011_54154586fbc871f126bb7ff3869bdb76": "$\\mathfrak{F}^{\\text{MMD}}$",
        "2211.03011_3840f3f2bdd5eaff9372d4c4f552ab8a": "\\begin{align}\n              \\Delta\n                \\le  2 \\frac{\\epsilon + \\discount\\delta\\kappa_{\\mathcal{U}}(\\hat \\valuefunction, \\hat f) } {(1-\\discount)},\n            \\end{align}",
        "2211.03011_cc2ea0baa9445308417324b39cdf2365": "is a RKHS space,",
        "2211.03011_b9eace90ae01078552d739c520dabb48": "$\\Vert\\cdot\\Vert_{\\mathcal{U}}$",
        "2211.03011_ee8906e8ce651d2b16ff6de108885fec": "its associated norm and",
        "2211.03011_eeb2f6d920067e4b5af5981c2f92c3ea": "$\\kappa_{\\mathcal{U}}(\\hat \\valuefunction, \\hat f) = \\sup_{\\feature, \\action}\\Vert(\\hat \\valuefunction(\\hat f(\\cdot, \\feature, \\action)))\\Vert_{\\mathcal{U}}$",
        "2211.03011_2f854e75afb261e86f0aca8f0c497458": "The proof follows from the properties of MMD described previously.",
        "2211.03011_7de41ab218f8a51f4742d4bc6393e346": "In the following section we will show how one can use these theoretical insights to design a policy search algorithm.",
        "2211.03011_52256f174893670edf771aa1a44bf816": "Reinforcement learning with history-based feature abstraction",
        "2211.03011_907be93d6900cb6322a28457a00be19f": "In this section, we leverage the approximation bounds of Theorem",
        "2211.03011_5739685465e47101af17e87a4e041bd5": "to develop a reinforcement learning algorithm",
        "2211.03011_ec65f5d7534b1662374e61357e748d8d": "The main idea is to add an additional block, which we call the AIS-approximator, to any standard RL algorithm",
        "2211.03011_b120aec3f9fd3de4b98cbb8685837222": "In this section, we explain an AIS-based generalization for policy-based algorithms such as REINFORCE and actor-critic, but the same idea could be used for value-based algorithms such as Q-learning as well.",
        "2211.03011_9aea9022772bd12b124976a1fe6e0631": "The AIS-approximator consists of two blocks: a recursively updatable history compressor and a reward and next-state predictor as shown in Fig.",
        "2211.03011_56c90c72ee03eaf1cbfd0b3bfc3e9a13": "In particular, we can consider any parameterised family of the history compression functions",
        "2211.03011_cbe3515ed3eebcbcd4793dbb47fce4e1": "$\\{\\aisfunction_\\timestep(\\cdot); \\aisparams) \\colon \\historyspace_\\timestep \\to\n\\aisspace\\}$",
        "2211.03011_e91001794c004c2af5954f032ffd9808": "which are recursively updatable via the function",
        "2211.03011_33e1d83a711fc841de2cb141a02865a1": "$\\hat{f}(\\cdot) \\colon \\aisspace \\times \\statespace\\times \\actionspace \\to\n\\aisspace$",
        "2211.03011_12966a89bd103bc2ebe9c171edd62b07": "as the history-compressor along with any parameterised family of functions",
        "2211.03011_5dff34889803b4166ae180f6115f5e85": "$\\hat \\cost(\\cdot; \\aisparams)\\colon\n\\aisspace \\times \\actionspace \\to \\real$",
        "2211.03011_bbce11cf64cf692ebfdc78942f6537f5": "as the reward approximator and any parameterised stochastic kernels",
        "2211.03011_91c363544a1bd897054e488148c2b2ac": "${\\hat\n\\transition}(\\cdot;\\aisparams)\\colon\\aisspace \\times \\actionspace \\to\n\\Delta(\\statespace)$",
        "2211.03011_15b961199224005e89dadb2a98bfdbff": "as the transition approximator",
        "2211.03011_b8a1f0e62ecad0b923a2f9ca21709188": "In the above notation",
        "2211.03011_970dafaa7c14cfd9848bef7872fbd2f5": "$\\aisparams$",
        "2211.03011_d858be959a61bcc0186add30321fcb16": "denotes the combined parameters of the family of functions",
        "2211.03011_cf314751d985625bee3a94732e4f2b20": "As a concrete example, we could use use memory-based neural networks such as LSTMs or GRUs as the history-compression functions",
        "2211.03011_fd4465ede00fb9e15e9c9c656a6186bb": "The memory update functions of such networks correspond to the update function",
        "2211.03011_9ab958cde46235a5390f3b24208ab5cf": "A multilayered perceptron (MLP) could be used as a reward approximator and a parameterized family of stochastic kernels such as the softmax function or a mixture of Gaussians could be used as the transition approximator",
        "2211.03011_09fba8401828805ea0f689dd15258255": "The parameters of all these networks together are denoted by",
        "2211.03011_27698df1b3e7bb191f26f5ff82890040": "We use a weighted combination of the reward prediction loss",
        "2211.03011_417e4da1fe71542c5fdaf19f8630d958": "$\\vert\n\\cost(\\State_\\timestep, \\Action_\\timestep) - \\hat\\cost (\\Ais_\\timestep,\n\\Action_\\timestep)\\vert$",
        "2211.03011_aab99765f036eebee95077c273e1047a": "and the transition-prediction loss",
        "2211.03011_e81783029edbfd7a55e9cd8508949415": "$\\ipm(\\transition,\n\\hat \\transition)$",
        "2211.03011_ebf40128156ab0e2fc7938c55e213b3e": "as the loss function for the AIS-generator",
        "2211.03011_6a3a86d06adfc92161740c6a8a62e568": "In particular, the AIS-loss is given by",
        "2211.03011_bdae20c77e141da651954bca5538526e": "\\begin{align}\n          \\aisloss(\\aisparams) &= \\frac{1}{\\Timestep}\\sum_{t = 0}^{\\Timestep}\\bigg( \\lambda \\underbrace{({\\hat{\\cost}}(\\Ais_{\\timestep}, \\Action_\\timestep; \\aisparams)  - \\cost(\\State_\\timestep, \\Action_\\timestep))^{2}}_{\\loss_{\\hat{\\Cost}(\\cdot;\\aisparams)}}+ (1-\\lambda)\\cdot \\underbrace{\\ipm({\\hat\\transition}(\\Ais_\\timestep, \\Action_\\timestep\\ ;\\aisparams),\\transition)^{2}}_{\\loss_{\\hat\\transition}(\\cdot;\\aisparams)}\\bigg),\\label{eq:pgt-loss}\n    \\end{align}",
        "2211.03011_b92316ae504a2fb2f3d16da40ccf6e9f": "$\\Timestep$",
        "2211.03011_357b6bf04e7ea005692b42b2350ea2e1": "is the length of the episode or the rollout length,",
        "2211.03011_f9944d2da56d55200f057fd68cb6dbc5": "$\\lambda \\in [0,1]$",
        "2211.03011_04467c67c40f985e1d21a20ad63fa44c": "is a hyper-parameter.",
        "2211.03011_963eff4af4d08409bdb1371f9bffd686": "The computation of",
        "2211.03011_dee3f6d54f342ccb4b31469dcc72b8b4": "$\\loss_{\\hat{\\transition}}(\\cdot; \\aisparams)$",
        "2211.03011_b65c6ae752bdc23595200f4dc491da29": ", depends on the choice of IPM",
        "2211.03011_f94d26acf323a1e57173cae1aa8ab9aa": "In principle we can pick any IPM, but we would want to use an IPM using which the distance",
        "2211.03011_2fbaa257a07c25f7b8a60c1e69722200": "$d_{\\mathfrak{F}}$",
        "2211.03011_dc985fdc331947f57190a3971b1cee84": "can be efficiently computed.",
        "2211.03011_18da2bdd534c73f2009d713b514efe2a": "Choice of an IPM",
        "2211.03011_01abb24fc07cf84277e789de05309c3a": "To compute the IPM",
        "2211.03011_8221020305c0f3980cc4ead639b7322b": "$d_\\mathfrak{F}$",
        "2211.03011_48aeb500324b4072267bf09dd38899d0": "we need to know the probability density functions",
        "2211.03011_a06d07217f55a942c9fdc785bb7489b6": "As we assume",
        "2211.03011_b60be6a81953815b24d345b979dd2065": "to belongs to a parametric family, we know its density function in closed form",
        "2211.03011_1a4b6584ea4021d75a3e85d78aeb02e9": "However, since we are in the learning setup, we can only access samples from",
        "2211.03011_a9bdefff0aceeae3f3e137a6223170a3": "$\\transition $",
        "2211.03011_4f9247764fc30ce11d7719bc11cfab14": "For a function a",
        "2211.03011_59827e8f485da2c058034386ed39e836": "$f\\in \\mathfrak{F}$",
        "2211.03011_9882316f07d1d00d1da83a21ff07f52b": ", and probability density functions",
        "2211.03011_ab6c8932f84f9a9f8c706c95cc956fa2": "such that,",
        "2211.03011_721bc0bab36a3dfec01945b0045cc803": "$\\nu_1 = \\transition$",
        "2211.03011_0f86b1da1c5442ebbb611e7063d7e822": "$\\nu_2 =\\hat \\transition$",
        "2211.03011_3db0f02bd709f90e5970aba9be6c3cca": ", we can estimate the IPM",
        "2211.03011_56e2a673d1d7c3c791682694d856443f": "between a distribution and samples using the duality",
        "2211.03011_0bdfd09448d04987df122dc149709944": "$|\\int_\\aisspace f d\\nu_1 - \\int_\\aisspace f d\\nu_2|$",
        "2211.03011_5f876b0dfd4a130bd1d4e9d9aa3f7fd5": "In this paper, we use two from of IPMs, the MMD distance and the Wasserstein/KantorovichRubinstein distance.",
        "2211.03011_0df11f7b998c0bd599d485dd6bb77cb9": "MMD Distance:",
        "2211.03011_1c398fa9c7e737f79674206d0744c460": "$m_\\aisparams$",
        "2211.03011_b3088c31250233434af984bc24de2562": "denote the mean of the distribution",
        "2211.03011_303a4a802ae9ac2c14cd8dbffe77c660": "$\\hat \\transition(\\cdot;\\aisparams)$",
        "2211.03011_6b3f5f7ff91660f69e1b99d2c78064c4": "Then, the AIS-loss when MMD is used as an IPM is given by",
        "2211.03011_dc48637b1fd3445cd9e524c2cfa903de": "\\begin{align}\n           \\aisloss(\\aisparams) &= \\frac{1}{\\Timestep}\\sum_{t = 0}^{\\Timestep}\\bigg( \\lambda ({\\hat{\\cost}}(\\Ais_{\\timestep}, \\Action_\\timestep; \\aisparams)  - \\cost(\\State_\\timestep, \\Action_\\timestep))^{2} + (1-\\lambda)(m^{\\State_\\timestep}_{\\aisparams} - 2\\State_\\timestep)^{\\top}m^{\\State_\\timestep}_{\\aisparams}  \\bigg),\\label{eq:mmd-ais-loss}\n        \\end{align}",
        "2211.03011_b439ffecec6f495e08fac5391edec7cc": "$m^{\\State_\\timestep}_{\\aisparams}$",
        "2211.03011_f31a6b3961c0ced78e6de25af79609df": "is obtained using the from the transition approximator,",
        "2211.03011_acafcd7e48299e06f23043c9731b2664": ", the mapping",
        "2211.03011_8d99c51053bf19326e3649052dc5fb47": "${\\hat\\transition}(\\aisparams): \\aisspace \\times \\actionspace \\to \\real$",
        "2211.03011_4db4f90e14af1e1ce17589002b8cbc9e": "For the detailed derivation of the above loss see",
        "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed": "Wasserstein/KantorovichRubinstein distance:",
        "2211.03011_a76caeda93e1b4be78b8dc08ba02357b": "In principle, the Wasserstein/Kantorovich distance can be computed by solving a linear program",
        "2211.03011_324132b8b604e662477be237bea3cb7f": "\\citep{Sriperumbudur}",
        "2211.03011_b223e666c754ff10c7d673db3487de47": ", but doing at every episode can be computationally expensive.",
        "2211.03011_944d3dea99f3c51512969013ac234f22": "Therefore, we propose to approximate the Wasserstein distance using a KL-divergence",
        "2211.03011_eb81e119a28284865d46cb1ae0c0d64c": "\\citep{kl}",
        "2211.03011_59b6354f947271f8f641d979ba1ba2a8": "based upper-bound",
        "2211.03011_61901bc3d118c51f5fbc488598b3fe25": "The simplified-KL divergence based AIS loss is given as:",
        "2211.03011_df6ac8524fdf4e2fe28871fdc4a27ad9": "\\begin{align}\n                \\aisloss(\\aisparams) &= \\frac{1}{\\Timestep} \\sum_{t = 0}^{\\Timestep}\\bigg( \\lambda ({\\hat{\\cost}}(\\Ais_{\\timestep}, \\Action_\\timestep; \\aisparams)  - \\cost(\\State_\\timestep, \\Action_\\timestep))^{2} + (1-\\lambda)\\log(\\hat \\transition(\\State_\\timestep;\\aisparams))  \\bigg),\\label{eq:w-ais-loss}\n        \\end{align}",
        "2211.03011_827dc1223ae9b2e70bbf5e1e18a4ee08": "where after dropping the terms which do not depend on",
        "2211.03011_17368be9281ffc9fbb69508d9d028d6e": ", we get",
        "2211.03011_149a1f9ac8ca567150ed34acef1f2c79": "$d_{\\mathfrak{F}^{\\text{W}}}^{2}(\\transition, \\hat \\transition)\\leq \\log(\\hat \\transition(\\State_\\timestep;\\aisparams))$",
        "2211.03011_9c68e5666b0f9f15bb31b01093c264b3": "is the simplified-KL-divergence based upper bound",
        "2211.03011_5f4f3cd158cea054ab7af98381b2a757": "For the details of this derivation see",
        "2211.03011_24247d37f026f5cf598677a503744c27": "Policy gradient algorithm",
        "2211.03011_158e5d694f9d6ec764a95970bf3ea094": "Following the design of the AIS block, we now provide a policy-gradient algorithm to learning both the AIS and policy",
        "2211.03011_7b2bd7eec2589eded953bc0a8b90c8fe": "The schematic of our agent architecture is given in",
        "2211.03011_154f6a6c2ed0a60ef028d2eb7d72a88c": ", and pseudo-code is given in",
        "2211.03011_440a512fefa486a6af8ae7068caf7e29": "Given a feature space",
        "2211.03011_c8a5665255e2aa7c057854802cc6e807": ", we can simultaneously learn the AIS-generator and the policy using a multi-timescale stochastic gradient ascent algorithm",
        "2211.03011_4821a01e8c2678434081e8e827c08cc4": "\\citep{borkar2008stochastic}",
        "2211.03011_7b7c8092996caf74be3c266582a01133": "$\\mu(\\cdot;\\actorparams):\\aisspace \\to \\Delta(\\actionspace)$",
        "2211.03011_3d4b381aeda7836005cde74bb7d31491": "be a parameterised stochastic policy with parameters",
        "2211.03011_c896490ae01a15acb4f6180e0d75cecf": "$\\actorparams$",
        "2211.03011_49b230030ea47d8f9e845643fe715a2f": "$\\performance(\\actorparams,\\aisparams)$",
        "2211.03011_4e380fab8ed1d552bc58439c20396e1f": "denote the performance of the policy",
        "2211.03011_006636c28a6cef805e6dca7ee08b4426": "$\\mu(\\cdot ;\\ \\actorparams)$",
        "2211.03011_444e96f61ddb5a7076cfd1581ab89180": "The policy gradient theorem",
        "2211.03011_8e03334edb8b06cf1774613f8e432bdc": "\\citep{pgt,Williams2004SimpleSG,baxter-bartlett}",
        "2211.03011_c0fac37badf15b7f7db4120a03537c0f": "states that:",
        "2211.03011_bdd64a63ac1453519db6b440881baf1a": "For a rollout horizon",
        "2211.03011_6eb39a59e49eea06a845abd04b95b3fc": ", we can estimate",
        "2211.03011_62fa5865d2eca2894cfd67c35c6b234a": "$\\grad_\\actorparams \\performance$",
        "2211.03011_2271c08579d5bce0ea9bda670137570a": "as:",
        "2211.03011_c0f8972a46447bc6f1fd39be148e2ec0": "\\begin{align*}\n            \\hat \\grad_{\\actorparams}\\performance(\\actorparams_\\timestep,\\aisparams_\\timestep) &= \\sum_{\\timestep=1}^{\\Timestep}\\discount^{\\timestep-1}\\cost_{\\timestep}\\bigg(\\sum_{\\tau =1}^{\\timestep}\\grad_{\\actorparams}\\log(\\mu(\\Action_\\timestep|\\Ais_\\timestep ;\\ \\actorparams_\\timestep))\\bigg).\n        \\end{align*}",
        "2211.03011_b89c49594d6142e8be55a59b05cd342e": "Following a rollout of length",
        "2211.03011_2e3a4efb51070982bf3a3e12aaa4267c": ", we can then update the parameters",
        "2211.03011_1c386b493b0f1a79f80f5e32ce070c20": "$\\{(\\aisparams_i, \\actorparams_i  )\\}_{i \\geq 1}$",
        "2211.03011_574bd3b0fd3ea20556b90498a2840b35": "as follows:",
        "2211.03011_791c15a2e2c6a8258361a6c2eda4e0d2": "\\begin{align}\n                \\aisparams_{i+1} = \\aisparams_i + \\aislr_i \\grad_\\aisparams\\aisloss(\\aisparams_i), &&\n                \\actorparams_{i+1} = \\actorparams_i + \\actorlr_i \\hat\\grad_{\\actorparams}\\performance(\\actorparams_{i},\\aisparams_{i})\\label{eq:actor-update},\n            \\end{align}",
        "2211.03011_c4d695002022924b24c54245c136547d": "where the step-size",
        "2211.03011_9369c90a47575b8d6d3c2aa5c9426a39": "$\\{\\aislr_{i}\\}_{i \\geq 0}$",
        "2211.03011_d6982b0caa2870b8c36a12b5102c1bff": "$\\{\\actorlr_{i}\\}_{i \\geq 0}$",
        "2211.03011_4f81b61cc5d59f6e56a1a1a9c39b0d4a": "satisfy the standard conditions",
        "2211.03011_30ab1b6f2ad63abacea648a1ecb657e0": "$\\sum_{i} \\aislr_{i} = \\infty$",
        "2211.03011_97a03a799eac122ae808bf5861076c98": "$\\sum_{i}\\aislr_{i}^{2}< \\infty$",
        "2211.03011_2fe38c2d7f8a4342067a8369cc13abea": "$\\sum_{i} \\actorlr_{i} = \\infty$",
        "2211.03011_067505d17c7ba9267c8a63a56095b50e": "$\\sum_{i}\\actorlr_{i}^{2}< \\infty$",
        "2211.03011_4b99ad4f32b999c31dae0b336712e6d9": "respectively",
        "2211.03011_40d8811b63bebcbd8518af147a5182ca": "Moreover, one can ensure that the AIS generator converges faster by choosing an appropriate learning rates such that,",
        "2211.03011_96219f12e8affcb7232ae1ce8194a010": "$\\lim_{i \\to \\infty} \\frac{\\actorlr_{i}}{\\aislr_{i}} = 0$",
        "2211.03011_d1b7471ae69e2d41c841032960318c1a": "Actor Critic Algorithm",
        "2211.03011_56071f195cdddfd9923c4b7b7ff4aef1": "We can also use the aforementioned ideas to design an AIS based actor-critic algorithm",
        "2211.03011_f22bd46c14d31be8169a017d00dabc90": "In addition to a parameterised policy",
        "2211.03011_c3136ebc76d421caa0bbef876f7d52ef": "$\\policy(\\cdot; \\actorparams)$",
        "2211.03011_fc016b8784169f2c59c7f79de4586740": "and AIS generator",
        "2211.03011_19392b59f9ea5b984b7b2a848cc19c1a": "$(\\aisfunction_\\timestep(\\cdot;\\aisparams), \\hat f, \\hat r, \\hat\\transition)$",
        "2211.03011_813dab5948b2660c366609ef9e553f82": "the actor-critic algorithm uses a parameterised critic",
        "2211.03011_24c70463433a5c604f8ee239c5bcf954": "$\\hat\\valuefunction(\\cdot;\\criticparams):\\featurespace \\to \\real$",
        "2211.03011_a6fc027f9a86244f2ddecfe7e666dc15": "$\\criticparams$",
        "2211.03011_8dc132ab63aae104455a69c8ea8ee285": "are the parameters for the critic",
        "2211.03011_35a51f0a4335da64ade2112c51c66454": "The performance of policy",
        "2211.03011_dac6d8b5a0f415bc3168d24f0b603a03": "$\\mu(\\cdot;\\actorparams)$",
        "2211.03011_59f57c86e6f2213433ead359cd04a688": "is then given by",
        "2211.03011_d323ecbabc1b4c42f65c5292ce88bd40": "$\\performance(\\actorparams, \\aisparams, \\criticparams)$",
        "2211.03011_7e6b25db27e508a13b13c5f1679a29b6": "According to policy gradient theorem",
        "2211.03011_162bf53c57d52944a035564d394767b4": "\\citep{pgt,baxter-bartlett}",
        "2211.03011_22e35f41f37ba2e4cdcc79dcf295be29": "the gradient of",
        "2211.03011_591148f4464556d53f2fba39a2dc9b98": ", is given as:",
        "2211.03011_be8fbae8a2446fbdd58239cc77e9e8d0": "\\begin{align}\n            \\grad_\\actorparams \\performance(\\actorparams, \\aisparams, \\criticparams) &= \\expecun{}\\bigg[ \\grad_{\\actorparams}\\log(\\mu(\\cdot;\\actorparams))\\hat{\\valuefunction}(\\cdot;\\criticparams)\\bigg].\n        \\end{align}",
        "2211.03011_7647a63940aefd5945973b8df63b93f0": "And for a trajectory of length",
        "2211.03011_cef000b9fea569d14741f1a350c6a144": ", we approximate it as:",
        "2211.03011_9121f9258a2b0636aceea9aa41a54bb0": "\\begin{align}\n            \\hat{\\grad}_\\actorparams \\performance(\\actorparams, \\aisparams, \\criticparams) &= \\frac{1}{\\Timestep}\\sum_{\\timestep =1}^{\\Timestep}\\bigg[ \\grad_{\\actorparams}\\log(\\mu(\\cdot;\\actorparams))\\hat{\\valuefunction}(\\cdot;\\criticparams)\\bigg].\n        \\end{align}",
        "2211.03011_59583a790bc35de30eb79ea67c534031": "The parameters",
        "2211.03011_4c3dd8cfbab383824ebff4b76d9c5236": "can be learnt by optimising the temporal difference loss given as:",
        "2211.03011_936a5232337a55713b04052771e2313b": "\\begin{align}\n            \\loss_{\\text{TD}}(\\actorparams, \\aisparams, \\criticparams) &= \\frac{1}{\\Timestep}\\sum_{\\timestep=0}^{\\Timestep}\\texttt{smoothL1}(\\hat{\\valuefunction}(\\Feature_\\timestep;\\criticparams) - \\cost(\\Feature_\\timestep,\\Action_\\timestep) - \\discount \\hat{\\valuefunction}(\\Feature_{\\timestep+1};\\criticparams)).\n        \\end{align}",
        "2211.03011_be841edf9f04f8608363771c3109061d": "$\\{(\\aisparams_i, \\actorparams_i, \\criticparams_i  )\\}_{i \\geq 1}$",
        "2211.03011_3d44097f9bf20dd20b7c8b3e9361817e": "can then be updated using a multi-timescale stochastic approximation algorithm as follows:",
        "2211.03011_eaa29129d6652f111ab82c7a812ea511": "\\begin{align}\n                \\aisparams_{i+1} &= \\aisparams_i + \\aislr_i \\grad_\\aisparams\\aisloss(\\aisparams_i)\\label{eq:ac-ais-update}\\\\\n                \\criticparams_{i+1} &= \\criticparams_i + \\criticlr_i \\grad_{\\criticparams}\\loss_{\\text{TD}}(\\actorparams_i, \\aisparams_i, \\criticparams_i)\\label{eq:ac-critic-update}\\\\\n                \\actorparams_{i+1} &= \\actorparams_i + \\actorlr_i \\hat{\\grad}_{\\actorparams}\\performance(\\actorparams_{i},\\aisparams_{i},\\criticparams)\\label{eq:ac-actor-update},\n            \\end{align}",
        "2211.03011_dede94acba140299b100e752612269de": "$\\{\\criticlr_{i}\\}_{i \\geq 0}$",
        "2211.03011_4d3ac98052fd66b10667c3677b516a4f": "$\\sum_{i}\\aislr_{i}^{2} < \\infty$",
        "2211.03011_fd34995973bfa0b61a9789d75c90d2ab": "$\\sum_{i} \\criticlr_{i} = \\infty$",
        "2211.03011_b5f82a018ca6c4110a41beda6de1ce7b": "$\\sum_{i}\\criticlr_{i}^{2} < \\infty$",
        "2211.03011_e767df161f7431f65a5f887b81dc3577": "Moreover, one can ensure that the AIS generator converges first, followed by the critic and the actor by choosing an appropriate step-sizes such that,",
        "2211.03011_a7a01aea3e9e9af61534a1b6840fed07": "$\\lim_{i \\to \\infty} \\frac{\\criticlr_{i}}{\\actorlr_{i}} = 0$",
        "2211.03011_398d6ee7abbdccbe4c577b31a362325d": "Convergence analysis",
        "2211.03011_8e415a75a8ec56081548e3d2e95674dd": "In this section we will discuss the convergence of the AIS-based policy gradient in",
        "2211.03011_26f4d5eaeeaa5cc1002b704ebfff1eaf": "as well as Actor-Critic algorithm presented in the previous subsection",
        "2211.03011_a1c196aea3bc5cad3e1e8962d2a04e4d": "The proof of convergence relies on multi-timescale stochastic approximation",
        "2211.03011_5428493f6985232d6110fb56c2980526": "\\citet{borkar2008stochastic}",
        "2211.03011_81ab6b32de6ea3d8dfe0ec400818153b": "under conditions similar to the standard conditions for convergence of policy gradient algorithms with function approximation stated below, therefore it would suffice to provide a proof sketch.",
        "2211.03011_7bade40785d1f5c1dbec8c0414eaac9a": "List (enumerate)",
        "2211.03011_ef1a4e152bddbe48fa5d5d1ca83b4e38": "The values of step-size parameters",
        "2211.03011_7db78a84b908ef06137488fa37eb894c": "$\\aislr, \\actorlr$",
        "2211.03011_277df80f00e305c27e6bb8a1136416f0": "$\\criticlr$",
        "2211.03011_bd5ca20389348d09baa7a85b3a513a03": "(for the actor critic algorithm) are set such that the timescales of the updates for",
        "2211.03011_0c2c3a1891f5a61118bfaa6d17c67522": "(for Actor-Critic algorithm) are separated,",
        "2211.03011_5ebe78f9784d1a2166426fb735547d5d": "$\\aislr_{\\timestep} \\gg \\actorlr_{\\timestep}$",
        "2211.03011_325e6eb6b8dbd820dc0c7d6dde145eb8": ", and for the Actor-Critic algorithm",
        "2211.03011_b4749aa0d6162a74976aad8cc5753348": "$\\aislr_{\\timestep} \\gg \\criticlr_\\timestep \\gg \\actorlr_{\\timestep}$",
        "2211.03011_8e0fbe05d282501663f38440ed36b6e1": "(for Actor-Critic algorithm) lie in a convex, compact and closed subset of Euclidean spaces.",
        "2211.03011_0372b17eed041163604d1e058279082e": "The gradient",
        "2211.03011_1d8d039300cba2ee4e9c5b8894549b7f": "$\\grad_{\\aisparams}\\aisloss$",
        "2211.03011_88215cc0f620daf24187d51a265c667a": "is Lipschitz in",
        "2211.03011_546e63d6df311d7a6973c6d9a3903236": "$\\aisparams_{\\timestep}$",
        "2211.03011_32fe3ccb2688cd8ed3bb64fcc402d54e": "$\\hat \\grad_{\\actorparams}\\performance(\\actorparams,\\aisparams)$",
        "2211.03011_859a5b6c9857f71088e2eb6467a735a3": "$\\actorparams_{\\timestep},~\\text{and}~\\aisparams_{\\timestep}$",
        "2211.03011_b48878031e46029c717aa6cb1e1a1385": "Whereas for the Actor-Critic algorithm the gradient of the TD loss",
        "2211.03011_bdf9a230788dfb29813774fb832df6a3": "$ \\grad_{\\criticparams}\\loss_{\\text{TD}}(\\aisparams, \\actorparams, \\criticparams)$",
        "2211.03011_39e59254269854d5b094be77c816d3ec": "and the policy gradient",
        "2211.03011_3d6540a21260bf2590c306b7e3c72856": "$\\hat \\grad_{\\actorparams} \\performance(\\aisparams, \\actorparams, \\criticparams)$",
        "2211.03011_52f751a6016239ae706f6d04100c761d": "$(\\aisparams_\\timestep, \\actorparams_\\timestep, \\criticparams_\\timestep)$",
        "2211.03011_8526f8cb4ee60d1e9a71eda47afb3fb3": "Estimates of gradients",
        "2211.03011_66d1c314806bbbe223e22edd7d4e55f0": "$\\grad_{\\actorparams}\\performance(\\actorparams,\\aisparams)$",
        "2211.03011_c873231acb31acc74ac6a46142309634": "and are unbiased with bounded variance",
        "2211.03011_68d030a234d18dcf903f0f8d0fb156f1": "The ordinary differential equation (ODE) corresponding to",
        "2211.03011_24bc09f1e24a313b92b0562e8d13a0cb": "is locally asymptotically stable.",
        "2211.03011_c6c5d00dba4aaa7f7149eb9ac88fa830": "The ODEs corresponding to",
        "2211.03011_9fdeee3445df77f58f4cb140e56712d2": "is globally asymptotically stable.",
        "2211.03011_195d4c0acdc6d451d53234f708c9e27b": "For the Actor-Critic algorithm, the ODE corresponding to",
        "2211.03011_d0a5f807296a92dfe81b6b43c3392187": "is globally asymptotically stable and has a fixed point which is Lipschitz in",
        "2211.03011_02552d3c116b8a96f9ff0001b6ae661c": "Under",
        "2211.03011_9423752703d6e2f0975768fc84b3a6dd": ", along any sample path, almost surely we have the following:",
        "2211.03011_146a65663a2131c31a29ae9125772ddc": "The iteration for",
        "2211.03011_13b5bfe96f3e2fe411c9f66f4a582adf": "in",
        "2211.03011_71d81097e8bfe7032b8756e4b879f402": "converges to an AIS generator that minimises the",
        "2211.03011_936038870b40ed9d6db5a68bee9cecad": "$\\aisloss$",
        "2211.03011_5e172f582b312eb9e56573ff44797f0d": "converges to a local maximum of the performance",
        "2211.03011_e40ed3712112070efb6d4185b5280b1e": "$\\performance(\\aisparams^\\star,\\actorparams)$",
        "2211.03011_8a9e5f829b0f5c7fa692c3cd88091ef0": "$\\aisparams^\\star$",
        "2211.03011_d249ff2188f5e41ccf986359eef9bdcd": "$\\criticparams^\\star$",
        "2211.03011_7d2574c098bab09b4bd5b65cf25c8488": "(for Actor Critic) are the converged value of",
        "2211.03011_d271f1eb1356e5de86d37de79fafac55": "For the Actor-Critic algorithm the iteration for",
        "2211.03011_732bf370673edb319836ce659247ba0a": "converges to critic that minimises the error with respect to the true value function.",
        "2211.03011_d0a04bd81791a4cd044fb46317f9ab07": "The proof for this theorem follows the technique used in",
        "2211.03011_f642b8438f9d13b33a7946791b020af8": "\\citep{Leslie2004ReinforcementLI,borkar2008stochastic}",
        "2211.03011_98119b89b3f1d17ca86d7a1ab1130c78": "Due to the specific choice of learning rate the AIS-generator is updated at a faster time-scale than the actor, therefore it is",
        "2211.03011_ce14e44c32596ba4db87c2d33335de0a": "quasi static",
        "2211.03011_4612b12a95d0a1cc3e17e9c69624f36a": "with respect to to the actor while the actor observes a",
        "2211.03011_613a608459fd4fe707cce6302c3978ba": "nearly equilibriated",
        "2211.03011_f450739281f61bd7e43cfca2dbe583d9": "AIS generator",
        "2211.03011_0717337655ee9686c573fe7fbb2747d7": "Similarly in the case of the Actor-Critic algorithm the AIS generator observes a stationary critic and actor, whereas the critic and actor see",
        "2211.03011_a96a205f77a5afc3e54ea326e48f72b3": "The Martingale difference condition (A3) of",
        "2211.03011_3d86ca69524d5df4591386a8da56ca9a": "is satisfied due to",
        "2211.03011_d550553c58511ccf7d17aef808551142": "As such since our algorithm satisfies all the four conditions by",
        "2211.03011_7c2467ac15a3ba24f33f983be748759f": "\\citep[page35]{Leslie2004ReinforcementLI}",
        "2211.03011_6ddcec9b2452cd4677ba64fe78711b9d": "\\citep[Theorem 23]{Borkar1997StochasticAW}",
        "2211.03011_717ef217f151015690a4e2a690518078": ", the result then follows by combining the theorem on",
        "2211.03011_d1a13e6cf856bd6099a4dbc410284cb5": "\\citep[page 35]{Leslie2004ReinforcementLI}",
        "2211.03011_db565d7c8132621a03bbd9e075a0d8f8": "\\citep[Theorem 23]{borkar2008stochastic}",
        "2211.03011_2d54d6e00f85eb1bc81281cb4f5f799c": "\\citep[Theorem 2.2]{Borkar1997StochasticAW}",
        "2211.03011_99cb3e6e64def1388fdd9aeba5bfbee9": "\\begin{figure*}[!htbp]\n      \\includegraphics[width=\\linewidth]{Results/combined-1.pdf}\n      % \\caption{This is a figure} \\label{fig:comb-results-1}\n      \\includegraphics[width=\\linewidth]{Results/combined-2.pdf}\n      \\caption{Empirical results averaged over 50 Monte Carlo runs with shaded regions showing the interquantile range.} \\label{fig:comb-results-2}\n    \\end{figure*}",
        "2211.03011_9cbb97584554b2dd4bf9d75401a89be1": "Empirical evaluation",
        "2211.03011_9e0d1aa452e77caa5296cf3a4bcf23fe": "Through our experiments, we seek to answer the following questions: (1) Can history-based feature representation policies help improve the quality solution found by a memory-less RL algorithm? (2) In regards to the solution quality and sample complexity, how does the proposed method compare with other memory-augmented policies? (3) How does the choice of IPM affect the algorithms performance? We answer question (1) and (2) by comparing our approach with the proximal policy gradient (PPO) algorithm which uses feed-forward neural networks",
        "2211.03011_98b533b7ef505c5b8eef8d711b44a0dc": "For question (2), we compare our method with an LSTM-based PPO variant which learns the feature representation using the history of states",
        "2211.03011_121ef0feb34ca2862519a3566c00bf7a": "$\\State_{1:\\Timestep}$",
        "2211.03011_caefd0d4340f2922c17d5ee7a2f18379": "in a trajectory",
        "2211.03011_315ca4207029f01e59a0144b0b11e57f": "For question (3) we compare the performance of our method using different MMD kernels and KL-divergence based approximation of Wasserstein distance",
        "2211.03011_46de29a73d2c10ee8c9d8e44ffbd942a": "All the approaches are evaluated on six continuous control tasks from the MuJoCo",
        "2211.03011_492d34a0afc04651cc29715b32f01cad": "\\citep{Todorov2012MuJoCoAP}",
        "2211.03011_3f5031c06c68453ee47a34b2621c42c0": "OpenAI-Gym suite",
        "2211.03011_c51af1b0c8b08142e705ffe2a5e63235": "To ensure a fair comparison, the baselines and their respective hyper-parameter settings are taken from well tested stand-alone implementations provided by",
        "2211.03011_22c208098b0e66e1963920348c94164c": "\\citet{baselines}",
        "2211.03011_e5faad8e760b842c06ee45b139da7c84": "From an implementation perspective, our framework can be used to modify any off-the-shelf policy-gradient algorithm by simply replacing (or augmenting) the feature abstraction layers of the policy and/or value networks with recurrent neural networks (RNNs), trained with the appropriate losses, as outlined previously",
        "2211.03011_0f754ea0299114bc58d8b07b4ef72f8c": "In these experiments, we replace the fully connected layers in PPO's architecture with a Gated Recurrent Unit (GRU)",
        "2211.03011_1dc6802e2d28f93a5b0f57b22b99dd65": "For all the implementations, we initialise the hidden state of the GRU to zero at the beginning of the trajectory",
        "2211.03011_6f645a269d5d765bac94484f2c5ce1f1": "This strategy simplifies the implementation and also allows for independent decorrelated sampling of sequences, therefore ensuring robust optimisation of the networks",
        "2211.03011_2eefecb3467363c670630dd30bc5df51": "\\citep{rnn-hausknecht}",
        "2211.03011_90e8e99ab47dccc0a64fa72bb81a5195": "It is important to note that we can extend our framework to other policy gradient methods such as SAC",
        "2211.03011_255ab85c5efeb5932e1f73e233b28f83": "\\citep{HaarnojaZAL18}",
        "2211.03011_9f5ef09d358b42a80967cde42f8fcb49": ", TD3",
        "2211.03011_3ef85492a3d5dff17f4c9de1ebd924de": "\\citep{td3}",
        "2211.03011_7f94c46bda25c3736df59c0f10b94d16": "or DDPG",
        "2211.03011_3b4dc2ec20788fcd52bf03ac349c4343": "\\citep{ddpg}",
        "2211.03011_bc0a7a44dc5dd0b6317e516b411c367f": ", after satisfying certain technical conditions",
        "2211.03011_ce7cf4a522724d83ea5f5fdbffabf49f": "However, we leave these extensions for future work",
        "2211.03011_ca5e71ff26312c8dd3681233b5b40bec": "Additional experimental details and results can be found in",
        "2211.03011_645703ef7a13a4dae90ff8d1d37191f5": "contains the results of our experiments averaged over 50 Monte-Carlo evaluations using MMD-based AIS loss in",
        "2211.03011_d45f8832ff0e2bcd79dfe600b0f6fe4d": "These results show that our algorithm improves over the performance of both the baselines, and the performance gain is significantly higher for high-dimensional environments like Humanoid and Ant",
        "2211.03011_484f4f68108bad902a7f30711afe7e94": "It is worth noticing that the GRU baseline also outperforms the feed-forward baseline for most environments",
        "2211.03011_966fcb0fd1083f4b39590964ff1931a3": "Overall, these findings lend credence to history-based encoding policies as a way to improve the quality of the solution learnt by the RL algorithm.",
        "2211.03011_cfea886fd374f551024e54bbb71bc11f": "\\begin{figure*}[!htbp]\n        \\includegraphics[width=\\linewidth]{Results/MMD.pdf}\n        \\caption{Comparison of different MMDs, averaged over 50 runs} \\label{fig:MMD-comp}\n    \\end{figure*}",
        "2211.03011_70835419281d4b593a54d6cba9f84b78": "Note that the MMD distance given by",
        "2211.03011_d757b502a6a80bc474f8b0a0e6090cee": ", can be computed using different types of characteristic kernels (for a detailed review see",
        "2211.03011_a0db68d122afd3aae68cc68478eb90dc": "\\citep{Sriperumbudur,NIPS2009_685ac8ca,sejdinovic}",
        "2211.03011_9371d7a2e3ae86a00aab4771e39d255d": ")",
        "2211.03011_79d20a30e742e10a127510c69fc848c6": "In this paper we consider computing",
        "2211.03011_3809d244bc6c35b0ba31cc125c5e54b9": "using the Laplace, Gaussian and energy distance kernels",
        "2211.03011_2cb1386d86ed46020f58d458d06d9be1": "In in",
        "2211.03011_a2106e1ccc0d4bef15470d52986adad6": "we compre the perfromance of our methods under different MMD kernels",
        "2211.03011_841a0ea8f1b740240917f28f5408c4a2": "It can be observed that for the continuous control tasks in the MuJoCo suite, the energy distance yields better performance, and therefore we implement",
        "2211.03011_24bda4238d181d34304ee8c5fdaeb3d3": "using the energy distance for the results in",
        "2211.03011_cc5eae04f9bdddfa6a76c58bb53d7015": "Next, we compare the performance of our method under MMD (Energy distance kernel) and Wasserstein distance",
        "2211.03011_5da618e8e4b89c66fe86e32cdafde142": "From",
        "2211.03011_c25579a103c8ff37432da4b319d17a8e": "we observe that for continuous control tasks, use of MMDs result in better performance as compared to Wasserstein distance.",
        "2211.03011_7d887bf4150670c319d5bf8dbb03bcec": "\\begin{figure*}[!htbp]\n            \\includegraphics[width=\\linewidth]{Results/wass.pdf}\n            \\caption{Comparison of Wasserstein vs MMDs, averaged over 50 runs.} \\label{fig:Wass-res}\n        \\end{figure*}",
        "2211.03011_aed402c3112b4749a9a98a72cbe9093d": "Related Work",
        "2211.03011_bc6414bfcf017fb6cb0280a74543e00d": "The development of RL algorithms with memory-based feature abstractions has been an active area of research, and most existing algorithms have tackled this problem using non-parametric methods like Nearest neighbour",
        "2211.03011_1156609d9e793b0c548dad47a6d3044d": "\\citep{Bentley1975MultidimensionalBS,Friedman1977AnAF,PENG1995438}",
        "2211.03011_ad4aac053ee32a824db54c9930d6946c": ", Locally-weighted regression",
        "2211.03011_a4dd8d5f9eeebbf3e760fd64ab2981d4": "\\citep{Baird1993ReinforcementLW,locallyweighedatekson,Moore1997EfficientLW}",
        "2211.03011_cac412793dd781c5b1c051a67aee658a": ", and Kernel-based regression",
        "2211.03011_d9cb76989fcf52818503542bacfe9a5f": "\\citep{Connell1987LearningTC,Dietterich2001BatchVF,kbrl,Xu2006KernelLT,Bhat2012NonparametricAD,BarretoAndr2016PracticalKR}",
        "2211.03011_58c0408c0cd9a5288c7ebf846b428192": "Despite their solid theoretical footing, these methods, have limited applicability as they are difficult to scale to high-dimensional state and action spaces",
        "2211.03011_1cf580f5834a0bcbef1d2ad832874207": "More recently, several methods that propose using recurrent neural networks for learning history-based abstractions have enjoyed considerable success in complex computer games",
        "2211.03011_b8caa259399b8c73e2c53b5921074b70": "\\citep{rnn-hausknecht,JaderbergMCSLSK17,impala,DBLP:conf/iclr/GruslysDAPBM18,ha}",
        "2211.03011_4e2426db6cca5b1604b12620bd8cba64": "however most of these methods have been designed for partially observable environments where use of history-based methods is often necessary",
        "2211.03011_3cf1a0604aa976d20bce2c80822c6759": "To the best of our knowledge, the only other work where a history-based RL algorithm is used for controlling a MDP is presented by",
        "2211.03011_0b39cbb4299716812e470b1d12e708b7": "\\citet{openai2019learning}",
        "2211.03011_89bc1f68e6c0f8f15bbda15134d2466a": "In this work the authors show that using an LSTM-based agent architecture results in superior performance for the object reorientation using robotic arms",
        "2211.03011_b9f6aaab9e68f61d8a357a6c68c80695": "However, the authors do not provide a theoretical analysis of their method.",
        "2211.03011_25f8fea261e42991fc947198f3db639c": "Bisimulation metrics",
        "2211.03011_aacd61784ed97251e71d57973fd2ead9": "On the theoretical front, our work is closely related to state aggregation techniques based on bisimulation metrics proposed by",
        "2211.03011_28ec91b78cf48ce017bb38ed6a0c5dcd": "\\citet{Givan2003EquivalenceNA,Ferns2004MetricsFF,Ferns2011BisimulationMF}",
        "2211.03011_6d0a721fff7b176b76feb35b6eddd697": "The bisimulation metric is the fixed point of an operator on the space of semi-metrics defined over the state space of an MDP with Lipschitz value functions",
        "2211.03011_e4a4680c97e576b0d9cdbea54ff1d262": "Apart from state aggregation, bisimulation metrics have been used for feature discovery",
        "2211.03011_e80ea5ab2e33f86f018e188a02772efb": "\\citep{Comanici2011BasisFD,Ruan2015RepresentationDF}",
        "2211.03011_c7f3ff975da2c9150c89676bf44e5a41": ", and transfer learning",
        "2211.03011_41fdb10829ab1f38ffd540572b32869e": "\\citep{Castro2010UsingBF}",
        "2211.03011_434d0c3e391b8519b28163b272a6d076": "However, computational impediments have prevented their broad adoption",
        "2211.03011_ef9987fe6a423971ca9aa7992de38f86": "Our work can be viewed as an alternative to bisimulation for the analysis of history-based state abstractions and deep RL methods",
        "2211.03011_2763f1eb3252b2372a10cc32bcee7d2d": "Our work can also be thought of as extension of the DeepMDP framework",
        "2211.03011_acbf8ee04ec594ac1494123d9829cc85": "to history-based policies and direct policy search methods.",
        "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81": "AIS and Agent state",
        "2211.03011_51cde8586d2e4d90c83c8732383e5cb8": "The notion of AIS is closely related to the epistemic state recently proposed by",
        "2211.03011_57f20561a25bb52b78be2d0b6b4cb79b": "\\citet{bvrstate}",
        "2211.03011_fa48d894f155c7cc2e74a5f5389117c3": "An epistemic state is a bounded representation of the history",
        "2211.03011_919824fab09a3faa2301012e7e6a9033": "It is updated recursively as the agent collects more information, and is represented as an environment proxy",
        "2211.03011_d20454f75a133b87c6351eed3d4bccc2": "$\\Upsilon$",
        "2211.03011_57cd7ebf89f5b756e648a48514a9d7d3": "which is learnt by optimising a target/objective function",
        "2211.03011_c91091e68f0e0113ff161179172813ac": "$\\chi$",
        "2211.03011_eb5105aae667cee681195170db93d37b": "is a random variable, its entropy",
        "2211.03011_ed80232bf64f1e0d3094d664092427da": "$\\mathbb{H}(\\Upsilon)$",
        "2211.03011_ee4d73072b2d7e2576dc3da99498f5f5": "is used to represent system's uncertainty about the environment",
        "2211.03011_731b613147b12bb1fa7d7e3433745f3c": "The framework proposed in this paper can considered as a practical way of constructing the system epistemic state where, the AIS",
        "2211.03011_cfaebeb9babc92d4af6917565c2b726e": "$\\Ais_\\timestep$",
        "2211.03011_5fffa36f7993121d3bfbc67f19daf5e9": "represents both the epistemic state and the environment proxy",
        "2211.03011_9a639daf68b7281f5bcd329f8d8a7812": "represents",
        "2211.03011_7e334ce7e9d3e2ebce8d3881692600f2": ", and instead of entropy, the constants",
        "2211.03011_7ccca27b5ccc533a2dd72dc6fa28ed84": "$\\epsilon$",
        "2211.03011_d9b2eaacb8de6c5e1da8dcdfb56d9d25": "represent the systems uncertainty about the environment",
        "2211.03011_ec4d61651b78865cda2d5b9b3669a481": "The study of the AIS framework in the regret minimisation paradigm could help establish a relationship between the",
        "2211.03011_d54c4beb5cfc0777ca7baa8c2bd31c82": ", thereby helping designers develop principled algorithms which synthesise ideas like information directed sampling for direct policy search algorithms.",
        "2211.03011_ce325d035fed6f391fca48c20d4825e5": "Analysis of RL algorithms with attention mechanism",
        "2211.03011_0cb6f451df2d212704cb9cb1180977cf": "Recently, there has been considerable interest in developing RL algorithms which use attention mechanism/transformer architectures",
        "2211.03011_4da4f9e1fe074a7d03e6dc220923b005": "\\citep{BahdanauCB14,Xu2015ShowAA}",
        "2211.03011_a2d7c81d737930d696eaa4704003d47b": "for learning feature abstractions",
        "2211.03011_6ddbae53c8d26c846975959d2e5cc494": "\\citep{Zambaldi2019DeepRL,Mott2019TowardsIR,Sorokin2015DeepAR,Oh,RitterFSSBR21,ParisottoSRPGJJ20,chen,LoyndFcSH20,TangNH20,PritzelUSBVHWB17}",
        "2211.03011_b9191bbd80124b88b90710745a264968": "Attention mechanism extract task relevant information from historical observations and can be used instead of RNNs for processing sequential data",
        "2211.03011_61d6a625abf75e8caf5bdefa244318bb": "\\citep{vaswani}",
        "2211.03011_be4ba36f97e08e5df0ce695e19659595": "As we do not impose a functional from on the history compression function",
        "2211.03011_5e6332f347c8ec7475e34b73a40a4284": "$\\aisfunction_\\timestep(\\cdot)$",
        "2211.03011_0bb1fd53ce422e3e84c83d91eb79bbfc": ", any attention mechanism can be interpreted as history compression function, and one can construct a valid information state by ensuring that the output of the attention mechanism satisfies (P1) and (P2)",
        "2211.03011_6ff130948188fdd3c3e9bef729ecb38e": "That being said, even without optimising",
        "2211.03011_9b2a2a372ff69d4e4a17b12c5232c3e5": ", the approximation bound in",
        "2211.03011_ac0ee800ecb7c8c0703f1e8e1a1cd757": "still applies for RL algorithms with attention mechanisms, with the caveat that the constants",
        "2211.03011_384fe64074bac3db45a47a22b4b2d254": "may be arbitrarily large",
        "2211.03011_4909dd2826edd32ce20331fd4a561ddd": "A thorough empirical analysis of the effect of different attention mechanisms, and the AIS loss on the on the error constants",
        "2211.03011_4b668622ebb4f8b5784a27631697b9a7": "could help us gain a better understanding of the way in which such design choices could influence the learning process.",
        "2211.03011_ca5b9bcf1c7d28ea5571305a1422be3c": "AIS for POMDPs",
        "2211.03011_500c307d9281d5ff38f0296446fa8467": "The concept of an AIS used in this paper is similar to the idea of AIS for POMDPs",
        "2211.03011_986977c7a963430cc8bbe314524d400b": "\\citep{ais-2, ais-1}",
        "2211.03011_021a6c0ad9af7c5a4a6e87b5174bf9e8": "Moreover, the literature also contains several other methods which have enjoyed empirical success in using history-based policies for controlling POMDPs",
        "2211.03011_a93dcb9142c1e0b277290b5f32880525": "\\citep{Isbell,hutte-1,hutter-2,Schaefer2007ARC,Dreamer,Pla-Net}",
        "2211.03011_24587e53567bba18d8324d72024b48c9": "In principle, one can use any of these methods for controlling MDPs",
        "2211.03011_824350fe9ed8fd5235f880f0dd103ce9": "However, this does not immediately provide a tight bound for the approximation error",
        "2211.03011_080b42ac97dec586f542419a3c4aeb68": "The MDP model has more structure than POMDPs, and our goal in this paper is to use this fact to present a tighter analysis of the approximation error.",
        "2211.03011_773fc6f17c333cb368c771d40121a558": "Conclusion and future work",
        "2211.03011_7d97df3f3edae22bf149336b8ed858f7": "This paper presents the design and analysis of a principled approach for learning history-based policies for controlling MDPs",
        "2211.03011_658277e0f80fb180651ad4bd9d20fe24": "We believe that our approximation bounds can be helpful for practitioners to study the effect of some of their design choices on the solution quality",
        "2211.03011_ee3ac473cc6382e17f5c2aeddeba27a2": "On the practical side, the proposed algorithm shows favourable results on high-dimensional control tasks",
        "2211.03011_a3b055404a1f6d44f22bad90cef557e2": "Note that one can also use the bounds in",
        "2211.03011_e4a81898673ac335bad4c8f88540120c": "to analyse the approximation error of other history-based methods",
        "2211.03011_bfb8c3827f197d6dc40f28fd51415064": "However, since some of these algorithms do not satisfy",
        "2211.03011_93c90e91a476e5383aa9eaa78da1eec4": ", the resulting approximation error might be arbitrarily large",
        "2211.03011_6a591b6d54756aaedfd8902e6d2a0f84": "Such blow-ups in the approximation error could be because the bound itself is loose or the optimality gap is large",
        "2211.03011_31d94d157b3a5c5160f2a9f9581affa7": "This would depend on the specifics of the methods and remains to be investigated",
        "2211.03011_1192d8cd240aab03b0876dcdda911ca9": "As such, a sharper analysis of the approximation error by factoring in the specific design choices of other methods is an interesting direction for future research",
        "2211.03011_f0a637a3d2b7138c1488ad327f4d6a77": "Another interesting direction would be to conduct a thorough empirical evaluation exploring the design choices of history compression functions.",
        "2211.03011_39ee9dae92bf620136f031b9afa38d34": "*Appendix",
        "2211.03011_b92673fa6044d0a8135d08edca87a36d": "Proof for Theorem",
        "2211.03011_8bd4df4efb7d8b5a677e3222400157ca": "For readability we will restate the theorem statement",
        "2211.03011_a74892bc7c77dcc4bef89e8edd3a7059": "For any time",
        "2211.03011_0d63c5ec809772715876b078eee0091e": ", any realisation",
        "2211.03011_8ccb58356c2caa4faeaaa7a2dfb34b3f": "$\\sts_\\timestep$",
        "2211.03011_8bf8854bebe108183caeb845c7676ae4": "of",
        "2211.03011_fe6b54052cf5f0165fd73650d28a1d41": "$\\action_\\timestep$",
        "2211.03011_d021765abce6a48bf9f1077b40fb264a": "$\\Action_\\timestep$",
        "2211.03011_86f39b2a7975cd9082784bb4059f2fe1": "$\\history_\\timestep = (\\sts_{1:\\timestep}, \\action_{1:\\timestep-1})$",
        "2211.03011_54143f14e12907fed4395b8b1491d47c": "$\\ais_\\timestep = \\aisfunction_\\timestep(\\history_\\timestep)$",
        "2211.03011_3c9c77ddc42d6926a86cfc36d3cdc10e": "is bounded as:",
        "2211.03011_86b748deecb350b4b5f9388b7f8b8280": "\\begin{align}\n                \\Delta\n                \\le 2 \\frac{\\varepsilon + \\discount\\delta \\kappa_{\\mathfrak{F}}(\\hat \\valuefunction^{\\mu}, \\hat{f})}{1 - \\discount},\n            \\end{align}",
        "2211.03011_6eb1c85fff16ab5f7b7716c538cbf8ce": "where,",
        "2211.03011_c4e905c4210a004675ca4048b5ee7e92": "$\\kappa_{\\mathfrak{F}}(\\hat \\valuefunction, \\hat f) = \\sup_{\\feature, \\action}\\rho_{\\mathfrak{F}}(\\hat \\valuefunction(\\hat f(\\cdot, \\feature, \\action)))$",
        "2211.03011_e8a79a65a15d84733a4600197d0c35d1": ". and",
        "2211.03011_ce852edfc885f2ca59af98510114d461": "For this proof we will use the following convention: For a generic history",
        "2211.03011_677a9cf23f254a7c5ebbabf077f9ac0e": "$\\history_\\timestep \\in \\historyspace_\\timestep$",
        "2211.03011_e064261950ecf935fbeba65877cddaaf": ", we assume that",
        "2211.03011_f6a9f7a6c5c03e1b7454b94cb13db5cb": "$\\history_\\timestep  = (\\sts_{1:\\timestep}, \\action_{1:\\timestep-1})$",
        "2211.03011_d97c3e4bbbcfeeac7933855b921bb9cf": ", moreover, note that",
        "2211.03011_4b9b66fe6f24dfd1732b4359ff267111": "Now from",
        "2211.03011_16cc25c6a719cf74c31d48ac8840462a": "for any",
        "2211.03011_d436b65a6933eae9076a4d4350801edc": "$\\action_\\timestep, \\sts_\\timestep, \\ais_\\timestep$",
        "2211.03011_f873b5e9f7a61df11b19e1ffbc2a4f4c": "\\begin{align}\n            \\nonumber  \\max_{\\history \\in \\historyspace_\\timestep, \\action_{\\timestep} \\in \\actionspace}\\bigg\\vert \\cost (\\sts_\\timestep, \\action_\\timestep) - \\hat \\cost(\\ais_\\timestep, \\action_\\timestep)\\bigg\\vert &\\leq \\epsilon.\\\\\n            \\max_{\\history \\in \\historyspace_\\timestep, \\action_{\\timestep} \\in \\actionspace}\\bigg\\vert \\sum_{\\sts_{\\timestep+1} \\in \\statespace} \\bigg(\\transition(\\sts_{\\timestep+1}\\vert \\sts_\\timestep,\\action_\\timestep)\\hat{\\valuefunction}^{\\mu}(\\hat{f}(\\sts_{_\\timestep+1}, \\ais_\\timestep, \\action_\\timestep)) - \\hat{\\transition}(\\sts_{\\timestep+1}\\vert \\ais_\\timestep,\\action_\\timestep)\\hat{\\valuefunction}^{\\mu}(\\hat{f}(\\sts_{\\timestep+1}, \\ais_\\timestep, \\action_\\timestep))\\bigg)\\bigg\\vert &\\leq \\delta \\rho_{\\mathfrak{F}}(\\hat{\\valuefunction}(\\hat f(\\cdot, \\feature_\\timestep, \\action_\\timestep))).\\label{eq:delta}\n        \\end{align}",
        "2211.03011_1a5ce6b6a320d359b0d546e6f9f9773f": "Now using triangle inequality we get:",
        "2211.03011_69c9aef16cd270653be97ed28c735952": "\\begin{align}\n            \\Vert \\valuefunction^{\\star}(\\sts_\\timestep) - \\valuefunction_{\\timestep}^{\\pi}(\\history_\\timestep)\\Vert_{\\infty}\n            &\\stackrel{(a)}{\\leq} \\underbrace{\\Vert \\valuefunction^{\\star}(\\sts_\\timestep) - \\hat \\valuefunction^{\\mu}(\\ais_\\timestep) \\Vert_{\\infty}}_{\\text{term 1}} +  \\underbrace{\\Vert \\valuefunction_{\\timestep}^{\\policy{}}(\\history_\\timestep) - \\hat\\valuefunction^{\\mu}(\\ais_\\timestep) \\Vert_{\\infty}}_{\\text{term 2}},\n        \\end{align}",
        "2211.03011_1ded1248b9f0794a801364bf2a647321": "$(a)$",
        "2211.03011_4ee05d5da2580193cf0f42bfaf48c3fe": "follows from triangle inequality",
        "2211.03011_944c5c9a90388fa9c2a894990fa74e58": "We will now proceed by bounding terms 1 and 2 separately",
        "2211.03011_ed2e81d6a8547f15cc1a3a462733c44f": "\\begin{align}\n           \\Vert \\valuefunction^{\\star}(\\sts_\\timestep) - \\hat \\valuefunction^{\\mu}(\\feature_\\timestep) \\Vert_\\infty\n            &\\leq \\max_{\\history \\in \\historyspace_\\timestep}\\bigg \\vert\\max_{\\action_{\\timestep} \\in \\actionspace}\\bigg[ Q^{\\star}(\\sts_\\timestep, \\action_\\timestep) - \\hat Q^{\\mu}(\\feature_\\timestep, \\action_\\timestep)\\bigg]\\bigg\\vert, \\label{eq:vq-diff-1}\n        \\end{align}",
        "2211.03011_98b49102fc0e6de5b99f7f598ab68fe5": "Therefore, for any action",
        "2211.03011_13e9ba4585909339169f64155b4650e4": "\\begin{align*}\n             \\max_{\\history \\in \\historyspace_\\timestep}\\bigg \\vert\\max_{\\action_{\\timestep} \\in \\actionspace}\\bigg[Q^\\star(\\sts_\\timestep, \\action_\\timestep) - \\hat Q^{\\mu}(\\feature_\\timestep, \\action_\\timestep)\\bigg]\\bigg\\vert \n             &= \\max_{\\history \\in \\historyspace_\\timestep}\\bigg \\vert \\max_{\\action_{\\timestep} \\in \\actionspace}\\bigg[\\cost(\\sts_{\\timestep},\\action_\\timestep) + \\discount\\sum_{\\sts_{\\timestep+1} \\in \\statespace} \\transition(\\sts_{\\timestep+1} \\vert \\sts_\\timestep,\\action_\\timestep)\\valuefunction^{\\star}(\\sts_{\\timestep+1})\\\\\n             &-  \\hat\\cost(\\ais_\\timestep,\\action_\\timestep) - \\discount\\sum_{\\sts_{\\timestep+1} \\in \\statespace} \\hat{\\transition}(\\sts_{\\timestep+1} \\vert \\ais_\\timestep,\\action_\\timestep)\\hat \\valuefunction^{\\mu}(\\hat{f}(\\sts_{\\timestep+1},\\ais_\\timestep, \\action_\\timestep))\\bigg]\\bigg\\vert\\\\\n             &\\stackrel{(a)}{\\leq} \\epsilon + \\max_{\\history \\in \\historyspace_\\timestep, \\action_{\\timestep} \\in \\actionspace} \\bigg\\vert \\discount\\sum_{\\sts_{\\timestep+1} \\in \\statespace} \\transition(\\sts_{\\timestep+1} \\vert \\sts_\\timestep,\\action_\\timestep)\\valuefunction^{\\star}(\\sts_{\\timestep+1})\n             - \\discount \\sum_{\\sts_{\\timestep+1} \\in \\statespace}\\transition(\\sts_{\\timestep+1}\\vert \\sts_\\timestep,\\action_\\timestep)\\hat{\\valuefunction}^{\\mu}(\\hat{f}(\\sts_{\\timestep+1}, \\ais_\\timestep, \\action_\\timestep)) \\bigg\\vert\\\\\n             &+ \\max_{\\history \\in \\historyspace_\\timestep, \\action_{\\timestep} \\in \\actionspace} \\bigg\\vert \\discount\\sum_{\\sts_{\\timestep+1} \\in \\statespace}\\transition(\\sts_{\\timestep+1}\\vert \\sts_\\timestep,\\action_\\timestep)\\hat{\\valuefunction}^{\\mu}(\\hat{f}(\\sts_{\\timestep+1}, \\ais_\\timestep, \\action_\\timestep)) - \\discount\\sum_{\\sts_{\\timestep+1} \\in \\statespace} \\hat{\\transition}(\\sts_{\\timestep+1} \\vert \\ais_\\timestep,\\action_\\timestep)\\hat \\valuefunction^{\\mu}(\\hat{f}(\\sts_{\\timestep+1},\\ais_\\timestep, \\action_\\timestep))\\bigg\\vert\\\\\n            &\\stackrel{(b)}{\\leq} \\epsilon + \\discount \\Vert (\\valuefunction^{\\star}(\\sts_\\timestep) -\\hat{\\valuefunction}^{\\mu}(\\ais_\\timestep))\\Vert_\\infty  +\\discount \\delta \\rho_{\\mathfrak{F}}(\\hat{\\valuefunction}^{\\mu}(\\hat f(\\cdot, \\feature_\\timestep, \\action_\\timestep))),\\\\\n            % &\\leq \\frac{\\epsilon + \\delta_{\\mathfrak{F}}\\rho_{\\mathfrak{F}}(\\hat{\\valuefunction})\\kappa_{\\mathfrak{F}}(\\hat f)}{(1-\\discount)}.\n        \\end{align*}",
        "2211.03011_a632ddc4011400c5e74408c91dbc0b28": "from triangle inequality and",
        "2211.03011_4a2d50fec399d204341797a80319353c": "$(b)$",
        "2211.03011_cc54ea38b33d44f855f5bbf102c4d420": "is due to",
        "2211.03011_0794f1b030ee56a0e25d788c17e6e077": "Now defining",
        "2211.03011_cccb4f5af2e9283cc9996a00a4681c0c": ", and substituting the above result in",
        "2211.03011_8be1ccec7187adf55ad901b04bfc7787": "we get",
        "2211.03011_917fbf142d32d2d5a979d4bef6745cfc": "\\begin{align}\n            \\vert \\valuefunction^{\\star}(\\sts_\\timestep) - \\hat \\valuefunction^{\\mu}(\\feature_\\timestep) \\vert \n            &\\leq \\frac{\\varepsilon + \\discount\\delta \\kappa_{\\mathfrak{F}}(\\hat \\valuefunction^{\\mu}, \\hat{f})}{1 - \\discount}\\label{eq:term1}.\n        \\end{align}",
        "2211.03011_5e62a9a96416862a489627f3c55f61f2": "\\begin{align}\n            \\Vert \\valuefunction_{\\timestep}^{\\policy{}}(\\history_\\timestep) - \\hat \\valuefunction^{\\mu}(\\feature_\\timestep) \\Vert_\\infty\n            &\\leq \\max_{\\history \\in \\historyspace_\\timestep}\\bigg \\vert\\max_{\\action_{\\timestep} \\in \\actionspace}\\bigg[ Q_{\\timestep}^{\\policy{}}(\\history_\\timestep, \\action_\\timestep) - \\hat Q^{\\mu}(\\feature_\\timestep, \\action_\\timestep)\\bigg]\\bigg\\vert,  \\label{eq:vq-diff-2}\n        \\end{align}",
        "2211.03011_2127163ede71545238ad6a2652a9256e": "\\begin{align*}\n           \\max_{\\history \\in \\historyspace_\\timestep}\\bigg \\vert\\max_{\\action_{\\timestep} \\in \\actionspace}\\bigg[ Q^{\\policy{}}(\\history_\\timestep, \\action_\\timestep) - \\hat Q^{\\mu}(\\feature_\\timestep, \\action_\\timestep)\\bigg]\\bigg\\vert\n            &=\\max_{\\history \\in \\historyspace_\\timestep}\\bigg \\vert \\max_{\\action_{\\timestep} \\in \\actionspace}\\bigg[\\cost(\\sts_{\\timestep},\\action_\\timestep) + \\discount\\sum_{\\sts_{\\timestep+1} \\in \\statespace} \\transition(\\sts_{\\timestep+1} \\vert \\sts_\\timestep,\\action_\\timestep)\\valuefunction_{\\timestep+1}^{\\policy{}}(\\history_{\\timestep+1}) \\\\\n            &-  \\hat\\cost(\\ais_\\timestep,\\action_\\timestep) - \\discount\\sum_{\\sts_{\\timestep+1} \\in \\statespace} \\hat{\\transition}(\\sts_{\\timestep+1} \\vert \\ais_\\timestep,\\action_\\timestep)\\hat \\valuefunction^{\\mu}(\\hat{f}(\\sts_{\\timestep+1},\\ais_\\timestep, \\action_\\timestep))\\bigg]\\bigg\\vert\\\\\n            &\\stackrel{(a)}{\\leq} \\epsilon + \\max_{\\history \\in \\historyspace_\\timestep, \\action_{\\timestep} \\in \\actionspace} \\bigg\\vert \\discount\\sum_{\\sts_{\\timestep+1} \\in \\statespace} \\transition(\\sts_{\\timestep+1} \\vert \\sts_\\timestep,\\action_\\timestep)\\valuefunction_{\\timestep+1}^{\\policy{}}(\\history_{\\timestep+1})\n            - \\discount \\sum_{\\sts_{\\timestep+1} \\in \\statespace}\\transition(\\sts_{\\timestep+1}\\vert \\sts_\\timestep,\\action_\\timestep)\\hat{\\valuefunction}^{\\mu}(\\hat{f}(\\sts_{\\timestep+1}, \\ais_\\timestep, \\action_\\timestep)) \\bigg\\vert\\\\\n            &+ \\max_{\\history \\in \\historyspace_\\timestep, \\action_{\\timestep} \\in \\actionspace} \\bigg\\vert \\discount\\sum_{\\sts_{\\timestep+1} \\in \\statespace}\\transition(\\sts_{\\timestep+1}\\vert \\sts_\\timestep,\\action_\\timestep)\\hat{\\valuefunction}^{\\mu}(\\hat{f}(\\sts_{\\timestep+1}, \\ais_\\timestep, \\action_\\timestep))\n            - \\discount\\sum_{\\sts_{\\timestep+1} \\in \\statespace} \\hat{\\transition}(\\sts_{\\timestep+1} \\vert \\ais_\\timestep,\\action_\\timestep)\\hat \\valuefunction^{\\mu}(\\hat{f}(\\sts_{\\timestep+1},\\ais_\\timestep, \\action_\\timestep))\\bigg\\vert\\\\\n            &\\stackrel{(b)}{\\leq} \\epsilon + \\discount \\Vert (\\valuefunction^{\\policy{}}(\\history_\\timestep) -\\hat{\\valuefunction}^{\\mu}(\\ais_\\timestep))\\Vert_\\infty  +\\discount \\delta \\rho_{\\mathfrak{F}}(\\hat{\\valuefunction}^{\\mu}(\\hat f(\\cdot, \\feature_\\timestep, \\action_\\timestep))),\\\\\n            % &\\leq \\frac{\\epsilon + \\delta_{\\mathfrak{F}}\\rho_{\\mathfrak{F}}(\\hat{\\valuefunction})\\kappa_{\\mathfrak{F}}(\\hat f)}{(1-\\discount)}.\n        \\end{align*}",
        "2211.03011_f417149572845e43e4f79d520f06c8da": "is from triangle inequality,",
        "2211.03011_1ba69d9d958607b4ee081f8e3b2ce120": "\\begin{align}\n            \\Vert \\valuefunction^{\\policy{}}_{\\timestep}(\\history_\\timestep) - \\hat \\valuefunction^{\\mu}(\\feature_\\timestep) \\Vert_\\infty\n            &\\leq \\frac{\\varepsilon + \\discount\\delta \\kappa_{\\mathfrak{F}}(\\hat \\valuefunction^{\\mu}, \\hat{f})}{1 - \\discount}\\label{eq:term2}.\n        \\end{align}",
        "2211.03011_60d9fae38ddd03e1dc50c17e2126d079": "The final result then follows by adding",
        "2211.03011_d06f4cba5a050035d0e49f79bd6a4373": "Proof for Corollary",
        "2211.03011_535e171f7ba172f8a61e7c49b7fc09eb": "$\\hat\\valuefunction$",
        "2211.03011_d9157b4d7ce70264082aaedb867b47c7": "is the optimal value function of the MDP",
        "2211.03011_72377cfd1f4bf9962dfa32598e511760": "$\\hat{\\mdp}$",
        "2211.03011_d477221d971f5913ffd804481f1d4650": "induced by the process",
        "2211.03011_df274248663c0fb8accd88d6deefea3d": "$\\{\\Feature_{\\timestep}\\}_{\\timestep \\geq 0}$",
        "2211.03011_8d1b81a8829d34955240667c7c273343": "\\begin{align}\n            \\spn(\\hat\\valuefunction) &\\leq \\frac{\\spn(\\hat \\cost)}{1 - \\discount}.\n        \\end{align}",
        "2211.03011_9ca358ff8623057bedd517e82def03d0": "The result follows by observing that the per-step reward",
        "2211.03011_a776543a0ba8830f8396028ce9d1cec5": "$ \\hat \\cost  (\\Feature_\\timestep, \\Action_\\timestep) \\in [\\min(\\hat \\cost), \\max(\\hat \\cost) ]$",
        "2211.03011_1f4771f80ff91af97f4b61509e0b71fc": "Therefore",
        "2211.03011_9389fb3b9f82f3662a1d76a8ee13e38e": "$\\max(\\hat\\valuefunction) \\leq \\max(\\hat \\cost)$",
        "2211.03011_19657811e726423a6546aafcc3d4a003": "$\\min(\\hat\\valuefunction) \\geq \\min(\\hat \\cost)$",
        "2211.03011_322f116eaf49326fec6cfb1b744a8d97": "defined in",
        "2211.03011_afe34d829e99c5612f9535877c8061f7": "\\begin{align}\n             \\Delta\n                \\le  \\frac{2\\epsilon}{1-\\discount} +\\frac{\\discount\\delta \\spn(\\hat\\cost)} {(1-\\discount)^2},\n            \\end{align}",
        "2211.03011_73d6dfd5521beeb40acf2ed2d8cfd657": "we know that for the Total variation distance",
        "2211.03011_f8b1019adb06bd0308f04cc422936ad7": "$\\rho_{\\mathfrak{F}^{\\text{TV}}}(\\hat\\valuefunction)$",
        "2211.03011_5f121f43705317418093b20f6f70ad5d": "$\\spn(\\hat\\valuefunction)$",
        "2211.03011_fa5e591ea54c997568c33b91a7b9aca3": "$\\kappa(\\hat f) = 1$",
        "2211.03011_2cdc4cee7115b69f95816a6a0457b31e": "The result in the corollary then follows from",
        "2211.03011_9eb407e0bb4332a376f6462b13143b54": "For any Lipschitz function",
        "2211.03011_96db65ac527a43b8dd98ec382395b9bb": "$f:(\\aisspace, d_{\\Ais}) \\to (\\real, \\vert \\cdot \\vert)$",
        "2211.03011_785136ac1f4a524af0413c343cf2d72b": ", and probability measures",
        "2211.03011_756b2946392459582fdadf766f3917d9": "$\\nu_1$",
        "2211.03011_9473daf474f8d39427c1a99ddb5ad626": "$\\nu_2$",
        "2211.03011_ed2b5c0139cec8ad2873829dc1117d50": "on",
        "2211.03011_2774fcd9e3cede89bcc58ae24bbddc24": "$(\\aisspace, d_{\\Ais})$",
        "2211.03011_f4fec29c3f53f19271431829c87b8e48": "\\begin{align}\n            \\bigg\\vert \\int_{\\aisspace} f d\\nu_1 - \\int_{\\aisspace} f d\\nu_2 \\bigg\\vert \\leq \\Vert f \\Vert_{L}. d_{\\mathfrak{F}^{\\text{W}}}(\\nu_1, \\nu_2) \\leq L_{f}d_{\\mathfrak{F}^{\\text{W}}}(\\nu_1, \\nu_2),\n        \\end{align}",
        "2211.03011_21838ca06931953c741fa06e65b2603e": "is the Lipschitz constant of",
        "2211.03011_665dbf1aed958225aa4be826cb288929": "$d_{\\mathfrak{F}^{\\text{W}}}$",
        "2211.03011_ba4734a904c72edc63fa9e087049df39": "is the Wasserstein distance.",
        "2211.03011_2103f85b8b1477f430fc407cad462224": "$d$",
        "2211.03011_c7286d1545e35f3149bf98e412db3a81": "be a metric on the AIS/Feature space",
        "2211.03011_bc40ad02d94f280570b850a7d2c747af": "The MDP",
        "2211.03011_bc710412ced8008611147422051dfe09": "is said to be",
        "2211.03011_4226060c0be497823941c78ce89e7a1e": "$(L_{\\hat\\cost}, L_{\\hat\\transition})$",
        "2211.03011_e76736666c67e09d3e94dee124f899d8": "- Lipschitz if for any",
        "2211.03011_3327eb1a8382684f0142b74c454f15a1": "$\\Feature_1, \\Feature_2 \\in \\aisspace$",
        "2211.03011_4fdeb1aa7e16d10fa329e9c1c2df7959": ", the reward",
        "2211.03011_ca7d99c583fefeafbce43908630d97d1": "$\\hat{\\cost}$",
        "2211.03011_75b7aebe6541ffc796947a9bbbb20a04": "and transition",
        "2211.03011_b062453f99d105dbc2cf476a2c3234d3": "$\\hat{\\transition}$",
        "2211.03011_2e5fa8665c8f41f8dcb908d68479cffd": "satisfy the following:",
        "2211.03011_73f0dd8537e3406985885378763bed3c": "\\begin{align}\n            \\bigg\\vert \\cost(\\Feature_1, \\Action) - \\cost(\\Feature_2, \\Action)\\bigg\\vert &\\leq L_{\\hat{\\cost}} d(\\Feature_1, \\Feature_2)\\\\\n            d_{\\mathfrak{F}^{\\text{W}}}(\\hat\\transition(\\cdot \\vert\\Feature_1,\\Action), \\hat\\transition(\\cdot\\vert\\Feature_2,\\Action) &\\leq L_{\\hat \\transition} d(\\Feature_1, \\Feature_2),\n        \\end{align}",
        "2211.03011_e2c809a5dfbb46d6dfd8195988dafd19": "is the Wasserstein or the Kantorovitch-Rubinstein distance.",
        "2211.03011_19f839dd2c511a907838e83605ac2830": "$\\hat \\valuefunction: \\aisspace \\to \\real$",
        "2211.03011_910955a907e739b81ec8855763108a29": "be",
        "2211.03011_19726c09036d56e620f92d8c5d4a5c03": "$L_{\\hat \\valuefunction}$",
        "2211.03011_d2884efee751eb4746b685827c7efcd9": "continuous",
        "2211.03011_222a78407bccdd31b5fa64b9e2c3bfaa": "Define:",
        "2211.03011_e9491f4ea013494b1c8504e94b4cc312": "\\begin{align*}\n            \\hat Q(\\ais,\\action) &= \\hat \\cost(\\ais, \\action) + \\discount\\sum_{\\sts'}\\hat\\transition(\n            \\sts'\\vert \\ais, \\action)\\hat \\valuefunction(\\hat f(\\sts', \\ais, \\action).\n        \\end{align*}",
        "2211.03011_d48a5cda4bc91670bf431856c381ab40": "Then",
        "2211.03011_079515917e0aae2a9b57ebaaf669c55e": "$\\hat Q$",
        "2211.03011_cd73039b4a54dbac1e114f0e78d7cc42": "$(L_{\\hat \\cost} + \\discount L_{\\hat \\valuefunction} L_{\\hat f} L_{\\hat \\transition})$",
        "2211.03011_87d5e6b6727c94fca50784aba9bedc57": "-Lipschitz continuous.",
        "2211.03011_cab6405b9ab9f41bb935f2b67e3e8aaf": "For any action",
        "2211.03011_5a63dc89e829a9d831818a98112de659": "$\\action$",
        "2211.03011_62232326af84bca8d3da9ec411cc326d": "\\begin{align}\n          \\bigg \\vert \\hat Q(\\ais_1, \\action) -\\hat Q(\\ais_2, \\action) \\bigg\\vert &\\stackrel{(a)}{\\leq} \\bigg\\vert \\hat\\cost(\\ais_1, \\action) - \\hat\\cost(\\ais_2, \\action) \\bigg\\vert + \\discount \\bigg\\vert \\sum_{\\sts'}\\hat \\transition(\\sts'\\vert \\ais_1, \\action) \\hat \\valuefunction(\\hat f(\\sts',\\ais_1, \\action)) - \\hat \\transition(\\sts' \\vert \\ais_2, \\action) \\hat \\valuefunction(\\hat f(\\sts',\\ais_2, \\action)) \\bigg\\vert \\\\\n          &\\stackrel{(b)}{\\leq} (L_{\\hat\\cost} + \\discount L_{\\hat \\valuefunction} L_{\\hat f} L_{\\hat \\transition}) d(\\ais_1, \\ais_2),\n        \\end{align}",
        "2211.03011_1c38f6b4805c23deab2a606a3905b5af": "due to triangle inequality, and",
        "2211.03011_66da95bbed3bacf0d79d5ba7abd41b83": "follows form",
        "2211.03011_7538e62486f5bd78e6744299137b0a01": ", and because",
        "2211.03011_c4cf758247d80a1f0f5c9bb6287130bd": "$\\Vert a \\circ b \\Vert_{L} \\leq \\Vert a\\Vert_{L} \\cdot \\Vert b \\Vert_{L}$",
        "2211.03011_36199d5c7ab66f759ed0835fd48514ad": "$\\hat Q: \\aisspace \\times \\actionspace \\to \\real$",
        "2211.03011_eeae1fd2d897ab2c80b592627bf73d56": "$L_{\\hat Q}$",
        "2211.03011_9d75e6bb163a38ad36645c0ad2f8c46c": "- Lipschitz continuous, Define",
        "2211.03011_1edc5ddb94cdf2a35f2fa3d0d14c3a98": "\\begin{align*}\n         \\hat \\valuefunction(\\ais) = \\max_{\\action_{\\timestep} \\in \\actionspace}\\hat Q(\\ais, \\action).\n     \\end{align*}",
        "2211.03011_a73d04b97e44d5f99381502779d5d14a": "$\\hat \\valuefunction$",
        "2211.03011_d8e85609d4175e96b4f783e2966de64b": "Lipschitz",
        "2211.03011_9bfb93aa9c186a1ee365e92efd7ccb7b": "Consider",
        "2211.03011_b82a44792a379a54952262c02663dd87": "$\\ais_1, \\ais_2 \\in \\aisspace$",
        "2211.03011_f1d590bd09cec61cd52a07d575f88581": ", and let",
        "2211.03011_6ae9887b847037e7b8cf82e28a76dce8": "$\\action_1$",
        "2211.03011_3f219d02f044cec6f68795b333434b0f": "$\\action_2$",
        "2211.03011_0e85b52364bd436e47a9e7e642826958": "denote the corresponding optimal action",
        "2211.03011_56bd379819b99029b1506ebac27e89e6": "Then,",
        "2211.03011_c060e9cc3d17a8a99831cb917e9d33bf": "\\begin{align}\n            \\hat \\valuefunction(\\ais_1) - \\hat\\valuefunction(\\ais_2) &= \\hat Q(\\ais_1, \\action_1) - \\hat Q(\\ais_2, \\action_2)\\\\\n            &\\stackrel{(a)}{\\leq} \\hat Q(\\ais_1, \\action_2) - \\hat Q(\\ais_2, \\action_2)\\\\\n            &\\stackrel{(b)}{\\leq}L_{\\hat Q}d(\\ais_1, \\ais_2),\n        \\end{align}",
        "2211.03011_87d068f811c538542044bcb811fe23c1": "By symmetry,",
        "2211.03011_f2621e3e1ae8221fd46c19b35e3a97cf": "\\begin{align*}\n             \\hat\\valuefunction(\\ais_2) - \\hat \\valuefunction(\\ais_1) &\\leq L_{\\hat Q}d(\\ais_1, \\ais_2).\n        \\end{align*}",
        "2211.03011_4612f9429bae7c8ccf75aa3f14ca550e": "Therefore,",
        "2211.03011_e41753014dc17e96078f6eee90544601": "\\begin{align*}\n            \\bigg \\vert  \\hat \\valuefunction(\\ais_1) - \\hat\\valuefunction(\\ais_2) \\bigg\\vert &\\leq L_{\\hat Q}d(\\ais_1, \\ais_2).\n        \\end{align*}",
        "2211.03011_0fcdfbcc37c8392c2fb8861040c776d7": "Consider the following dynamic program defined in",
        "2211.03011_10e1c37636645264eba147561b05ddbd": "\\begin{align*}\n                \\hat Q_\\timestep(\\ais_\\timestep, \\action_\\timestep) &= \\hat \\cost(\\ais_\\timestep, \\action_\\timestep) + \\discount \\sum_{\\sts_\\timestep \\in \\statespace}\n                \\hat \\transition(\\sts_\\timestep|\\ais_\\timestep,\\action_\\timestep) \\hat \\valuefunction(\\hat{f}(\\ais_\\timestep,\\sts_\\timestep,\\action_\\timestep)), \\ \\forall \\feature \\in \\featurespace, \\action \\in \\actionspace \\\\\n                \\hat \\valuefunction_\\timestep(\\ais_\\timestep) &= \\max_{\\action \\in \\actionspace} \\hat Q_\\timestep(\\ais_\\timestep,\\action_\\timestep), \\ \\forall \\feature \\in \\featurespace \\label{eq:ais-dp-2}\n            \\end{align*}",
        "2211.03011_f9b5ac549cc7f3b93750bc3af617155c": "Then at any time",
        "2211.03011_0f4ad8ed6f8abf5056019865b8b13f87": ", we have:",
        "2211.03011_9ab6fe894c4a852325e0dc713134226e": "\\begin{align*}\n            L_{\\hat \\valuefunction_{\\timestep+1}} & = L_{\\hat\\cost} + \\discount L_{\\hat \\transition}L_{\\hat f}L_{\\hat \\valuefunction_\\timestep}.\n        \\end{align*}",
        "2211.03011_7b799b102f230848294be8524e5fb7ff": "We prove this by induction",
        "2211.03011_56d564d0ed00a23301345d4636534f86": "At time",
        "2211.03011_b2a662b51de89196af0915bffb787b77": "$\\timestep =1$",
        "2211.03011_4d2fc01b05a2b35fb61dfb81433a527a": "$\\hat Q_{1}(\\ais, \\action) = \\hat \\cost(\\ais, \\action)$",
        "2211.03011_3ccbdd710d24730fe06c06afb1e80a5e": ", therefore",
        "2211.03011_39666dbd4111abcd9b0dd21c50ee996e": "$ L_{\\hat Q_{1}} = L_{\\hat \\cost}$",
        "2211.03011_669ffff679790ef8ff6733609831cecf": "Then according to",
        "2211.03011_72f1e97389e53cdefbb77e0e970b9b0e": "$\\hat \\valuefunction_1$",
        "2211.03011_458868aedfce22e929b9f3c3efbf2016": "is Lipschitz with Lipschitz constant",
        "2211.03011_699abb9dede050251eea809f484be566": "$L_{\\hat \\valuefunction_1} = L_{\\hat Q_1} = L_{\\hat\\cost}$",
        "2211.03011_bd0d15ad85aae0936df7d6aa65d88602": "This forms the basis of induction",
        "2211.03011_32993d595757c83f9f92b5eb9d3d93ed": "Now assume that at time",
        "2211.03011_be57d2c97b32d70f1d50ce19bfb83248": "$\\hat \\valuefunction_{\\timestep}$",
        "2211.03011_f8766d08ecc308753a297be83eebbaea": "$L_{\\hat \\valuefunction_\\timestep}$",
        "2211.03011_8c8304bf1f58fa8c3edbb8efecdbfa61": "- Lipschitz",
        "2211.03011_53e5aa2c97fef1555d2511de8218c544": "By",
        "2211.03011_d2697472a1735c3a68f7a74862289136": "$\\hat Q_{\\timestep+1}$",
        "2211.03011_f262374752801d7684eff221bc876f3b": "$L_{\\hat \\cost} + \\discount L_{\\hat f}, L_{\\hat \\transition} L_{\\hat \\valuefunction_{\\timestep}}$",
        "2211.03011_47d4acab76b606a7670544a5bf36cb9e": "Therefore by",
        "2211.03011_051bcd9071057802751df32ca8b158d9": "$\\hat \\valuefunction_{(\\timestep+1)}$",
        "2211.03011_452b2fa0bc5d0a706984e3cc902586fa": "is Lipschitz with constant:",
        "2211.03011_19319e324278cf6dd5d5aa0b22c09f85": "\\begin{align*}\n            L_{\\hat\\valuefunction_{\\timestep+1}} &= L_{\\hat \\cost} + \\discount L_{\\hat f}L_{\\hat \\transition}L_{\\hat \\valuefunction_\\timestep}.\n        \\end{align*}",
        "2211.03011_adbcde67e9f9127b23cefe6312e2e547": "Given any",
        "2211.03011_7f2e5932b931b595b5fea90cc037049c": "- Lipschitz MDP, if",
        "2211.03011_b3be42135b8f3315876e267fee9e28a8": "$\\discount L_{\n        \\hat \\transition}L_{\\hat f} \\leq 1$",
        "2211.03011_e9787690d9de6e4470974bcf7834932d": ", then the infinite horizon",
        "2211.03011_7bcecb93918e70645ea180ce68fbcd3b": "$\\discount$",
        "2211.03011_77cc9437985e6e190575d1d1e55a51ff": "-discounted value function",
        "2211.03011_b5210c80c2d00c6720fe76e9d96d1df6": "is Lipschitz continuous with Lipschitz constant",
        "2211.03011_ff881d4e7aeac92ee7fbbdd1325a3837": "\\begin{align*}\n            L_{\\hat \\valuefunction} &= \\frac{L_{\\hat \\cost}}{1 - \\discount L_{\\hat f}L_{\\hat \\transition}}.\n        \\end{align*}",
        "2211.03011_603812bcd528b93b0b18a12effbcf215": "Consider the sequence of",
        "2211.03011_d4814ddb1b4a0cd34f60beecc3aa458b": "$L_{\\timestep} = L_{\\hat \\valuefunction_{\\timestep}}$",
        "2211.03011_f09cc7ee3a9a93273f4b80601cafb00c": "values",
        "2211.03011_8d4abc8d8b1e0f573e147378885fe62f": "For simplicity write",
        "2211.03011_1efe4068b7e779165362586541a26c62": "$\\alpha = \\discount L_{\\hat \\transition}L_{\\hat f}$",
        "2211.03011_41f5e83f1d7ec2ffdaf23ece684f8924": "Then the sequence",
        "2211.03011_a9c40b8b3e5fa771d788bedacd5f54a8": "$\\{L_{\n        \\timestep}\\}_{\\timestep \\geq 1}$",
        "2211.03011_00b88f1f58d22e9d8bb3f383e5073fe2": "is given by :",
        "2211.03011_b3479a7f04bb1e7aabbb93a1ca03029d": "$L_1 = L_{\\hat \\cost}$",
        "2211.03011_13a40798b93cc7ee8b6fc0d3422289bc": "and for",
        "2211.03011_2088824e0896f5c317c759f396cfc290": "$\\timestep \\geq 1$",
        "2211.03011_25273871ddc86384d21392e2e356cc69": "\\begin{align*}\n            L_{\\timestep+1} &= L_{\\hat \\cost} + \\alpha L_{\\timestep}, \\\\\n            \\text{Therefore,} \\\\\n            L_{\\timestep} &= L_{\\hat \\cost} + \\alpha L_{\\hat \\cost}+ \\ldots + \\alpha_{\\timestep+1} = \\frac{1 - \\alpha^{\\timestep}}{1 - \\alpha}L_{\\hat\\cost}.\n        \\end{align*}",
        "2211.03011_f07195ce9b03d6b8d6656f224bd0d982": "This sequence converges if",
        "2211.03011_1013056b24ebf89b67337481ec6d9dae": "$\\vert \\alpha \\vert \\leq 1$",
        "2211.03011_c745b9b57c145ec5577b82542b2df546": "$\\alpha$",
        "2211.03011_f2d5c59e104aa3f90a07fbbe0d6db0d8": "is non-negative, this is equivalent to",
        "2211.03011_d302578609975743a904e9b003ae41e3": "$\\alpha\\leq 1$",
        "2211.03011_1f5163c012fd300f41e2cab925f3b91d": ", which is true by hypothesis",
        "2211.03011_1a62f6d03b6f8061a7ef1e5bc9a9c32b": "Hence",
        "2211.03011_76aaa888420eb794a937310f7be7780e": "$L_\\timestep$",
        "2211.03011_1adb0ba246f4d947ee41ea97cfbf7340": "is a convergent sequence",
        "2211.03011_3234e75097003865f3857e41bc9abf15": "At convergence, the limit",
        "2211.03011_0d6a063c0241a5e25a32cfbba87f4a1b": "must satisfy the fixed point of the recursion relationship introduced in",
        "2211.03011_63ab4bc006c5f69cd6a3a5980ba7a043": ", hence,",
        "2211.03011_ab71e51680d9edd6b0fcfc846a9606ca": "\\begin{align*}\n            L_{\\hat \\valuefunction}&= L_{\\hat\\cost} + \\discount L_{\\hat f}L_{\\hat \\transition}L_{\\hat \\valuefunction}.\n        \\end{align*}",
        "2211.03011_e3ae5a58c0fa42904473ed0cd53f09e3": "Consequently, the limit is equal to,",
        "2211.03011_4bd358bddfff89932cadc46b3d11fba6": "\\begin{align*}\n            L_{\\hat \\valuefunction} = \\frac{L_{\\hat \\cost}}{1 - \\discount L_{\\hat f}L_{\\hat \\transition}}.\n        \\end{align*}",
        "2211.03011_18a9755b0b7eef24c25a587dc5625221": "\\begin{align}\n               \\Delta\n                \\le  \\frac{2\\epsilon}{(1-\\discount)} + \\frac{2\\discount\\delta L_{\\hat \\cost} }{(1- \\discount)(1-\\discount L_{\\hat f}L_{\\hat\\transition})},\n            \\end{align}",
        "2211.03011_e23b5c8ada47eefc23ca517db43c5b8c": "The proof follows from the observation that for",
        "2211.03011_137af169c322965991c135c29b0371c4": "$\\rho_{\\mathfrak{F}^{\\text{W}}}$",
        "2211.03011_c3ba632cc26c78095cb444b6879a43aa": ", and then using the result from",
        "2211.03011_3fb2719f6516f6410d50804082467a97": "Algorithmic Details",
        "2211.03011_7f55c49228839915b376896902afe230": "Choice of an IPM:",
        "2211.03011_d026936df1996241d1cda58970b2a242": "MMD",
        "2211.03011_5bd6f68f7260a41ae481185f27b1a7da": "One advantage of choosing",
        "2211.03011_48ea2f23d8b3e4714b51feda8aa9f9b3": "as the MMD distance is that unlike the Wasserstein distance, its computation does not require solving an optimisation problem",
        "2211.03011_164c12257f29dee73cf9dd553d4afb8b": "Another advantage is that we can leverage some of their properties to further simplify our computation, as follows:",
        "2211.03011_085b7f84e37d0ce1f2729b2af96d9f25": "$\\mathcal{X} \\subseteq \\real^{m}$",
        "2211.03011_418c2ed6f2e77230b72281894b202318": "$d_{\\mathcal{X},p}: \\mathcal{X}\\times \\mathcal{X} \\to \\real$",
        "2211.03011_05a81dbc81b020811e13babb0767785d": "be a metric given by",
        "2211.03011_e2820bc65a9849d9fbbd69047177475c": "$d_{\\mathcal{X},p}(x,x') = \\Vert x - x'\\Vert^{p}_{2}$",
        "2211.03011_7e762fbe4c626a825b191c3ef617dca0": ", for",
        "2211.03011_289c3eb783f6bb127d6be12d328537eb": "$p \\in (0,2]$",
        "2211.03011_88f81460e3ff4946fd661ed481e529cc": "$k_p :\\mathcal{X} \\times \\mathcal{X} \\to \\real$",
        "2211.03011_0f0bce3fae33d68b11fe1601ee8d9372": "be any kernel given:",
        "2211.03011_55c74e383b8a0cacf47508265823cdaf": "\\begin{align}\n                    k_p (x,x') &= \\frac{1}{2}(d_{\\mathcal{X},p}(x,x_0) + d_{\\mathcal{X},p}(x',x_0) - d_{\\mathcal{X},p}(x,x')),\n                \\end{align}",
        "2211.03011_76329327e943ed2b24816e56c23f107d": "$x_0 \\in \\mathcal{X}$",
        "2211.03011_43238f406a7cb5a1a90d78abc6bd1f0f": "is arbitrary, and let",
        "2211.03011_24224b0661e579850080ee3c9b7afcfd": "$\\mathcal{U}_p$",
        "2211.03011_b6262e2cd6967f60b90a5b1c905917cd": "be a RKHS kernel with kernel",
        "2211.03011_b19efe18c84e5887c52c1c0fd15160eb": "$k_p$",
        "2211.03011_af39428cf24465e389a9341f85f7f6a8": "$\\mathfrak{F}_p = \\{f \\in \\mathcal{U}_p : \\Vert f \\Vert_{\\mathcal{U}_p} \\geq 1  \\}$",
        "2211.03011_911990acc31d329408eb7920ae7d53fc": "Then for any distributions",
        "2211.03011_2765bb4d5d327c70b5805609ae608a2c": "$\\nu_2 \\in \\Delta{\\mathcal{X}}$",
        "2211.03011_96149022da269cd1a90bef84ee328b25": ", the IPM can be expressed as:",
        "2211.03011_c209a5e9d81a0b7cb7d9652bfd828fb5": "\\begin{align}\n                    \\ipm(\\nu_1,\\nu_2) &= \\bigg(\\expecun{}[d_{\\mathcal{X},p}(X_1, W_1)] - \\frac{1}{2}\\expecun{}[d_{\\mathcal{X},p}(X_1, X_2)] - \\frac{1}{2}\\expecun{}[d_{\\mathcal{X},p}(W_1, W_2)]\\bigg)^{\\frac{1}{2}},\\label{eq:prop-ipm1}\n                \\end{align}",
        "2211.03011_56f60b6e6263029f30aef0f18ff8aeb1": "$X_1,X_2$",
        "2211.03011_28ebe2b1df526c25df0a200b80fe626e": "$W_1,W_2$",
        "2211.03011_7e9a94dd421e5c141274db572204c3ae": "are i.i.d. samples from",
        "2211.03011_6e64174cc720cf11e2c6a1315b8cbf18": "respectively.",
        "2211.03011_01fd231593eeaeb8b947f512e7cdab6a": "The main implication of",
        "2211.03011_8ca0bd77c6ab0a0516cb4056698359dc": "is that, instead of using",
        "2211.03011_02f15ff26a5b8283fc9f5e6a2300572d": "$p\\in (0,2]$",
        "2211.03011_a9edd8d411ea0df4213c21fc0e242c0c": "we can use the following as a surrogate for",
        "2211.03011_532b8955f7633ab752fb5da43aac3472": "$d_{\\mathfrak{F}_p}$",
        "2211.03011_a1a7e3759bcaee97cf59d51d5de8cd6b": "\\begin{align}\n                     \\int_\\mathcal{X}\\int_\\mathcal{X}\\Vert x_1 - w_1 \\Vert_{2}^{p}\\nu_1(dx_1)\\nu_2(dw_1) - \\frac{1}{2}\\int_\\mathcal{X}\\int_\\mathcal{X}\\Vert w_1 - w_2 \\Vert_{2}^{p}\\nu_{2}(dw_1)\\nu_{2}(dw_1). \\label{eq:ipm-surrogate}\n                \\end{align}",
        "2211.03011_f298e81134db96e2537df2a820a98dd1": "Moreover, according to",
        "2211.03011_42204961f266ec22fbbeb497a0a0f188": "\\citet{Sriperumbudur}",
        "2211.03011_6a68c16bcf90c80245ebcebfa8e1f40e": "for n identically and independently distributed (i.i.d) samples",
        "2211.03011_22cff9efeba6e6ba92a891244a23c066": "$\\{X_i\\}_{i=0}^{n} \\sim \\nu_1$",
        "2211.03011_17681253939a23b1833914f8bd65bbc1": "an unbiased estimator of",
        "2211.03011_7021bf33e44a17601d724c06e342c595": "is given as:",
        "2211.03011_b00ae41c141b44d1d2639d06d8911a5d": "\\begin{align}\n                    \\frac{1}{n}\\sum_{i=1}^{n}\\int_\\mathcal{X}\\Vert X_i -w_1\\Vert_{2}^{p}\\nu_1 d(w_1)  - \\frac{1}{2}\\int_\\mathcal{X}\\int_\\mathcal{X}\\Vert w_1 - w_2 \\Vert_{2}^{p}\\nu_1(dw_1)\\nu_2(dw_2). \\label{eq:ipm-surrogate-2}\n                \\end{align}",
        "2211.03011_86d21ca84519a4386b3cd927084a246d": "We implement a simplified version of the surrogate loss in",
        "2211.03011_5e0000888d0698281eeec2a2c549df21": "Given the setup in",
        "2211.03011_90264925fb137831c8f410cd14c75cff": "$p=2$",
        "2211.03011_94e3944c54537cc1ca609a603ce1fb08": ", Let",
        "2211.03011_63aeb650a7c5ec0f0aa37f4150f40ce6": "$\\nu_2(\\aisparams)$",
        "2211.03011_60580bbbc3dfc30f57283aa0f16cabd5": "be a parametric distribution with mean",
        "2211.03011_0e51a2dede42189d77627c4d742822c3": "$m$",
        "2211.03011_17fc4fc8d2910ca193ac5f142651a09c": "and let",
        "2211.03011_92305326663866c39d8dd15c5dadda22": "$X \\sim \\nu_1$",
        "2211.03011_2e38f0e620cb90ab52a194e06693b29b": ", then the gradient",
        "2211.03011_8dcf91ec0259e6d92e274d16129c40f2": "$\\grad_\\aisparams (m_\\aisparams - 2X)^{\\top}m_\\aisparams$",
        "2211.03011_0860e9fe49979d1d14b12f0769049814": "is an unbiased estimator of",
        "2211.03011_ea7ec500a22dca4543fec2fa09ce8d8d": "$\\grad_\\aisparams d_{\\mathfrak{F}_{2}}(\\alpha, \\nu_\\aisparams)^{2}$",
        "2211.03011_b9ec59ef719f08cd6a8c50592284c623": "$X_1,X_2 \\sim \\nu_1$",
        "2211.03011_2fb8adcc63192871e19bcd9880a553cc": "$W_1,W_2 \\sim \\nu_2(\\aisparams)$",
        "2211.03011_5e77827bbab4835edfc1e4ab5218f6ce": "\\begin{align}\n                    \\therefore \\grad_{\\aisparams} d_{\\mathfrak{F}_2}(\\nu_1,\\nu_2(\\aisparams))^2 &= \\grad_{\\aisparams} \\bigg[\\expecun{}\\Vert X_1 - W_1 \\Vert_{2}^{2} - \\frac{1}{2}\\expecun{}\\Vert X_1 - X_2 \\Vert_{2}^{2} - \\frac{1}{2}\\expecun{}\\Vert W_1 - W_2 \\Vert_{2}^{2}\\bigg ]\\\\\n                    &\\stackrel{(a)}{=}\\grad_\\aisparams \\bigg[ \\expecun{}\\Vert W_1\\Vert_{2}^{2} - 2\\expecun{}\\Vert X_1\\Vert^{\\top}\\expecun{}\\Vert W_1 \\Vert \\bigg]\\label{eq:mmd-grad}, \n                \\end{align}",
        "2211.03011_6506476a422da8ee0f4b6b805e84021e": "follows from the fact that",
        "2211.03011_cbfb1b2a33b28eab8a3e59464768e810": "$X$",
        "2211.03011_7f8091df5462faa7a88c312b2f8bdbf5": "does not depend on",
        "2211.03011_174d0b6d6cda5454b4508f9faf4e709e": ", which simplifies the implementation of the MMD distance.",
        "2211.03011_698dc5b3d8e55cff953b65956038b372": "In this way we can simplify the computation of",
        "2211.03011_452930616271c680f24ada7e65cabad8": "using a parametric stochastic kernel approximator and MMD metric",
        "2211.03011_6787dbdc5cbd959f828ea4a0ce2ac2d8": "Note that when are trying to approximate a continuous distribution we can readily use the loss function",
        "2211.03011_3f9f07a1c939260f63f92713d628e9fd": "as long as the mean",
        "2211.03011_f6c0cc6f187210373d69a85765e50487": "is given in closed form",
        "2211.03011_f90c570aeadf7e5563db23507e9f9b3a": "The AIS loss is then given as:",
        "2211.03011_bca01f0a544f09b42c40cb0a6992c322": "\\begin{align}\n                 \\aisloss(\\aisparams) &= \\frac{1}{\\Timestep}\\sum_{t = 0}^{\\Timestep}\\bigg( \\lambda (f_{\\hat{\\cost}}(\\Ais_{\\timestep}, \\Action_\\timestep; \\aisparams)  - \\cost(\\State_\\timestep, \\Action_\\timestep))^{2} + (1-\\lambda)(m^{\\State_\\timestep}_{\\aisparams} - 2\\State_\\timestep)^{\\top}m^{\\State_\\timestep}_{\\aisparams}  \\bigg),\\label{eq:mmd-ais-loss-2}\n            \\end{align}",
        "2211.03011_5e96330591cb970b741f4c83d4fc9d4f": "$f_{\\hat\\transition}(\\aisparams): \\aisspace \\times \\actionspace \\to \\real$",
        "2211.03011_82fd2ac9221b670412337f84a176d13f": "Wasserstein Distance",
        "2211.03011_e928200d7fc522f1ba51fc57ba2e91cb": "The the KL-divergence between two densities",
        "2211.03011_8fff84b6aaf52c89c49fda6ae6147794": "on for any",
        "2211.03011_f243e6ae20381498736c70fd13bae939": "$X \\in \\mathcal{X} \\subset \\real^{m} $",
        "2211.03011_058bae3b0862942f5cf64e200aeab1be": "\\begin{align}\n            d_{\\text{KL}}(\\nu_1 \\Vert \\nu_2) &= \\int_{\\mathcal X} \\log(\\nu_1(x))\\nu_1(dx) - \\int_{\\mathcal X} \\log(\\nu_2(x))\\nu_1(dx)\n        \\end{align}",
        "2211.03011_4bc58beb582698fc642082053c0cd6e1": "Moreover, if",
        "2211.03011_7da75f4e61cdeabf944740206b511812": "$\\mathcal{X}$",
        "2211.03011_324ecc9541dcfbbd88f2faf8ae698239": "is bounded space with diameter",
        "2211.03011_78ec2b7008296ce0561cf83393cb746d": "$D$",
        "2211.03011_67438bc80ded7aeafea4ef32442850d7": ", then the relation between the Wasserstein distance",
        "2211.03011_91288ff048149763c39a4e492c17285e": ", Total variation distance",
        "2211.03011_ba184f232e44447b0bbbec9432548000": "$d_{\\mathfrak{F}^{\\text{TV}}}$",
        "2211.03011_1851854038db5f7c973768f11874b954": ", and the KL divergence is given as :",
        "2211.03011_3398dbde4d704eb321fb9f4a33168d67": "\\begin{align}\n            d_{\\mathfrak{F}^{\\text{W}}}(\\nu_1, \\nu_2)\\leq D d_{\\mathfrak{F}^{\\text{TV}}}(\\nu_1, \\nu_2)\\stackrel{(a)}{\\leq} \\sqrt{2d_{\\text{KL}}(\\nu_1\\Vert \\nu_2)},\n        \\end{align}",
        "2211.03011_2948d94fa881d94caece8e30a4c21e55": "follows from the Pinsker's inequality",
        "2211.03011_3f1b8315b62f0d9237cb8d0a8782699c": "Note that in",
        "2211.03011_8b07999d33807f030ea3232a15ecb183": "we use",
        "2211.03011_580820984f61e5c8aa9a6fdcc36bada5": "$d_{\\mathfrak{F}}^{2}$",
        "2211.03011_783d4f6985b9977748d45f0547ada824": "Therefore, we can use a (simplified) KL-divergence based surrogate objective given as:",
        "2211.03011_decdcb505840847086d8f8010de9e4b3": "\\begin{align}\n            \\int_{\\mathcal{X}}\\log(\\nu_2(x;\\aisparams)) \\nu_1(dx),\n        \\end{align}",
        "2211.03011_ba2a327845aff1c11adc8445815fdcab": "where we have dropped the terms which do not depend on",
        "2211.03011_098666a03f4aa6b01dd283248689d5b2": "Note that the above expression is same as the cross entropy between",
        "2211.03011_62bf920bcc27eabbe57568fac305d4c0": "which can be effectively computed using samples",
        "2211.03011_312ea82d78b3b15156791660298921b9": "In particular, if we get",
        "2211.03011_266a711a23c5b0f8b06959aab3c5aac9": "i.i.d samples from",
        "2211.03011_7f580809c491e6907c1703f36a1cde0a": ", then,",
        "2211.03011_bdefa891599d79c18af0aefc274b100f": "\\begin{align}\n            \\frac{1}{\\Timestep}\\sum_{i=0}^{\\Timestep}\\log(\\nu_2(x_i;\\aisparams))\\label{eq:klwass}\n        \\end{align}",
        "2211.03011_3a07fbc35a5479784758bf25daac8fe4": "$\\int_{\\mathcal X}\\log(\\nu_2(x;\\aisparams)) \\nu_1(dx)$",
        "2211.03011_c3d2ddd3f25f150ed093547da6e1aab8": "The KL divergence based AIS loss is then given as:",
        "2211.03011_e3a78400d5bd6ffbb5ef3477808b94e4": "\\begin{align}\n                \\aisloss(\\aisparams) &= \\frac{1}{\\Timestep}\\sum_{t = 0}^{\\Timestep}\\bigg( \\lambda (f_{\\hat{\\cost}}(\\Ais_{\\timestep}, \\Action_\\timestep; \\aisparams)  - \\cost(\\State_\\timestep, \\Action_\\timestep))^{2} + (1-\\lambda)\\log(\\hat \\transition(\\State_\\timestep;\\aisparams))  \\bigg),\\label{eq:w-ais-loss-2}\n            \\end{align}",
        "2211.03011_1c8ac169bf87c6204ae5b545c02dacbc": "Experimental Details",
        "2211.03011_a30014a1b3a1c6ab9cc18a45def9f22c": "\\begin{table}[!htbp]\n        \\begin{center}\n            \\begin{tabular}{|c|l|c|}\n                \\hline\n                \\multirow{9}{*}{Common}&Optimiser& Adam \\\\\n                & Discount Factor $\\discount$ & 0.99 \\\\\n                % & GAE parameter $\\lambda$& 0.95\\\\\n                &Inital standard deviation for the policy &0.0\\\\\n                & PPO-Epochs & 12\\\\\n                & Clipping Coefficient & 0.2\\\\\n                & Entropy-Regulariser& 0\\\\\n                & Batch Size & 512\\\\\n                & Episode Length & 2048\\\\\n                \\hline\n                \\multirow{3}{*}{AIS generator}& History Compressor& GRU \\\\\n                & Hidden layer dimension & 256 \\\\\n                & Step size & 1.5e-3\\\\\n                & $\\lambda$ & 0.3\\\\\n                \\hline\n                \\multirow{3}{*}{Actor}\n                & Step size & 3.5e-4\\\\\n                & No of hidden layers& 1 \\\\\n                & Hidden layer Dimension& 32\\\\\n                % \\hline\n                % \\multirow{3}{*}{Critic} \n                % & Step size & 1.5e-3\\\\\n                % & No of hidden layers& 1 \\\\\n                % & Hidden layer Dimension& 32\\\\\n                \\hline\n            \\end{tabular}\n        \\end{center}\n        \\caption{Hyperparameters}\\label{tab:hyperparams}\n    \\end{table}",
        "2211.03011_551388036a08752c7fb78efbb27a659a": "Environments",
        "2211.03011_d66bdd827c54a25ad996b96cc5496cfe": "Our algorithms are evaluated on MuJoCo",
        "2211.03011_c8e68787f7ba40a1ea9b9b3e4fcdea45": "\\citep[mujoco-py version 2.0.2.9 ]{Todorov2012MuJoCoAP}",
        "2211.03011_6811defec4c4bbd8ac5e5b036e615ef3": "via OpenAI gym",
        "2211.03011_0b9375a09ecbc0aae573a18b083b113c": "\\citep[version 0.17.1]{gym}",
        "2211.03011_1f86f5858875d61f1306ada5bc0f8d74": "interface, using the v2 environments",
        "2211.03011_4435f20b008803676186f05173d25165": "The environment, state-space, action space, and reward function are not modified or pre-processed in any way for easy reproducibility and fair comparison with previous results",
        "2211.03011_62c04cc2675ad9467387f6a3cd6baf64": "Each environment runs for a maximum of 2048 time steps or until some termination condition and has a multi-dimensional action space with values in the range of (-1, 1), except for Humanoid which uses the range of (-0.4, 0.4).",
        "2211.03011_cb78d72f87a301b96457f08965121cc0": "Hyper-parameters",
        "2211.03011_980fbaaaa04b39d7258e21f485b5ecbb": "contains all the hyper-parameters used in our experiments",
        "2211.03011_8f703f4283457e67023e6e6b0d44214c": "Both the policy and AIS networks are trained with Adam optimiser",
        "2211.03011_defc2ab3051af1fd3c386c26b1d40073": "\\citep{adam}",
        "2211.03011_d7bdb41128d7733e9731374109575391": ", with a batch size of 512",
        "2211.03011_00bbd95ed734afc6f9d81774219f4867": "We follow",
        "2211.03011_d0706c01f3e3846186c48b71406207b9": "\\citet{Raichuk2021WhatMF}",
        "2211.03011_de52e37d0eb3e4b38f6a9722c5d6c970": "'s recommended protocol for training on-policy policy based methods, and perform 12 PPO updates after every policy evaluation subroutine",
        "2211.03011_5ffd88956a19d4f2dbcd1d18aeaf474c": "To ensure separation of time-scales the step size of the AIS generator and the policy network is set to",
        "2211.03011_2e8fe082d4f69d6565735199779a33f1": "$1.5\\text{e}^{-3}$",
        "2211.03011_7e2289f8dabff6bc7fcba9b7135d3bad": "$3.5\\text{e}^{-4}$",
        "2211.03011_d5292aee9173415a22b860aea5545dee": "Hyper-parameters of our approach are searched over a grid of values, but an exhaustive grid search is not carried out due to prohibitive computational cost",
        "2211.03011_7ad602f1f41b6b9f089cd9fe5e29e689": "We start with the recommended hyper-parameters for the baseline implementations and tune them further around promising values by an iterative process of performing experiments and observing results",
        "2211.03011_0ea97206fd20c6d4676200f457999e9c": "For the state-based RNN baseline we have tuned the learning rate over a grid of values starting from 1e-4 to 4e-4 and settled on 3.5e-4 as it achieved the best performance",
        "2211.03011_b37b381f750749293783aa21417dcbd7": "Similarly the hidden layer size set to 256 as it is observed to achieve best performance",
        "2211.03011_854d95bd42066e7fa25631e5b0b82adb": "For the feed-forward baselines we use the implementation by OpenAI baselines",
        "2211.03011_cd2a043a3a57ae49378755d03b24e47b": "\\citep{baselines}",
        "2211.03011_393256c911347ec6d0af757f5c31335e": "with their default hyper-parameters."
    },
    "hierarchy": {
        "1": {
            "2211.03011_a6164cb182c9c63cdade753c99db465d": "2211.03011_69c686a1e3ae00d774ff1ed33d0e844b",
            "2211.03011_e353dbe42c8654f33588d4da0b517469": "2211.03011_69c686a1e3ae00d774ff1ed33d0e844b",
            "2211.03011_309dbf8458328e395c671cae07c6faec": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_0c9fad2047a0144d8a5a7ddb84a53cc1": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_41b3f373fef3861bb0a21ba80dd9ca30": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_e62ac68e0e6f632a9559ba36337b32d8": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_0b79795d3efc95b9976c7c5b933afce2": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_c48ed3abb0b442a8a0b00712244bd4b2": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_23d6f0f417d35bd3726ccbaec51d18db": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_115206c0b4fca0ad7b71a2bed3cdf021": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_c7231e55199e87dcf992f08907f0d61c": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_c400beb29df46b5f1761d7a7d23afc82": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_1e097e51e848d784b7ffde3ffe400f7a": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_03594021c0847b4e978fd76a543f9d05": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_9686d6292766d1578881b636553ed415": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_e181aefaeb9c81936d245d31ed94aff5": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_857bbeec4a3c27b158f8f1be2467e856": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_133363f4b1207280c00979b53ee2853f": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_2aab0c594c5501bf23fc166a31ee0182": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_c0a798e82236a6762399268e1cf49226": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_ad688e4fb07f43fcf3af44dc0356c228": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_611ef6d6b926987a67ee0000b8824acc": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_f16115e11f3fec0df0bba0188997604a": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_4fd4634702bde2a84ecc10dd483f39ed": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_250fb37be40d8c4ee899534c7ab2939d": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_efeb369cccbd560588a756610865664c": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_5b855c0e62aa57999cebffc1bab7fefe": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_94921a6af06e2c002df8b6773e67d78a": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_e164aacf273e30e3eb7babf410f64a4e": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_726cf58b3134bb1639a1fbf7a2bfb338": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_78b53c8af37d83d5b641399c9d6d2d69": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_649c029cd1e36da8c4411b700a49ae82": "2211.03011_0b79795d3efc95b9976c7c5b933afce2",
            "2211.03011_5058f1af8388633f609cadb75a75dc9d": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_90b9329bb5fae8bf53157c208cfd0f41": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_2a4a1702cc16a9af03b0fefdc904bf1c": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_f5c11bbc96a07bd58f1453788494ad38": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_567904efe9e64d9faf3e41ef402cb568": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_7d615cff00854eb008f6935851a8dc40": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_13c5e6d37a12c103d101f33792657813": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_a77ea1c78604675d469ea4c00f3d8d7a": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_5d633873b74b3d2fc8219f7b6fed16ca": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_9d0db5860cea7c0f877d88f2b7cb89f3": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_c19d342cdd06314295ae448ce0788ae7": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_3d150ce9a23a96fbcc39157621a7aa8c": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_0409ee49c2b2992eaf178e32001cd941": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_17020815ddb68bbaf27882c1e1c090b5": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_52cfab0882f919785a5e6d78d107c56f": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_129fbe35c8b59dc7cb48fabd25dfa8dd": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_b70b0410af3bacd1a2650ce40d764af3": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_7113382170194148d3b8f2bb94904f82": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_ad2733f7bf355eb5e6910be61cc52765": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_3879179cf3369405c240fb9cf8fa487a": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_d1fcf32671e73130ad006b801e9e2de3": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_75b1566874df6e167895b97e9d5cc6ce": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_25c96da87fff397bb6b670de6872851e": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_182077805b4455ee5e248ffc2c484a84": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_ac3148a5746b81298cb0c456b661f197": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_91b5743fdd53120f6c436c6024926707": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_a4fd57cf9b7e271e6f7f2f6e9080b858": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_e02031955d01cb0fa61a4134633fbc26": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_be5d5d37542d75f93a87094459f76678": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_663ac504054da2f05b16d57c8619acee": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_553e7f860b7472c2f2bc05c01ec43462": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_9b8a6316da8572e95ee2a8893ceb7a89": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_916d023e6698347703907ac981760857": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_d7ccf09047620d0ccbb8acc42a973163": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_0456dd8f50aa0f8130656d2a9578765f": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_a7304ccb43dc45cca018f063a85efa85": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_06bc7f8e9ec46f50a4114bf4774c855b": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_3b3366a82828c93815199234712018f6": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_32d7acc03bad6dc7366ea9f30fa2391b": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_8116c304f9fd2c642b7ddfa73d1252b6": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_2028d4bf2377b0e049fbf01f2ab44805": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_558d4c50d0aa62bc5e711ecedfe47a76": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_b246202a262be48b42d8d3b2c8b9e96c": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_c48233c6260f0fa184932bf7d0a57506": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_1d5d9fedd9ab44a19d9f30a0b81f79a8": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_f4af8b5789576c000ce9105b25609bd6": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_61a0362e2ef15a5393c1e2ef3c17ca89": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_c0cb5f0fcf239ab3d9c1fcd31fff1efc": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_42081e04497930d1590b479463780d25": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_b6e12ce80d7402ec3522658fe951b47e": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_023913f46506fb2019b7b5ae199db7f9": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_d02d0bf2da68e0b45b9026104009fb19": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_b84c7ddc5de69e9d368d34b993fa7e33": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_0564adc1219854e49dc38bcca131968f": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_90de545e518547f74725a635ebec2dd8": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_b4a1d6ce4f047fe1d7014db1adf79e85": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_88f9ef04de08b686edfdad1239813e25": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_a0d587998e0d897ece232d64ccbe43c3": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_8a0e2b549d223aba55a866c38d1c9275": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_ee257ffc352abf7ad3b733f83bed702d": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_f7cbd707910b7e6593b9f1bff795cb4f": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_a6aacf7d8cb19e2a518e34700f9a1ff0": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_a29f17caa3e965b909d1aef183a202e4": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_d6328eaebbcd5c358f426dbea4bdbf70": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_73d7111aa2ae526fe9a3bbd506581205": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_968245e65658f9d3e05d53982cadc1e2": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_5dc642f297e291cfdde8982599601d7e": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_3754a68e6e8664f579833c2aa0674ebf": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_76c5792347bb90ef71cfbace628572cf": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_52b3705250c5dc8eab0832260e994d88": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_d843094204e89edfe33c45294236051a": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_42818217a9b0df80c535a410f960d34d": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_23696bb68b987bce23cc98449556961d": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_b0316aaeba84883d381280f13ee0faf0": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_a551e9bd38d0e342200ac07a78d0e766": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_df3c5c096fb5ea6306d3aef228df303f": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_01efb56f09b9fae19d23eea5e3a1afdf": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_ca5c354431b9a7c3aecb72c34335b5de": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_b60daaf917fe7189c930f3897a8badec": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_ae5e735a1d01159500fc22b615c993dc": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_a0efe538bc60d7ea680e07ee447bce0f": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_e72173254f9184557dc20ed431dd5239": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_30676fc4ffafdba645e3c9a3e350345a": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_e19f6ce00803d7973fb3d333cc0245d7": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_dd426d100a1f1018b013ef22fdaf0e6c": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_d867a7dd8f9e1a969b8d5d955bffe7a2": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_080d640843c41cad228d403d485c75aa": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_d3bb8c8185a90a90b00a6ba3d5ea6d36": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_72ce25fac81027cf9c291e00caf6701b": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_f50853d41be7d55874e952eb0d80c53e": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_fddd4e2b3810c8754e7cbc17a0924a3b": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_4b9e818fb2bd9e00a29de1201046856b": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_b02e8d6380570e7c0efe76c50c742079": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_48ae4dc4b4d97fca35e813b1145f23be": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_0aec6b1a0d42f1b68747d84dec7ea11e": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_cc7817a435708b499a822f10d683815f": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_45792bf40eb282f17d94ae6706f58ed5": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_a4c5f57a05898c2553c4f9d013ab5872": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_a457efa02bc2946e2ed0d285d3cdee39": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_68e43b5e8afd5bd4c4c7af8f90d20055": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_07617f9d8fe48b4a7b3f523d6730eef0": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_d6d59c3bbd4d584005553e21a67da915": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_94f19ee28014814bfdb82451d32d2f42": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_fcdec8ba599ba9dfe201e0429fddbf15": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_5f4fe13d269f7b4ac974f259a69df6e7": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_d4a00b1d35fd099d00a5cc8c375493f1": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_df02e7666c632d22547b9c75b98c49bf": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_13ff0587ded1b3fbee98b34bfbc95c17": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_9af86d2fb9ab92c17a27eb048efa059f": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_a5b810f11c231d03bc9b97562ea31d37": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_0ee5ab8da7e9ac050cecd67156094799": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_bc3b923190e79ef890240ac314a257f9": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_8be2b487eaf0803a532a2261cc333d39": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_9c03e5dc4ebd91a76554c8dd7caa3067": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_da6ee250684be292f9d142c973800ebf": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_ea84c470b3d33b1c19c076d7b79277cd": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_388fa2509025674d7dbf4715a4a9783e": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_f6d372fb0fa29ea70ccc0a6e9e11a2af": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_5f9cdc7c0f48165b1d9b0fee2f66a6ce": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_d66a45e0022537aed886b2499dd0bb1f": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_2a4da47a84f6723d6613e55a12bebe29": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_6a93aa1aaa01ce4e27009e2ea0d54022": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_0beef5f09952c04d06b5e9d05056c4de": "2211.03011_90b9329bb5fae8bf53157c208cfd0f41",
            "2211.03011_7e5ad6c44bdd82b92e2a85e02f7ecc8c": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_06052e031fcec39412abab363aae1858": "2211.03011_7e5ad6c44bdd82b92e2a85e02f7ecc8c",
            "2211.03011_d05487b62ecaa382f68c6cba959464f2": "2211.03011_7e5ad6c44bdd82b92e2a85e02f7ecc8c",
            "2211.03011_9ce854c86fdbe257d83b04d555a5f8be": "2211.03011_7e5ad6c44bdd82b92e2a85e02f7ecc8c",
            "2211.03011_f39292ba4bcbe105d3d0f7b621d751d6": "2211.03011_7e5ad6c44bdd82b92e2a85e02f7ecc8c",
            "2211.03011_f78791212eacc17b98cda7fb89b8ee66": "2211.03011_7e5ad6c44bdd82b92e2a85e02f7ecc8c",
            "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c": "2211.03011_7e5ad6c44bdd82b92e2a85e02f7ecc8c",
            "2211.03011_cae6404c4aecf46684930fe2a86676a6": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_d2261afb4f5870a503af555cfd555f82": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_be000cebcfa2819d0babf38547d4d282": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_3ca8a5bc0cb9795f4ad5d27a7115a724": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_943cd266063e8c7e8be27e11ec6c1acd": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_d2c40e49b4bf1485686005f0a508feb9": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_5e02bba5687e9e8a407ff781d9a1f220": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_550d39a8b64a62afab5c9d93110e4437": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_a5e5171912cb444cdfd0c4a51c5eb32d": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_5422695d6e310d32f83a899e61ff6b6a": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_0ce69584400d17167fe3e8473096d814": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_190083ef7a1625fbc75f243cffb9c96d": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_0e64568b6ef4b66acd2a78f70be08c6e": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_aa050808fa16b2f412c2311a0dfc2f74": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_fcc4599bb28793b7c46200b26b57eb1a": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_e43699be9d3f7ce528d1d65b30c31be7": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_e8ca9d5653a2bcf11929ad07319ba346": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_a04cd6274f661fddda9a98418e6bd2c4": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_8921bd7af5f9b2a5d1f7a2ebc8465744": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_b45319a6ea1b7bfcf4e8ffd4cf1e77e8": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_853ae90f0351324bd73ea615e6487517": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_21af775c9ed3aa7b482106b6ae258a5c": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_8a02376b6f01a3fb1454923f077894bf": "2211.03011_ff8ec9f2965aa91b0a0982d70bb07a6c",
            "2211.03011_7d74f3b92b19da5e606d737d339a9679": "2211.03011_7bade40785d1f5c1dbec8c0414eaac9a",
            "2211.03011_82352ddd1e83a90141bb3e886e37bd21": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_f516594d92a2e17ef62a163402708713": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_43ec3e5dee6e706af7766fffea512721": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_331d387eaa82aaec976361033ca4b142": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_fb4f3b79910c6656439b97f695d3f658": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_ec9acb8d24391abfe49520372e152d95": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_ef7544b2a30b56b41e08158b0851e1a3": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_7114e8b70a29f3808a4b0ac1fc360fba": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_46dd09e3809614efce3edce6320ee3b8": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_9e1e035b868fe89dd4993bd73d8a6f74": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_a37302359718eac43a8aec1b5310a2be": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_0504bf39f9472c4390ca69cee15073be": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_b476478a059076bd064cd484e55bd3e8": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_55b540d6c3a545cda9559d252c21bc20": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_1161abc180af83c1b4834b02611f99e2": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_acf1fb1972262e375f7bad3701f5fa20": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_a9ee20bb24959381e80f233187ff8777": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_010e18380f5127e518be1e9d671cf408": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_e06ba62f2bfed5cf8a0fae61c45d4ac8": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_973ca979fddfba1db6f2774ff446c0fc": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_cd67b575db4ed2b9a0f5719d2aef7411": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_d58a0703ef6dd2b7ac6da1692fdd1f6d": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_ed5660e33d3f46f0905d6f49bfe917c8": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_981d477f5229e728371e1b646520a85b": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_d2e2a17faf26552552438906e4d97300": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_0a2e541fccb9ace28f513ad172e9c378": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_507377f5cf3fe310cd1151d38a0bfce6": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_7e2d84b1ce02eea574681dad45d216c4": "2211.03011_7e5ad6c44bdd82b92e2a85e02f7ecc8c",
            "2211.03011_94936736293272f345248e3e9f1918e8": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_98e10b206ba5c38ed19851d91569adf7": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_a437cbf5bde9cd9b6bac6d3d707e4c0d": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_0623934d2e2051c6ead0528a661c4313": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_0b551d05d8f005791675d74ca69d5a73": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_f2194a2def0670a6e2847f4d5c73a3b7": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_9fab0f1ebc8b22cb954b711d5be4210c": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_86ddd25d2aad69191512313db911ea1d": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_154cb8d316ec563a2e66124100ceda6b": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_4f4f4e395762a3af4575de74c019ebb5": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_993ac1f820cba3f1950d647e1902e3b8": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_3722e629d21484eb98395ce6fe1ec3cd": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_3e93a51b47befb7fc9a3eed5987b6188": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_0e7260f478363af724aaeaab04f00264": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_aae7ea259cda5d1b4f6faa85909060f3": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_63eaac49b782ffcdc3d196ede3d84daf": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_255c484618de115b366dcf63e8d07d96": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_e49d39947a7721d621e78d117e8b0b98": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_f01bcf651e471dc7a0be43aa70bf370f": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_6da074617278fa6887c1bbea881ac6a2": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_a62dd5e9bb41e370575bc9cdfd7beb19": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_d828a642e2875ea6e1dcb1c9ac405c16": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_d9acee9e485ccc1a2fc8666788180979": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_38e7ce80eb790ab23fe30624874ca027": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_cbd26d8a2e0dd465562853e9a6d0b1b9": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_353fb0bc1e64e5e8ff0b91f12a84e19f": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_df917871e6e0a195c3af28139415826a": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_3f3e9703de90cdb1716d333e9e41f74c": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_0dba8d672be3e549f965eee412c53b00": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_7a3e7da148fc31147168eac5bba0bc7a": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_3426a75cfa4681e9db4a5072c1d0ca3e": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_a2d3b68aa6e7c3ea60302ce7ed17c727": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_d270f59d2e4eb02af7c441259600b3e7": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_ee1140d9db0c0b3cf1b25e624d18b984": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_feab7d6a29c0a324bffb00555a6f6b00": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_1cd3069d934e1d8d87343f04d2c74a23": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_d8aeb8fc900f9df2e2cfc8eff9cddaec": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_30f13dbcbbf37c30f11c95546b51e34a": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_cb5d947b1298828ef4f2a044865d448f": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_eac332f2e2ca0ca5c060d49f8224ed8d": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_84bf347f1aca4e9ce6474a71766c50b6": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_6eee077716ccf603a3d82e7abaaa9602": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_1ce2becc7cf407aa96a91bf6fbb1a409": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_209490dfc8a074f8872110ef5181b9ef": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_67dcdcd2ff7dc889aba14d086a1df24f": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_f511d79370a09218cddeb4a5c211f3da": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_f6b770da20531eb1d40e0ebaf980a4da": "2211.03011_7e2d84b1ce02eea574681dad45d216c4",
            "2211.03011_261e15505a1b57e5b4be78e5da1dcddb": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_9aaa3b585b9a624b8bf1bc0d3e610f02": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_21634175583d382933cfab688766e011": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_125dcf0f5c8334292df8199cc9e89ea4": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_ecab5095068222371d477f91210d408b": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_ef146718ccf9bbccaabada4e11c30dbc": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_f75a6ebb7d62ba04bc8436b0cd00dc38": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_95fe648d8ef5b9f5e6a3994d5b02b84a": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_d0a00029da440e685800066a561c1fc9": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_b9a404206d6325b011ab48e48d773dd6": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_553e3bbf8c8e52b44f20006a2b842d7b": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_054bae8b1788c0e689593e295073e0cf": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_c8688043cf931f9c7dbc0b16cafce959": "2211.03011_25f8fea261e42991fc947198f3db639c",
            "2211.03011_4d469e1fdc5c758ee9f00cb173a0e422": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_37fcda0a5906bf1fa06532e46426996e": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_c79a3536ee84a4f0eeb16e5e512e47c4": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_be77643eceb3ba0dbb4c9537d25223ee": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_2d474ccb5029bc3ffae00e5749f11b35": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_039cda6a8e9c320eba9f4ff4e47f3fce": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_fa5852d9aeb8e1b90fb7c3b243a06b3f": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_bd4e8467f6eec50d4e9925fc9898e71f": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_238cee5f86f23f9555aa7a234e17e7f4": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_fbaf8464a019b9e45099f724e3f00347": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_7ad23a7d9ac23635efd21b62518d85f0": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_302aea2d3b31f04bd1d05493219b0d54": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_a79f2ffb7acbdbd5c2284cbe7a1d1694": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_b9f85bb6936a624e5e7a704877a71bfd": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_79170fd81965ed64f415720cbafe70ed": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_564537e0001836961460a712b29e9663": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_54e0854b89560b1abd88c023e66f22fa": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_041db6f5daba6dbd91357c47d95c766b": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_1a402055656256eb35e7c20e801793a3": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_fc0e4a6be1e579a5fef887ba9f3d9923": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_9312cc2f02f42764c70c3713f80fe219": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_2290a08ae763688f9bba3b655014288f": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_98b9573060feb65ec13ee45e58de3a6b": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_3a91b239db7ab1491dbb3d8fa230a1d0": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_38c50b731f70abc42c8baa3e7399b413": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_4c657fe19c72fc65b04316256e6a12b4": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_7a35d1fa95c69c610bcab0b249be58c4": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_74d39a1573fbbbcb294d2d1c501d3330": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_1d69035ba0a7b66abf73d33831d447cb": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_279915c30453c198bd50e15933483959": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_1f07d90c713d8527d15006a8b597e7ce": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_0e5a6d3cbc4ff3e000f672a3e01300b3": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_1197d9626a1b0e3281bb2efca3904bc1": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_52b4477b11df35c367d2d120b455026e": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_ac57fe7ea9f1d98a76f25879c7d59013": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_c2974f727435bd2c19ec136a88d9596e": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_272bc0a063b6e9e2c707e5b0502b63ea": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_03c7f1f643b33a6d66d37a8912115497": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_bc64324d237711542f964071370112a2": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_c203b52408cf1e6bffbc24e359fa538e": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_f855a9b5e6cbb183788a1d10e3d7c6b6": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_383b8f0cc76e5fe96b4aa1a6c7145be8": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_3268b2e3bf243528a48b49c2ae6d2de6": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_54d44a61c5a51a5bc85f4da72eb49a0a": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_b3940a5c778684336ddd47fc77f59e3e": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_4dc022b61f6390353ed66650618a4d0f": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_b66ba1272e68cba980c14a2ae282c1fe": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_80b5a17a7721b02954b2b4cab4435772": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_edfb19ec72ce4dceb1ad9373e4f005a1": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_e5fcd666784e5b4114c35e63cbad39a8": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_7a16c6cb5fc758239fb71d6e042e4a24": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_3f60d645c75bd0702e9a0823975fa8e8": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_8e1a3770649d8f6cf5de38bd1b72bee3": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_38f1e2a089e53d5c990a82f284948953": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_8ba57dc1cba1a007e8322893a2560734": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_82c7448328656e4dda7c3f7a2e61e736": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_626549a5a258e01c9934431b3b0314c6": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_d28adb314e6a80173dce44cd0349b945": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_2a346a4ac761168c11c82c6eb4abb447": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_55b6594a2cd55065bce98f7391553d20": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_eaad832ea9586c9dae667826d11b3bfc": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_ce48df8cf209c244d6932b1a6dd59648": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_808671871e2c98e867f7a585d494d54d": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_3fb7c4a26dd24c6aeb24aa6a4791cf8c": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_5888411c8c645faa2939a83f56fa61b6": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_b325ebe03975476152e00dde21c3590f": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_a2a551a6458a8de22446cc76d639a9e9": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_a7def38322784e91536f66bc462c20a7": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_7e9fe18dc67705c858c077c5ee292ab4": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_80853d56d52219996de2b6d480b5ba1b": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_56a67126096b69d4b7ef1fb9a898b239": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_5f1a3ba5270897a6dcbcab46bbdc99ed": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_8e832b62b7f4ade29bb75dad80ef556d": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_e3c3774c048954142b7df64c642da3c1": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_ff06d36da55702702c927a9c0bbfcde5": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_5721336c579cf4af612914063bead318": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_c77d742d712aa220ddc49c56a3396a0b": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_597e3fc63df677f2a118a2acdcdef73e": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_b1d5e6dec7426978d6ccd2ccfd42328b": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_162d61d860f7dca965c813f623a40bf7": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_87eed07edd7f3955b7a2419c88ed9bd1": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_8389d291126551be45600c4e4abbdd64": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_e37493765bac92d7dc767dc402c3425a": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_786887572f6ef1c20f2d8177cb2f1639": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_fae7d06d34c04831a201a8cafff26811": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_390cd41b15655a50c7ebbb6509e95c6f": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_1838ee43e599ed9f21e6f0278031c52b": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_587037d7cf426d56a4240e03919348ca": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_54154586fbc871f126bb7ff3869bdb76": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_3840f3f2bdd5eaff9372d4c4f552ab8a": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_cc2ea0baa9445308417324b39cdf2365": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_b9eace90ae01078552d739c520dabb48": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_ee8906e8ce651d2b16ff6de108885fec": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_eeb2f6d920067e4b5af5981c2f92c3ea": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_2f854e75afb261e86f0aca8f0c497458": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_7de41ab218f8a51f4742d4bc6393e346": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_52256f174893670edf771aa1a44bf816": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_907be93d6900cb6322a28457a00be19f": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_5739685465e47101af17e87a4e041bd5": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_ec65f5d7534b1662374e61357e748d8d": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_b120aec3f9fd3de4b98cbb8685837222": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_9aea9022772bd12b124976a1fe6e0631": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_56c90c72ee03eaf1cbfd0b3bfc3e9a13": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_cbe3515ed3eebcbcd4793dbb47fce4e1": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_e91001794c004c2af5954f032ffd9808": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_33e1d83a711fc841de2cb141a02865a1": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_12966a89bd103bc2ebe9c171edd62b07": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_5dff34889803b4166ae180f6115f5e85": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_bbce11cf64cf692ebfdc78942f6537f5": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_91c363544a1bd897054e488148c2b2ac": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_15b961199224005e89dadb2a98bfdbff": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_b8a1f0e62ecad0b923a2f9ca21709188": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_970dafaa7c14cfd9848bef7872fbd2f5": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_d858be959a61bcc0186add30321fcb16": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_cf314751d985625bee3a94732e4f2b20": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_fd4465ede00fb9e15e9c9c656a6186bb": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_9ab958cde46235a5390f3b24208ab5cf": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_09fba8401828805ea0f689dd15258255": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_27698df1b3e7bb191f26f5ff82890040": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_417e4da1fe71542c5fdaf19f8630d958": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_aab99765f036eebee95077c273e1047a": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_e81783029edbfd7a55e9cd8508949415": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_ebf40128156ab0e2fc7938c55e213b3e": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_6a3a86d06adfc92161740c6a8a62e568": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_bdae20c77e141da651954bca5538526e": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_b92316ae504a2fb2f3d16da40ccf6e9f": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_357b6bf04e7ea005692b42b2350ea2e1": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_f9944d2da56d55200f057fd68cb6dbc5": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_04467c67c40f985e1d21a20ad63fa44c": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_963eff4af4d08409bdb1371f9bffd686": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_dee3f6d54f342ccb4b31469dcc72b8b4": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_b65c6ae752bdc23595200f4dc491da29": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_f94d26acf323a1e57173cae1aa8ab9aa": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_2fbaa257a07c25f7b8a60c1e69722200": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_dc985fdc331947f57190a3971b1cee84": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_18da2bdd534c73f2009d713b514efe2a": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_01abb24fc07cf84277e789de05309c3a": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_8221020305c0f3980cc4ead639b7322b": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_48aeb500324b4072267bf09dd38899d0": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_a06d07217f55a942c9fdc785bb7489b6": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_b60be6a81953815b24d345b979dd2065": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_1a4b6584ea4021d75a3e85d78aeb02e9": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_a9bdefff0aceeae3f3e137a6223170a3": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_4f9247764fc30ce11d7719bc11cfab14": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_59827e8f485da2c058034386ed39e836": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_9882316f07d1d00d1da83a21ff07f52b": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_ab6c8932f84f9a9f8c706c95cc956fa2": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_721bc0bab36a3dfec01945b0045cc803": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_0f86b1da1c5442ebbb611e7063d7e822": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_3db0f02bd709f90e5970aba9be6c3cca": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_56e2a673d1d7c3c791682694d856443f": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_0bdfd09448d04987df122dc149709944": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_5f876b0dfd4a130bd1d4e9d9aa3f7fd5": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_0df11f7b998c0bd599d485dd6bb77cb9": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_1c398fa9c7e737f79674206d0744c460": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_b3088c31250233434af984bc24de2562": "2211.03011_0df11f7b998c0bd599d485dd6bb77cb9",
            "2211.03011_303a4a802ae9ac2c14cd8dbffe77c660": "2211.03011_0df11f7b998c0bd599d485dd6bb77cb9",
            "2211.03011_6b3f5f7ff91660f69e1b99d2c78064c4": "2211.03011_0df11f7b998c0bd599d485dd6bb77cb9",
            "2211.03011_dc48637b1fd3445cd9e524c2cfa903de": "2211.03011_0df11f7b998c0bd599d485dd6bb77cb9",
            "2211.03011_b439ffecec6f495e08fac5391edec7cc": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_f31a6b3961c0ced78e6de25af79609df": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_acafcd7e48299e06f23043c9731b2664": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_8d99c51053bf19326e3649052dc5fb47": "2211.03011_0df11f7b998c0bd599d485dd6bb77cb9",
            "2211.03011_4db4f90e14af1e1ce17589002b8cbc9e": "2211.03011_0df11f7b998c0bd599d485dd6bb77cb9",
            "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed": "2211.03011_18da2bdd534c73f2009d713b514efe2a",
            "2211.03011_a76caeda93e1b4be78b8dc08ba02357b": "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed",
            "2211.03011_324132b8b604e662477be237bea3cb7f": "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed",
            "2211.03011_b223e666c754ff10c7d673db3487de47": "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed",
            "2211.03011_944d3dea99f3c51512969013ac234f22": "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed",
            "2211.03011_eb81e119a28284865d46cb1ae0c0d64c": "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed",
            "2211.03011_59b6354f947271f8f641d979ba1ba2a8": "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed",
            "2211.03011_61901bc3d118c51f5fbc488598b3fe25": "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed",
            "2211.03011_df6ac8524fdf4e2fe28871fdc4a27ad9": "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed",
            "2211.03011_827dc1223ae9b2e70bbf5e1e18a4ee08": "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed",
            "2211.03011_17368be9281ffc9fbb69508d9d028d6e": "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed",
            "2211.03011_149a1f9ac8ca567150ed34acef1f2c79": "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed",
            "2211.03011_9c68e5666b0f9f15bb31b01093c264b3": "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed",
            "2211.03011_5f4f3cd158cea054ab7af98381b2a757": "2211.03011_f4a569c2ca22c9c2b1a1126dbf3abeed",
            "2211.03011_24247d37f026f5cf598677a503744c27": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_158e5d694f9d6ec764a95970bf3ea094": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_7b2bd7eec2589eded953bc0a8b90c8fe": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_154f6a6c2ed0a60ef028d2eb7d72a88c": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_440a512fefa486a6af8ae7068caf7e29": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_c8a5665255e2aa7c057854802cc6e807": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_4821a01e8c2678434081e8e827c08cc4": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_7b7c8092996caf74be3c266582a01133": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_3d4b381aeda7836005cde74bb7d31491": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_c896490ae01a15acb4f6180e0d75cecf": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_49b230030ea47d8f9e845643fe715a2f": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_4e380fab8ed1d552bc58439c20396e1f": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_006636c28a6cef805e6dca7ee08b4426": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_444e96f61ddb5a7076cfd1581ab89180": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_8e03334edb8b06cf1774613f8e432bdc": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_c0fac37badf15b7f7db4120a03537c0f": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_bdd64a63ac1453519db6b440881baf1a": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_6eb39a59e49eea06a845abd04b95b3fc": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_62fa5865d2eca2894cfd67c35c6b234a": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_2271c08579d5bce0ea9bda670137570a": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_c0f8972a46447bc6f1fd39be148e2ec0": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_b89c49594d6142e8be55a59b05cd342e": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_2e3a4efb51070982bf3a3e12aaa4267c": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_1c386b493b0f1a79f80f5e32ce070c20": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_574bd3b0fd3ea20556b90498a2840b35": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_791c15a2e2c6a8258361a6c2eda4e0d2": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_c4d695002022924b24c54245c136547d": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_9369c90a47575b8d6d3c2aa5c9426a39": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_d6982b0caa2870b8c36a12b5102c1bff": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_4f81b61cc5d59f6e56a1a1a9c39b0d4a": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_30ab1b6f2ad63abacea648a1ecb657e0": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_97a03a799eac122ae808bf5861076c98": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_2fe38c2d7f8a4342067a8369cc13abea": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_067505d17c7ba9267c8a63a56095b50e": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_4b99ad4f32b999c31dae0b336712e6d9": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_40d8811b63bebcbd8518af147a5182ca": "2211.03011_24247d37f026f5cf598677a503744c27",
            "2211.03011_96219f12e8affcb7232ae1ce8194a010": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_d1b7471ae69e2d41c841032960318c1a": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_56071f195cdddfd9923c4b7b7ff4aef1": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_f22bd46c14d31be8169a017d00dabc90": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_c3136ebc76d421caa0bbef876f7d52ef": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_fc016b8784169f2c59c7f79de4586740": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_19392b59f9ea5b984b7b2a848cc19c1a": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_813dab5948b2660c366609ef9e553f82": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_24c70463433a5c604f8ee239c5bcf954": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_a6fc027f9a86244f2ddecfe7e666dc15": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_8dc132ab63aae104455a69c8ea8ee285": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_35a51f0a4335da64ade2112c51c66454": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_dac6d8b5a0f415bc3168d24f0b603a03": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_59f57c86e6f2213433ead359cd04a688": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_d323ecbabc1b4c42f65c5292ce88bd40": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_7e6b25db27e508a13b13c5f1679a29b6": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_162bf53c57d52944a035564d394767b4": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_22e35f41f37ba2e4cdcc79dcf295be29": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_591148f4464556d53f2fba39a2dc9b98": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_be8fbae8a2446fbdd58239cc77e9e8d0": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_7647a63940aefd5945973b8df63b93f0": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_cef000b9fea569d14741f1a350c6a144": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_9121f9258a2b0636aceea9aa41a54bb0": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_59583a790bc35de30eb79ea67c534031": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_4c3dd8cfbab383824ebff4b76d9c5236": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_936a5232337a55713b04052771e2313b": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_be841edf9f04f8608363771c3109061d": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_3d44097f9bf20dd20b7c8b3e9361817e": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_eaa29129d6652f111ab82c7a812ea511": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_dede94acba140299b100e752612269de": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_4d3ac98052fd66b10667c3677b516a4f": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_fd34995973bfa0b61a9789d75c90d2ab": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_b5f82a018ca6c4110a41beda6de1ce7b": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_e767df161f7431f65a5f887b81dc3577": "2211.03011_d1b7471ae69e2d41c841032960318c1a",
            "2211.03011_a7a01aea3e9e9af61534a1b6840fed07": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_398d6ee7abbdccbe4c577b31a362325d": "2211.03011_52256f174893670edf771aa1a44bf816",
            "2211.03011_8e415a75a8ec56081548e3d2e95674dd": "2211.03011_398d6ee7abbdccbe4c577b31a362325d",
            "2211.03011_26f4d5eaeeaa5cc1002b704ebfff1eaf": "2211.03011_398d6ee7abbdccbe4c577b31a362325d",
            "2211.03011_a1c196aea3bc5cad3e1e8962d2a04e4d": "2211.03011_398d6ee7abbdccbe4c577b31a362325d",
            "2211.03011_5428493f6985232d6110fb56c2980526": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_81ab6b32de6ea3d8dfe0ec400818153b": "2211.03011_398d6ee7abbdccbe4c577b31a362325d",
            "2211.03011_7bade40785d1f5c1dbec8c0414eaac9a": "2211.03011_398d6ee7abbdccbe4c577b31a362325d",
            "2211.03011_ef1a4e152bddbe48fa5d5d1ca83b4e38": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_7db78a84b908ef06137488fa37eb894c": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_277df80f00e305c27e6bb8a1136416f0": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_bd5ca20389348d09baa7a85b3a513a03": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_0c2c3a1891f5a61118bfaa6d17c67522": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_5ebe78f9784d1a2166426fb735547d5d": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_325e6eb6b8dbd820dc0c7d6dde145eb8": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_b4749aa0d6162a74976aad8cc5753348": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_8e0fbe05d282501663f38440ed36b6e1": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_0372b17eed041163604d1e058279082e": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_1d8d039300cba2ee4e9c5b8894549b7f": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_88215cc0f620daf24187d51a265c667a": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_546e63d6df311d7a6973c6d9a3903236": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_32fe3ccb2688cd8ed3bb64fcc402d54e": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_859a5b6c9857f71088e2eb6467a735a3": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_b48878031e46029c717aa6cb1e1a1385": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_bdf9a230788dfb29813774fb832df6a3": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_39e59254269854d5b094be77c816d3ec": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_3d6540a21260bf2590c306b7e3c72856": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_52f751a6016239ae706f6d04100c761d": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_8526f8cb4ee60d1e9a71eda47afb3fb3": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_66d1c314806bbbe223e22edd7d4e55f0": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_c873231acb31acc74ac6a46142309634": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_68d030a234d18dcf903f0f8d0fb156f1": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_24bc09f1e24a313b92b0562e8d13a0cb": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_c6c5d00dba4aaa7f7149eb9ac88fa830": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_9fdeee3445df77f58f4cb140e56712d2": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_195d4c0acdc6d451d53234f708c9e27b": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_d0a5f807296a92dfe81b6b43c3392187": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_02552d3c116b8a96f9ff0001b6ae661c": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_9423752703d6e2f0975768fc84b3a6dd": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_146a65663a2131c31a29ae9125772ddc": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_13b5bfe96f3e2fe411c9f66f4a582adf": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_71d81097e8bfe7032b8756e4b879f402": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_936038870b40ed9d6db5a68bee9cecad": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_5e172f582b312eb9e56573ff44797f0d": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_e40ed3712112070efb6d4185b5280b1e": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_8a9e5f829b0f5c7fa692c3cd88091ef0": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_d249ff2188f5e41ccf986359eef9bdcd": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_7d2574c098bab09b4bd5b65cf25c8488": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_d271f1eb1356e5de86d37de79fafac55": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_732bf370673edb319836ce659247ba0a": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_d0a04bd81791a4cd044fb46317f9ab07": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_f642b8438f9d13b33a7946791b020af8": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_98119b89b3f1d17ca86d7a1ab1130c78": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_ce14e44c32596ba4db87c2d33335de0a": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_4612b12a95d0a1cc3e17e9c69624f36a": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_613a608459fd4fe707cce6302c3978ba": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_f450739281f61bd7e43cfca2dbe583d9": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_0717337655ee9686c573fe7fbb2747d7": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_a96a205f77a5afc3e54ea326e48f72b3": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_3d86ca69524d5df4591386a8da56ca9a": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_d550553c58511ccf7d17aef808551142": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_7c2467ac15a3ba24f33f983be748759f": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_6ddcec9b2452cd4677ba64fe78711b9d": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_717ef217f151015690a4e2a690518078": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_d1a13e6cf856bd6099a4dbc410284cb5": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_db565d7c8132621a03bbd9e075a0d8f8": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_2d54d6e00f85eb1bc81281cb4f5f799c": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_99cb3e6e64def1388fdd9aeba5bfbee9": "2211.03011_7d74f3b92b19da5e606d737d339a9679",
            "2211.03011_9cbb97584554b2dd4bf9d75401a89be1": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_9e0d1aa452e77caa5296cf3a4bcf23fe": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_98b533b7ef505c5b8eef8d711b44a0dc": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_121ef0feb34ca2862519a3566c00bf7a": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_caefd0d4340f2922c17d5ee7a2f18379": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_315ca4207029f01e59a0144b0b11e57f": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_46de29a73d2c10ee8c9d8e44ffbd942a": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_492d34a0afc04651cc29715b32f01cad": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_3f5031c06c68453ee47a34b2621c42c0": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_c51af1b0c8b08142e705ffe2a5e63235": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_22c208098b0e66e1963920348c94164c": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_e5faad8e760b842c06ee45b139da7c84": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_0f754ea0299114bc58d8b07b4ef72f8c": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_1dc6802e2d28f93a5b0f57b22b99dd65": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_6f645a269d5d765bac94484f2c5ce1f1": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_2eefecb3467363c670630dd30bc5df51": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_90e8e99ab47dccc0a64fa72bb81a5195": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_255ab85c5efeb5932e1f73e233b28f83": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_9f5ef09d358b42a80967cde42f8fcb49": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_3ef85492a3d5dff17f4c9de1ebd924de": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_7f94c46bda25c3736df59c0f10b94d16": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_3b4dc2ec20788fcd52bf03ac349c4343": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_bc0a7a44dc5dd0b6317e516b411c367f": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_ce7cf4a522724d83ea5f5fdbffabf49f": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_ca5e71ff26312c8dd3681233b5b40bec": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_645703ef7a13a4dae90ff8d1d37191f5": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_d45f8832ff0e2bcd79dfe600b0f6fe4d": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_484f4f68108bad902a7f30711afe7e94": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_966fcb0fd1083f4b39590964ff1931a3": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_cfea886fd374f551024e54bbb71bc11f": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_70835419281d4b593a54d6cba9f84b78": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_d757b502a6a80bc474f8b0a0e6090cee": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_a0db68d122afd3aae68cc68478eb90dc": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_9371d7a2e3ae86a00aab4771e39d255d": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_79d20a30e742e10a127510c69fc848c6": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_3809d244bc6c35b0ba31cc125c5e54b9": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_2cb1386d86ed46020f58d458d06d9be1": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_a2106e1ccc0d4bef15470d52986adad6": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_841a0ea8f1b740240917f28f5408c4a2": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_24bda4238d181d34304ee8c5fdaeb3d3": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_cc5eae04f9bdddfa6a76c58bb53d7015": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_5da618e8e4b89c66fe86e32cdafde142": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_c25579a103c8ff37432da4b319d17a8e": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_7d887bf4150670c319d5bf8dbb03bcec": "2211.03011_9cbb97584554b2dd4bf9d75401a89be1",
            "2211.03011_aed402c3112b4749a9a98a72cbe9093d": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_bc6414bfcf017fb6cb0280a74543e00d": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_1156609d9e793b0c548dad47a6d3044d": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_ad4aac053ee32a824db54c9930d6946c": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_a4dd8d5f9eeebbf3e760fd64ab2981d4": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_cac412793dd781c5b1c051a67aee658a": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_d9cb76989fcf52818503542bacfe9a5f": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_58c0408c0cd9a5288c7ebf846b428192": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_1cf580f5834a0bcbef1d2ad832874207": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_b8caa259399b8c73e2c53b5921074b70": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_4e2426db6cca5b1604b12620bd8cba64": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_3cf1a0604aa976d20bce2c80822c6759": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_0b39cbb4299716812e470b1d12e708b7": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_89bc1f68e6c0f8f15bbda15134d2466a": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_b9f6aaab9e68f61d8a357a6c68c80695": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_25f8fea261e42991fc947198f3db639c": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_aacd61784ed97251e71d57973fd2ead9": "2211.03011_25f8fea261e42991fc947198f3db639c",
            "2211.03011_28ec91b78cf48ce017bb38ed6a0c5dcd": "2211.03011_25f8fea261e42991fc947198f3db639c",
            "2211.03011_6d0a721fff7b176b76feb35b6eddd697": "2211.03011_25f8fea261e42991fc947198f3db639c",
            "2211.03011_e4a4680c97e576b0d9cdbea54ff1d262": "2211.03011_25f8fea261e42991fc947198f3db639c",
            "2211.03011_e80ea5ab2e33f86f018e188a02772efb": "2211.03011_25f8fea261e42991fc947198f3db639c",
            "2211.03011_c7f3ff975da2c9150c89676bf44e5a41": "2211.03011_25f8fea261e42991fc947198f3db639c",
            "2211.03011_41fdb10829ab1f38ffd540572b32869e": "2211.03011_25f8fea261e42991fc947198f3db639c",
            "2211.03011_434d0c3e391b8519b28163b272a6d076": "2211.03011_25f8fea261e42991fc947198f3db639c",
            "2211.03011_ef9987fe6a423971ca9aa7992de38f86": "2211.03011_25f8fea261e42991fc947198f3db639c",
            "2211.03011_2763f1eb3252b2372a10cc32bcee7d2d": "2211.03011_25f8fea261e42991fc947198f3db639c",
            "2211.03011_acbf8ee04ec594ac1494123d9829cc85": "2211.03011_25f8fea261e42991fc947198f3db639c",
            "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_51cde8586d2e4d90c83c8732383e5cb8": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_57f20561a25bb52b78be2d0b6b4cb79b": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_fa48d894f155c7cc2e74a5f5389117c3": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_919824fab09a3faa2301012e7e6a9033": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_d20454f75a133b87c6351eed3d4bccc2": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_57cd7ebf89f5b756e648a48514a9d7d3": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_c91091e68f0e0113ff161179172813ac": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_eb5105aae667cee681195170db93d37b": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_ed80232bf64f1e0d3094d664092427da": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_ee4d73072b2d7e2576dc3da99498f5f5": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_731b613147b12bb1fa7d7e3433745f3c": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_cfaebeb9babc92d4af6917565c2b726e": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_5fffa36f7993121d3bfbc67f19daf5e9": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_9a639daf68b7281f5bcd329f8d8a7812": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_7e334ce7e9d3e2ebce8d3881692600f2": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_7ccca27b5ccc533a2dd72dc6fa28ed84": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_d9b2eaacb8de6c5e1da8dcdfb56d9d25": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_ec4d61651b78865cda2d5b9b3669a481": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_d54c4beb5cfc0777ca7baa8c2bd31c82": "2211.03011_0fd9e1dad26816c4d8c7aa2966ebca81",
            "2211.03011_ce325d035fed6f391fca48c20d4825e5": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_0cb6f451df2d212704cb9cb1180977cf": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_4da4f9e1fe074a7d03e6dc220923b005": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_a2d7c81d737930d696eaa4704003d47b": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_6ddbae53c8d26c846975959d2e5cc494": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_b9191bbd80124b88b90710745a264968": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_61d6a625abf75e8caf5bdefa244318bb": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_be4ba36f97e08e5df0ce695e19659595": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_5e6332f347c8ec7475e34b73a40a4284": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_0bb1fd53ce422e3e84c83d91eb79bbfc": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_6ff130948188fdd3c3e9bef729ecb38e": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_9b2a2a372ff69d4e4a17b12c5232c3e5": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_ac0ee800ecb7c8c0703f1e8e1a1cd757": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_384fe64074bac3db45a47a22b4b2d254": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_4909dd2826edd32ce20331fd4a561ddd": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_4b668622ebb4f8b5784a27631697b9a7": "2211.03011_ce325d035fed6f391fca48c20d4825e5",
            "2211.03011_ca5b9bcf1c7d28ea5571305a1422be3c": "2211.03011_aed402c3112b4749a9a98a72cbe9093d",
            "2211.03011_500c307d9281d5ff38f0296446fa8467": "2211.03011_ca5b9bcf1c7d28ea5571305a1422be3c",
            "2211.03011_986977c7a963430cc8bbe314524d400b": "2211.03011_ca5b9bcf1c7d28ea5571305a1422be3c",
            "2211.03011_021a6c0ad9af7c5a4a6e87b5174bf9e8": "2211.03011_ca5b9bcf1c7d28ea5571305a1422be3c",
            "2211.03011_a93dcb9142c1e0b277290b5f32880525": "2211.03011_ca5b9bcf1c7d28ea5571305a1422be3c",
            "2211.03011_24587e53567bba18d8324d72024b48c9": "2211.03011_ca5b9bcf1c7d28ea5571305a1422be3c",
            "2211.03011_824350fe9ed8fd5235f880f0dd103ce9": "2211.03011_ca5b9bcf1c7d28ea5571305a1422be3c",
            "2211.03011_080b42ac97dec586f542419a3c4aeb68": "2211.03011_ca5b9bcf1c7d28ea5571305a1422be3c",
            "2211.03011_773fc6f17c333cb368c771d40121a558": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_7d97df3f3edae22bf149336b8ed858f7": "2211.03011_773fc6f17c333cb368c771d40121a558",
            "2211.03011_658277e0f80fb180651ad4bd9d20fe24": "2211.03011_773fc6f17c333cb368c771d40121a558",
            "2211.03011_ee3ac473cc6382e17f5c2aeddeba27a2": "2211.03011_773fc6f17c333cb368c771d40121a558",
            "2211.03011_a3b055404a1f6d44f22bad90cef557e2": "2211.03011_773fc6f17c333cb368c771d40121a558",
            "2211.03011_e4a81898673ac335bad4c8f88540120c": "2211.03011_773fc6f17c333cb368c771d40121a558",
            "2211.03011_bfb8c3827f197d6dc40f28fd51415064": "2211.03011_773fc6f17c333cb368c771d40121a558",
            "2211.03011_93c90e91a476e5383aa9eaa78da1eec4": "2211.03011_773fc6f17c333cb368c771d40121a558",
            "2211.03011_6a591b6d54756aaedfd8902e6d2a0f84": "2211.03011_773fc6f17c333cb368c771d40121a558",
            "2211.03011_31d94d157b3a5c5160f2a9f9581affa7": "2211.03011_773fc6f17c333cb368c771d40121a558",
            "2211.03011_1192d8cd240aab03b0876dcdda911ca9": "2211.03011_773fc6f17c333cb368c771d40121a558",
            "2211.03011_f0a637a3d2b7138c1488ad327f4d6a77": "2211.03011_773fc6f17c333cb368c771d40121a558",
            "2211.03011_39ee9dae92bf620136f031b9afa38d34": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_b92673fa6044d0a8135d08edca87a36d": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_8bd4df4efb7d8b5a677e3222400157ca": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_a74892bc7c77dcc4bef89e8edd3a7059": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_0d63c5ec809772715876b078eee0091e": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_8ccb58356c2caa4faeaaa7a2dfb34b3f": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_8bf8854bebe108183caeb845c7676ae4": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_fe6b54052cf5f0165fd73650d28a1d41": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_d021765abce6a48bf9f1077b40fb264a": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_86f39b2a7975cd9082784bb4059f2fe1": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_54143f14e12907fed4395b8b1491d47c": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_3c9c77ddc42d6926a86cfc36d3cdc10e": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_86b748deecb350b4b5f9388b7f8b8280": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_6eb1c85fff16ab5f7b7716c538cbf8ce": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_c4e905c4210a004675ca4048b5ee7e92": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_e8a79a65a15d84733a4600197d0c35d1": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_ce852edfc885f2ca59af98510114d461": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_677a9cf23f254a7c5ebbabf077f9ac0e": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_e064261950ecf935fbeba65877cddaaf": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_f6a9f7a6c5c03e1b7454b94cb13db5cb": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_d97c3e4bbbcfeeac7933855b921bb9cf": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_4b9b66fe6f24dfd1732b4359ff267111": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_16cc25c6a719cf74c31d48ac8840462a": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_d436b65a6933eae9076a4d4350801edc": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_f873b5e9f7a61df11b19e1ffbc2a4f4c": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_1a5ce6b6a320d359b0d546e6f9f9773f": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_69c9aef16cd270653be97ed28c735952": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_1ded1248b9f0794a801364bf2a647321": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_4ee05d5da2580193cf0f42bfaf48c3fe": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_944c5c9a90388fa9c2a894990fa74e58": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_ed2e81d6a8547f15cc1a3a462733c44f": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_98b49102fc0e6de5b99f7f598ab68fe5": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_13e9ba4585909339169f64155b4650e4": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_a632ddc4011400c5e74408c91dbc0b28": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_4a2d50fec399d204341797a80319353c": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_cc54ea38b33d44f855f5bbf102c4d420": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_0794f1b030ee56a0e25d788c17e6e077": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_cccb4f5af2e9283cc9996a00a4681c0c": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_8be1ccec7187adf55ad901b04bfc7787": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_917fbf142d32d2d5a979d4bef6745cfc": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_5e62a9a96416862a489627f3c55f61f2": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_2127163ede71545238ad6a2652a9256e": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_f417149572845e43e4f79d520f06c8da": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_1ba69d9d958607b4ee081f8e3b2ce120": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_60d9fae38ddd03e1dc50c17e2126d079": "2211.03011_b92673fa6044d0a8135d08edca87a36d",
            "2211.03011_d06f4cba5a050035d0e49f79bd6a4373": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_535e171f7ba172f8a61e7c49b7fc09eb": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_d9157b4d7ce70264082aaedb867b47c7": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_72377cfd1f4bf9962dfa32598e511760": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_d477221d971f5913ffd804481f1d4650": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_df274248663c0fb8accd88d6deefea3d": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_8d1b81a8829d34955240667c7c273343": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_9ca358ff8623057bedd517e82def03d0": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_a776543a0ba8830f8396028ce9d1cec5": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_1f4771f80ff91af97f4b61509e0b71fc": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_9389fb3b9f82f3662a1d76a8ee13e38e": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_19657811e726423a6546aafcc3d4a003": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_322f116eaf49326fec6cfb1b744a8d97": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_afe34d829e99c5612f9535877c8061f7": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_73d6dfd5521beeb40acf2ed2d8cfd657": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_f8b1019adb06bd0308f04cc422936ad7": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_5f121f43705317418093b20f6f70ad5d": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_fa5e591ea54c997568c33b91a7b9aca3": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_2cdc4cee7115b69f95816a6a0457b31e": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_9eb407e0bb4332a376f6462b13143b54": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_96db65ac527a43b8dd98ec382395b9bb": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_785136ac1f4a524af0413c343cf2d72b": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_756b2946392459582fdadf766f3917d9": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_9473daf474f8d39427c1a99ddb5ad626": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_ed2b5c0139cec8ad2873829dc1117d50": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_2774fcd9e3cede89bcc58ae24bbddc24": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_f4fec29c3f53f19271431829c87b8e48": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_21838ca06931953c741fa06e65b2603e": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_665dbf1aed958225aa4be826cb288929": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_ba4734a904c72edc63fa9e087049df39": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_2103f85b8b1477f430fc407cad462224": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_c7286d1545e35f3149bf98e412db3a81": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_bc40ad02d94f280570b850a7d2c747af": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_bc710412ced8008611147422051dfe09": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_4226060c0be497823941c78ce89e7a1e": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_e76736666c67e09d3e94dee124f899d8": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_3327eb1a8382684f0142b74c454f15a1": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_4fdeb1aa7e16d10fa329e9c1c2df7959": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_ca7d99c583fefeafbce43908630d97d1": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_75b7aebe6541ffc796947a9bbbb20a04": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_b062453f99d105dbc2cf476a2c3234d3": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_2e5fa8665c8f41f8dcb908d68479cffd": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_73f0dd8537e3406985885378763bed3c": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_e2c809a5dfbb46d6dfd8195988dafd19": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_19f839dd2c511a907838e83605ac2830": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_910955a907e739b81ec8855763108a29": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_19726c09036d56e620f92d8c5d4a5c03": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_d2884efee751eb4746b685827c7efcd9": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_222a78407bccdd31b5fa64b9e2c3bfaa": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_e9491f4ea013494b1c8504e94b4cc312": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_d48a5cda4bc91670bf431856c381ab40": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_079515917e0aae2a9b57ebaaf669c55e": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_cd73039b4a54dbac1e114f0e78d7cc42": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_87d5e6b6727c94fca50784aba9bedc57": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_cab6405b9ab9f41bb935f2b67e3e8aaf": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_5a63dc89e829a9d831818a98112de659": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_62232326af84bca8d3da9ec411cc326d": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_1c38f6b4805c23deab2a606a3905b5af": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_66da95bbed3bacf0d79d5ba7abd41b83": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_7538e62486f5bd78e6744299137b0a01": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_c4cf758247d80a1f0f5c9bb6287130bd": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_36199d5c7ab66f759ed0835fd48514ad": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_eeae1fd2d897ab2c80b592627bf73d56": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_9d75e6bb163a38ad36645c0ad2f8c46c": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_1edc5ddb94cdf2a35f2fa3d0d14c3a98": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_a73d04b97e44d5f99381502779d5d14a": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_d8e85609d4175e96b4f783e2966de64b": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_9bfb93aa9c186a1ee365e92efd7ccb7b": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_b82a44792a379a54952262c02663dd87": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_f1d590bd09cec61cd52a07d575f88581": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_6ae9887b847037e7b8cf82e28a76dce8": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_3f219d02f044cec6f68795b333434b0f": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_0e85b52364bd436e47a9e7e642826958": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_56bd379819b99029b1506ebac27e89e6": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_c060e9cc3d17a8a99831cb917e9d33bf": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_87d068f811c538542044bcb811fe23c1": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_f2621e3e1ae8221fd46c19b35e3a97cf": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_4612f9429bae7c8ccf75aa3f14ca550e": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_e41753014dc17e96078f6eee90544601": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_0fcdfbcc37c8392c2fb8861040c776d7": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_10e1c37636645264eba147561b05ddbd": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_f9b5ac549cc7f3b93750bc3af617155c": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_0f4ad8ed6f8abf5056019865b8b13f87": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_9ab6fe894c4a852325e0dc713134226e": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_7b799b102f230848294be8524e5fb7ff": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_56d564d0ed00a23301345d4636534f86": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_b2a662b51de89196af0915bffb787b77": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_4d2fc01b05a2b35fb61dfb81433a527a": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_3ccbdd710d24730fe06c06afb1e80a5e": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_39666dbd4111abcd9b0dd21c50ee996e": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_669ffff679790ef8ff6733609831cecf": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_72f1e97389e53cdefbb77e0e970b9b0e": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_458868aedfce22e929b9f3c3efbf2016": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_699abb9dede050251eea809f484be566": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_bd0d15ad85aae0936df7d6aa65d88602": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_32993d595757c83f9f92b5eb9d3d93ed": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_be57d2c97b32d70f1d50ce19bfb83248": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_f8766d08ecc308753a297be83eebbaea": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_8c8304bf1f58fa8c3edbb8efecdbfa61": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_53e5aa2c97fef1555d2511de8218c544": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_d2697472a1735c3a68f7a74862289136": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_f262374752801d7684eff221bc876f3b": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_47d4acab76b606a7670544a5bf36cb9e": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_051bcd9071057802751df32ca8b158d9": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_452b2fa0bc5d0a706984e3cc902586fa": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_19319e324278cf6dd5d5aa0b22c09f85": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_adbcde67e9f9127b23cefe6312e2e547": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_7f2e5932b931b595b5fea90cc037049c": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_b3be42135b8f3315876e267fee9e28a8": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_e9787690d9de6e4470974bcf7834932d": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_7bcecb93918e70645ea180ce68fbcd3b": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_77cc9437985e6e190575d1d1e55a51ff": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_b5210c80c2d00c6720fe76e9d96d1df6": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_ff881d4e7aeac92ee7fbbdd1325a3837": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_603812bcd528b93b0b18a12effbcf215": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_d4814ddb1b4a0cd34f60beecc3aa458b": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_f09cc7ee3a9a93273f4b80601cafb00c": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_8d4abc8d8b1e0f573e147378885fe62f": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_1efe4068b7e779165362586541a26c62": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_41f5e83f1d7ec2ffdaf23ece684f8924": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_a9c40b8b3e5fa771d788bedacd5f54a8": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_00b88f1f58d22e9d8bb3f383e5073fe2": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_b3479a7f04bb1e7aabbb93a1ca03029d": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_13a40798b93cc7ee8b6fc0d3422289bc": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_2088824e0896f5c317c759f396cfc290": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_25273871ddc86384d21392e2e356cc69": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_f07195ce9b03d6b8d6656f224bd0d982": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_1013056b24ebf89b67337481ec6d9dae": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_c745b9b57c145ec5577b82542b2df546": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_f2d5c59e104aa3f90a07fbbe0d6db0d8": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_d302578609975743a904e9b003ae41e3": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_1f5163c012fd300f41e2cab925f3b91d": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_1a62f6d03b6f8061a7ef1e5bc9a9c32b": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_76aaa888420eb794a937310f7be7780e": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_1adb0ba246f4d947ee41ea97cfbf7340": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_3234e75097003865f3857e41bc9abf15": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_0d6a063c0241a5e25a32cfbba87f4a1b": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_63ab4bc006c5f69cd6a3a5980ba7a043": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_ab71e51680d9edd6b0fcfc846a9606ca": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_e3ae5a58c0fa42904473ed0cd53f09e3": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_4bd358bddfff89932cadc46b3d11fba6": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_18a9755b0b7eef24c25a587dc5625221": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_e23b5c8ada47eefc23ca517db43c5b8c": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_137af169c322965991c135c29b0371c4": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_c3ba632cc26c78095cb444b6879a43aa": "2211.03011_d06f4cba5a050035d0e49f79bd6a4373",
            "2211.03011_3fb2719f6516f6410d50804082467a97": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_7f55c49228839915b376896902afe230": "2211.03011_3fb2719f6516f6410d50804082467a97",
            "2211.03011_d026936df1996241d1cda58970b2a242": "2211.03011_7f55c49228839915b376896902afe230",
            "2211.03011_5bd6f68f7260a41ae481185f27b1a7da": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_48ea2f23d8b3e4714b51feda8aa9f9b3": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_164c12257f29dee73cf9dd553d4afb8b": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_085b7f84e37d0ce1f2729b2af96d9f25": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_418c2ed6f2e77230b72281894b202318": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_05a81dbc81b020811e13babb0767785d": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_e2820bc65a9849d9fbbd69047177475c": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_7e762fbe4c626a825b191c3ef617dca0": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_289c3eb783f6bb127d6be12d328537eb": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_88f81460e3ff4946fd661ed481e529cc": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_0f0bce3fae33d68b11fe1601ee8d9372": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_55c74e383b8a0cacf47508265823cdaf": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_76329327e943ed2b24816e56c23f107d": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_43238f406a7cb5a1a90d78abc6bd1f0f": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_24224b0661e579850080ee3c9b7afcfd": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_b6262e2cd6967f60b90a5b1c905917cd": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_b19efe18c84e5887c52c1c0fd15160eb": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_af39428cf24465e389a9341f85f7f6a8": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_911990acc31d329408eb7920ae7d53fc": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_2765bb4d5d327c70b5805609ae608a2c": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_96149022da269cd1a90bef84ee328b25": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_c209a5e9d81a0b7cb7d9652bfd828fb5": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_56f60b6e6263029f30aef0f18ff8aeb1": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_28ebe2b1df526c25df0a200b80fe626e": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_7e9a94dd421e5c141274db572204c3ae": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_6e64174cc720cf11e2c6a1315b8cbf18": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_01fd231593eeaeb8b947f512e7cdab6a": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_8ca0bd77c6ab0a0516cb4056698359dc": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_02f15ff26a5b8283fc9f5e6a2300572d": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_a9edd8d411ea0df4213c21fc0e242c0c": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_532b8955f7633ab752fb5da43aac3472": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_a1a7e3759bcaee97cf59d51d5de8cd6b": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_f298e81134db96e2537df2a820a98dd1": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_42204961f266ec22fbbeb497a0a0f188": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_6a68c16bcf90c80245ebcebfa8e1f40e": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_22cff9efeba6e6ba92a891244a23c066": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_17681253939a23b1833914f8bd65bbc1": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_7021bf33e44a17601d724c06e342c595": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_b00ae41c141b44d1d2639d06d8911a5d": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_86d21ca84519a4386b3cd927084a246d": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_5e0000888d0698281eeec2a2c549df21": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_90264925fb137831c8f410cd14c75cff": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_94e3944c54537cc1ca609a603ce1fb08": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_63aeb650a7c5ec0f0aa37f4150f40ce6": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_60580bbbc3dfc30f57283aa0f16cabd5": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_0e51a2dede42189d77627c4d742822c3": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_17fc4fc8d2910ca193ac5f142651a09c": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_92305326663866c39d8dd15c5dadda22": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_2e38f0e620cb90ab52a194e06693b29b": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_8dcf91ec0259e6d92e274d16129c40f2": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_0860e9fe49979d1d14b12f0769049814": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_ea7ec500a22dca4543fec2fa09ce8d8d": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_b9ec59ef719f08cd6a8c50592284c623": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_2fb8adcc63192871e19bcd9880a553cc": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_5e77827bbab4835edfc1e4ab5218f6ce": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_6506476a422da8ee0f4b6b805e84021e": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_cbfb1b2a33b28eab8a3e59464768e810": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_7f8091df5462faa7a88c312b2f8bdbf5": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_174d0b6d6cda5454b4508f9faf4e709e": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_698dc5b3d8e55cff953b65956038b372": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_452930616271c680f24ada7e65cabad8": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_6787dbdc5cbd959f828ea4a0ce2ac2d8": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_3f9f07a1c939260f63f92713d628e9fd": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_f6c0cc6f187210373d69a85765e50487": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_f90c570aeadf7e5563db23507e9f9b3a": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_bca01f0a544f09b42c40cb0a6992c322": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_5e96330591cb970b741f4c83d4fc9d4f": "2211.03011_d026936df1996241d1cda58970b2a242",
            "2211.03011_82fd2ac9221b670412337f84a176d13f": "2211.03011_7f55c49228839915b376896902afe230",
            "2211.03011_e928200d7fc522f1ba51fc57ba2e91cb": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_8fff84b6aaf52c89c49fda6ae6147794": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_f243e6ae20381498736c70fd13bae939": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_058bae3b0862942f5cf64e200aeab1be": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_4bc58beb582698fc642082053c0cd6e1": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_7da75f4e61cdeabf944740206b511812": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_324ecc9541dcfbbd88f2faf8ae698239": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_78ec2b7008296ce0561cf83393cb746d": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_67438bc80ded7aeafea4ef32442850d7": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_91288ff048149763c39a4e492c17285e": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_ba184f232e44447b0bbbec9432548000": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_1851854038db5f7c973768f11874b954": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_3398dbde4d704eb321fb9f4a33168d67": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_2948d94fa881d94caece8e30a4c21e55": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_3f1b8315b62f0d9237cb8d0a8782699c": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_8b07999d33807f030ea3232a15ecb183": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_580820984f61e5c8aa9a6fdcc36bada5": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_783d4f6985b9977748d45f0547ada824": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_decdcb505840847086d8f8010de9e4b3": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_ba2a327845aff1c11adc8445815fdcab": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_098666a03f4aa6b01dd283248689d5b2": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_62bf920bcc27eabbe57568fac305d4c0": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_312ea82d78b3b15156791660298921b9": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_266a711a23c5b0f8b06959aab3c5aac9": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_7f580809c491e6907c1703f36a1cde0a": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_bdefa891599d79c18af0aefc274b100f": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_3a07fbc35a5479784758bf25daac8fe4": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_c3d2ddd3f25f150ed093547da6e1aab8": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_e3a78400d5bd6ffbb5ef3477808b94e4": "2211.03011_82fd2ac9221b670412337f84a176d13f",
            "2211.03011_1c8ac169bf87c6204ae5b545c02dacbc": "2211.03011_e353dbe42c8654f33588d4da0b517469",
            "2211.03011_a30014a1b3a1c6ab9cc18a45def9f22c": "2211.03011_1c8ac169bf87c6204ae5b545c02dacbc",
            "2211.03011_551388036a08752c7fb78efbb27a659a": "2211.03011_1c8ac169bf87c6204ae5b545c02dacbc",
            "2211.03011_d66bdd827c54a25ad996b96cc5496cfe": "2211.03011_551388036a08752c7fb78efbb27a659a",
            "2211.03011_c8e68787f7ba40a1ea9b9b3e4fcdea45": "2211.03011_551388036a08752c7fb78efbb27a659a",
            "2211.03011_6811defec4c4bbd8ac5e5b036e615ef3": "2211.03011_551388036a08752c7fb78efbb27a659a",
            "2211.03011_0b9375a09ecbc0aae573a18b083b113c": "2211.03011_551388036a08752c7fb78efbb27a659a",
            "2211.03011_1f86f5858875d61f1306ada5bc0f8d74": "2211.03011_551388036a08752c7fb78efbb27a659a",
            "2211.03011_4435f20b008803676186f05173d25165": "2211.03011_551388036a08752c7fb78efbb27a659a",
            "2211.03011_62c04cc2675ad9467387f6a3cd6baf64": "2211.03011_551388036a08752c7fb78efbb27a659a",
            "2211.03011_cb78d72f87a301b96457f08965121cc0": "2211.03011_1c8ac169bf87c6204ae5b545c02dacbc",
            "2211.03011_980fbaaaa04b39d7258e21f485b5ecbb": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_8f703f4283457e67023e6e6b0d44214c": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_defc2ab3051af1fd3c386c26b1d40073": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_d7bdb41128d7733e9731374109575391": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_00bbd95ed734afc6f9d81774219f4867": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_d0706c01f3e3846186c48b71406207b9": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_de52e37d0eb3e4b38f6a9722c5d6c970": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_5ffd88956a19d4f2dbcd1d18aeaf474c": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_2e8fe082d4f69d6565735199779a33f1": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_7e2289f8dabff6bc7fcba9b7135d3bad": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_d5292aee9173415a22b860aea5545dee": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_7ad602f1f41b6b9f089cd9fe5e29e689": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_0ea97206fd20c6d4676200f457999e9c": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_b37b381f750749293783aa21417dcbd7": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_854d95bd42066e7fa25631e5b0b82adb": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_cd2a043a3a57ae49378755d03b24e47b": "2211.03011_cb78d72f87a301b96457f08965121cc0",
            "2211.03011_393256c911347ec6d0af757f5c31335e": "2211.03011_cb78d72f87a301b96457f08965121cc0"
        }
    }
}