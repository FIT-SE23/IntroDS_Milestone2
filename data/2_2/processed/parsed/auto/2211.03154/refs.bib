@inproceedings{nguyen-etal-2020-bertweet,
  author = {Nguyen, Dat Quoc and Vu, Thanh and Tuan Nguyen, Anh},
  title = {{BERT}weet: A pre-trained language model for {E}nglish Tweets},
  year = {2020},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages = {9--14},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{ma-etal-2019-domain,
  author = {Ma, Xiaofei and Xu, Peng and Wang, Zhiguo and Nallapati, Ramesh and Xiang, Bing},
  title = {Domain Adaptation with {BERT}-based Domain Classification and Data Selection},
  year = {2019},
  booktitle = {Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)},
  pages = {76--83},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{liu-etal-2022-p,
  author = {Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  title = {{P}-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks},
  year = {2022},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages = {61--68},
  publisher = {Association for Computational Linguistics},
}
@article{ryu2022knowledge,
  author = {Ryu, Minho and Lee, Geonseok and Lee, Kichun},
  title = {Knowledge distillation for bert unsupervised domain adaptation},
  year = {2022},
  journal = {Knowledge and Information Systems},
  volume = {64},
  number = {11},
  pages = {3113--3128},
  publisher = {Springer},
}
@inproceedings{blitzer-etal-2007-biographies,
  author = {Blitzer, John and Dredze, Mark and Pereira, Fernando},
  title = {Biographies, {B}ollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification},
  year = {2007},
  booktitle = {Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics},
  pages = {440--447},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{wuebker-etal-2018-compact,
  author = {Wuebker, Joern and Simianer, Patrick and DeNero, John},
  title = {Compact Personalized Models for Neural Machine Translation},
  year = {2018},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages = {881--886},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{lester-etal-2021-power,
  author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  year = {2021},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages = {3045--3059},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{karouzos-etal-2021-udalm,
  author = {Karouzos, Constantinos and Paraskevopoulos, Georgios and Potamianos, Alexandros},
  title = {{UDALM}: Unsupervised Domain Adaptation through Language Modeling},
  year = {2021},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages = {2579--2590},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{strauss-etal-2016-results,
  author = {Strauss, Benjamin and Toma, Bethany and Ritter, Alan and de Marneffe, Marie-Catherine and Xu, Wei},
  title = {Results of the {WNUT}16 Named Entity Recognition Shared Task},
  year = {2016},
  booktitle = {Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT})},
  pages = {138--144},
  publisher = {The COLING 2016 Organizing Committee},
}
@article{patel2015visual,
  author = {Patel, Vishal M and Gopalan, Raghuraman and Li, Ruonan and Chellappa, Rama},
  title = {Visual domain adaptation: A survey of recent advances},
  year = {2015},
  journal = {IEEE signal processing magazine},
  volume = {32},
  number = {3},
  pages = {53--69},
  publisher = {IEEE},
}
@article{radford2018gpt,
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  title = {Improving language understanding by generative pre-training},
  year = {2018},
  publisher = {OpenAI},
}
@inproceedings{saenko2010adapting,
  author = {Saenko, Kate and Kulis, Brian and Fritz, Mario and Darrell, Trevor},
  title = {Adapting visual category models to new domains},
  year = {2010},
  booktitle = {European conference on computer vision},
  pages = {213--226},
  organization = {Springer},
}
@inproceedings{response-sigir-2017,
  author = {Yang, Min and Zhao, Zhou and Zhao, Wei and Chen, Xiaojun and Zhu, Jia and Zhou, Lianqiang and Cao, Zigang},
  title = {Personalized Response Generation via Domain Adaptation},
  year = {2017},
  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages = {1021–1024},
  publisher = {Association for Computing Machinery},
  isbn = {9781450350228},
  series = {SIGIR '17},
}
@book{mcbook,
  author = {Art B. Owen},
  title = {Monte Carlo theory, methods and examples},
}
@inproceedings{karimi-mahabadi-etal-2022-prompt,
  author = {Rabeeh, Karimi Mahabadi and Luke, Zettlemoyer and James, Henderson and Lambert, Mathias and Marzieh, Saeidi and Veselin, Stoyanov and Majid, Yazdani},
  title = {Prompt-free and Efficient Few-shot Learning with Language Models},
  year = {2022},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages = {3638--3652},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{tang2020unsupervised,
  author = {Tang, Hui and Chen, Ke and Jia, Kui},
  title = {Unsupervised domain adaptation via structurally regularized deep clustering},
  year = {2020},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages = {8725--8735},
}
@inproceedings{ramponi-plank-2020-neural,
  author = {Ramponi, Alan and Plank, Barbara},
  title = {Neural Unsupervised Domain Adaptation in {NLP}{---}{A} Survey},
  year = {2020},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  pages = {6838--6855},
  publisher = {International Committee on Computational Linguistics},
}
@inproceedings{drift-2021,
  author = {Wang, Zhuoyi and Chen, Yuqiao and Zhao, Chen and Lin, Yu and Zhao, Xujiang and Tao, Hemeng and Wang, Yigong and Khan, Latifur},
  title = {CLEAR: Contrastive-Prototype Learning with Drift Estimation for Resource Constrained Stream Mining},
  year = {2021},
  pages = {1351–1362},
  publisher = {Association for Computing Machinery},
  isbn = {9781450383127},
  series = {WWW '21},
}
@inproceedings{ye-etal-2020-feature,
  author = {Ye, Hai and Tan, Qingyu and He, Ruidan and Li, Juntao and Ng, Hwee Tou and Bing, Lidong},
  title = {Feature Adaptation of Pre-Trained Language Models across Languages and Domains with Robust Self-Training},
  year = {2020},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages = {7386--7399},
  publisher = {Association for Computational Linguistics},
}
@article{bilen2017universal,
  author = {Bilen, Hakan and Vedaldi, Andrea},
  title = {Universal representations: The missing link between faces, text, planktons, and cat breeds},
  year = {2017},
  journal = {arXiv preprint arXiv:1701.07275},
}
@inproceedings{schick-schutze-2021-just,
  author = {Schick, Timo and Sch{\"u}tze, Hinrich},
  title = {It{'}s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
  year = {2021},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages = {2339--2352},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{lewis-etal-2020-bart,
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  year = {2020},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  publisher = {Association for Computational Linguistics},
}
@article{reposnse-per-2022,
  author = {Zhang, Wei-Nan and Zhu, Qingfu and Wang, Yifa and Zhao, Yanyan and Liu, Ting},
  title = {Neural Personalized Response Generation as Domain Adaptation},
  year = {2019},
  journal = {World Wide Web},
  volume = {22},
  number = {4},
  pages = {1427–1446},
  publisher = {Kluwer Academic Publishers},
  issn = {1386-145X},
  month = {jul},
}
@article{saunders2022domain,
  author = {Saunders, Danielle},
  title = {Domain adaptation and multi-domain adaptation for neural machine translation: A survey},
  year = {2022},
  journal = {Journal of Artificial Intelligence Research},
  volume = {75},
  pages = {351--424},
}
@article{wang2018deep,
  author = {Wang, Mei and Deng, Weihong},
  title = {Deep visual domain adaptation: A survey},
  year = {2018},
  journal = {Neurocomputing},
  volume = {312},
  pages = {135--153},
  publisher = {Elsevier},
}
@inproceedings{wright-augenstein-2020-transformer,
  author = {Wright, Dustin and Augenstein, Isabelle},
  title = {Transformer Based Multi-Source Domain Adaptation},
  year = {2020},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages = {7963--7974},
  publisher = {Association for Computational Linguistics},
}
@article{min2021recent,
  author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heinz, Ilana and Roth, Dan},
  title = {Recent advances in natural language processing via large pre-trained language models: A survey},
  year = {2021},
  journal = {arXiv preprint arXiv:2111.01243},
}
@inproceedings{yu-etal-2021-adaptsum,
  author = {Yu, Tiezheng and Liu, Zihan and Fung, Pascale},
  title = {{A}dapt{S}um: Towards Low-Resource Domain Adaptation for Abstractive Summarization},
  year = {2021},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages = {5892--5904},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{cao2019learning,
  author = {Cao, Zhangjie and You, Kaichao and Long, Mingsheng and Wang, Jianmin and Yang, Qiang},
  title = {Learning to transfer examples for partial domain adaptation},
  year = {2019},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages = {2985--2994},
}
@inproceedings{he2022-paralleladapter,
  author = {Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
  title = {Towards a Unified View of Parameter-Efficient Transfer Learning},
  year = {2022},
  booktitle = {International Conference on Learning Representations},
}
@article{iter2021complementarity,
  author = {Iter, Dan and Grangier, David},
  title = {On the Complementarity of Data Selection and Fine Tuning for Domain Adaptation},
  year = {2021},
  journal = {arXiv preprint arXiv:2109.07591},
}
@article{hase2021language,
  author = {Hase, Peter and Diab, Mona and Celikyilmaz, Asli and Li, Xian and Kozareva, Zornitsa and Stoyanov, Veselin and Bansal, Mohit and Iyer, Srinivasan},
  title = {Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs},
  year = {2021},
  journal = {arXiv preprint arXiv:2111.13654},
}
@article{hinton2015distilling,
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  title = {Distilling the knowledge in a neural network},
  year = {2015},
  journal = {arXiv preprint arXiv:1503.02531},
  volume = {2},
  number = {7},
}
@inproceedings{hambardzumyan-etal-2021-warp,
  author = {Hambardzumyan, Karen and Khachatrian, Hrant and May, Jonathan},
  title = {{WARP}: {W}ord-level {A}dversarial {R}e{P}rogramming},
  year = {2021},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages = {4921--4933},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{he-etal-2021-effectiveness,
  author = {He, Ruidan and Liu, Linlin and Ye, Hai and Tan, Qingyu and Ding, Bosheng and Cheng, Liying and Low, Jiawei and Bing, Lidong and Si, Luo},
  title = {On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation},
  year = {2021},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages = {2208--2222},
  publisher = {Association for Computational Linguistics},
}
@misc{liu2020roberta,
  author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  year = {2020},
}
@inproceedings{han-eisenstein-2019-unsupervised,
  author = {Han, Xiaochuang and Eisenstein, Jacob},
  title = {Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling},
  year = {2019},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages = {4238--4248},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{aharoni-goldberg-2017-towards,
  author = {Aharoni, Roee and Goldberg, Yoav},
  title = {Towards String-To-Tree Neural Machine Translation},
  year = {2017},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages = {132--140},
  publisher = {Association for Computational Linguistics},
}
@article{parisi2019continual,
  author = {Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
  title = {Continual lifelong learning with neural networks: A review},
  year = {2019},
  journal = {Neural Networks},
  volume = {113},
  pages = {54--71},
  publisher = {Elsevier},
  issn = {0893-6080},
}
@inproceedings{lee-etal-2019-domain,
  author = {Lee, Seanie and Kim, Donggyu and Park, Jangwon},
  title = {Domain-agnostic Question-Answering with Adversarial Training},
  year = {2019},
  booktitle = {Proceedings of the 2nd Workshop on Machine Reading for Question Answering},
  pages = {196--202},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{NEURIPS2021_c1fea270,
  author = {Liu, Hong and Wang, Jianmin and Long, Mingsheng},
  title = {Cycle Self-Training for Domain Adaptation},
  year = {2021},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {22968--22981},
  publisher = {Curran Associates, Inc.},
  editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
}
@article{lee2020biobert,
  author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  title = {BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  year = {2020},
  journal = {Bioinformatics},
  volume = {36},
  number = {4},
  pages = {1234--1240},
  publisher = {Oxford University Press},
}
@inproceedings{nishida-etal-2020-unsupervised,
  author = {Nishida, Kosuke and Nishida, Kyosuke and Saito, Itsumi and Asano, Hisako and Tomita, Junji},
  title = {Unsupervised Domain Adaptation of Language Models for Reading Comprehension},
  year = {2020},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  pages = {5392--5399},
  publisher = {European Language Resources Association},
}
@article{group-lasso-2017,
  author = {Scardapane, Simone and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio},
  title = {Group Sparse Regularization for Deep Neural Networks},
  year = {2017},
  journal = {Neurocomput.},
  volume = {241},
  number = {C},
  pages = {81–89},
  publisher = {Elsevier Science Publishers B. V.},
  issn = {0925-2312},
  month = {jun},
}
@inproceedings{lekhtman-etal-2021-dilbert,
  author = {Lekhtman, Entony and Ziser, Yftah and Reichart, Roi},
  title = {{DILBERT}: Customized Pre-Training for Domain Adaptation with Category Shift, with an Application to Aspect Extraction},
  year = {2021},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages = {219--230},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{zou-etal-2021-unsupervised,
  author = {Zou, Han and Yang, Jianfei and Wu, Xiaojian},
  title = {Unsupervised Energy-based Adversarial Domain Adaptation for Cross-domain Text Classification},
  year = {2021},
  booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages = {1208--1218},
  publisher = {Association for Computational Linguistics},
}
@article{makhzani2015adversarial,
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  title = {Adversarial autoencoders},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.05644},
}
@inproceedings{zhang-etal-2020-multi-stage,
  author = {Zhang, Rong and Gangi Reddy, Revanth and Sultan, Md Arafat and Castelli, Vittorio and Ferritto, Anthony and Florian, Radu and Sarioglu Kayi, Efsun and Roukos, Salim and Sil, Avi and Ward, Todd},
  title = {Multi-Stage Pre-training for Low-Resource Domain Adaptation},
  year = {2020},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages = {5461--5468},
  publisher = {Association for Computational Linguistics},
}
@article{xie2020unsupervised,
  author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Thang and Le, Quoc},
  title = {Unsupervised data augmentation for consistency training},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {6256--6268},
}
@inproceedings{salinas-alvarado-etal-2015-domain,
  author = {Salinas Alvarado, Julio Cesar and Verspoor, Karin and Baldwin, Timothy},
  title = {Domain Adaption of Named Entity Recognition to Support Credit Risk Assessment},
  year = {2015},
  booktitle = {Proceedings of the Australasian Language Technology Association Workshop 2015},
  pages = {84--90},
}
@inproceedings{pmlr-v97-liu19b,
  author = {Liu, Hong and Long, Mingsheng and Wang, Jianmin and Jordan, Michael},
  title = {Transferable Adversarial Training: A General Approach to Adapting Deep Classifiers},
  year = {2019},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  volume = {97},
  pages = {4013--4022},
  publisher = {PMLR},
  month = {09--15 Jun},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  series = {Proceedings of Machine Learning Research},
}
@inproceedings{xu-etal-2021-gradual,
  author = {Xu, Haoran and Ebner, Seth and Yarmohammadi, Mahsa and White, Aaron Steven and Van Durme, Benjamin and Murray, Kenton},
  title = {Gradual Fine-Tuning for Low-Resource Domain Adaptation},
  year = {2021},
  booktitle = {Proceedings of the Second Workshop on Domain Adaptation for NLP},
  pages = {214--221},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{zaken-2022-bitfit,
  author = {Ben Zaken, Elad and Goldberg, Yoav and Ravfogel, Shauli},
  title = {{B}it{F}it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  year = {2022},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages = {1--9},
  publisher = {Association for Computational Linguistics},
}
@article{conneau2019cross,
  author = {Conneau, Alexis and Lample, Guillaume},
  title = {Cross-lingual language model pretraining},
  year = {2019},
  journal = {Advances in neural information processing systems},
  volume = {32},
}
@inproceedings{denkowski-etal-2014-learning,
  author = {Denkowski, Michael and Dyer, Chris and Lavie, Alon},
  title = {Learning from Post-Editing: Online Model Adaptation for Statistical Machine Translation},
  year = {2014},
  booktitle = {Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics},
  pages = {395--404},
  publisher = {Association for Computational Linguistics},
}
@article{huang2019clinicalbert,
  author = {Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh},
  title = {Clinicalbert: Modeling clinical notes and predicting hospital readmission},
  year = {2019},
  journal = {arXiv preprint arXiv:1904.05342},
}
@inproceedings{ghosal-etal-2020-kingdom,
  author = {Ghosal, Deepanway and Hazarika, Devamanyu and Roy, Abhinaba and Majumder, Navonil and Mihalcea, Rada and Poria, Soujanya},
  title = {{K}in{GDOM}: {K}nowledge-{G}uided {DOM}ain {A}daptation for {S}entiment {A}nalysis},
  year = {2020},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages = {3198--3210},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{jakob-gurevych-2010-extracting,
  author = {Jakob, Niklas and Gurevych, Iryna},
  title = {Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields},
  year = {2010},
  booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
  pages = {1035--1045},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{yao-etal-2021-adapt,
  author = {Yao, Yunzhi and Huang, Shaohan and Wang, Wenhui and Dong, Li and Wei, Furu},
  title = {Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains},
  year = {2021},
  booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{grangier-iter-2022-trade,
  author = {Grangier, David and Iter, Dan},
  title = {The Trade-offs of Domain Adaptation for Neural Language Models},
  year = {2022},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages = {3802--3813},
  publisher = {Association for Computational Linguistics},
}
@article{kirkpatrick2017overcoming,
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  title = {Overcoming catastrophic forgetting in neural networks},
  year = {2017},
  journal = {Proceedings of the national academy of sciences},
  volume = {114},
  number = {13},
  pages = {3521--3526},
  publisher = {National Acad Sciences},
}
@article{pruthi2020estimating,
  author = {Pruthi, Garima and Liu, Frederick and Kale, Satyen and Sundararajan, Mukund},
  title = {Estimating training data influence by tracing gradient descent},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {19920--19930},
}
@inproceedings{li-etal-2018-named,
  author = {Li, Zhongwei and Wang, Xuancong and Aw, Ai Ti and Chng, Eng Siong and Li, Haizhou},
  title = {Named-Entity Tagging and Domain adaptation for Better Customized Translation},
  year = {2018},
  booktitle = {Proceedings of the Seventh Named Entities Workshop},
  pages = {41--46},
  publisher = {Association for Computational Linguistics},
}
@article{turchi2017continuous,
  author = {Turchi, Marco and Negri, Matteo and Farajian, M Amin and Federico, Marcello},
  title = {Continuous learning from human post-edits for neural machine translation},
  year = {2017},
  journal = {The Prague Bulletin of Mathematical Linguistics},
  volume = {108},
  number = {1},
  pages = {233},
  publisher = {De Gruyter Poland},
}
@article{mansour2008domain,
  author = {Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
  title = {Domain adaptation with multiple sources},
  year = {2008},
  journal = {Advances in neural information processing systems},
  volume = {21},
}
@article{tzeng2014deep,
  author = {Tzeng, Eric and Hoffman, Judy and Zhang, Ning and Saenko, Kate and Darrell, Trevor},
  title = {Deep domain confusion: Maximizing for domain invariance},
  year = {2014},
  journal = {arXiv preprint arXiv:1412.3474},
}
@inproceedings{label-proportion-2008-icml,
  author = {Quadrianto, Novi and Smola, Alex J. and Caetano, Tiberio S. and Le, Quoc V.},
  title = {Estimating Labels from Label Proportions},
  year = {2008},
  booktitle = {Proceedings of the 25th International Conference on Machine Learning},
  pages = {776–783},
  publisher = {Association for Computing Machinery},
  isbn = {9781605582054},
  series = {ICML '08},
}
@article{mohiuddin2022data,
  author = {Mohiuddin, Tasnim and Koehn, Philipp and Chaudhary, Vishrav and Cross, James and Bhosale, Shruti and Joty, Shafiq},
  title = {Data Selection Curriculum for Neural Machine Translation},
  year = {2022},
  journal = {arXiv preprint arXiv:2203.13867},
}
@inproceedings{sachidananda-etal-2021-efficient,
  author = {Sachidananda, Vin and Kessler, Jason and Lai, Yi-An},
  title = {Efficient Domain Adaptation of Language Models via Adaptive Tokenization},
  year = {2021},
  booktitle = {Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing},
  pages = {155--165},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{axelrod-etal-2011-domain,
  author = {Axelrod, Amittai and He, Xiaodong and Gao, Jianfeng},
  title = {Domain Adaptation via Pseudo In-Domain Data Selection},
  year = {2011},
  booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
  pages = {355--362},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{vu-etal-2022-spot,
  author = {Vu, Tu and Lester, Brian and Constant, Noah and Al-Rfou{'}, Rami and Cer, Daniel},
  title = {{SP}o{T}: Better Frozen Model Adaptation through Soft Prompt Transfer},
  year = {2022},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages = {5039--5059},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{dai-etal-2022-knowledge,
  author = {Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  title = {Knowledge Neurons in Pretrained Transformers},
  year = {2022},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages = {8493--8502},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{pmlr-v119-kumar20c,
  author = {Kumar, Ananya and Ma, Tengyu and Liang, Percy},
  title = {Understanding Self-Training for Gradual Domain Adaptation},
  year = {2020},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  volume = {119},
  pages = {5468--5479},
  publisher = {PMLR},
  month = {13--18 Jul},
  editor = {III, Hal Daumé and Singh, Aarti},
  series = {Proceedings of Machine Learning Research},
}
@inproceedings{sun2016deep,
  author = {Sun, Baochen and Saenko, Kate},
  title = {Deep coral: Correlation alignment for deep domain adaptation},
  year = {2016},
  booktitle = {European conference on computer vision},
  pages = {443--450},
  organization = {Springer},
}
@article{meng2022locating,
  author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  title = {Locating and Editing Factual Associations in GPT},
  year = {2022},
  journal = {arXiv preprint arXiv:2202.05262},
}
@inproceedings{gururangan-etal-2020-dont,
  author = {Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
  title = {Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks},
  year = {2020},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages = {8342--8360},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{kim-etal-2021-learn,
  author = {Kim, Gangwoo and Kim, Hyunjae and Park, Jungsoo and Kang, Jaewoo},
  title = {Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering},
  year = {2021},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages = {6130--6141},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{beltagy-etal-2019-scibert,
  author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  title = {{S}ci{BERT}: A Pretrained Language Model for Scientific Text},
  year = {2019},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages = {3615--3620},
  publisher = {Association for Computational Linguistics},
}
@article{wang2021gradient,
  author = {Wang, Xinyi and Bapna, Ankur and Johnson, Melvin and Firat, Orhan},
  title = {Gradient-guided loss masking for neural machine translation},
  year = {2021},
  journal = {arXiv preprint arXiv:2102.13549},
}
@inproceedings{chen2020adversarial,
  author = {Chen, Minghao and Zhao, Shuai and Liu, Haifeng and Cai, Deng},
  title = {Adversarial-learned loss for domain adaptation},
  year = {2020},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {3521--3528},
}
@article{creswell2022faithful,
  author = {Creswell, Antonia and Shanahan, Murray},
  title = {Faithful reasoning using large language models},
  year = {2022},
  journal = {arXiv preprint arXiv:2208.14271},
}
@article{guo-et-al-2022-fedhumor,
  author = {Guo, Xu and Yu, Han and Li, Boyang and Wang, Hao and Xing, Pengwei and Feng, Siwei and Nie, Zaiqing and Miao, Chunyan},
  title = {Federated Learning for Personalized Humor Recognition},
  year = {2022},
  journal = {ACM Trans. Intell. Syst. Technol.},
  volume = {13},
  number = {4},
  publisher = {Association for Computing Machinery},
  issn = {2157-6904},
  month = {may},
}
@inproceedings{jiang2020bidirectional,
  author = {Jiang, Pin and Wu, Aming and Han, Yahong and Shao, Yunfeng and Qi, Meiyu and Li, Bingshuai},
  title = {Bidirectional Adversarial Training for Semi-Supervised Domain Adaptation.},
  year = {2020},
  booktitle = {IJCAI},
  pages = {934--940},
}
@inproceedings{pmlr-v70-koh17a,
  author = {Pang Wei Koh and Percy Liang},
  title = {Understanding Black-box Predictions via Influence Functions},
  year = {2017},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  volume = {70},
  pages = {1885--1894},
  publisher = {PMLR},
  month = {06--11 Aug},
  editor = {Precup, Doina and Teh, Yee Whye},
  series = {Proceedings of Machine Learning Research},
}
@article{gu2021domain,
  author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  title = {Domain-specific language model pretraining for biomedical natural language processing},
  year = {2021},
  journal = {ACM Transactions on Computing for Healthcare (HEALTH)},
  volume = {3},
  number = {1},
  pages = {1--23},
  publisher = {ACM New York, NY},
}
@article{li2021pretrained,
  author = {Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Wen, Ji-Rong},
  title = {Pretrained language models for text generation: A survey},
  year = {2021},
  journal = {arXiv preprint arXiv:2105.10311},
}
@article{brown2020language,
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  title = {Language models are few-shot learners},
  year = {2020},
  journal = {Advances in neural information processing systems},
  volume = {33},
  pages = {1877--1901},
}
@article{pan2009survey,
  author = {Pan, Sinno Jialin and Yang, Qiang},
  title = {A survey on transfer learning},
  year = {2009},
  journal = {IEEE Transactions on knowledge and data engineering},
  volume = {22},
  number = {10},
  pages = {1345--1359},
  publisher = {IEEE},
}
@article{rebuffi2017learning,
  author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
  title = {Learning multiple visual domains with residual adapters},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
}
@inproceedings{gu-etal-2022-ppt,
  author = {Gu, Yuxian and Han, Xu and Liu, Zhiyuan and Huang, Minlie},
  title = {{PPT}: Pre-trained Prompt Tuning for Few-shot Learning},
  year = {2022},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages = {8410--8423},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{park2021consistency,
  author = {Park, Jungsoo and Kim, Gyuwan and Kang, Jaewoo},
  title = {Consistency training with virtual adversarial discrete perturbation},
  year = {2022},
  booktitle = {Proceedings of the 2022 Annual Conference of the North American Chapter of the Association for Computational Linguistics (Short Papers)},
}
@article{huang2019clinical,
  author = {Huang, Kexin and Singh, Abhishek and Chen, Sitong and Moseley, Edward T and Deng, Chih-ying and George, Naomi and Lindvall, Charlotta},
  title = {Clinical XLNet: Modeling sequential clinical notes and predicting prolonged mechanical ventilation},
  year = {2019},
  journal = {arXiv preprint arXiv:1912.11975},
}
@article{guo2021bertweetfr,
  author = {Guo, Yanzhu and Rennard, Virgile and Xypolopoulos, Christos and Vazirgiannis, Michalis},
  title = {BERTweetFR: Domain Adaptation of Pre-Trained Language Models for French Tweets},
  year = {2021},
  journal = {arXiv preprint arXiv:2109.10234},
}
@inproceedings{schick-schutze-2021-exploiting,
  author = {Schick, Timo and Sch{\"u}tze, Hinrich},
  title = {Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference},
  year = {2021},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages = {255--269},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{stress-per-2018,
  author = {Saeed, Aaqib and Ozcelebi, Tanir and Lukkien, Johan and van Erp, Jan B.F. and Trajanovski, Stojan},
  title = {Model Adaptation and Personalization for Physiological Stress Detection},
  year = {2018},
  booktitle = {2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)},
  pages = {209-216},
}
@article{rietzler2019adapt,
  author = {Rietzler, Alexander and Stabinger, Sebastian and Opitz, Paul and Engl, Stefan},
  title = {Adapt or get left behind: Domain adaptation through bert language model finetuning for aspect-target sentiment classification},
  year = {2019},
  journal = {arXiv preprint arXiv:1908.11860},
}
@inproceedings{tzeng2017adversarial,
  author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
  title = {Adversarial discriminative domain adaptation},
  year = {2017},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages = {7167--7176},
}
@inproceedings{sun2016return,
  author = {Sun, Baochen and Feng, Jiashi and Saenko, Kate},
  title = {Return of frustratingly easy domain adaptation},
  year = {2016},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {30},
  number = {1},
}
@inproceedings{zhang2018importance,
  author = {Zhang, Jing and Ding, Zewei and Li, Wanqing and Ogunbona, Philip},
  title = {Importance weighted adversarial nets for partial domain adaptation},
  year = {2018},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages = {8156--8164},
}
@inproceedings{cooper-stickland-etal-2021-multilingual,
  author = {Cooper Stickland, Asa and Berard, Alexandre and Nikoulina, Vassilina},
  title = {Multilingual Domain Adaptation for {NMT}: Decoupling Language and Domain Information with Adapters},
  year = {2021},
  booktitle = {Proceedings of the Sixth Conference on Machine Translation},
  pages = {578--598},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{chalkidis-etal-2020-legal,
  author = {Chalkidis, Ilias and Fergadiotis, Manos and Malakasiotis, Prodromos and Aletras, Nikolaos and Androutsopoulos, Ion},
  title = {{LEGAL}-{BERT}: The Muppets straight out of Law School},
  year = {2020},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages = {2898--2904},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{pmlr-v28-yu13a,
  author = {Yu, Felix and Liu, Dong and Kumar, Sanjiv and Tony, Jebara and Chang, Shih-Fu},
  title = {$\propto$SVM for Learning with Label Proportions},
  year = {2013},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  volume = {28},
  number = {3},
  pages = {504--512},
  publisher = {PMLR},
  month = {17--19 Jun},
  editor = {Dasgupta, Sanjoy and McAllester, David},
  series = {Proceedings of Machine Learning Research},
}
@inproceedings{mutualinfo-belghazi-2018-icml,
  author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  title = {Mutual Information Neural Estimation},
  year = {2018},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  volume = {80},
  pages = {531--540},
  publisher = {PMLR},
  month = {10--15 Jul},
  editor = {Dy, Jennifer and Krause, Andreas},
  series = {Proceedings of Machine Learning Research},
}
@article{ganin2016domain,
  title = {Domain-adversarial training of neural networks},
  year = {2016},
  journal = {The journal of machine learning research},
  volume = {17},
  number = {1},
  pages = {2096--2030},
  publisher = {JMLR. org},
}
@inproceedings{hu2022lora,
  author = {Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  title = {Lo{RA}: Low-Rank Adaptation of Large Language Models},
  year = {2022},
  booktitle = {International Conference on Learning Representations},
}
@inproceedings{jin-etal-2022-lifelong,
  author = {Jin, Xisen and Zhang, Dejiao and Zhu, Henghui and Xiao, Wei and Li, Shang-Wen and Wei, Xiaokai and Arnold, Andrew and Ren, Xiang},
  title = {Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora},
  year = {2022},
  booktitle = {Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models},
  pages = {1--16},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{li-liang-2021-prefix,
  author = {Li, Xiang Lisa and Liang, Percy},
  title = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  year = {2021},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages = {4582--4597},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{Lan2020ALBERT,
  author = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  year = {2020},
  booktitle = {International Conference on Learning Representations},
}
@inproceedings{budzianowski-vulic-2019-hello,
  author = {Budzianowski, Pawe{\l} and Vuli{\'c}, Ivan},
  title = {Hello, It{'}s {GPT}-2 - How Can {I} Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems},
  year = {2019},
  booktitle = {Proceedings of the 3rd Workshop on Neural Generation and Translation},
  pages = {15--22},
  publisher = {Association for Computational Linguistics},
}
@article{ECG-per-2019,
  author = {Wenfeng Yin and Xiuzhu Yang and Lei Li and Lin Zhang and Nattapong Kitsuwan and Ryoichi Shinkuma and Eiji Oki},
  title = {Self-adjustable domain adaptation in personalized ECG monitoring integrated with IR-UWB radar},
  year = {2019},
  journal = {Biomedical Signal Processing and Control},
  volume = {47},
  pages = {75-87},
  issn = {1746-8094},
}
@incollection{mccloskey1989catastrophic,
  author = {McCloskey, Michael and Cohen, Neal J},
  title = {Catastrophic interference in connectionist networks: The sequential learning problem},
  year = {1989},
  booktitle = {Psychology of learning and motivation},
  volume = {24},
  pages = {109--165},
  publisher = {Elsevier},
}
@inproceedings{wang-etal-2019-adversarial,
  author = {Wang, Huazheng and Gan, Zhe and Liu, Xiaodong and Liu, Jingjing and Gao, Jianfeng and Wang, Hongning},
  title = {Adversarial Domain Adaptation for Machine Reading Comprehension},
  year = {2019},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages = {2510--2520},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{chronopoulou-etal-2022-efficient,
  author = {Chronopoulou, Alexandra and Peters, Matthew and Dodge, Jesse},
  title = {Efficient Hierarchical Domain Adaptation for Pretrained Language Models},
  year = {2022},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages = {1336--1351},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{vu-etal-2020-effective,
  author = {Vu, Thuy-Trang and Phung, Dinh and Haffari, Gholamreza},
  title = {Effective Unsupervised Domain Adaptation with Adversarially Trained Language Models},
  year = {2020},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages = {6163--6173},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{per-facial-2014,
  author = {Zen, Gloria and Sangineto, Enver and Ricci, Elisa and Sebe, Nicu},
  title = {Unsupervised Domain Adaptation for Personalized Facial Emotion Recognition},
  year = {2014},
  booktitle = {Proceedings of the 16th International Conference on Multimodal Interaction},
  pages = {128–135},
  publisher = {Association for Computing Machinery},
  isbn = {9781450328852},
  series = {ICMI '14},
}
@article{weiss2016survey,
  author = {Weiss, Karl and Khoshgoftaar, Taghi M and Wang, DingDing},
  title = {A survey of transfer learning},
  year = {2016},
  journal = {Journal of Big data},
  volume = {3},
  number = {1},
  pages = {1--40},
  publisher = {SpringerOpen},
}
@article{tan2022towards,
  author = {Tan, Alysa Ziying and Yu, Han and Cui, Lizhen and Yang, Qiang},
  title = {Towards personalized federated learning},
  year = {2022},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  publisher = {IEEE},
}
@inproceedings{yoon-etal-2021-ssmix,
  author = {Yoon, Soyoung and Kim, Gyuwan and Park, Kyumin},
  title = {{SSM}ix: Saliency-Based Span Mixup for Text Classification},
  year = {2021},
  booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages = {3225--3234},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{mcclosky-etal-2006-effective,
  author = {McClosky, David and Charniak, Eugene and Johnson, Mark},
  title = {Effective Self-Training for Parsing},
  year = {2006},
  booktitle = {Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference},
  pages = {152--159},
  publisher = {Association for Computational Linguistics},
}
@article{chaudhry2019tiny,
  author = {Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K and Torr, Philip HS and Ranzato, Marc'Aurelio},
  title = {On tiny episodic memories in continual learning},
  year = {2019},
  journal = {arXiv preprint arXiv:1902.10486},
}
@article{gretton2012kernel,
  author = {Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  title = {A kernel two-sample test},
  year = {2012},
  journal = {The Journal of Machine Learning Research},
  volume = {13},
  number = {1},
  pages = {723--773},
  publisher = {JMLR. org},
}
@inproceedings{sennrich-haddow-2016-linguistic,
  author = {Sennrich, Rico and Haddow, Barry},
  title = {Linguistic Input Features Improve Neural Machine Translation},
  year = {2016},
  booktitle = {Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers},
  pages = {83--91},
  publisher = {Association for Computational Linguistics},
}
@article{qiu2020pre,
  author = {Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
  title = {Pre-trained models for natural language processing: A survey},
  year = {2020},
  journal = {Science China Technological Sciences},
  volume = {63},
  number = {10},
  pages = {1872--1897},
  publisher = {Springer},
}
@article{peris2017online,
  author = {Peris, Alvaro and Cebri{\'a}n, Luis and Casacuberta, Francisco},
  title = {Online learning for neural machine translation post-editing},
  year = {2017},
  journal = {arXiv preprint arXiv:1706.03196},
}
@inproceedings{ahuja2022low,
  author = {Ahuja, Chaitanya and Lee, Dong Won and Morency, Louis-Philippe},
  title = {Low-Resource Adaptation for Personalized Co-Speech Gesture Generation},
  year = {2022},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {20566--20576},
}
@inproceedings{guo-etal-2021-latent,
  author = {Guo, Xu and Li, Boyang and Yu, Han and Miao, Chunyan},
  title = {Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection},
  year = {2021},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages = {5394--5407},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{beltagy-etal-2020-arabic,
  author = {Beltagy, Ahmad and Abouelenin, Abdelrahman and ElSherief, Omar},
  title = {{A}rabic Dialect Identification Using {BERT}-Based Domain Adaptation},
  year = {2020},
  booktitle = {Proceedings of the Fifth Arabic Natural Language Processing Workshop},
  pages = {262--267},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{alsentzer-etal-2019-publicly,
  author = {Alsentzer, Emily and Murphy, John and Boag, William and Weng, Wei-Hung and Jindi, Di and Naumann, Tristan and McDermott, Matthew},
  title = {Publicly Available Clinical {BERT} Embeddings},
  year = {2019},
  booktitle = {Proceedings of the 2nd Clinical Natural Language Processing Workshop},
  pages = {72--78},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{du-etal-2020-adversarial,
  author = {Du, Chunning and Sun, Haifeng and Wang, Jingyu and Qi, Qi and Liao, Jianxin},
  title = {Adversarial and Domain-Aware {BERT} for Cross-Domain Sentiment Analysis},
  year = {2020},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages = {4019--4028},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{poerner-etal-2020-inexpensive,
  author = {Poerner, Nina and Waltinger, Ulli and Sch{\"u}tze, Hinrich},
  title = {Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical {NER} and Covid-19 {QA}},
  year = {2020},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages = {1482--1490},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{mahabadi2021compacter,
  author = {Rabeeh Karimi mahabadi and James Henderson and Sebastian Ruder},
  title = {Compacter: Efficient Low-Rank Hypercomplex Adapter Layers},
  year = {2021},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
}
@inproceedings{de-cao-etal-2021-editing,
  author = {De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  title = {Editing Factual Knowledge in Language Models},
  year = {2021},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages = {6491--6506},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{miller-etal-2021-domain,
  author = {Miller, Timothy and Laparra, Egoitz and Bethard, Steven},
  title = {Domain adaptation in practice: Lessons from a real-world information extraction pipeline},
  year = {2021},
  booktitle = {Proceedings of the Second Workshop on Domain Adaptation for NLP},
  pages = {105--110},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{budzianowski-etal-2018-multiwoz,
  author = {Budzianowski, Pawe{\l} and Wen, Tsung-Hsien and Tseng, Bo-Hsiang and Casanueva, I{\~n}igo and Ultes, Stefan and Ramadan, Osman and Ga{s}i{\'c}, Milica},
  title = {{M}ulti{WOZ} - A Large-Scale Multi-Domain {W}izard-of-{O}z Dataset for Task-Oriented Dialogue Modelling},
  year = {2018},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{NEURIPS2021_PFL,
  author = {Sun, Benyuan and Huo, Hongxing and YANG, YI and Bai, Bo},
  title = {PartialFed: Cross-Domain Personalized Federated Learning via Partial Initialization},
  year = {2021},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {23309--23320},
  publisher = {Curran Associates, Inc.},
  editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
}
@inproceedings{label-dist-2016-ijcai,
  author = {Ardehaly, Ehsan Mohammady and Culotta, Aron},
  title = {Domain Adaptation for Learning from Label Proportions Using Self-Training},
  year = {2016},
  booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
  pages = {3670–3676},
  publisher = {AAAI Press},
  isbn = {9781577357704},
  series = {IJCAI'16},
}
@inproceedings{Michel2018ExtremeAF,
  author = {Paul Michel and Graham Neubig},
  title = {Extreme Adaptation for Personalized Neural Machine Translation},
  year = {2018},
  booktitle = {ACL},
}
@inproceedings{NIPS2017_3f5ee243,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  title = {Attention is All you Need},
  year = {2017},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
}
@article{rahman2020correlation,
  author = {Rahman, Mohammad Mahfujur and Fookes, Clinton and Baktashmotlagh, Mahsa and Sridharan, Sridha},
  title = {Correlation-aware adversarial domain adaptation and generalization},
  year = {2020},
  journal = {Pattern Recognition},
  volume = {100},
  pages = {107124},
  publisher = {Elsevier},
}
@inproceedings{kim-etal-2011-overview,
  author = {Kim, Jin-Dong and Pyysalo, Sampo and Ohta, Tomoko and Bossy, Robert and Nguyen, Ngan and Tsujii, Jun{'}ichi},
  title = {Overview of {B}io{NLP} Shared Task 2011},
  year = {2011},
  booktitle = {Proceedings of {B}io{NLP} Shared Task 2011 Workshop},
  pages = {1--6},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{ijcai2020p0508,
  author = {Li, Juntao and He, Ruidan and Ye, Hai and Ng, Hwee Tou and Bing, Lidong and Yan, Rui},
  title = {Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language Model},
  year = {2020},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, {IJCAI-20}},
  pages = {3672--3678},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  month = {7},
  editor = {Christian Bessiere},
}
@article{bommasani2021-foundation,
  author = {Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  title = {On the opportunities and risks of foundation models},
  year = {2021},
  journal = {arXiv preprint arXiv:2108.07258},
}
@inproceedings{wang-etal-2018-denoising,
  author = {Wang, Wei and Watanabe, Taro and Hughes, Macduff and Nakagawa, Tetsuji and Chelba, Ciprian},
  title = {Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection},
  year = {2018},
  booktitle = {Proceedings of the Third Conference on Machine Translation: Research Papers},
  pages = {133--143},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{hjelm2018learning,
  author = {R Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Phil Bachman and Adam Trischler and Yoshua Bengio},
  title = {Learning deep representations by mutual information estimation and maximization},
  year = {2019},
  booktitle = {International Conference on Learning Representations},
}
@inproceedings{thompson-etal-2019-overcoming,
  author = {Thompson, Brian and Gwinnup, Jeremy and Khayrallah, Huda and Duh, Kevin and Koehn, Philipp},
  title = {Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation},
  year = {2019},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages = {2062--2068},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{ruckle-2021-adapterdrop,
  author = {R{\"u}ckl{\'e}, Andreas and Geigle, Gregor and Glockner, Max and Beck, Tilman and Pfeiffer, Jonas and Reimers, Nils and Gurevych, Iryna},
  title = {{AdapterDrop}: {O}n the Efficiency of Adapters in Transformers},
  year = {2021},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{kim-etal-2009-overview,
  author = {Kim, Jin-Dong and Ohta, Tomoko and Pyysalo, Sampo and Kano, Yoshinobu and Tsujii, Jun{'}ichi},
  title = {Overview of {B}io{NLP}{'}09 Shared Task on Event Extraction},
  year = {2009},
  booktitle = {Proceedings of the {B}io{NLP} 2009 Workshop Companion Volume for Shared Task},
  pages = {1--9},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{yang2020adversarial,
  author = {Yang, Jihan and Xu, Ruijia and Li, Ruiyu and Qi, Xiaojuan and Shen, Xiaoyong and Li, Guanbin and Lin, Liang},
  title = {An adversarial perturbation oriented domain adaptation approach for semantic segmentation},
  year = {2020},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {07},
  pages = {12613--12620},
}
@inproceedings{alexandari2020maximum,
  author = {Alexandari, Amr and Kundaje, Anshul and Shrikumar, Avanti},
  title = {Maximum likelihood with bias-corrected calibration is hard-to-beat at label shift adaptation},
  year = {2020},
  booktitle = {International Conference on Machine Learning},
  pages = {222--232},
  organization = {PMLR},
}
@inproceedings{NIPS2006_b1b0432c,
  author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
  title = {Analysis of Representations for Domain Adaptation},
  year = {2006},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {19},
  publisher = {MIT Press},
  editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
}
@inproceedings{chen-etal-2020-low,
  author = {Chen, Xilun and Ghoshal, Asish and Mehdad, Yashar and Zettlemoyer, Luke and Gupta, Sonal},
  title = {Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing},
  year = {2020},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{adapter-houlsby19a-icml,
  author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  title = {Parameter-Efficient Transfer Learning for {NLP}},
  year = {2019},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  volume = {97},
  pages = {2790--2799},
  publisher = {PMLR},
  month = {09--15 Jun},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  series = {Proceedings of Machine Learning Research},
}
@article{ding2022delta,
  author = {Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  title = {Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models},
  year = {2022},
  journal = {arXiv preprint arXiv:2203.06904},
}
@article{csurka2017comprehensive,
  author = {Csurka, Gabriela},
  title = {A comprehensive survey on domain adaptation for visual applications},
  year = {2017},
  journal = {Domain adaptation in computer vision applications},
  pages = {1--35},
  publisher = {Springer},
}
@article{ben2010theory,
  author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  title = {A theory of learning from different domains},
  year = {2010},
  journal = {Machine learning},
  volume = {79},
  number = {1},
  pages = {151--175},
  publisher = {Springer},
}
@inproceedings{NIPS2016_30ef30b6,
  author = {Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen, Tolga},
  title = {Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning},
  year = {2016},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {29},
  publisher = {Curran Associates, Inc.},
  editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
}
@inproceedings{devlin-etal-2019-bert,
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year = {2019},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{el-mekki-etal-2021-domain,
  author = {El Mekki, Abdellah and El Mahdaouy, Abdelkader and Berrada, Ismail and Khoumsi, Ahmed},
  title = {Domain Adaptation for {A}rabic Cross-Domain and Cross-Dialect Sentiment Analysis from Contextualized Word Embedding},
  year = {2021},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages = {2824--2837},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{antoun-etal-2020-arabert,
  author = {Antoun, Wissam and Baly, Fady and Hajj, Hazem},
  title = {{A}ra{BERT}: Transformer-based Model for {A}rabic Language Understanding},
  year = {2020},
  booktitle = {Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection},
  pages = {9--15},
  publisher = {European Language Resource Association},
  isbn = {979-10-95546-51-1},
}
@inproceedings{NIPS2014_5ca3e9b1,
  author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  title = {Generative Adversarial Nets},
  year = {2014},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {27},
  publisher = {Curran Associates, Inc.},
  editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
}
@article{mikolov2013efficient,
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  title = {Efficient estimation of word representations in vector space},
  year = {2013},
  journal = {arXiv preprint arXiv:1301.3781},
}
@inproceedings{wang2019towards,
  author = {Wang, Xudong and Cai, Zhaowei and Gao, Dashan and Vasconcelos, Nuno},
  title = {Towards universal object detection by domain attention},
  year = {2019},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages = {7289--7298},
}
@article{sun2015survey,
  author = {Sun, Shiliang and Shi, Honglei and Wu, Yuanbin},
  title = {A survey of multi-source domain adaptation},
  year = {2015},
  journal = {Information Fusion},
  volume = {24},
  pages = {84--92},
  publisher = {Elsevier},
}
@inproceedings{chen-etal-2022-modular,
  author = {Chen, Junshen and Card, Dallas and Jurafsky, Dan},
  title = {Modular Domain Adaptation},
  year = {2022},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{mitchell2022fast,
  author = {Eric Mitchell and Charles Lin and Antoine Bosselut and Chelsea Finn and Christopher D Manning},
  title = {Fast Model Editing at Scale},
  year = {2022},
  booktitle = {International Conference on Learning Representations},
}
@article{guo2022improving,
  author = {Guo, Xu and Li, Boyang and Yu, Han},
  title = {Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation},
  year = {2022},
  journal = {arXiv preprint arXiv:2210.02952},
}
@article{app11051974,
  author = {Lee, Chanhee and Yang, Kisu and Whang, Taesun and Park, Chanjun and Matteson, Andrew and Lim, Heuiseok},
  title = {Exploring the Data Efficiency of Cross-Lingual Post-Training in Pretrained Language Models},
  year = {2021},
  journal = {Applied Sciences},
  volume = {11},
  number = {5},
  issn = {2076-3417},
}
@article{araci2020finbert,
  author = {Araci, Dogu},
  title = {Finbert: Financial sentiment analysis with pre-trained language models},
  year = {2019},
  journal = {arXiv preprint arXiv:1908.10063},
}
@inproceedings{van-der-wees-etal-2017-dynamic,
  author = {van der Wees, Marlies and Bisazza, Arianna and Monz, Christof},
  title = {Dynamic Data Selection for Neural Machine Translation},
  year = {2017},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages = {1400--1410},
  publisher = {Association for Computational Linguistics},
}
@inproceedings{label-proportion-2010-icml,
  author = {Rueping, Stefan},
  title = {SVM Classifier Estimation from Group Probabilities},
  year = {2010},
  booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
  pages = {911–918},
  publisher = {Omnipress},
  isbn = {9781605589077},
  series = {ICML'10},
}
@inproceedings{gao-etal-2021-making,
  author = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  title = {Making Pre-trained Language Models Better Few-shot Learners},
  year = {2021},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages = {3816--3830},
  publisher = {Association for Computational Linguistics},
}
@article{Reffel2020T5,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {140},
  pages = {1--67},
}
@inproceedings{vsipka2022hitchhiker,
  title = {The Hitchhiker's Guide to Prior-Shift Adaptation},
  year = {2022},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages = {1516--1524},
}
@article{radford2019language,
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  title = {Language models are unsupervised multitask learners},
  year = {2019},
  journal = {OpenAI blog},
  volume = {1},
  number = {8},
  pages = {9},
}
