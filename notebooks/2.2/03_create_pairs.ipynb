{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Create Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "MANUAL_DATA_PATH = os.path.join('..', 'data', 'raw', 'manual')\n",
    "AUTO_DATA_PATH = os.path.join('..', 'data', 'raw', 'auto')\n",
    "\n",
    "GT_PATH = os.path.join('..', 'output', 'ground_truth')\n",
    "AUTO_GT_PATH = os.path.join(GT_PATH, 'auto.json')     \n",
    "MANUAL_GT_PATH = os.path.join(GT_PATH, 'manual.json') \n",
    "\n",
    "OUTPUT_PATH = os.path.join('..', 'data', 'processed', 'pairs')\n",
    "\n",
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'and', 'or', 'of', 'to', 'in', 'on', 'at', 'by', 'for', \n",
    "    'with', 'from', 'as', 'is', 'are', 'was', 'were', 'be', 'been', 'this', 'that'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_latex_string(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # remove \\bibinfo{type}{content} -> content\n",
    "    text = re.sub(r'\\\\bibinfo\\{.*?\\}\\{(.*?)\\}', r'\\1', text)\n",
    "\n",
    "    # remove latex commands\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{(.*?)\\}', r'\\1', text)\n",
    "\n",
    "    # remove newlines\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_field(field_name, entry):\n",
    "    field_pattern = re.compile(fr'\\b{field_name}\\s*=\\s*\\{{((?:[^{{}}]|\\{{[^{{}}]*\\}})*)\\}}', re.IGNORECASE | re.DOTALL)\n",
    "    match = field_pattern.search(entry)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    pat_quotes = re.compile(fr'\\b{field_name}\\s*=\\s*\\\"(.*?)\\\"', re.IGNORECASE | re.DOTALL)\n",
    "    match = pat_quotes.search(entry)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1)  \n",
    "    else: \n",
    "        return None\n",
    "\n",
    "def extract_from_bib(file_content):\n",
    "    references = []\n",
    "    raw_entries = re.split(r'^@', file_content, flags=re.MULTILINE)\n",
    "    \n",
    "    for entry in raw_entries:\n",
    "        entry = entry.strip()\n",
    "        if not entry or entry.startswith('%'): \n",
    "            continue\n",
    "            \n",
    "        key_match = re.search(r'^(\\w+)\\s*\\{\\s*([^,]+),', entry)\n",
    "        if not key_match:\n",
    "            continue\n",
    "            \n",
    "        ref_type = key_match.group(1)\n",
    "        ref_id = key_match.group(2).strip()\n",
    "\n",
    "        if ref_type.lower() in ['string', 'comment', 'preamble']:\n",
    "            continue\n",
    "        \n",
    "        title_raw = get_field('title', entry)\n",
    "        year_raw = get_field('year', entry)\n",
    "        author_raw = get_field('author', entry)\n",
    "        \n",
    "        if title_raw:\n",
    "            title = clean_latex_string(title_raw) \n",
    "        else:\n",
    "            title = \"\"\n",
    "        \n",
    "        if year_raw:\n",
    "            year = year_raw \n",
    "        else: \n",
    "            year = \"\"\n",
    "        \n",
    "        authors = []\n",
    "        if author_raw:\n",
    "            raw_authors = clean_latex_string(author_raw)\n",
    "            authors = [a.strip() for a in raw_authors.split(' and ')]\n",
    "\n",
    "        references.append({\n",
    "            \"id\": ref_id,\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"year\": year,\n",
    "            \"source_type\": \"bib\"\n",
    "        })\n",
    "        \n",
    "    return references\n",
    "\n",
    "def extract_from_bibitem(file_content):\n",
    "    references = []\n",
    "    \n",
    "    raw_items = re.split(r'\\\\bibitem', file_content)\n",
    "    \n",
    "    bibinfo_pattern = re.compile(r'\\\\bibinfo\\{(.*?)\\}\\{((?:[^{}]|\\{[^{}]*\\})*)\\}', re.DOTALL | re.IGNORECASE)\n",
    "    title_italic_pattern = re.compile(r'(?:\\{\\\\em\\s+|\\\\emph\\{|\\\\textit\\{)((?:[^{}]|\\{[^{}]*\\})*)\\}', re.IGNORECASE | re.DOTALL)\n",
    "    year_pattern = re.compile(r'\\((\\d{4})\\)')\n",
    "\n",
    "    for item in raw_items:\n",
    "        if not item.strip(): continue\n",
    "        \n",
    "        id_match = re.search(r'^\\{([^}]+)\\}', item.strip())\n",
    "        if not id_match: \n",
    "            continue\n",
    "\n",
    "        ref_id = id_match.group(1).strip()\n",
    "        \n",
    "        if r'\\bibinfo' in item:\n",
    "            fields = {'author': [], 'title': '', 'year': ''}\n",
    "            for key, value in bibinfo_pattern.findall(item):\n",
    "                key = key.lower()\n",
    "                clean_val = clean_latex_string(value)\n",
    "                if key == 'author':\n",
    "                    fields['author'].append(clean_val)\n",
    "                else:\n",
    "                    fields[key] = clean_val\n",
    "            \n",
    "            title = fields['title']\n",
    "            authors = fields['author']\n",
    "            year = fields['year']\n",
    "            \n",
    "        else:\n",
    "            title_match = title_italic_pattern.search(item)\n",
    "            if title_match:\n",
    "                title = clean_latex_string(title_match.group(1))\n",
    "\n",
    "                title_start_index = item.find(title_match.group(0))\n",
    "                id_end_index = item.find('}') + 1\n",
    "                \n",
    "                if title_start_index > id_end_index:\n",
    "                    raw_authors = item[id_end_index:title_start_index]\n",
    "                    clean_auth_str = clean_latex_string(raw_authors)\n",
    "                    authors = [clean_auth_str]\n",
    "                else:\n",
    "                    authors = []\n",
    "            else:\n",
    "                id_end_index = item.find('}') + 1\n",
    "                title = clean_latex_string(item[id_end_index:])\n",
    "                authors = []\n",
    "\n",
    "            year_match = year_pattern.search(item)\n",
    "            year = year_match.group(1) if year_match else \"\"\n",
    "\n",
    "        references.append({\n",
    "            \"id\": ref_id,\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"year\": year,\n",
    "            \"source_type\": \"bibitem\"\n",
    "        })\n",
    "            \n",
    "    return references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_references(paper_id, data_path):\n",
    "    if not data_path:\n",
    "        paper_path = os.path.join(AUTO_DATA_PATH, paper_id)\n",
    "    else:\n",
    "        paper_path = os.path.join(data_path, paper_id)\n",
    "\n",
    "    unique_references = {} \n",
    "    \n",
    "    # search for all .bib files\n",
    "    for root, dirs, files in os.walk(paper_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".bib\"):\n",
    "                try:\n",
    "                    with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                        extracted_references = extract_from_bib(content)\n",
    "                        \n",
    "                        for reference in extracted_references:\n",
    "                            if reference['id'] not in unique_references:\n",
    "                                unique_references[reference['id']] = reference\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {file}: {e}\")\n",
    "\n",
    "    # search for all .tex files\n",
    "    for root, dirs, files in os.walk(paper_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".tex\"):\n",
    "                try:\n",
    "                    with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                        \n",
    "                        extracted_references = extract_from_bibitem(content)\n",
    "                        \n",
    "                        for reference in extracted_references:\n",
    "                            if reference['id'] not in unique_references:\n",
    "                                unique_references[reference['id']] = reference\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {file}: {e}\")\n",
    "            \n",
    "    return list(unique_references.values())\n",
    "\n",
    "def load_references_json_file(paper_id, data_path):\n",
    "    if not data_path:\n",
    "        json_path = os.path.join(AUTO_DATA_PATH, paper_id, 'references.json')\n",
    "    else:\n",
    "        json_path = os.path.join(data_path, paper_id, 'references.json')\n",
    "    \n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"references.json not found for paper {paper_id}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        if data is None or not isinstance(data, dict):\n",
    "            print(f\"Warning: references.json for {paper_id} is empty or invalid format.\")\n",
    "            return []\n",
    "            \n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Could not decode JSON for paper {paper_id}\")\n",
    "        return []\n",
    "        \n",
    "    target_references = []\n",
    "    \n",
    "    for arxiv_id, metadata in data.items():\n",
    "        \n",
    "        title = metadata.get('title', \"\")\n",
    "        authors = metadata.get('authors', [])\n",
    "        date_str = metadata.get('submission_date', \"\")\n",
    "        year = \"\"\n",
    "\n",
    "        if date_str:\n",
    "            year = date_str.split('-')[0]\n",
    "            \n",
    "        target_references.append({\n",
    "            \"id\": arxiv_id,             \n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"year\": year,\n",
    "            \"source_type\": \"references_json\"\n",
    "        })\n",
    "        \n",
    "    return target_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # filter stop words\n",
    "    tokens = text.split()\n",
    "    clean_tokens = [t for t in tokens if t not in STOP_WORDS]\n",
    "    \n",
    "    return \" \".join(clean_tokens)\n",
    "\n",
    "def tokenize_author_list(authors_list):\n",
    "    if not authors_list:\n",
    "        return set()\n",
    "    \n",
    "    all_authors = \" \".join(authors_list)\n",
    "    all_authors = all_authors.lower()\n",
    "    all_authors = re.sub(r'[^a-z\\s]', ' ', all_authors)\n",
    "    \n",
    "    # separate all author string into tokens\n",
    "    tokens = set(all_authors.split())\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def clean_references(reference_list):\n",
    "    for reference in reference_list:\n",
    "        reference['clean_title'] = clean_text(reference.get('title', ''))\n",
    "        reference['clean_author_tokens'] = tokenize_author_list(reference.get('authors', []))\n",
    "        \n",
    "    return reference_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "### 2.1 Load References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_folder(folder_path, label_type):\n",
    "    print(f\"Loading {label_type} data from: {os.path.abspath(folder_path)}\")\n",
    "    \n",
    "    pids = [d for d in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, d))]\n",
    "    pids.sort()\n",
    "    \n",
    "    refs_dict = {}\n",
    "    targets_dict = {}\n",
    "    \n",
    "    for pid in pids:\n",
    "        refs = get_paper_references(pid, folder_path)\n",
    "        targets = load_references_json_file(pid, folder_path)\n",
    "        \n",
    "        clean_references(refs)\n",
    "        clean_references(targets)\n",
    "        \n",
    "        refs_dict[pid] = refs\n",
    "        targets_dict[pid] = targets\n",
    "        \n",
    "    print(f\"  -> Loaded content for {len(pids)} {label_type} papers.\")\n",
    "    return refs_dict, targets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MANUAL data from: /Users/thomas200905/Documents/Thomas/HCMUS/Third Year/Semester 7/Intro to Data Science/Milestones/MS02/data/raw/manual\n",
      "  -> Loaded content for 5 MANUAL papers.\n",
      "Loading AUTO data from: /Users/thomas200905/Documents/Thomas/HCMUS/Third Year/Semester 7/Intro to Data Science/Milestones/MS02/data/raw/auto\n",
      "Warning: references.json for 2211.03130 is empty or invalid format.\n",
      "Warning: references.json for 2211.03225 is empty or invalid format.\n",
      "Warning: references.json for 2211.03324 is empty or invalid format.\n",
      "Warning: references.json for 2211.03333 is empty or invalid format.\n",
      "Warning: references.json for 2211.03339 is empty or invalid format.\n",
      "Warning: references.json for 2211.03403 is empty or invalid format.\n",
      "Warning: references.json for 2211.03406 is empty or invalid format.\n",
      "Warning: references.json for 2211.03432 is empty or invalid format.\n",
      "Warning: references.json for 2211.03446 is empty or invalid format.\n",
      "  -> Loaded content for 498 AUTO papers.\n",
      "----------------------------------------\n",
      "Total Papers Loaded: 503\n"
     ]
    }
   ],
   "source": [
    "manual_refs, manual_target_refs = load_data_from_folder(MANUAL_DATA_PATH, \"MANUAL\")\n",
    "auto_refs, auto_target_refs = load_data_from_folder(AUTO_DATA_PATH, \"AUTO\")\n",
    "\n",
    "master_refs = {**auto_refs, **manual_refs}               \n",
    "master_targets = {**auto_target_refs, **manual_target_refs} \n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total Papers Loaded: {len(master_refs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Ground Truth Labels...\n",
      "  -> Loaded 498 auto-labeled papers.\n",
      "  -> Loaded 5 manually-labeled papers.\n"
     ]
    }
   ],
   "source": [
    "def load_ground_truth(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        print(f\"[WARNING] Ground truth file not found: {filepath}\")\n",
    "        return {}\n",
    "\n",
    "print(\"Loading Ground Truth Labels...\")\n",
    "gt_auto = load_ground_truth(AUTO_GT_PATH)\n",
    "gt_manual = load_ground_truth(MANUAL_GT_PATH)\n",
    "\n",
    "print(f\"  -> Loaded {len(gt_auto)} auto-labeled papers.\")\n",
    "print(f\"  -> Loaded {len(gt_manual)} manually-labeled papers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grouped_dataset(ground_truth, all_refs, all_targets):\n",
    "    grouped_data = {}\n",
    "    \n",
    "    for pid, matches in ground_truth.items():\n",
    "        if pid not in all_refs or pid not in all_targets:\n",
    "            continue\n",
    "            \n",
    "        paper_pairs = []\n",
    "        source_lookup = {r['id']: r for r in all_refs[pid]}\n",
    "        target_lookup = {t['id']: t for t in all_targets[pid]}\n",
    "        all_target_ids = list(target_lookup.keys())\n",
    "        \n",
    "        for source_id, true_target_id in matches.items():\n",
    "            source_obj = source_lookup.get(source_id)\n",
    "            target_obj = target_lookup.get(true_target_id)\n",
    "            \n",
    "            if not source_obj or not target_obj: continue\n",
    "            \n",
    "            paper_pairs.append({\n",
    "                \"source\": source_obj,\n",
    "                \"candidate\": target_obj,\n",
    "                \"label\": 1\n",
    "            })\n",
    "            \n",
    "            negative_candidates = [tid for tid in all_target_ids if tid != true_target_id]\n",
    "            \n",
    "            k = min(len(negative_candidates), 5) \n",
    "            selected_negatives = random.sample(negative_candidates, k)\n",
    "            \n",
    "            for neg_id in selected_negatives:\n",
    "                neg_obj = target_lookup.get(neg_id)\n",
    "                if neg_obj:\n",
    "                    paper_pairs.append({\n",
    "                        \"source\": source_obj,\n",
    "                        \"candidate\": neg_obj,\n",
    "                        \"label\": 0\n",
    "                    })\n",
    "        \n",
    "        if paper_pairs:\n",
    "            grouped_data[pid] = paper_pairs\n",
    "\n",
    "        if not paper_pairs:\n",
    "            if len(ground_truth) < 100: \n",
    "                print(f\"Dropping Paper {pid}, no valid pairs generated\")\n",
    "                    \n",
    "    return grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Grouped Datasets...\n",
      "Saved manual pairs for 5 papers.\n",
      "Saved auto pairs for 222 papers.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating Grouped Datasets...\")\n",
    "\n",
    "def set_to_list(obj):\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "\n",
    "    raise TypeError\n",
    "\n",
    "manual_grouped = generate_grouped_dataset(gt_manual, master_refs, master_targets)\n",
    "with open(os.path.join(OUTPUT_PATH, 'manual_pairs.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(manual_grouped, f, indent=2, ensure_ascii = False, default = set_to_list) \n",
    "print(f\"Saved manual pairs for {len(manual_grouped)} papers.\")\n",
    "\n",
    "auto_grouped = generate_grouped_dataset(gt_auto, master_refs, master_targets)\n",
    "with open(os.path.join(OUTPUT_PATH, 'auto_pairs.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(auto_grouped, f, indent=2, ensure_ascii = False, default = set_to_list)\n",
    "print(f\"Saved auto pairs for {len(auto_grouped)} papers.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
