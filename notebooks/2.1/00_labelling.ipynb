{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "RAW_DATA_PATH = os.path.join('..', 'testing')\n",
    "\n",
    "print(f\"Data path: {os.path.abspath(RAW_DATA_PATH)}\")\n",
    "\n",
    "paper_ids = [d for d in os.listdir(RAW_DATA_PATH) if os.path.isdir(os.path.join(RAW_DATA_PATH, d))]\n",
    "paper_ids.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract References\n",
    "### 1.1 From .bib and .tex files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_latex_string(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # remove \\bibinfo{type}{content} -> content\n",
    "    text = re.sub(r'\\\\bibinfo\\{.*?\\}\\{(.*?)\\}', r'\\1', text)\n",
    "\n",
    "    # remove latex commands\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{(.*?)\\}', r'\\1', text)\n",
    "\n",
    "    # remove newlines\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_field(field_name, entry):\n",
    "    field_pattern = re.compile(fr'\\b{field_name}\\s*=\\s*\\{{((?:[^{{}}]|\\{{[^{{}}]*\\}})*)\\}}', re.IGNORECASE | re.DOTALL)\n",
    "    match = field_pattern.search(entry)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    pat_quotes = re.compile(fr'\\b{field_name}\\s*=\\s*\\\"(.*?)\\\"', re.IGNORECASE | re.DOTALL)\n",
    "    match = pat_quotes.search(entry)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1)  \n",
    "    else: \n",
    "        return None\n",
    "\n",
    "def extract_from_bib(file_content):\n",
    "    references = []\n",
    "    raw_entries = re.split(r'^@', file_content, flags=re.MULTILINE)\n",
    "    \n",
    "    for entry in raw_entries:\n",
    "        entry = entry.strip()\n",
    "        if not entry or entry.startswith('%'): \n",
    "            continue\n",
    "            \n",
    "        key_match = re.search(r'^(\\w+)\\s*\\{\\s*([^,]+),', entry)\n",
    "        if not key_match:\n",
    "            continue\n",
    "            \n",
    "        ref_type = key_match.group(1)\n",
    "        ref_id = key_match.group(2).strip()\n",
    "\n",
    "        if ref_type.lower() in ['string', 'comment', 'preamble']:\n",
    "            continue\n",
    "        \n",
    "        title_raw = get_field('title', entry)\n",
    "        year_raw = get_field('year', entry)\n",
    "        author_raw = get_field('author', entry)\n",
    "        \n",
    "        if title_raw:\n",
    "            title = clean_latex_string(title_raw) \n",
    "        else:\n",
    "            title = \"\"\n",
    "        \n",
    "        if year_raw:\n",
    "            year = year_raw \n",
    "        else: \n",
    "            year = \"\"\n",
    "        \n",
    "        authors = []\n",
    "        if author_raw:\n",
    "            raw_authors = clean_latex_string(author_raw)\n",
    "            authors = [a.strip() for a in raw_authors.split(' and ')]\n",
    "\n",
    "        references.append({\n",
    "            \"id\": ref_id,\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"year\": year,\n",
    "            \"source_type\": \"bib\"\n",
    "        })\n",
    "        \n",
    "    return references\n",
    "\n",
    "def extract_from_bibitem(file_content):\n",
    "    bib_section = re.search(r'\\\\begin\\{thebibliography\\}.*?\\n(.*?)\\\\end\\{thebibliography\\}', file_content, re.DOTALL)\n",
    "    if not bib_section:\n",
    "        return []\n",
    "    \n",
    "    raw_items = re.split(r'\\\\bibitem', bib_section.group(1))\n",
    "    references = []\n",
    "\n",
    "    for item in raw_items:\n",
    "        if not item.strip(): \n",
    "            continue\n",
    "        \n",
    "        id_match = re.search(r'\\{([^}]+)\\}', item)\n",
    "        \n",
    "        if not id_match: \n",
    "            continue\n",
    "\n",
    "        ref_id = id_match.group(1).strip()\n",
    "        \n",
    "        title_match = re.search(r'\\\\bibinfo\\{title\\}\\{((?:[^{}]|\\{[^{}]*\\})*)\\}', item, re.DOTALL | re.IGNORECASE)\n",
    "        if title_match:\n",
    "            title = clean_latex_string(title_match.group(1)) \n",
    "        else:\n",
    "            title = \"\"\n",
    "        \n",
    "        year_match = re.search(r'\\\\bibinfo\\{year\\}\\{([0-9]{4})\\}', item)\n",
    "        if year_match:\n",
    "            year = year_match.group(1) \n",
    "        else: \n",
    "            year = \"\"\n",
    "        \n",
    "        author_matches = re.findall(r'\\\\bibinfo\\{author\\}\\{((?:[^{}]|\\{[^{}]*\\})*)\\}', item)\n",
    "        authors = [clean_latex_string(a) for a in author_matches]\n",
    "\n",
    "        references.append({\n",
    "            \"id\": ref_id,\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"year\": year,\n",
    "            \"source_type\": \"bibitem\"\n",
    "        })\n",
    "            \n",
    "    return references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_references(paper_id):\n",
    "    paper_path = os.path.join(RAW_DATA_PATH, paper_id)\n",
    "    unique_references = {} \n",
    "    found_bib = False\n",
    "    \n",
    "    # search for all .bib files\n",
    "    for root, dirs, files in os.walk(paper_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".bib\"):\n",
    "                try:\n",
    "                    with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                        extracted_references = extract_from_bib(content)\n",
    "                        \n",
    "                        for reference in extracted_references:\n",
    "                            if reference['id'] not in unique_references:\n",
    "                                unique_references[reference['id']] = reference\n",
    "                                \n",
    "                        if extracted_references:\n",
    "                            found_bib = True\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {file}: {e}\")\n",
    "\n",
    "    # no .bib file found, check .tex files\n",
    "    if not found_bib:\n",
    "        for root, dirs, files in os.walk(paper_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".tex\"):\n",
    "                    try:\n",
    "                        with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                            content = f.read()\n",
    "                            \n",
    "                            if \"\\\\begin{thebibliography}\" in content:\n",
    "                                extracted_references = extract_from_bibitem(content)\n",
    "                                \n",
    "                                for reference in extracted_references:\n",
    "                                    if reference['id'] not in unique_references:\n",
    "                                        unique_references[reference['id']] = reference\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing {file}: {e}\")\n",
    "            \n",
    "    return list(unique_references.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_refs = {}\n",
    "\n",
    "for pid in paper_ids:\n",
    "    print(f\"Processing Paper ID: {pid}\")\n",
    "    \n",
    "    refs = get_paper_references(pid)\n",
    "    all_papers_refs[pid] = refs\n",
    "    \n",
    "    print(f\"  -> Extracted {len(refs)} references.\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# DEBUG\n",
    "k = 5\n",
    "if len(paper_ids) > 0:\n",
    "    sample_pid = paper_ids[2]\n",
    "    print(f\"\\nExample Output for {sample_pid} (First {k} refs):\")\n",
    "    print(json.dumps(all_papers_refs[sample_pid][-k:-1], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "debug_dir = os.path.join('..', 'output', 'debug_extracted_refs')\n",
    "os.makedirs(debug_dir, exist_ok=True)\n",
    "\n",
    "target_pid = sample_pid \n",
    "data_to_save = all_papers_refs[target_pid]\n",
    "\n",
    "output_filename = f\"{target_pid}_extracted_refs_2.json\"\n",
    "output_path = os.path.join(debug_dir, output_filename)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_to_save, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Successfully saved extracted references for {target_pid} to:\")\n",
    "print(os.path.abspath(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 From references.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_references_json_file(paper_id):\n",
    "    json_path = os.path.join(RAW_DATA_PATH, paper_id, 'references.json')\n",
    "    \n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"references.json not found for paper {paper_id}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        if data is None or not isinstance(data, dict):\n",
    "            print(f\"Warning: references.json for {paper_id} is empty or invalid format.\")\n",
    "            return []\n",
    "            \n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Could not decode JSON for paper {paper_id}\")\n",
    "        return []\n",
    "        \n",
    "    target_references = []\n",
    "    \n",
    "    for arxiv_id, metadata in data.items():\n",
    "        \n",
    "        title = metadata.get('title', \"\")\n",
    "        authors = metadata.get('authors', [])\n",
    "        date_str = metadata.get('submission_date', \"\")\n",
    "        year = \"\"\n",
    "\n",
    "        if date_str:\n",
    "            year = date_str.split('-')[0]\n",
    "            \n",
    "        target_references.append({\n",
    "            \"id\": arxiv_id,             \n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"year\": year,\n",
    "            \"source_type\": \"references_json\"\n",
    "        })\n",
    "        \n",
    "    return target_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_target_refs = {}\n",
    "\n",
    "for pid in paper_ids:\n",
    "    targets = load_references_json_file(pid)\n",
    "    all_papers_target_refs[pid] = targets\n",
    "    \n",
    "\n",
    "print(f\"\\nLoaded targets for {len(all_papers_target_refs)} papers.\")\n",
    "\n",
    "# DEBUG\n",
    "if len(paper_ids) > 0:\n",
    "    sample_pid = paper_ids[1]\n",
    "    if all_papers_target_refs[sample_pid]:\n",
    "        print(f\"\\nExample Target Entry for {sample_pid}:\")\n",
    "        print(json.dumps(all_papers_target_refs[sample_pid][0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = {\n",
    "    'a', 'an', 'the', 'and', 'or', 'of', 'to', 'in', 'on', 'at', 'by', 'for', \n",
    "    'with', 'from', 'as', 'is', 'are', 'was', 'were', 'be', 'been', 'this', 'that'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # filter stop words\n",
    "    tokens = text.split()\n",
    "    clean_tokens = [t for t in tokens if t not in STOP_WORDS]\n",
    "    \n",
    "    return \" \".join(clean_tokens)\n",
    "\n",
    "def tokenize_author_list(authors_list):\n",
    "    if not authors_list:\n",
    "        return set()\n",
    "    \n",
    "    all_authors = \" \".join(authors_list)\n",
    "    all_authors = all_authors.lower()\n",
    "    all_authors = re.sub(r'[^a-z\\s]', ' ', all_authors)\n",
    "    \n",
    "    # separate all author string into tokens\n",
    "    tokens = set(all_authors.split())\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_references(reference_list):\n",
    "    for reference in reference_list:\n",
    "        reference['clean_title'] = clean_text(reference.get('title', ''))\n",
    "        reference['clean_author_tokens'] = tokenize_author_list(reference.get('authors', []))\n",
    "        \n",
    "    return reference_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning extracted references (Source)...\")\n",
    "for pid in all_papers_refs:\n",
    "    clean_references(all_papers_refs[pid])\n",
    "\n",
    "print(\"Cleaning target references (Target)...\")\n",
    "for pid in all_papers_target_refs:\n",
    "    clean_references(all_papers_target_refs[pid])\n",
    "    \n",
    "print(\"Cleaning completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "if len(paper_ids) > 0:\n",
    "    sample_pid = paper_ids[0]\n",
    "    \n",
    "    # check source (bib)\n",
    "    if all_papers_refs[sample_pid]:\n",
    "        ref = all_papers_refs[sample_pid][0]\n",
    "        print(f\"--- Source (BibTeX) ---\")\n",
    "        print(f\"Original Title:  {ref['title']}\")\n",
    "        print(f\"Clean Title:     {ref['clean_title']}\")\n",
    "        print(f\"Original Authors:{ref['authors']}\")\n",
    "        print(f\"Author Tokens:   {ref['clean_author_tokens']}\")\n",
    "        print(\"\")\n",
    "\n",
    "    # check target (json)\n",
    "    if all_papers_target_refs[sample_pid]:\n",
    "        target = all_papers_target_refs[sample_pid][0]\n",
    "        print(f\"--- Target (JSON) ---\")\n",
    "        print(f\"Original Title:  {target['title']}\")\n",
    "        print(f\"Clean Title:     {target['clean_title']}\")\n",
    "        print(f\"Original Authors:{target['authors']}\")\n",
    "        print(f\"Author Tokens:   {target['clean_author_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Match reference ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_automatic_labels(source_reference_list, target_reference_list):\n",
    "    target_map = {}\n",
    "    for reference in target_reference_list:\n",
    "        # avoid titles that are too short, short titles doesnt contain enough info\n",
    "        if reference['clean_title'] and len(reference['clean_title']) > 10: \n",
    "            target_map[reference['clean_title']] = reference['id']\n",
    "            \n",
    "    matches = {}\n",
    "    \n",
    "    for reference in source_reference_list:\n",
    "        s_title = reference['clean_title']\n",
    "        s_id = reference['id']\n",
    "        \n",
    "        if not s_title or len(s_title) <= 10:\n",
    "            continue\n",
    "            \n",
    "        if s_title in target_map:\n",
    "            matched_target_reference = target_map[s_title]\n",
    "            matches[s_id] = matched_target_reference\n",
    "            \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers_gt = {} # ground truth for all papers\n",
    "total_matches = 0\n",
    "total_refs = 0\n",
    "\n",
    "print(\"Generating Automatic Ground Truth...\")\n",
    "\n",
    "for pid in paper_ids:\n",
    "    sources = all_papers_refs[pid]\n",
    "    targets = all_papers_target_refs[pid]\n",
    "    \n",
    "    paper_matches = generate_automatic_labels(sources, targets)\n",
    "    all_papers_gt[pid] = paper_matches\n",
    "    \n",
    "    n_source = len(sources)\n",
    "    n_match = len(paper_matches)\n",
    "    total_refs += n_source\n",
    "    total_matches += n_match\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total References Processed: {total_refs}\")\n",
    "print(f\"Total Automatic Matches:    {total_matches}\")\n",
    "print(f\"Global Match Rate:          {total_matches/total_refs:.1%}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_gt_dir = os.path.join('..', 'output', 'debug_ground_truth')\n",
    "os.makedirs(debug_gt_dir, exist_ok=True)\n",
    "\n",
    "filename = \"automatic_labels_check_2.json\"\n",
    "output_gt_path = os.path.join(debug_gt_dir, filename)\n",
    "\n",
    "with open(output_gt_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_papers_gt, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Successfully saved automatic labels to:\")\n",
    "print(os.path.abspath(output_gt_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
