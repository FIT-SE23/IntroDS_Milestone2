{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: /Users/thomas200905/Documents/Thomas/HCMUS/Third Year/Semester 7/Intro to Data Science/Milestones/MS02/data/testing/input\n",
      "Output: /Users/thomas200905/Documents/Thomas/HCMUS/Third Year/Semester 7/Intro to Data Science/Milestones/MS02/data/testing/output\n"
     ]
    }
   ],
   "source": [
    "# # Notebook 1: Hierarchical Parsing & Standardization\n",
    "# \n",
    "# **Objectives:**\n",
    "# 1. Parse raw LaTeX into a structured JSON hierarchy.\n",
    "# 2. Normalize text and math formulas.\n",
    "# 3. Extract and deduplicate references (`refs.bib`), filtering only those cited in the text.\n",
    "# 4. Generate unique IDs (Hashes) for content to handle deduplication across versions.\n",
    "\n",
    "import os, re, json, hashlib, shutil, sys\n",
    "from pylatexenc.latexwalker import LatexWalker, LatexEnvironmentNode, LatexCharsNode, \\\n",
    "                                    LatexGroupNode, LatexMathNode, LatexMacroNode, LatexCommentNode, LatexSpecialsNode\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "RAW_DATA_PATH = os.path.join(\"..\", \"data\",'raw', 'auto') \n",
    "OUTPUT_PATH = os.path.join(\"..\", 'data', 'processed', 'parsed', 'auto')\n",
    "\n",
    "# RAW_DATA_PATH = os.path.join(\"..\", \"data\",'testing', 'input') \n",
    "# OUTPUT_PATH = os.path.join(\"..\", 'data', 'testing', 'output')\n",
    "\n",
    "LEVELS = {\n",
    "    \"root\": 0,\n",
    "    \"abstract\": 1,\n",
    "    \"section\": 2,\n",
    "    \"subsection\": 3,\n",
    "    \"subsubsection\": 4,\n",
    "    \"paragraph\": 5,\n",
    "    \"subparagraph\": 6,\n",
    "    \"itemize\": 7,   \n",
    "    \"enumerate\": 7,\n",
    "    \"item\": 8,\n",
    "    \"leaf\": 9       # Sentences, Figures, Tables, Formulas\n",
    "}\n",
    "\n",
    "print(f\"Input: {os.path.abspath(RAW_DATA_PATH)}\")\n",
    "print(f\"Output: {os.path.abspath(OUTPUT_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_latex_string(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\\\bibinfo\\{.*?\\}\\{(.*?)\\}', r'\\1', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{(.*?)\\}', r'\\1', text)\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def clean_body_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    cmds_to_remove = r'\\\\(centering|newpage|clearpage|tableofcontents|maketitle|hrule|vfill)'\n",
    "    text = re.sub(cmds_to_remove, '', text)\n",
    "    text = re.sub(r'\\\\[hv]space\\{.*?\\}', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def normalize_math(latex_code, is_inline=True):\n",
    "    clean = latex_code.strip()\n",
    "    clean = re.sub(r'^\\\\\\(|\\\\\\)$', '', clean)\n",
    "    clean = re.sub(r'^\\\\\\[|\\\\\\]$', '', clean)\n",
    "    clean = re.sub(r'^(\\$)|(\\$)$', '', clean)\n",
    "    \n",
    "    if is_inline:\n",
    "        return f\"${clean}$\"\n",
    "    else:\n",
    "        if clean.startswith(r'\\begin'):\n",
    "            return clean\n",
    "        return f\"\\\\begin{{equation}}{clean}\\\\end{{equation}}\"\n",
    "\n",
    "def generate_id(content, paper_id):\n",
    "    if not content: return None\n",
    "    hash_obj = hashlib.md5(content.encode('utf-8'))\n",
    "    return f\"{paper_id}_{hash_obj.hexdigest()}\"\n",
    "\n",
    "# Avoids splitting on \"et al.\", \"Fig.\", \"Eq.\"\n",
    "SENTENCE_PATTERN = re.compile(r\"(?<!\\b[A-Z])(?<![Ee][Tt] [Aa][Ll])(?<![Ff][Ii][Gg])(?<![Ee][Qq])\\.\\s+(?=[A-Z])\")\n",
    "\n",
    "def split_sentences(text):\n",
    "    if not text: return []\n",
    "    sentences = SENTENCE_PATTERN.split(text)\n",
    "    return [s.strip() for s in sentences if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field(field_name, entry):\n",
    "    pat_braces = re.compile(fr'\\b{field_name}\\s*=\\s*\\{{((?:[^{{}}]|\\{{[^{{}}]*\\}})*)\\}}', re.IGNORECASE | re.DOTALL)\n",
    "    match = pat_braces.search(entry)\n",
    "    if match: return match.group(1)\n",
    "    pat_quotes = re.compile(fr'\\b{field_name}\\s*=\\s*\\\"(.*?)\\\"', re.IGNORECASE | re.DOTALL)\n",
    "    match = pat_quotes.search(entry)\n",
    "    if match: return match.group(1)\n",
    "    return None\n",
    "\n",
    "ADDITIONAL_FIELDS = [\n",
    "    'journal', 'booktitle', 'volume', 'number', 'pages', \n",
    "    'publisher', 'organization', 'school', 'institution', \n",
    "    'doi', 'url', 'issn', 'isbn', 'month', 'editor', 'series'\n",
    "]\n",
    "\n",
    "def extract_from_bib(file_content):\n",
    "    references = []\n",
    "    raw_entries = re.split(r'^@', file_content, flags=re.MULTILINE)\n",
    "    \n",
    "    for entry in raw_entries:\n",
    "        entry = entry.strip()\n",
    "        if not entry or entry.startswith('%'): continue\n",
    "            \n",
    "        key_match = re.search(r'^(\\w+)\\s*\\{\\s*([^,]+),', entry)\n",
    "        if not key_match: continue\n",
    "            \n",
    "        ref_type = key_match.group(1)\n",
    "        ref_id = key_match.group(2).strip()\n",
    "\n",
    "        if ref_type.lower() in ['string', 'comment', 'preamble']: continue\n",
    "        ref_data = {\n",
    "            \"id\": ref_id,\n",
    "            \"type\": ref_type,\n",
    "            \"title\": \"\",\n",
    "            \"authors\": [],\n",
    "            \"year\": \"\",\n",
    "            \"source_type\": \"bib\"\n",
    "        }\n",
    "        raw_authors = get_field('author', entry)\n",
    "        if raw_authors:\n",
    "            cleaned_auth = clean_latex_string(raw_authors)\n",
    "            ref_data[\"authors\"] = [a.strip() for a in cleaned_auth.split(' and ')]\n",
    "            \n",
    "        ref_data[\"title\"] = clean_latex_string(get_field('title', entry))\n",
    "        ref_data[\"year\"] = get_field('year', entry) or \"\"\n",
    "        \n",
    "        for field in ADDITIONAL_FIELDS:\n",
    "            val = get_field(field, entry)\n",
    "            if val:\n",
    "                ref_data[field] = clean_latex_string(val)\n",
    "\n",
    "        references.append(ref_data)\n",
    "    return references\n",
    "\n",
    "def extract_from_bibitem(file_content):\n",
    "    bib_section = re.search(r'\\\\begin\\{thebibliography\\}.*?\\n(.*?)\\\\end\\{thebibliography\\}', file_content, re.DOTALL)\n",
    "    if not bib_section:\n",
    "        bib_section = re.search(r'\\\\begin\\{references\\}.*?\\n(.*?)\\\\end\\{references\\}', file_content, re.DOTALL)\n",
    "    \n",
    "    content_to_parse = bib_section.group(1) if bib_section else file_content\n",
    "    raw_items = re.split(r'\\\\bibitem', content_to_parse)\n",
    "    \n",
    "    references = []\n",
    "    pat_bibinfo = re.compile(r'\\\\bibinfo\\{(.*?)\\}\\{((?:[^{}]|\\{[^{}]*\\})*)\\}', re.DOTALL | re.IGNORECASE)\n",
    "    pat_italic = re.compile(r'(?:\\{\\\\em\\s+|\\\\emph\\{|\\\\textit\\{)((?:[^{}]|\\{[^{}]*\\})*)\\}', re.IGNORECASE | re.DOTALL)\n",
    "    pat_quotes = re.compile(r'``(.*?)''|\"(.*?)\"', re.DOTALL)\n",
    "    pat_year = re.compile(r'\\((\\d{4})\\)')\n",
    "\n",
    "    for item in raw_items:\n",
    "        if not item.strip():\n",
    "            continue\n",
    "        \n",
    "        id_match = re.search(r'^\\{([^}]+)\\}', item.strip())\n",
    "        if not id_match: continue\n",
    "        ref_id = id_match.group(1).strip()\n",
    "        id_end_idx = item.find('}') + 1\n",
    "        \n",
    "        ref_data = {\n",
    "            \"id\": ref_id, \"title\": \"\", \"authors\": [], \"year\": \"\", \n",
    "            \"journal\": \"\", \"source_type\": \"bibitem\"\n",
    "        }\n",
    "        \n",
    "        if r'\\bibinfo' in item:\n",
    "            for key, value in pat_bibinfo.findall(item):\n",
    "                key = key.lower()\n",
    "                val = clean_latex_string(value)\n",
    "                if key == 'author': ref_data['authors'].append(val)\n",
    "                else: ref_data[key] = val\n",
    "            \n",
    "        else:\n",
    "            y_match = pat_year.search(item)\n",
    "            if y_match: ref_data['year'] = y_match.group(1)\n",
    "            quote_match = pat_quotes.search(item)\n",
    "            italic_match = pat_italic.search(item)\n",
    "            if quote_match and (not italic_match or quote_match.start() < italic_match.start()):\n",
    "                raw_title = quote_match.group(1) or quote_match.group(2)\n",
    "                ref_data['title'] = clean_latex_string(raw_title)\n",
    "                \n",
    "                title_start = quote_match.start()\n",
    "                if title_start > id_end_idx:\n",
    "                    raw_auth = item[id_end_idx:title_start]\n",
    "                    ref_data['authors'] = [clean_latex_string(raw_auth).rstrip(',').strip()]\n",
    "                    \n",
    "                if italic_match:\n",
    "                    ref_data['journal'] = clean_latex_string(italic_match.group(1))\n",
    "\n",
    "            elif italic_match:\n",
    "                italic_text = clean_latex_string(italic_match.group(1))\n",
    "                \n",
    "                italic_start = italic_match.start()\n",
    "                pre_text = item[id_end_idx:italic_start].strip()\n",
    "                if pre_text.endswith(',') or pre_text.endswith('.'):\n",
    "                    pre_text = pre_text[:-1]\n",
    "                \n",
    "                last_comma = pre_text.rfind(',')\n",
    "                \n",
    "                if last_comma != -1:\n",
    "                    potential_author = pre_text[:last_comma]\n",
    "                    potential_title = pre_text[last_comma+1:]\n",
    "                    \n",
    "                    # too short, the comma was part of the name\n",
    "                    if len(potential_title.strip()) < 5:\n",
    "                        # case: Book. Italic = Title.\n",
    "                        ref_data['title'] = italic_text\n",
    "                        ref_data['authors'] = [clean_latex_string(pre_text)]\n",
    "                    else:\n",
    "                        # case: Article. Plain = Title, Italic = Journal.\n",
    "                        ref_data['title'] = clean_latex_string(potential_title)\n",
    "                        ref_data['authors'] = [clean_latex_string(potential_author)]\n",
    "                        ref_data['journal'] = italic_text\n",
    "                else:\n",
    "                    # case: \"Author \\emph{BookTitle}\"\n",
    "                    ref_data['title'] = italic_text\n",
    "                    ref_data['authors'] = [clean_latex_string(pre_text)]\n",
    "            else:\n",
    "                ref_data['title'] = clean_latex_string(item[id_end_idx:])\n",
    "\n",
    "        references.append(ref_data)\n",
    "    return references\n",
    "\n",
    "def get_ref_fingerprint(ref):\n",
    "    title = ref.get('title', '')\n",
    "    if not title: return None \n",
    "    \n",
    "    clean_title = re.sub(r'[^a-z0-9]', '', title.lower())\n",
    "    \n",
    "    year = ref.get('year', '')\n",
    "    clean_year = re.sub(r'[^0-9]', '', str(year))\n",
    "    \n",
    "    return f\"{clean_title}_{clean_year}\"\n",
    "\n",
    "def unionize_refs(target, source):\n",
    "    for key, value in source.items():\n",
    "        if key not in target or not target[key]:\n",
    "            target[key] = value\n",
    "    return target\n",
    "\n",
    "def process_batch_get_paper_refs(extracted_list, content_map, key_map):\n",
    "    for ref in extracted_list:\n",
    "        ref_id = ref['id']\n",
    "        fp = get_ref_fingerprint(ref)\n",
    "        \n",
    "        if not fp: \n",
    "            fp = f\"ID_{ref_id}\"\n",
    "\n",
    "        if fp in content_map:\n",
    "            master_ref = content_map[fp]\n",
    "            unionize_refs(master_ref, ref)\n",
    "            key_map[ref_id] = master_ref['id']\n",
    "        else:\n",
    "            content_map[fp] = ref\n",
    "            key_map[ref_id] = ref_id\n",
    "\n",
    "def get_paper_references(paper_id, root_path):\n",
    "    paper_path = os.path.join(root_path, paper_id)\n",
    "    content_map = {}\n",
    "    key_map = {} \n",
    "    \n",
    "    for root, _, files in os.walk(paper_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".bib\"):\n",
    "                try:\n",
    "                    with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        refs = extract_from_bib(f.read())\n",
    "                        process_batch_get_paper_refs(refs, content_map, key_map)\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    for root, _, files in os.walk(paper_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".tex\"):\n",
    "                try:\n",
    "                    with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                        if \"bibitem\" in content:\n",
    "                            refs = extract_from_bibitem(content)\n",
    "                            process_batch_get_paper_refs(refs, content_map, key_map)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "    return list(content_map.values()), key_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_latex_nodes(nodes, base_dir, current_level_name=\"leaf\"):\n",
    "    elements = []\n",
    "    citations = set()\n",
    "    \n",
    "    for node in nodes:\n",
    "        if node is None or isinstance(node, LatexCommentNode):\n",
    "            continue\n",
    "\n",
    "        if isinstance(node, LatexCharsNode):\n",
    "            text = clean_body_text(node.chars)\n",
    "            if text:\n",
    "                for s in split_sentences(text):\n",
    "                    elements.append({\n",
    "                        'content': s,\n",
    "                        'level': LEVELS['leaf'],\n",
    "                        'type': 'sentence'\n",
    "                    })\n",
    "\n",
    "        elif isinstance(node, LatexMathNode):\n",
    "            is_inline = (node.displaytype == 'inline')\n",
    "            norm_math = normalize_math(node.latex_verbatim(), is_inline)\n",
    "            elements.append({\n",
    "                'content': norm_math,\n",
    "                'level': LEVELS['leaf'],\n",
    "                'type': 'math'\n",
    "            })\n",
    "\n",
    "        elif isinstance(node, LatexEnvironmentNode):\n",
    "            env = node.environmentname.lower()\n",
    "            if env in ['thebibliography', 'references']:\n",
    "                continue\n",
    "            if env in ['figure', 'table', 'figure*', 'table*']:\n",
    "                elements.append({\n",
    "                    'content': node.latex_verbatim(), \n",
    "                    'level': LEVELS['leaf'],\n",
    "                    'type': 'figure'\n",
    "                })\n",
    "            elif env in ['itemize', 'enumerate']:\n",
    "                elements.append({\n",
    "                    'content': f\"List ({env})\", \n",
    "                    'level': LEVELS['itemize'],\n",
    "                    'type': env\n",
    "                })\n",
    "                sub_elems, sub_cites = parse_latex_nodes(node.nodelist, base_dir, 'item')\n",
    "                elements.extend(sub_elems)\n",
    "                citations.update(sub_cites)\n",
    "            elif env in ['equation', 'align', 'equation*', 'align*']:\n",
    "                norm_math = normalize_math(node.latex_verbatim(), is_inline=False)\n",
    "                elements.append({\n",
    "                    'content': norm_math,\n",
    "                    'level': LEVELS['leaf'],\n",
    "                    'type': 'math_block'\n",
    "                })\n",
    "                \n",
    "            else:\n",
    "                if env == 'abstract':\n",
    "                    elements.append({'content': 'Abstract', 'level': LEVELS['abstract'], 'type': 'section'})\n",
    "                \n",
    "                sub_elems, sub_cites = parse_latex_nodes(node.nodelist, base_dir, env)\n",
    "                elements.extend(sub_elems)\n",
    "                citations.update(sub_cites)\n",
    "\n",
    "        elif isinstance(node, LatexMacroNode):\n",
    "            name = node.macroname\n",
    "            \n",
    "            if name == 'bibliography':\n",
    "                continue\n",
    "\n",
    "            if name in ['section', 'subsection', 'subsubsection', 'paragraph', 'subparagraph']:\n",
    "                if node.nodeargd and node.nodeargd.argnlist:\n",
    "                    title = \"\"\n",
    "                    for arg in node.nodeargd.argnlist:\n",
    "                        if not arg:\n",
    "                            continue\n",
    "                        \n",
    "                        if hasattr(arg, 'nodelist'):\n",
    "                            title += \"\".join([n.chars for n in arg.nodelist if isinstance(n, LatexCharsNode)])\n",
    "                        \n",
    "                        elif isinstance(arg, LatexCharsNode):\n",
    "                            title += arg.chars\n",
    "                    \n",
    "                    elements.append({\n",
    "                        'content': clean_body_text(title),\n",
    "                        'level': LEVELS.get(name, LEVELS['section']),\n",
    "                        'type': name\n",
    "                    })\n",
    "\n",
    "            elif name == 'item':\n",
    "                elements.append({\n",
    "                    'content': \"Item\", \n",
    "                    'level': LEVELS['item'],\n",
    "                    'type': 'item'\n",
    "                })\n",
    "            elif name in ['cite', 'citep', 'citet', 'citeauthor']:\n",
    "                if node.nodeargd and node.nodeargd.argnlist:\n",
    "                    for arg in node.nodeargd.argnlist:\n",
    "                        if not arg:\n",
    "                            continue\n",
    "\n",
    "                        if hasattr(arg, 'nodelist'):\n",
    "                            for n in arg.nodelist:\n",
    "                                if isinstance(n, LatexCharsNode):\n",
    "                                    keys = n.chars.split(',')\n",
    "                                    for k in keys: citations.add(k.strip())\n",
    "                        elif isinstance(arg, LatexCharsNode):\n",
    "                            keys = arg.chars.split(',')\n",
    "                            for k in keys: citations.add(k.strip())\n",
    "\n",
    "                elements.append({\n",
    "                    'content': node.latex_verbatim(), \n",
    "                    'level': LEVELS['leaf'],\n",
    "                    'type': 'citation'\n",
    "                })\n",
    "\n",
    "            elif name in ['input', 'include']:\n",
    "                if node.nodeargd and node.nodeargd.argnlist:\n",
    "                    arg_node = node.nodeargd.argnlist[0]\n",
    "                    fname = \"\"\n",
    "                    if hasattr(arg_node, 'nodelist') and arg_node.nodelist:\n",
    "                         if isinstance(arg_node.nodelist[0], LatexCharsNode):\n",
    "                             fname = arg_node.nodelist[0].chars\n",
    "                    elif isinstance(arg_node, LatexCharsNode):\n",
    "                        fname = arg_node.chars\n",
    "                        \n",
    "                    if fname:\n",
    "                        if not fname.endswith('.tex'):\n",
    "                            fname += '.tex'\n",
    "                        \n",
    "                        full_path = os.path.join(base_dir, fname)\n",
    "                        if os.path.exists(full_path):\n",
    "                            print(f\"    -> Parsing input: {fname}\")\n",
    "                            try:\n",
    "                                with open(full_path, 'r', encoding='latin-1') as f:\n",
    "                                    sub_nodes, _, _ = LatexWalker(f.read()).get_latex_nodes()\n",
    "                                    sub_elems, sub_cites = parse_latex_nodes(sub_nodes, base_dir, current_level_name)\n",
    "                                    elements.extend(sub_elems)\n",
    "                                    citations.update(sub_cites)\n",
    "                            except Exception as e:\n",
    "                                print(f\"    [Error] Failed to parse input {fname}: {e}\")\n",
    "\n",
    "    return elements, citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hierarchy_tree(flat_elements, paper_id, global_elements_store):\n",
    "    hierarchy = {}\n",
    "    \n",
    "    root_content = f\"Document Root {paper_id}\"\n",
    "    root_id = generate_id(root_content, paper_id)\n",
    "    global_elements_store[root_id] = root_content\n",
    "    \n",
    "    stack = [(0, root_id)]\n",
    "    \n",
    "    for item in flat_elements:\n",
    "        content = item['content']\n",
    "        level = item['level']\n",
    "        if not content:\n",
    "            continue\n",
    "        \n",
    "        node_id = generate_id(content, paper_id)\n",
    "        global_elements_store[node_id] = content\n",
    "\n",
    "        while stack and stack[-1][0] >= level:\n",
    "            stack.pop()\n",
    "            \n",
    "        if stack:\n",
    "            parent_id = stack[-1][1]\n",
    "            hierarchy[node_id] = parent_id\n",
    "        \n",
    "        if level < LEVELS['leaf']:\n",
    "            stack.append((level, node_id))\n",
    "            \n",
    "    return hierarchy\n",
    "\n",
    "def write_refs_bib(references, output_path):\n",
    "    internal_keys = {'id', 'type', 'source_type', 'authors'}\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for ref in references:\n",
    "            entry_type = ref.get('type', 'misc').lower()\n",
    "            key = ref.get('id', 'unknown')\n",
    "            \n",
    "            lines = [f\"@{entry_type}{{{key},\"]\n",
    "            \n",
    "            if ref.get('authors'):\n",
    "                auth_str = \" and \".join(ref['authors'])\n",
    "                lines.append(f\"  author = {{{auth_str}}},\")\n",
    "            \n",
    "            for k, v in ref.items():\n",
    "                if k not in internal_keys and v:\n",
    "                    lines.append(f\"  {k} = {{{v}}},\")\n",
    "                \n",
    "            lines.append(\"}\\n\")\n",
    "            f.write(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel processing of 6 papers with 30 threads...\n",
      "--------------------------------------------------\n",
      "[2211.03026] Done. Elements: 310, Refs: 26\n",
      "[2211.03016] Done. Elements: 121, Refs: 6\n",
      "    -> Parsing input: LaTeX/1_introduction.tex\n",
      "    -> Parsing input: LaTeX/2_related_works.tex\n",
      "    -> Parsing input: LaTeX/3_middleGAN.tex\n",
      "    -> Parsing input: content/1-introduction.tex\n",
      "    -> Parsing input: content/2-related-work.tex\n",
      "    -> Parsing input: LaTeX/4_evaluation.tex\n",
      "    -> Parsing input: content/3-method.tex\n",
      "    -> Parsing input: LaTeX/6_conclusion.tex\n",
      "    -> Parsing input: content/4-evaluation.tex\n",
      "    -> Parsing input: content/5-discussion.tex\n",
      "    -> Parsing input: content/6-conclusions.tex\n",
      "[2211.03144] Done. Elements: 429, Refs: 45\n",
      "    -> Parsing input: content/8-appendix.tex\n",
      "[2211.03002] Done. Elements: 612, Refs: 29\n",
      "[2211.03001] Done. Elements: 674, Refs: 43\n",
      "    -> Parsing input: tex/abstract.tex\n",
      "    -> Parsing input: tex/intro.tex\n",
      "    -> Parsing input: tex/related_works.tex\n",
      "    -> Parsing input: tex/method.tex\n",
      "    -> Parsing input: tex/experiments2.tex\n",
      "    -> Parsing input: tex/Tables/Architectures.tex\n",
      "    -> Parsing input: tex/Tables/position.tex\n",
      "    -> Parsing input: tex/Tables/movinet_clean.tex\n",
      "    -> Parsing input: tex/Tables/FinalResults.tex\n",
      "    -> Parsing input: tex/Tables/Device.tex\n",
      "    -> Parsing input: tex/conclusion.tex\n",
      "    -> Parsing input: tex/abstract.tex\n",
      "    -> Parsing input: tex/intro.tex\n",
      "    -> Parsing input: tex/related_works.tex\n",
      "    -> Parsing input: tex/method.tex\n",
      "    -> Parsing input: tex/experiments.tex\n",
      "    -> Parsing input: tex/Tables/Architectures.tex\n",
      "    -> Parsing input: tex/Tables/position.tex\n",
      "    -> Parsing input: tex/Tables/movinet_clean.tex\n",
      "    -> Parsing input: tex/Tables/FinalResults.tex\n",
      "    -> Parsing input: tex/Tables/delta.tex\n",
      "    -> Parsing input: tex/Tables/Device.tex\n",
      "    -> Parsing input: tex/conclusion.tex\n",
      "[2211.03004] Done. Elements: 437, Refs: 41\n",
      "--------------------------------------------------\n",
      "All papers processed.\n"
     ]
    }
   ],
   "source": [
    "def process_paper(paper_id):\n",
    "    source_dir = os.path.join(RAW_DATA_PATH, paper_id)\n",
    "    target_dir = os.path.join(OUTPUT_PATH, paper_id)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    for fname in ['metadata.json', 'references.json']:\n",
    "        src = os.path.join(source_dir, fname)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, os.path.join(target_dir, fname))\n",
    "            \n",
    "    all_raw_refs, key_map = get_paper_references(paper_id, RAW_DATA_PATH)\n",
    "    ref_lookup = {r['id']: r for r in all_raw_refs}\n",
    "    \n",
    "    tex_root = os.path.join(source_dir, 'tex')\n",
    "    if not os.path.exists(tex_root):\n",
    "        return\n",
    "\n",
    "    global_elements = {}\n",
    "    hierarchy_output = {}\n",
    "    all_cited_keys = set()\n",
    "    \n",
    "    for v_folder in sorted(os.listdir(tex_root)):\n",
    "        full_v_path = os.path.join(tex_root, v_folder)\n",
    "        if not os.path.isdir(full_v_path):\n",
    "            continue\n",
    "\n",
    "        main_file = None\n",
    "        for f in os.listdir(full_v_path):\n",
    "            if f.endswith('.tex'):\n",
    "                try:\n",
    "                    with open(os.path.join(full_v_path, f), 'r', encoding='latin-1') as tf:\n",
    "                        if r'\\documentclass' in tf.read():\n",
    "                            main_file = f\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if not main_file:\n",
    "            continue\n",
    "        \n",
    "        with open(os.path.join(full_v_path, main_file), 'r', encoding='latin-1') as f:\n",
    "            w = LatexWalker(f.read())\n",
    "            nodes, _, _ = w.get_latex_nodes()\n",
    "            \n",
    "        elements, citations = parse_latex_nodes(nodes, full_v_path)\n",
    "        \n",
    "        normalized_citations = set()\n",
    "        for c in citations:\n",
    "            master_key = key_map.get(c, c)\n",
    "            normalized_citations.add(master_key)\n",
    "            \n",
    "        all_cited_keys.update(normalized_citations)\n",
    "\n",
    "        version_key = v_folder.split('v')[-1]\n",
    "        hierarchy_output[version_key] = build_hierarchy_tree(elements, paper_id, global_elements)\n",
    "\n",
    "    final_json = {\"elements\": global_elements, \"hierarchy\": hierarchy_output}\n",
    "    with open(os.path.join(target_dir, 'hierarchy.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_json, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "    final_refs = []\n",
    "    for k in all_cited_keys:\n",
    "        if k in ref_lookup:\n",
    "            final_refs.append(ref_lookup[k])\n",
    "    \n",
    "    if not final_refs and all_raw_refs:\n",
    "        final_refs = all_raw_refs\n",
    "        \n",
    "    write_refs_bib(final_refs, os.path.join(target_dir, 'refs.bib'))\n",
    "    \n",
    "    print(f\"[{paper_id}] Done. Elements: {len(global_elements)}, Refs: {len(final_refs)}\")\n",
    "\n",
    "paper_ids = [d for d in os.listdir(RAW_DATA_PATH) if os.path.isdir(os.path.join(RAW_DATA_PATH, d))]\n",
    "paper_ids.sort()\n",
    "\n",
    "MAX_WORKERS = 30 \n",
    "\n",
    "print(f\"Starting parallel processing of {len(paper_ids)} papers with {MAX_WORKERS} threads...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(process_paper, pid): pid for pid in paper_ids}\n",
    "    \n",
    "    for future in as_completed(futures):\n",
    "        pid = futures[future]\n",
    "        try:\n",
    "            future.result() \n",
    "        except Exception as e:\n",
    "            print(f\"\\n[CRITICAL ERROR] Failed to process {pid}: {e}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"All papers processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
