{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c961a512-b86b-4c15-b330-ac27d4724431",
   "metadata": {},
   "source": [
    "### 0. Import libraries and implement functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af9fddae-53b6-47c1-afd0-334fd7327c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylatexenc.latexwalker import LatexWalker, LatexEnvironmentNode, LatexCharsNode, LatexCommentNode,\\\n",
    "                                    LatexGroupNode, LatexMathNode, LatexMacroNode, LatexSpecialsNode\n",
    "import sys, re, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0141011f-5f72-4c24-bfba-c62469f61772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tex_files(dir, max_depth):\n",
    "    if max_depth == 0 or not os.path.isdir(dir):\n",
    "        return []\n",
    "\n",
    "    fs = os.listdir(dir)\n",
    "\n",
    "    if dir != \".\":\n",
    "        fs = [dir + \"/\" + f for f in fs]\n",
    "\n",
    "    sub = [f for f in fs if os.path.isdir(f)]\n",
    "    fs = [f for f in fs if os.path.isfile(f) and f.endswith(\".tex\")]\n",
    "\n",
    "    for d in sub:\n",
    "        fs.extend(find_tex_files(d, max_depth - 1))\n",
    "    return fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ce43a3-471c-4036-90f9-4242c9edacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_begin_document(files):\n",
    "    for file in files:\n",
    "        with open(file, \"r\") as f:\n",
    "            text = f.read()\n",
    "            if r\"\\begin{document}\" in text:\n",
    "                return file\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae037fc7-1ad4-45ee-a8fb-9b07131d95ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_include(tex):\n",
    "    pattern = r\"\\\\(?:include|input)\\{([\\w\\d\\/_]+)\\}\"\n",
    "    lines = tex.split(\"\\n\")\n",
    "    files = []\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i]\n",
    "\n",
    "        percent_idx = 0\n",
    "        while True:\n",
    "            percent_idx = line.find(\"%\", percent_idx)\n",
    "            if percent_idx == -1:\n",
    "                break\n",
    "\n",
    "            if percent_idx == 0 or (percent_idx > 0 and line[percent_idx - 1] != '\\\\'):\n",
    "                break\n",
    "            percent_idx += 1\n",
    "\n",
    "        if percent_idx == 0:\n",
    "            continue\n",
    "        elif percent_idx != -1:\n",
    "            line = line[:percent_idx - 1]\n",
    "\n",
    "        captured = re.findall(pattern, line)\n",
    "        if len(captured) > 0:\n",
    "            files.extend(captured)\n",
    "\n",
    "    return list(set(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5449c50-5e32-42f1-8831-e330dce3c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latex_nodes(fp):\n",
    "    with open(fp, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    newcommand_pattern = r\"^\\\\newcommand\\{([^}]+)\\}(?:\\[[^]]+\\])?\\{(.+)\\}$\"\n",
    "    newcommands = re.findall(newcommand_pattern, text, flags=re.M)\n",
    "    # print(newcommands)\n",
    "    for (short_hand, cmd) in newcommands:\n",
    "        short_hand = short_hand.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "        cmd = cmd.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "        text = re.sub(short_hand, cmd, text)\n",
    "\n",
    "    text = re.sub(newcommand_pattern, \"\", text)\n",
    "\n",
    "    w = LatexWalker(text)\n",
    "    nodes, _, _ = w.get_latex_nodes()\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6725286-7dd3-4e8a-b1e7-e4f628db6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_PATTERN = re.compile(r\"(?<!\\b[A-Z])(?<![Ee][Tt] [Aa][Ll])\\.\\s+(?=[A-Z])\")\n",
    "\n",
    "def split_sentences(text, level):\n",
    "    sentences = SENTENCE_PATTERN.split(text)\n",
    "    sentences = [(sentence, level) for sentence in sentences]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11883365-066b-4143-b992-a1910971761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVELS = {\n",
    "    \"document\": 0,\n",
    "    \"abstract\": 1,\n",
    "    \"section\": 1,\n",
    "    \"subsection\": 2,\n",
    "    \"subsubsection\": 3,\n",
    "    \"paragraph\": 4,\n",
    "    \"subparagraph\": 5,\n",
    "    \"itemize\": 6,\n",
    "    \"item\": 7,\n",
    "    \"leaf\": 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c8a3170-a61f-4310-94c4-e5aeba572524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchy_nodes(nodes, dir, append_trailing=False):\n",
    "    text = \"\"\n",
    "    tokens = []\n",
    "    refs = []\n",
    "\n",
    "    for node in nodes:\n",
    "        if node == None or isinstance(node, LatexCommentNode):\n",
    "            continue\n",
    "\n",
    "        if isinstance(node, LatexCharsNode):\n",
    "            chars = node.chars\n",
    "            text += chars\n",
    "        elif isinstance(node, LatexMathNode):\n",
    "            text += node.latex_verbatim()\n",
    "        elif isinstance(node, LatexGroupNode):\n",
    "            text += node.latex_verbatim()\n",
    "            # tokens.extend(node.nodelist)\n",
    "        elif isinstance(node, LatexEnvironmentNode):\n",
    "            env_name = node.environmentname.lower()\n",
    "            if env_name in [\"figure\", \"figure*\", \"equation\", \"equation*\", \"align\", \"align*\", \"table\", \"remark\", \"remark*\"]:\n",
    "                tokens.extend(split_sentences(text, LEVELS[\"leaf\"]))\n",
    "                tokens.append((node.latex_verbatim(), LEVELS[\"leaf\"]))\n",
    "                text = \"\"\n",
    "            elif env_name == \"itemize\":\n",
    "                tokens.extend(split_sentences(text, LEVELS[\"leaf\"]))\n",
    "                tokens.append((env_name, LEVELS[env_name]))\n",
    "                latex = node.latex_verbatim()\n",
    "                # pattern = r\"\\\\(begin|end)\\{\" + env_name + r\"\\}\" + r\"(\\[[^]]+\\])?\"\n",
    "                pattern = r\"\\\\(begin|end)\\{itemize\\}(\\[[^]]+\\])?\"\n",
    "                latex = re.sub(pattern, \"\", latex, flags=re.IGNORECASE).strip()\n",
    "\n",
    "                items = re.split(r\"\\\\item\", latex, flags=re.IGNORECASE)\n",
    "                for item in items:\n",
    "                    item = item.strip()\n",
    "                    if len(item) == 0:\n",
    "                        continue\n",
    "\n",
    "                    tokens.append((\"item\", LEVELS[\"item\"]))\n",
    "                    tokens.extend(split_sentences(item, LEVELS[\"leaf\"]))\n",
    "\n",
    "            elif env_name in \"document\":\n",
    "                tokens.append((env_name, LEVELS[env_name]))\n",
    "                sub_tokens, sub_refs = hierarchy_nodes(node.nodelist, dir, True)\n",
    "                tokens.extend(sub_tokens)\n",
    "                refs.extend(sub_refs)\n",
    "            elif env_name in \"abstract\":\n",
    "                tokens.append((env_name, LEVELS[env_name]))\n",
    "                latex = node.latex_verbatim()\n",
    "                latex = re.sub(r\"\\\\(begin|end)\\{abstract\\}\", \"\", latex, flags=re.IGNORECASE).strip()\n",
    "                tokens.extend(split_sentences(latex, LEVELS[\"leaf\"]))\n",
    "            else:\n",
    "                # print(env_name, \"[Environment]\")\n",
    "                sub_tokens, sub_refs = hierarchy_nodes(node.nodelist, dir, True)\n",
    "                tokens.extend(sub_tokens)\n",
    "                refs.extend(sub_refs)\n",
    "        elif isinstance(node, LatexMacroNode):\n",
    "            if node.macroname in [\"input\", \"include\"]:\n",
    "                if len(node.nodeargd.argnlist) != 1 or len(node.nodeargd.argnlist[0].nodelist) != 1 \\\n",
    "                        or not isinstance(node.nodeargd.argnlist[0].nodelist[0], LatexCharsNode):\n",
    "                    print(\"\\t\"*tabs, \"?Empty?\", \"[\\\\include]\")\n",
    "                    exit(1)\n",
    "                else:\n",
    "                    tokens.extend(split_sentences(text, LEVELS[\"leaf\"]))\n",
    "                    text = \"\"\n",
    "\n",
    "                    fp = os.path.join(dir, node.nodeargd.argnlist[0].nodelist[0].chars)\n",
    "                    dependencies_nodes = get_latex_nodes(fp)\n",
    "                    print(\"Parse\", fp)\n",
    "                    sub_tokens, sub_refs = hierarchy_nodes(dependencies_nodes, dir, True)\n",
    "                    tokens.extend(sub_tokens)\n",
    "                    refs.extend(sub_refs)\n",
    "\n",
    "                    text = \"\"\n",
    "            elif node.macroname in [\"section\", \"subsection\", \"subsubsection\", \"paragraph\", \"subparagraph\"]:\n",
    "                tokens.extend(split_sentences(text, LEVELS[\"leaf\"]))\n",
    "                tokens.append((node.latex_verbatim(), LEVELS[node.macroname]))\n",
    "                text = \"\"\n",
    "            elif node.macroname in [\"label\", \"footnote\"]:\n",
    "                latex = node.latex_verbatim()\n",
    "                pattern = fr\"\\\\{node.macroname}\" + \"{[^}]+}\"\n",
    "                latex = re.sub(pattern, \"\", latex)\n",
    "                text += latex\n",
    "            elif node.macroname in [\"cite\", \"citep\", \"citet\"]:\n",
    "                latex = node.latex_verbatim()\n",
    "                latex = re.sub(r\"\\\\\" + node.macroname + r\"\\{\", \"\", latex)\n",
    "                latex = latex[:-2]\n",
    "                refs.extend(latex.split(\",\"))\n",
    "            else:\n",
    "                text += node.latex_verbatim()\n",
    "                pass\n",
    "        elif isinstance(node, LatexSpecialsNode):\n",
    "            text += node.specials_chars\n",
    "\n",
    "    if append_trailing and text != \"\":\n",
    "        tokens.extend(split_sentences(text, LEVELS[\"leaf\"]))\n",
    "\n",
    "    return tokens, refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aca8668e-9707-4132-86dc-6ed4171c60dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchy_version(version_directory):\n",
    "    main_tex_fp = os.path.join(version_directory, \"main.tex\")\n",
    "\n",
    "    if not os.path.exists(main_tex_fp):\n",
    "        depth1_tex_files = find_tex_files(version_directory, 1)\n",
    "        main_tex_fp = find_begin_document(depth1_tex_files)\n",
    "        if main_tex_fp == None:\n",
    "            print(\"Not found main tex file\")\n",
    "            return None\n",
    "\n",
    "    nodes = get_latex_nodes(main_tex_fp)\n",
    "    nodes, refs = hierarchy_nodes(nodes, version_directory)\n",
    "    nodes = [node for node in nodes if len(node[0]) > 0]\n",
    "    refs = list(set(refs))\n",
    "\n",
    "    ignore_idx = 0\n",
    "    while ignore_idx < len(nodes) and nodes[ignore_idx] != (\"document\", 0):\n",
    "        ignore_idx += 1\n",
    "    nodes = nodes[ignore_idx:]\n",
    "\n",
    "    node_stack = [0]\n",
    "    node_hierarchy = {0: 0}\n",
    "\n",
    "    for i in range(1, len(nodes)):\n",
    "        last_node = nodes[node_stack[-1]]\n",
    "        last_node_level = last_node[1]\n",
    "\n",
    "        current_node = nodes[i]\n",
    "        current_node_level = current_node[1]\n",
    "\n",
    "        if current_node_level > last_node_level:\n",
    "            node_hierarchy[i] = node_stack[-1]\n",
    "            node_stack.append(i)\n",
    "        elif current_node_level == last_node_level:\n",
    "            node_hierarchy[i] = node_hierarchy[node_stack[-1]]\n",
    "        else:\n",
    "            while len(nodes) > 0 and current_node_level <= nodes[node_stack[-1]][1]:\n",
    "                node_stack.pop()\n",
    "\n",
    "            if len(nodes) == 0:\n",
    "                node_hierarchy[i] = 0\n",
    "            else:\n",
    "                node_hierarchy[i] = node_stack[-1]\n",
    "\n",
    "            node_stack.append(i)\n",
    "\n",
    "    return nodes, node_hierarchy, refs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c4818-3d77-46d8-bd95-8526a5cf1dd8",
   "metadata": {},
   "source": [
    "### 1. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f86e2c17-b992-463d-82df-dd4f3056af4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse ../../23127247_milestone1/2210.16424/tex/2210.16424v1/math_commands.tex\n",
      "Refs: ['zhu2019deep', 'cao2015toward', 'bonawitz2021federate', 'wang2021field', 'liu2021federaser', 'thomee2016yfcc100', 'Berlekamp1968AlgebraicCT', 'chen2020breaking', 'pmlr-v119-guo20', 'gardner2014measurin', 'so2022lightsecagg', 'geiping2020invertin', 'pmlr-v119-guo20c', 'hartigan1979algorith', 'ghosh2020efficient', 'gardner2014measuring', 'gardner20143', 'wu2022federated', 'bourtoule2021machin', 'caldas2018lea', 'bourtoule2021machine', 'bonawitz2017practical', 'kedlaya2011fas', 'gan2017', 'vassilvitskii2006', 'blackard1999comparativ', 'gandikota2021vqsg', 'kissner2005privacy', 'deng2009imagene', 'hutter2018cance', 'han2018mapping', 'chen2022fundamenta', 'chung2022federate', 'kairouz2021advances', 'dennis2021heterogeneit', 'so2022lightsecag', 'mahajan2012plana', 'fredrikson2015model', 'dennis2021heterogeneity', 'bell2020secure', 'chien2018query', 'li2022secur', 'guha2003clustering', 'seo2012constan', 'acharya2019communication', 'chien2022certifie', 'lee2017improve', 'sudlow2015u', 'veale2018algorithm', 'sekhari2021remembe', 'wu2022federate', 'mcmahan2017communication', 'ginart2019making', 'sattler2020clustere', 'reed1960polynomia', 'chawla2013k', 'eisinberg2006inversio', 'cohen2017emnis', 'chen2021grap', 'lloyd1982leas', 'massey1969shif', 'bahmani2012scalabl', 'wang2022federate', 'frikken2007privacy', 'liu2021federase', 'peterson2009ni', 'mansour2020thre', 'ginart2019makin', 'beguier2020efficient', 'ergun2021sparsifie', 'ailon2009streamin']\n",
      "('document', 0) ('document', 0)\n",
      "('abstract', 1) ('document', 0)\n",
      "('Federated clustering is an unsupervised learning problem that arises in a number of practical applications, including personalized recommender and healthcare systems', 8) ('abstract', 1)\n",
      "(\"With the adoption of recent laws ensuring the ``right to be forgotten'', the problem of machine unlearning for federated clustering methods has become of significant importance\", 8) ('abstract', 1)\n",
      "('This work proposes the first known unlearning mechanism for federated clustering with privacy criteria that support simple, provable, and efficient data removal at the client and server level', 8) ('abstract', 1)\n",
      "('The gist of our approach is to combine special initialization procedures with quantization methods that allow for secure aggregation of estimated local cluster counts at the server unit', 8) ('abstract', 1)\n",
      "('As part of our platform, we introduce \\\\emph{secure compressed multiset aggregation (SCMA)}, which is of independent interest for secure sparse model aggregation', 8) ('abstract', 1)\n",
      "('In order to simultaneously facilitate low communication complexity and secret sharing protocols, we integrate Reed-Solomon encoding with special evaluation points into the new SCMA pipeline and derive bounds on the time and communication complexity of different components of the scheme', 8) ('abstract', 1)\n",
      "('Compared to completely retraining $K$-means++ locally and globally for each removal request, we obtain an average speed-up of roughly $84$x across seven datasets, two of which contain biological and medical information that is subject to frequent unlearning requests.', 8) ('abstract', 1)\n",
      "('\\n\\n\\n\\\\maketitle\\n\\n\\n\\n', 8) ('abstract', 1)\n",
      "('\\\\section{Introduction}', 1) ('document', 0)\n",
      "('\\nThe availability of large volumes of user training data has contributed to the success of modern machine learning models', 8) ('\\\\section{Introduction}', 1)\n",
      "('For example, most state-of-the-art computer vision models are trained on large-scale image datasets including Flickr~ and ImageNet~', 8) ('\\\\section{Introduction}', 1)\n",
      "('Organizations and repositories that collect and store user data must comply with privacy regulations, such as the recent European Union General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and the Canadian Consumer Privacy Protection Act (CPPA), all of which guarantee the right of users to remove their data from the datasets (\\\\emph{Right to be Forgotten})', 8) ('\\\\section{Introduction}', 1)\n",
      "('Data removal requests frequently arise in practice, especially for sensitive datasets pertaining to medical records (numerous machine learning models in computational biology are trained using UK Biobank~ which hosts a collection of genetic and medical records of roughly half a million patients~)', 8) ('\\\\section{Introduction}', 1)\n",
      "('Removing user data from a dataset is insufficient to ensure sufficient privacy, since training data can often be reconstructed from trained models~', 8) ('\\\\section{Introduction}', 1)\n",
      "('This motivates the study of \\\\emph{machine unlearning}~ which aims to efficiently eliminate the influence of certain data points on a model', 8) ('\\\\section{Introduction}', 1)\n",
      "('Naively, one can retrain the model from scratch to ensure complete removal, yet retraining comes at a high computational cost and is thus not practical when accommodating frequent removal requests', 8) ('\\\\section{Introduction}', 1)\n",
      "('To avoid complete retraining, specialized approaches have to be developed for each unlearning application~', 8) ('\\\\section{Introduction}', 1)\n",
      "('At the same time, federated learning (FL) has emerged as a promising approach to enable distributed training over a large number of users while protecting their privacy~', 8) ('\\\\section{Introduction}', 1)\n",
      "('The key idea of FL is to keep user data on their devices and train global models by aggregating local models in a communication-efficient and secure manner', 8) ('\\\\section{Introduction}', 1)\n",
      "('Due to model inversion attacks~, secure local model aggregation at the server is a critical consideration in FL, as it guarantees that the server cannot get specific information about client data based on their local models~', 8) ('\\\\section{Introduction}', 1)\n",
      "('Since data privacy is the main goal in FL, it should be natural for a FL framework to allow for frequent data removal of a subset of client data in a cross-silo setting (e.g., when several patients request their data to be removed in the hospital database), or the entire local dataset for clients in a cross-device setting (e.g., when users request apps not to track their data on their phones)', 8) ('\\\\section{Introduction}', 1)\n",
      "('This leads to the largely unstudied problem termed \\\\emph{federated unlearning}~', 8) ('\\\\section{Introduction}', 1)\n",
      "('However, existing federated unlearning methods do not guarantee exact or approximate unlearning~, they do not come with theoretical performance guarantees after model updates, and often, they are vulnerable to adversarial attacks.\\n\\n\\\\textbf{Our contributions.} We describe the first unlearning framework for a newly developed federated clustering approach (Figure~\\\\ref{fig:framework_overview}) which addresses the following problems: \\n', 8) ('\\\\section{Introduction}', 1)\n",
      "('itemize', 6) ('\\\\section{Introduction}', 1)\n",
      "('item', 7) ('itemize', 6)\n",
      "(\"Unlearning of clusters at the client level via an intuitive application of $K$-means++ initialization without subsequent Lloyd's iterations\", 8) ('item', 7)\n",
      "('The advantage of using $K$-means++ is that the models do not need to be updated unless removed points were selected as initial centroids', 8) ('item', 7)\n",
      "('At the server level, full $K$-means++  can be performed to improve the overall federated clustering performance.', 8) ('item', 7)\n",
      "('item', 7) ('itemize', 6)\n",
      "('Enabling secure aggregation of sparse vectors via a novel approach termed \\\\emph{secure compressed multiset aggregation} (SCMA)', 8) ('item', 7)\n",
      "('The gist of SCMA is to encode quantized local data into information pertaining to the identifiers of quantization bins and their occupancies using specialized Reed-Solomon code evaluations, and followed by secure model aggregations.', 8) ('item', 7)\n",
      "('item', 7) ('itemize', 6)\n",
      "('Ensuring privacy-accuracy-efficiency trade-offs through the introduction of a privacy criterion suitable for federated clustering and unlearning with simple communication protocols', 8) ('item', 7)\n",
      "('In this context, we derive theoretical performance guarantees for the computational and communication complexity of model training and unlearning procedures.', 8) ('item', 7)\n",
      "('item', 7) ('itemize', 6)\n",
      "('Compiling a collection of datasets for benchmarking unlearning of federated clusters, including two new datasets which contain anonymized cancer genomics and microbiome information that is subject to frequent unlearning requests', 8) ('item', 7)\n",
      "('Experimental results reveal that our one-shot algorithm offers an average speed-up of roughly $84$x compared to complete retraining across seven datasets.', 8) ('item', 7)\n",
      "('\\nThe availability of large volumes of user training data has contributed to the success of modern machine learning models', 8) ('item', 7)\n",
      "('For example, most state-of-the-art computer vision models are trained on large-scale image datasets including Flickr~ and ImageNet~', 8) ('item', 7)\n",
      "('Organizations and repositories that collect and store user data must comply with privacy regulations, such as the recent European Union General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and the Canadian Consumer Privacy Protection Act (CPPA), all of which guarantee the right of users to remove their data from the datasets (\\\\emph{Right to be Forgotten})', 8) ('item', 7)\n",
      "('Data removal requests frequently arise in practice, especially for sensitive datasets pertaining to medical records (numerous machine learning models in computational biology are trained using UK Biobank~ which hosts a collection of genetic and medical records of roughly half a million patients~)', 8) ('item', 7)\n",
      "('Removing user data from a dataset is insufficient to ensure sufficient privacy, since training data can often be reconstructed from trained models~', 8) ('item', 7)\n",
      "('This motivates the study of \\\\emph{machine unlearning}~ which aims to efficiently eliminate the influence of certain data points on a model', 8) ('item', 7)\n",
      "('Naively, one can retrain the model from scratch to ensure complete removal, yet retraining comes at a high computational cost and is thus not practical when accommodating frequent removal requests', 8) ('item', 7)\n",
      "('To avoid complete retraining, specialized approaches have to be developed for each unlearning application~', 8) ('item', 7)\n",
      "('At the same time, federated learning (FL) has emerged as a promising approach to enable distributed training over a large number of users while protecting their privacy~', 8) ('item', 7)\n",
      "('The key idea of FL is to keep user data on their devices and train global models by aggregating local models in a communication-efficient and secure manner', 8) ('item', 7)\n",
      "('Due to model inversion attacks~, secure local model aggregation at the server is a critical consideration in FL, as it guarantees that the server cannot get specific information about client data based on their local models~', 8) ('item', 7)\n",
      "('Since data privacy is the main goal in FL, it should be natural for a FL framework to allow for frequent data removal of a subset of client data in a cross-silo setting (e.g., when several patients request their data to be removed in the hospital database), or the entire local dataset for clients in a cross-device setting (e.g., when users request apps not to track their data on their phones)', 8) ('item', 7)\n",
      "('This leads to the largely unstudied problem termed \\\\emph{federated unlearning}~', 8) ('item', 7)\n",
      "('However, existing federated unlearning methods do not guarantee exact or approximate unlearning~, they do not come with theoretical performance guarantees after model updates, and often, they are vulnerable to adversarial attacks.\\n\\n\\\\textbf{Our contributions.} We describe the first unlearning framework for a newly developed federated clustering approach (Figure~\\\\ref{fig:framework_overview}) which addresses the following problems: \\n\\n\\n', 8) ('item', 7)\n",
      "('\\\\begin{figure}[t]\\n    \\\\centering\\n    \\\\includegraphics[width=\\\\linewidth]{figs/framework_overview.png}\\n    \\\\caption{Overview of our proposed federated clustering scheme. $K$-means++ initialization and quantization are performed at each client in parallel. The SCMA procedure ensures that only the server knows the aggregated statistics of clients, without revealing who contributed the points in each individual cluster. The server generates points from the quantization bins with prescribed weights and performs full $K$-means++ clustering to infer the global model.}\\n    \\\\label{fig:framework_overview}\\n\\\\end{figure}', 8) ('item', 7)\n",
      "('\\n\\n', 8) ('item', 7)\n",
      "('\\\\section{Related Works}', 1) ('document', 0)\n",
      "('\\n\\\\textbf{Machine unlearning.} For \\\\emph{centralized} machine unlearning problems, two types of unlearning requirements were proposed in previous works: exact unlearning and approximate unlearning', 8) ('\\\\section{Related Works}', 1)\n",
      "('For exact unlearning, the unlearned model is required to perform identically as a completely retrained model', 8) ('\\\\section{Related Works}', 1)\n",
      "('To achieve this,~ introduced distributed learners,~ proposed sharding-based methods,~ used quantization to eliminate the effect of removed data in clustering problems, and~ applied sharding-based methods to Graph Neural Networks', 8) ('\\\\section{Related Works}', 1)\n",
      "(\"For approximate unlearning, the ``differences'' in behavior between the unlearned model and the completely retrained model should be appropriately bounded, similarly to what is done in the context of differential privacy\", 8) ('\\\\section{Related Works}', 1)\n",
      "('Following this latter direction,~ introduced the inverse Newton update for linear models,~ studied the generalization performance of approximately unlearned models, while~ extended the analysis to linearized Graph Neural Networks', 8) ('\\\\section{Related Works}', 1)\n",
      "('A limited number of recent works also investigated data removal in the FL settings:~ proposed to use fewer iterations during retraining for federated unlearning,~ introduced Knowledge Distillation into the unlearning procedure to eliminate the effect of data points requested for removal, and~ considered removing all data from one particular class via inspection of the internal influence of each channel in Convolutional Neural Networks', 8) ('\\\\section{Related Works}', 1)\n",
      "('These federated unlearning methods are (mostly) empirical and do not come with theoretical guarantees for model performance after removal and/or for the unlearning efficiency', 8) ('\\\\section{Related Works}', 1)\n",
      "('In contrast, our proposed federated clustering framework not only enables efficient data removal in practice, but also provides theoretical guarantees for the unlearned model performance and for the expected time complexity of the unlearning procedure.\\n\\n\\\\textbf{Federated clustering.} The idea of federated clustering is to perform clustering using data that resides at different edge devices', 8) ('\\\\section{Related Works}', 1)\n",
      "('It is closely related to clustered FL~, whose goal is to learn several global models simultaneously, based on the cluster structure of the dataset, as well as personalization according to the cluster assignments of client data in FL~', 8) ('\\\\section{Related Works}', 1)\n",
      "('One difference between federated clustering and distributed clustering~ is the assumption of data heterogeneity across different clients', 8) ('\\\\section{Related Works}', 1)\n",
      "('Recent works~ exploit the non-i.i.d nature of client data to improve the performance of some learners', 8) ('\\\\section{Related Works}', 1)\n",
      "('Another difference pertains to data privacy', 8) ('\\\\section{Related Works}', 1)\n",
      "('Most previous methods were centered around the idea of sending exact~ or quantized client (local) centroids~ to the server, which may not be considered private as it leaks the data statistics or cluster information of all the clients', 8) ('\\\\section{Related Works}', 1)\n",
      "('To avoid sending exact centroids,~ proposes sending distances between data points and centroids to the server without revealing the membership of data points to any of the parties involved', 8) ('\\\\section{Related Works}', 1)\n",
      "('Note that there is currently no formal definition of computational or information-theoretic secrecy/privacy for federated clustering problems, making it hard to compare methods addressing different aspects of FL', 8) ('\\\\section{Related Works}', 1)\n",
      "('Our method introduces a simple-to-unlearn clustering process and new privacy mechanism that is intuitively appealing as it involves communicating obfuscated point counts of the clients to the server. \\n\\n', 8) ('\\\\section{Related Works}', 1)\n",
      "('\\\\section{Preliminaries}', 1) ('document', 0)\n",
      "('\\nWe start with a formal definition of the centralized $K$-means problem', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('Given a set of $n$ points $\\\\mathcal{X}$ arranged into a matrix $X \\\\in \\\\mathbb{R}^{n\\\\times d}$, and the number of clusters $K$, the $K$-means problem asks for finding a set of points $\\\\mathbf{C}=\\\\{c_1,...,c_K\\\\}, c_k \\\\in \\\\mathbb{R}^d, \\\\forall k\\\\in[K]$ that minimizes the objective\\n', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('\\\\begin{equation}\\\\label{eq:central_kmeans}\\n   \\\\phi_c(X;\\\\mathbf{C}) = \\\\|X-C\\\\|_F^2,\\n\\\\end{equation}', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('\\nwhere $||\\\\cdot||_F$ denotes the Frobenius norm of a matrix, $\\\\|\\\\cdot\\\\|$ denotes the $\\\\ell_2$ norm of a vector, and $C\\\\in\\\\mathbb{R}^{n\\\\times d}$ records the closest centroid in $\\\\mathbf{C}$ to each data point $x_i$ (i.e., $c_i=\\\\argmin_{c_j\\\\in\\\\mathbf{C}}\\\\|x_i-c_j\\\\|$)', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('Without loss of generality, we make the assumption that the optimal solution is unique in order to facilitate simpler analysis and discussion, and denote the optimum by $\\\\mathbf{C}^* = \\\\{c_1^*,...,c_K^*\\\\}$', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('The set of centroids $\\\\mathbf{C}^*$ induces an optimal partition $\\\\bigcup_{k=1}^{K}\\\\mathcal{C}_k^*$ over $\\\\mathcal{X}$, where $\\\\forall k\\\\in[K], \\\\mathcal{C}_k^* = \\\\{x_i: ||x_i-c_k^*||\\\\leq ||x_i-c_j^*||\\\\;\\\\forall i\\\\in[n],j\\\\in[K]\\\\}$', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('We use $\\\\phi_c^*(X)$ to denote the optimal value of the objective function for the centralized $K$-means problem', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('With a slight abuse of notation, we also use $\\\\phi_c^*(\\\\mathcal{C}_k^*)$ to denote the objective value computed over the optimal cluster $\\\\mathcal{C}_k^*$', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('A detailed description of a commonly used approach for solving the $K$-means problem, $K$-means++, is available in Appendix~\\\\ref{app:kpp_init}', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('In federated clustering, the dataset $\\\\mathcal{X}$ is no longer available at the centralized server', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('Instead, data is stored on $L$ edge devices (clients) and the goal of federated clustering is to learn a global set of $K$ centroids $\\\\mathbf{C}_s$ over the union of client data', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('For simplicity, we assume that there exists no identical data points across clients, and that the overall dataset $\\\\mathcal{X}$ is the union of the datasets $\\\\mathcal{X}^{(l)}$ arranged as $X^{(l)}\\\\in\\\\mathbb{R}^{n^{(l)}\\\\times d}$ on devices $l, \\\\forall l\\\\in[L]$', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('To make sure the global model can be learned securely, we allow all clients to communicate with each other as well as the server only to perform secure model aggregation~', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('The server will receive the aggregated cluster statistics of all clients, while each client will only know the cluster assignment of its own data $\\\\mathcal{X}^{(l)}$ and not that of the global server clusters', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('In this case, the federated $K$-means problem asks for finding $K$ global centroids $\\\\mathbf{C}_s$ that minimize the objective\\n', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('\\\\begin{equation}\\\\label{eq:fl_kmeans}\\n% \\\\vspace{-5pt}\\n    \\\\phi_f(X;\\\\mathbf{C}_s) = \\\\sum_{l=1}^L\\\\|X^{(l)}-C_s^{(l)}\\\\|_F^2,\\n% \\\\vspace{-3pt}\\n\\\\end{equation}', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('\\nwhere $C_s^{(l)}\\\\in\\\\mathbb{R}^{n^{(l)}\\\\times d}$ records the centroids of the global clusters that data points $\\\\{x_i^{(l)}\\\\}_{i=1}^{n^{(l)}}$ on client $l$ belong to', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('A formal definition of global clusters is given in Definition~\\\\ref{def:induced_cluster}', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('Here, $\\\\phi_f$ stands for the objective of federated clustering, not to be confused with the centralized $\\\\phi_c$ from Equation~(\\\\ref{eq:central_kmeans}).\\n\\n\\\\textbf{Exact unlearning.} For clustering problems, the \\\\emph{exact unlearning} criterion may be formulated as follows', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('Let $\\\\mathcal{X}$ be a given dataset and $\\\\mathcal{A}$ a (randomized) clustering algorithm that trains on $\\\\mathcal{X}$ and outputs a set of centroids $\\\\mathbf{C}\\\\in \\\\mathcal{M}$, where $\\\\mathcal{M}$ is the chosen space of models (centroid sets)', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('Let $\\\\mathcal{U}$ be an unlearning algorithm that is applied to $\\\\mathcal{A}(\\\\mathcal{X})$ to remove the effects of one data point $x\\\\in\\\\mathcal{X}$', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('Then $\\\\mathcal{U}$ is an exact unlearning algorithm if $\\\\forall \\\\mathbf{C}\\\\in \\\\mathcal{M}, x\\\\in\\\\mathcal{X}, \\\\mathbb{P}(\\\\mathcal{U}(\\\\mathcal{A}(\\\\mathcal{X}), \\\\mathcal{X}, x)=\\\\mathbf{C}) = \\\\mathbb{P}(\\\\mathcal{A}(\\\\mathcal{X}\\\\backslash x)=\\\\mathbf{C})$', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('To avoid confusion, in certain cases, this criterion is referred to as \\\\emph{probabilistic (model) equivalence.}\\n\\n\\\\textbf{Privacy-accuracy-efficiency trilemma.} How to trade-off data privacy, model performance, communication and computational efficiency is a long-standing problem in distributed learning~ that also carries over to FL and federated clustering', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('Solutions that simultaneously address all these challenges in the latter context are still lacking', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('For example,~ proposed a one-shot algorithm that takes model performance and communication efficiency into consideration by sending the \\\\emph{exact} centroids of each client to the server in a \\\\emph{nonanonymous} fashion', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('This approach may not be desirable under stringent privacy constraints as the server can gain information about individual client data', 8) ('\\\\section{Preliminaries}', 1)\n",
      "(\"On the other hand, privacy considerations were addressed in~ by performing $K$-means Lloyd's iterations anonymously via distribution of computations across different clients\", 8) ('\\\\section{Preliminaries}', 1)\n",
      "('Since the method relies on obfuscating pairwise distances for each client, it incurs computational overheads to hide the identity of contributing clients at the server and communication overheads due to interactive computations', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('None of the above methods is suitable for unlearning applications', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('To simultaneously enable unlearning and address the trilemma in the unlearning context, our privacy criterion involves transmitting \\\\emph{the number of client data points within local client clusters} in such a manner that the server cannot learn the data statistics of any specific client, but only the overall statistics of the union of client datasets', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('In this case, computations are limited and the clients on their end can perform efficient unlearning, unlike the case when presented with data point/centroid distances.\\n\\n\\\\textbf{Random and adversarial removal.} All previous unlearning literature focuses on the case where all data points are equally likely to be removed, a setting known as \\\\emph{random removal}', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('However, adversarial data removal requests may arise when users are sensitive towards certain points that are critical for model training (i.e., boundary points in optimal clusters)', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('We refer to such a removal request as \\\\emph{adversarial removal}', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('Note that adversarial removal may be of more importance in practice, especially for medical data', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('For example, if the data points correspond to gene mutations of patients, the sparsely distributed boundary points usually represent rare mutations where most patients would like to keep their privacy, leading to a larger probability to be removed', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('In Section~\\\\ref{sec:mu_complexity} and~\\\\ref{sec:fl_complexity}, we provide theoretical analysis for both types of removal requests.\\n\\n', 8) ('\\\\section{Preliminaries}', 1)\n",
      "('\\\\section{Machine Unlearning via Specialized Seeding}', 1) ('document', 0)\n",
      "('[1]\\n    \\\\STATE \\\\textbf{input:} Dataset $\\\\mathcal{X}$, centroid set $\\\\mathbf{C}$ obtained by $K$-means++ initialization on $\\\\mathcal{X}$, removal request set $\\\\mathcal{X}_R=\\\\{x_{r_1},\\\\ldots,x_{r_R}\\\\}$.\\n    \\\\IF{$c_j\\\\notin \\\\mathcal{X}_R,\\\\;\\\\forall c_j\\\\in\\\\mathbf{C}$}\\n    \\\\STATE $\\\\mathbf{C}^\\\\prime \\\\leftarrow \\\\mathbf{C}$\\n    \\\\ELSE\\n    \\\\STATE $i\\\\leftarrow (\\\\argmin_j c_j\\\\in \\\\mathcal{X}_R) - 1$\\n    \\\\IF{$i=0$}\\n    \\\\STATE $\\\\mathbf{C}^\\\\prime \\\\leftarrow \\\\varnothing$, $\\\\mathcal{X}^\\\\prime\\\\leftarrow \\\\mathcal{X}\\\\backslash\\\\mathcal{X}_R$.\\n    \\\\ELSE\\n    \\\\STATE $\\\\mathbf{C}^\\\\prime \\\\leftarrow \\\\{c_1,\\\\ldots,c_i\\\\}$, $\\\\mathcal{X}^\\\\prime\\\\leftarrow \\\\mathcal{X}\\\\backslash\\\\mathcal{X}_R$.\\n    \\\\ENDIF\\n    \\\\FOR{$j = i+1,\\\\ldots,K$}\\n      \\\\STATE Sample $x$ from $\\\\mathcal{X}^\\\\prime$ with probability\\n        $\\\\frac{d^2(x,\\\\mathbf{C}^\\\\prime)}{\\\\phi_c(X^\\\\prime;\\\\mathbf{C}^\\\\prime)}$.\\n      \\\\STATE $\\\\mathbf{C}^\\\\prime \\\\leftarrow\\\\mathbf{C}^\\\\prime\\\\cup \\\\{x\\\\}$.\\n    \\\\ENDFOR\\n    \\\\ENDIF\\n    \\\\RETURN $\\\\mathbf{C}^\\\\prime$\\n  ', 8) ('\\\\section{Machine Unlearning via Specialized Seeding}', 1)\n",
      "('\\n   \\\\caption{Unlearning via $K$-means++ Initialization.}\\n   \\n   \\n', 8) ('\\\\section{Machine Unlearning via Specialized Seeding}', 1)\n",
      "('\\nWe exhibit next a useful connection between exact unlearning and the $K$-means++ initialization procedure', 8) ('\\\\section{Machine Unlearning via Specialized Seeding}', 1)\n",
      "('Throughout this section we consider the problem in the centralized setting so as to set the stage for the discussion of the federated clustering setting in Section~\\\\ref{sec:fl}', 8) ('\\\\section{Machine Unlearning via Specialized Seeding}', 1)\n",
      "('The first step is to introduce the unlearning mechanism based on the initial centroid set, as described in Algorithm~\\\\ref{alg:mu_kpp}.\\n\\n\\n\\n', 8) ('\\\\section{Machine Unlearning via Specialized Seeding}', 1)\n",
      "('\\\\subsection{Performance Analysis}', 2) ('\\\\section{Machine Unlearning via Specialized Seeding}', 1)\n",
      "('\\nFor any set of data points $\\\\mathcal{X}$ and removal set $\\\\mathcal{X}_R$, assuming that the remaining dataset is $\\\\mathcal{X}^\\\\prime$ and the centroid set returned by Algorithm~\\\\ref{alg:mu_kpp} is $\\\\mathbf{C}^\\\\prime$, we have $\\\\mathbb{E}(\\\\phi_c(X^\\\\prime;\\\\mathbf{C}^\\\\prime))\\\\leq 8(\\\\ln K+2)\\\\phi^*_c(X^\\\\prime)$.\\n', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('\\nSince the centroids chosen through $K$-means++ initialization are true data points, the results returned by Algorithm~\\\\ref{alg:mu_kpp} are guaranteed to contain no information about the data points that have been removed', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('To verify that Algorithm~\\\\ref{alg:mu_kpp} is an exact unlearning method, we also need to check that $\\\\mathbf{C}^\\\\prime$ is probabilistically equivalent to the models generated by rerunning the $K$-means++ initialization process on $\\\\mathcal{X}^\\\\prime$, the set of point remaining after removal', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('This observation is best explained through an example, while a formal proof is provided in Appendix~\\\\ref{app:mu_kpp}', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('Suppose that one would like to unlearn the data point $x_i$ from the clustering model represented by an initial centroid set $\\\\mathbf{C}$', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('For the first  chosen centroid $c_1$ in $\\\\mathbf{C}$, there are only two possibilities: $c_1=x_i$ and $c_1\\\\neq x_i$', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('In the first case, we will have to re-run the initialization over $\\\\mathcal{X}^\\\\prime$, which is retraining the model', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('In the second case, since we know $c_1\\\\neq x_i$, the probability of choosing $c_1$ from $\\\\mathcal{X}$ as the first centroid becomes the conditional probability $\\\\mathbb{P}($choose $c_1$ from $\\\\mathcal{X}$ as the first centroid$|c_1\\\\neq x_i)=\\\\frac{1}{n-1}=\\\\mathbb{P}($choose $c_1$ from $\\\\mathcal{X}^\\\\prime$ as the first centroid$)$, where $n$ is the number of data points in $\\\\mathcal{X}$', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('Therefore, the first centroid $c_1^\\\\prime$ returned by Algorithm~\\\\ref{alg:mu_kpp} can always be viewed as the first centroid obtained by re-running the initialization over $\\\\mathcal{X}^\\\\prime$', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('This can be generalized for $c_j\\\\;\\\\forall j\\\\in[K]$ and arbitrary $\\\\mathcal{X}_R$', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('Since Algorithm~\\\\ref{alg:mu_kpp} is an exact unlearning method, we can have the performance guarantees in Lemma~\\\\ref{lma:mu_kpp} based on Theorem 1.1 of~.\\n\\n', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('\\\\subsection{Complexity Analysis}', 2) ('\\\\section{Machine Unlearning via Specialized Seeding}', 1)\n",
      "('\\nLet $\\\\epsilon_1=\\\\frac{n}{K s_{\\\\min}}$ be a constant denoting what we refer to as \\\\emph{cluster size imbalance,} where $s_{\\\\min}$ equals the size of the smallest cluster in the optimal clustering; when $\\\\epsilon_1=1$, all clusters are of the same size $\\\\frac{n}{K}$.\\n', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('\\nAssume that $\\\\epsilon_2\\\\geq 1$ is a fixed constant', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('An outlier $x_i$ in $\\\\mathcal{X}$ satisfies\\n', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('\\\\begin{align}\\\\label{eq:outlier}\\n    \\\\|x_i-c_j^*\\\\|\\\\leq \\\\|x_i-c_k^*\\\\|, \\\\forall k\\\\in[K] \\\\quad \\\\text{ and } \\\\quad \\\\|x_i-c_j^*\\\\| > \\\\sqrt{\\\\frac{\\\\epsilon_2\\\\phi_c^*(\\\\mathcal{C}_j^*)}{|\\\\mathcal{C}_j^*|}}. \\n\\\\end{align}', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "(\"\\nThe assumption is not overly restrictive, as outliers are commonly defined as points that are well-separated from all ``regular'' clusters: the distance between an outlier and its closest centroid is larger than a scaled proxy for the empirical standard deviation of the average distance between the cluster points and that centroid~\", 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('There also exist many different approaches for removing outliers for clustering purposes~, which effectively reduce the probability of outliers to negligible values.\\n', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('\\nAssume that the number of data points in $\\\\mathcal{X}$ is $n$ and the probability of the dataset containing at least one outlier is upper bounded by $O\\\\left({1}/{n}\\\\right)$', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('Let $\\\\mathbf{C}$ be the centroid set obtained by running $K$-means++ on $\\\\mathcal{X}$', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('For unlearning a data point $x_i \\\\in \\\\mathcal{X}$ we have that for random removal: $\\\\mathbb{P}(x_i\\\\in\\\\mathbf{C})<O\\\\left({K}/{n}\\\\right)$; for adversarial removal: $\\\\mathbb{P}(x_i\\\\in\\\\mathbf{C})<O\\\\left({K^2\\\\epsilon_1\\\\epsilon_2}/{n}\\\\right)$.\\n', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('\\nAssume that the number of data points in $\\\\mathcal{X}$ is $n$ and the probability of the dataset containing at least one outlier is upper bounded by $O\\\\left({1}/{n}\\\\right)$', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('Algorithm~\\\\ref{alg:mu_kpp} supports removing $R$ points with time $O(RK^2d)$ in expectation for random removals, and with time $O(RK^3\\\\epsilon_1\\\\epsilon_2d)$ in expectation for adversarial removals', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('The time complexity of complete retraining equals $O(nKRd)$.\\n', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('\\nWe show next that Algorithm~\\\\ref{alg:mu_kpp} supports removing $R$ random points in expected time $O(RK^2d)$ and $R$ adversarial points in expected time $O(RK^3\\\\epsilon_1\\\\epsilon_2d)$', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('In contrast, the corresponding time complexity of retraining is $O(nKRd)$', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('To analyze the removal time complexity of Algorithm~\\\\ref{alg:mu_kpp}, we first state two assumptions pertaining to optimal cluster sizes and outliers', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('Assumptions~\\\\ref{def:imbalance} and~\\\\ref{def:outlier} can be used to derive an expression for the probability of $x_i\\\\in\\\\mathbf{C}$ when $x_i$ needs to be unlearned, which equals the probability of retraining with $\\\\mathcal{X}^\\\\prime$', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('Based on Algorithm~\\\\ref{alg:mu_kpp}, we need to reinitialize the centroids only when the point that requests removal lie in the original centroid set', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('We need $O(K)$ time to check if this is the case, and we need extra $O(nd)$ time to sample every new centroid due to the distance computation', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('Therefore, we arrive at an estimate of the expected removal time as shown in Theorem~\\\\ref{thm:expected_removal_time}, which is independent of the problem size $n$. \\n\\n\\n\\n', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('\\\\section{Unlearning Federated Clusters}', 1) ('document', 0)\n",
      "('[1]\\n    \\\\STATE \\\\textbf{input:} Dataset $\\\\mathcal{X}$ distributed on $L$ clients ($\\\\mathcal{X}^{(1)},\\\\ldots,\\\\mathcal{X}^{(L)}$), client oversampling coefficient $\\\\alpha$.\\n    \\\\STATE Run $(\\\\alpha K)$-means++ initialization on each client $l$ in parallel, obtain the initial centroid sets $\\\\mathbf{C}^{(l)}$, and record the corresponding cluster sizes $(|\\\\mathcal{C}_1^{(l)}|, \\\\ldots, |\\\\mathcal{C}_{\\\\alpha K}^{(l)}|),\\\\;\\\\forall l\\\\in[L]$.\\n    \\\\STATE Perform uniform quantization of $\\\\mathbf{C}^{(l)}$ on each dimension, and flatten the quantization bins into a vector $q^{(l)}$, $\\\\forall l\\\\in[L]$.\\n    \\\\STATE Set $q^{(l)}_j=\\\\left|\\\\mathcal{C}_k^{(l)}\\\\right|$ with $j$ being the index of the quantization bin where $c_k^{(l)}$ lies in for $\\\\forall k\\\\in[\\\\alpha K]$', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('Set $q^{(l)}_j=0$ for all other indices.\\n    \\\\STATE Securely sum up $q^{(l)}$ at server by Algorithm~\\\\ref{alg:fl_secure}, with the aggregated vector denoted as $q$.\\n    \\\\STATE For index $j\\\\in\\\\{t:q_t\\\\neq 0\\\\}$, sample $q_j$ points based on pre-defined distribution and denote their union as new dataset $\\\\mathcal{X}_s$ at server.\\n    \\\\STATE Run full $K$-means++ clustering at server with $\\\\mathcal{X}_s$ to obtain the centroid set $\\\\mathbf{C}_s$ at server.\\n    \\\\RETURN Each client retains its own centroid set $\\\\mathbf{C}^{(l)}$, server retains $\\\\mathcal{X}_s, q$ and $\\\\mathbf{C}_s$.\\n', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('\\n   \\\\caption{Secure Federated Clustering}\\n   \\n   \\n', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('[1]\\n    \\\\STATE \\\\textbf{input:} Dataset $\\\\mathcal{X}$ distributed on $L$ clients ($\\\\mathcal{X}^{(1)},\\\\ldots,\\\\mathcal{X}^{(L)}$), ($\\\\mathbf{C}^{(l)}, \\\\mathcal{X}_s, q, \\\\mathbf{C}_s$) obtained by Algorithm~\\\\ref{alg:fl_clustering} on $\\\\mathcal{X}$, removal request set $\\\\mathcal{X}_R^{(l)}$ for single-client removal or $\\\\mathcal{L}_R$ for multi-client removal.\\n    \\\\IF{single-client removal}\\n    \\\\STATE Run Algorithm~\\\\ref{alg:mu_kpp} on client $l$ and update $q^{(l)}$ if client $l$ has to perform retraining.\\n    \\\\ELSE\\n    \\\\STATE $q^{(l)}\\\\leftarrow\\\\mathbf{0}$ on client $l$, $\\\\forall l\\\\in\\\\mathcal{L}_R$.\\n    \\\\ENDIF\\n    \\\\STATE Securely sum up $q^{(l)}$ at server by Algorithm~\\\\ref{alg:fl_secure}, with the aggregated vector denoted as $q^\\\\prime$.\\n    \\\\IF{$q^\\\\prime=q$}\\n    \\\\STATE $\\\\mathbf{C}_s^\\\\prime\\\\leftarrow \\\\mathbf{C}_s$.\\n    \\\\ELSE\\n    \\\\STATE Generate $\\\\mathcal{X}_s^\\\\prime$ with $q^\\\\prime$.\\n    \\\\STATE Run full $K$-means++ at the server with $\\\\mathcal{X}_s^\\\\prime$ to obtain $\\\\mathbf{C}_s^\\\\prime$.\\n    \\\\ENDIF\\n    \\\\RETURN Client centroid sets $\\\\mathbf{C}^{(l)\\\\prime}$, server data $\\\\mathcal{X}_s^\\\\prime, q^\\\\prime$ and centroids $\\\\mathbf{C}_s^\\\\prime$.\\n  ', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('\\n   \\\\caption{Unlearning of Federated Clusters}\\n   \\n   \\n', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('\\n\\nWe introduce next our federated clustering framework (Algorithm~\\\\ref{alg:fl_clustering}) based on the discussion from Section~\\\\ref{sec:mu}', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('The removal procedure is described in Algorithm~\\\\ref{alg:mu_fl}', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('We introduce an oversampling coefficient $\\\\alpha$ for clients, following a common strategy for distributed $K$-means clustering~ to improve the final federated clustering performance', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('In practice, a constant value $\\\\alpha$ suffices, and for simplicity, we set $\\\\alpha=1$ in our subsequent analysis', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('For Algorithm~\\\\ref{alg:fl_clustering} to satisfy the privacy criterion stated in Section~\\\\ref{sec:prelim}, we need to design an efficient protocol for aggregating information at the server pertaining to the overall dataset without leaking individual client data statistics', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('This approach is discussed in Section~\\\\ref{sec:secure_agg}', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('A simplified version of Algorithm~\\\\ref{alg:fl_clustering} is discussed in Appendix~\\\\ref{app:fl_nonsecure_clustering}, for applications where the privacy criterion is not an imperative concern', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('Note that Step 6 of Algorithm~\\\\ref{alg:fl_clustering} involves generating $q_j$ points for the $j$-th quantization bin', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('The simplest idea is to choose the center of the quantization bin as the representative point and assign weight $q_j$ to it', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('Then, in Step 7, we can use the weighted $K$-means++ algorithm at the server to further reduce the computational complexity at the server side, by reducing the problem size at the server from $n$, which is the total number of data points, to $KL$', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('However, in practice we find that when the server computational power is not the bottleneck in the FL system, generating data points uniformly at random for the quantization bins with non-zero weights can often lead to better clustering performance', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('Thus, this is the default choice in our subsequent simulations.\\n\\n', 8) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('\\\\subsection{SCMA}', 2) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('[1]\\n    \\\\STATE \\\\textbf{input:} $L$ different vectors $q^{(l)}$ of length $B^d$ to be securely aggregated, a finite field $\\\\mathbb{F}_p$.\\n    \\\\STATE Each client $l\\\\in [L],i\\\\in[2KL]$ communicates $(S^{(l)}_1,\\\\ldots,S^{(l)}_{2KL})$ to the server, where\\n    $S^{(l)}_i=(\\\\sum_{j:q^{(l)}_j\\\\ne 0}q^{(l)}_j \\\\cdot j^{i-1}+z^{(l)}_i) \\\\text{ mod } p,$\\n     and $z^{(l)}_i$ is a random key uniformly distributed over $\\\\mathbb{F}_p$ and hidden from the server', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('The keys $\\\\{z^{(l)}_i\\\\}_{l\\\\in[L],i\\\\in[2KL]}$ are generated offline using standard secure model aggregation so that $\\\\sum_{l}z^{(l)}_i=0$.\\n    \\\\STATE The server first computes the sum $S_i=(\\\\sum_{l\\\\in[L]}S^{(l)}_i) \\\\text{ mod } p, \\\\forall i\\\\in[2KL]$', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('Given $S_i$, the server computes the coefficients of the polynomial $g(x)=\\\\prod_{j:q_j\\\\ne 0}(1-j \\\\cdot x)$ using the Berlekamp-Massey algorithm~', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('Then, the server factorizes $g(x)$ over the field $\\\\mathbb{F}_p$ to determine the roots $j^{-1}$, $q_j\\\\ne 0$, using the polynomial factorizing algorithm~', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('Finally, the server solves a set of $2KL$ linear equations\\n    $S_i=\\\\sum_{l\\\\in[L]}S^{(l)}_i=\\\\sum_{j:q_j\\\\ne 0}q_j \\\\cdot j^{i-1}$  for $i\\\\in[2KL]$, by considering $q_j$ as unknowns and $j^{i-1}$ as known coefficients for $q_j\\\\ne 0$. \\n        \\\\RETURN $q$ reconstructed at the server.\\n  ', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('[H]\\n   \\\\caption{SCMA}\\n   \\n   \\n', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('\\nTo utilize off-the-shelf secure aggregation methods developed for FL~, we first need to quantize the centroids of client data and convert the information into vectors', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('For simplicity, we use uniform quantization with step size $\\\\gamma$ for each dimension in Step 3 of Algorithm~\\\\ref{alg:fl_clustering}', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('A detailed description of the quantization process is available in Appendix~\\\\ref{app:quant}', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('Once the vector representations $q^{(l)}$ of length $B^d\\\\;(B=1/\\\\gamma)$, $l\\\\in[L]$ of client data are generated, we can use standard secure model aggregation methods~ to sum up all $q^{(l)}$ securely and obtain the aggregated results $q$ at the server', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('However, since the length of each $q^{(l)}$ is $B^d$, securely aggregating the whole vector would lead to very high communication complexity for each client if $\\\\gamma$ is small (i.e., if we need finer quantization) or $d$ is large (i.e., if we have to cluster high dimensional data)', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('Moreover, each $q^{(l)}$ is a sparse vector since the number of client centroids is much smaller than the number of quantization bins (i.e., $K\\\\ll B^d$)', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('So it is inefficient and unnecessary to send out the entire $q^{(l)}$ for aggregation', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('This motivates us to first compress the vectors and then perform the secure aggregation, and we refer to this process as SCMA', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('By observing that there can be at most $K$ nonzero entries in $q^{(l)},\\\\forall l\\\\in[L]$ and at most $KL$ nonzero entries in $q$, we invoke the Reed-Solomon code construction~ for designing SCMA', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('Let $\\\\mathbb{F}_p=\\\\{0,1,\\\\ldots,p-1\\\\}$ be a finite field of prime order $p\\\\ge \\\\max\\\\{n,B^d\\\\}$', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('We treat the indices of the quantization bins as distinct elements from the underlying finite field, and use them as evaluation points of the encoder polynomial', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('In addition, we treat a nonzero entry $q^{(l)}_j$ in vector $q^{(l)}$ as a substitution error at the $j$-th entry in a codeword', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('Then, we use our SCMA scheme shown in Algorithm~\\\\ref{alg:fl_secure}, where the messages that the clients send to server can be treated as syndromes in Reed-Solomon decoding', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('The discussion about the differences between SCMA and a related problem termed private set union is available in Appendix~\\\\ref{app:private_set_union}', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('Note that in our scheme, the server does not know $q^{(l)},l\\\\in[L]$ beyond the fact that $\\\\sum_{l\\\\in[L]}q^{(l)}=q$, which fits into our privacy criterion', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('This follows because $z^{(l)}_i$ is uniformly distributed over $\\\\mathbb{F}_p$ and independently chosen for different $l\\\\in[L]$ and $i\\\\in[2KL]$', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('For details, please refer to Appendix~\\\\ref{sec:uniqueness}', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('One example of SCMA procedure is illustrated in Figure~\\\\ref{fig:secure_example}.\\n\\n', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('\\\\begin{figure}[htb]\\n    \\\\centering\\n  \\\\includegraphics[width=0.5\\\\linewidth]{figs/secure_agg_example.png}\\n    \\\\caption{Example of the SCMA procedure for $K=2,L=2,B^d=4,n=12,p=13$.}\\n    \\\\label{fig:secure_example}\\n\\\\end{figure}', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('\\n\\n', 8) ('\\\\subsection{SCMA}', 2)\n",
      "('\\\\subsection{Performance Analysis}', 2) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('\\nSuppose that the local clusters at client $l$ are denoted by $\\\\mathcal{C}_k^{(l)},\\\\forall k\\\\in[K],l\\\\in[L]$, and that the clusters at the server are denoted by $\\\\mathcal{C}_k^s,\\\\forall k\\\\in[K]$', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('The global clustering equals\\n$\\n\\\\mathcal{P}_k=\\\\left\\\\{x_i^{(l)}| x_i^{(l)}\\\\in\\\\mathcal{C}_j^{(l)}, c_j^{(l)}\\\\in\\\\mathcal{C}_k^s,\\\\forall j\\\\in[K], l\\\\in[L]\\\\right\\\\},\\n$\\nwhere $c_j^{(l)}$ is the centroid of $\\\\mathcal{C}_j^{(l)}$', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('Note that $(\\\\mathcal{P}_1,\\\\ldots,\\\\mathcal{P}_K)$ forms a partition of the entire dataset $\\\\mathcal{X}$, and the representative centroid for $\\\\mathcal{P}_k$ is defined as $c_{s,k}$ in $\\\\mathbf{C}_s$ returned by Algorithm~\\\\ref{alg:fl_clustering}.\\n', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('\\nSuppose that the entire dataset across clients is denoted by $\\\\mathcal{X}$, and the set of server centroids returned by Algorithm~\\\\ref{alg:fl_nonsecure_clustering} is $\\\\mathbf{C}_s$', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('Then $\\\\mathbb{E}\\\\left(\\\\phi_f(X;\\\\mathbf{C}_s)\\\\right) < O(\\\\log^2 K)\\\\cdot \\\\phi_c^*(X).$\\n', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('\\nSuppose that we performed uniform quantization with step size $\\\\gamma$ in Algorithm~\\\\ref{alg:fl_clustering}', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('Then we have\\n$\\\\mathbb{E}\\\\left(\\\\phi_f(X;\\\\mathbf{C}_s)\\\\right) < O(\\\\log^2 K)\\\\cdot \\\\phi_c^*(X) + O(nd\\\\gamma^2\\\\log K).$\\n', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('\\nWe describe next the performance guarantees of Algorithm~\\\\ref{alg:fl_clustering} w.r.t. the objective defined in Equation~(\\\\ref{eq:fl_kmeans}), using the definition of global clusters from Definition~\\\\ref{def:induced_cluster} originally proposed in~', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('Note that only the global clustering information can be sent to clients by the server, if requested (i.e., global centroids will not be interactively communicated between clients and server)', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('We first consider the case where no quantization is used (see Algorithm~\\\\ref{alg:fl_nonsecure_clustering})', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('The proof is relegated to Appendix~\\\\ref{app:pf_fl_performance_analog}', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('The result of Lemma~\\\\ref{lma:fl_performance_analog} holds for any distribution of data across clients, and Algorithm~\\\\ref{alg:fl_nonsecure_clustering} may perform even better than for the case of homogeneous assignments', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('This is best explained through the following example', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('In the extreme case of data heterogeneity, each client stores a different cluster w.r.t. the global optimal clustering ($L=K$)', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('Then Algorithm~\\\\ref{alg:fl_nonsecure_clustering} can be viewed as seeding each optimal cluster by a data point uniformly at random chosen from that cluster, through $1$-means clustering at each client', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('This leads to a performance guarantee of $\\\\mathbb{E}\\\\left(\\\\phi_f(X;\\\\mathbf{C}_s)\\\\right) =2\\\\phi_c^*(X)$, where the approximation factor is independent of $K$', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('A more detailed discussion how data heterogeneity can benefit our unlearning method is available in Appendix~\\\\ref{app:pf_fl_performance_analog}', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('Next, we consider the distortion introduced by quantization', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('The performance guarantee is given in Theorem~\\\\ref{thm:fl_performance_quantize}', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('Note that SCMA does not contribute to the distortion as it always returns the exact sum', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('Furthermore, following the same reasoning as in Lemma~\\\\ref{lma:mu_kpp}, one can show that the result of Theorem~\\\\ref{thm:fl_performance_quantize} also holds for the remaining dataset $\\\\mathcal{X}^\\\\prime$ after running the unlearning procedure in Algorithm~\\\\ref{alg:mu_fl}, as shown in Corollary~\\\\ref{coro:fl_unlearn_obj}.\\n\\n', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('\\\\begin{remark*}\\nTo make the second term in the upper bound a constant w.r.t. $n$, we can choose $\\\\gamma=\\\\Theta(1/\\\\sqrt{n})$, which is a good choice in practice offering good performance across different datasets.\\n\\\\end{remark*}', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('\\nDenote by $X^\\\\prime$ the data set after removal, $\\\\mathbf{C}_s^\\\\prime$ the updated global centroids, and $R$ the number of removals', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('After unlearning, we have\\n$\\\\mathbb{E}\\\\left(\\\\phi_f(X^\\\\prime;\\\\mathbf{C}_s^\\\\prime)\\\\right) < O(\\\\log^2 K)\\\\cdot \\\\phi_c^*(X^\\\\prime) + O((n-R)d\\\\gamma^2\\\\log K).\\n$\\n', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('\\n\\n\\n\\n', 8) ('\\\\subsection{Performance Analysis}', 2)\n",
      "('\\\\subsection{Complexity Analysis}', 2) ('\\\\section{Unlearning Federated Clusters}', 1)\n",
      "('\\nWe derive a cohort of in-depth analysis pertaining to the computational complexity of both types of data removals, SCMA at the client and server side, as well as the communication complexity at the clients', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('The results are listed as follows.\\n\\n\\\\textbf{Removal time complexity.} We consider the removal requests in two federated clustering settings: removing $R$ points from only one client $l$ (cross-silo, single-client removal), and removing all data points from $R$ clients $l_1,\\\\ldots,l_R$ (cross-device, multi-client removal)', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('For the case where multiple clients want to unlearn only a part of their data, the approach is similar to that of single-client removal and the corresponding analysis is based on a simple union bound', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('For single-client removal, we know from Theorem~\\\\ref{thm:expected_removal_time} that the expected removal time for client $l$ is $O(RK^2d)$ for random removal requests and $O(RK^3\\\\epsilon_1\\\\epsilon_2d)$ for adversarial removal requests', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('Other clients do not require additional computations, because our federated clustering algorithm is one-shot and thus their centroids will not be affected by the removal requests', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "(\"On the other hand, the expected removal time for the server is upper bounded by $O(K^2LdT)$ for both types of removal requests, where $T$ is the maximum number of iterations of Lloyd's algorithm at the server before convergence\", 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('For multi-client removal, no client needs to perform additional computations', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('Since in this case we always re-run the full $K$-means++ clustering at the server, the expected removal time complexity at the server equals $O((L-R)K^2Td)$.\\n\\n\\\\textbf{Communication complexity of SCMA at the client end.}\\nSince each $S^{(l)}_i$ can be represented by $\\\\lceil\\\\log p\\\\rceil$ bits, the information $\\\\{S_i^{(l)}\\\\}_{i\\\\in[2KL]}$ sent by each client $l$ can be represented by $2KL \\\\lceil\\\\log p\\\\rceil\\\\le\\\\max\\\\{2KL\\\\log n,2KLd\\\\log B\\\\}+1$ bits', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('Note that there are at most $\\\\sum_{k\\\\in[KL]}\\\\binom{B^d}{k}\\\\binom{n}{k-1}$ $q$-ary vectors of length $B^d$', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('Hence, the cost for communicating $q^{(l)}$ from the client to server $l$ is at least $\\\\log \\\\big(\\\\sum_{k\\\\in[KL]}\\\\binom{B^d}{k}\\\\binom{n}{k-1} \\\\big)=\\\\max\\\\{O(KL\\\\log n),O(KLd\\\\log B)\\\\}$ bits, which implies that our scheme is order-optimal w.r.t. the communication cost', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('Note that following standard practice in the area, we do not take into account the complexity of noise generation in secure model aggregation, as it can be done offline and independently of the Reed-Solomon encoding procedure. \\n\\n\\\\textbf{Computational complexity of SCMA at the client end.} The computation of $S_i^{(l)}$ on client $l$ requires at most $O(K\\\\log i)$ multiplications over $\\\\mathbb{F}_p,$ $i\\\\in[2KL]$', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('The total computational complexity equals $O(K^2L\\\\log (KL))$ multiplication and addition operations over $\\\\mathbb{F}_p$.\\n\\n\\\\textbf{Computational complexity of SCMA at the server end.}\\nThe computational complexity at the server is dominated by the complexity of the Berlekamp-Massey decoding algorithm~, factorizing the polynomial $g(x)$ over $\\\\mathbb{F}_p$~, and solving the linear equations $S_i=\\\\sum_{l\\\\in[L]}S^{(l)}_i=\\\\sum_{j:q_j\\\\ne 0}q_j\\\\cdot j^{i-1}$ with known $j$, $q_j\\\\ne 0$', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('The complexity of Berlekamp-Massey decoding over $\\\\mathbb{F}_p$ is $O(K^2L^2)$', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('The complexity of factorizing a polynomial $g(x)$ over $\\\\mathbb{F}_p$ using the algorithm in~ is $O((KL)^{1.5}\\\\log p+ KL\\\\log^2 p)$ operations over $\\\\mathbb{F}_p$', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('The complexity of solving for $S_i=\\\\sum_{l\\\\in[L]}S^{(l)}_i$ equals that of finding the inverse of a Vandermonde matrix, which takes $O(K^2L^2)$ operations over $\\\\mathbb{F}_p$~', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('Hence, the total computational complexity at the server side is $\\\\max\\\\{O(K^2L^2), O((KL)^{1.5}\\\\log p+ KL\\\\log^2 p)\\\\}$ operations over $\\\\mathbb{F}_p$.\\n\\n', 8) ('\\\\subsection{Complexity Analysis}', 2)\n",
      "('\\\\section{Experimental Results}', 1) ('document', 0)\n",
      "('\\nTo empirically characterize the trade-off between the efficiency of data removal and performance of our newly proposed federated clustering method, we compare it with baseline methods on both synthetic and real datasets', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('Due to space limitations, more in-depth experiments and discussions are delegated to Appendix~\\\\ref{app:exp}.\\n\\n\\\\textbf{Datasets and baselines.} We use one synthetic dataset generated by a Gaussian Mixture Model (Gaussian) and six real datasets (Celltype, Covtype, FEMNIST, Postures, TMI, TCGA) in our experiments', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('We preprocess the datasets such that the data distribution is non-i.i.d. across different clients', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('The symbol $K^\\\\prime$ in Figure~\\\\ref{fig:main_exp} represents the maximum number of (true) clusters among clients, while $K$ represents the number of true clusters in the global dataset', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('A detailed description of the data statistics and the preprocessing procedure is available in Appendix~\\\\ref{app:exp}', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('Since there is currently no off-the-shelf algorithm designed for unlearning federated clusters, we adapt DC-Kmeans (DC-KM) from~ to apply to our problem setting, and use complete retraining as the baseline comparison method', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('To evaluate federated clustering performance on the complete dataset (before data removals), we also include the K-FED algorithm from~ as the baseline method', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('In all plots, our Algorithm~\\\\ref{alg:fl_clustering} is referred to as MUFC', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('Note that in FL, clients are usually trained in parallel so that the estimated time complexity equals the sum of the longest processing time of a client and the processing time of the server.\\n\\n\\\\textbf{Clustering performance.} The clustering performance of all methods on the complete dataset is shown in the first row of Table~\\\\ref{tab:performance}', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('The loss ratio is defined as ${\\\\phi_f(X;\\\\mathbf{C}_s)}/{\\\\phi_c^*(X)}$, which is the metric used to evaluate the quality of the obtained clusters', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('For the seven datasets, MUFC offered the best performance on TMI and Celltype, datasets for which the numbers of data points in different clusters are highly imbalanced', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('This can be explained by pointing out an important difference between MUFC and K-FED/DC-KM: the quantized centroids sent by the clients may have non-unit weights, and MUFC is essentially performing weighted $K$-means++ at the server', 8) ('\\\\section{Experimental Results}', 1)\n",
      "(\"In contrast, both K-FED and DC-KM assign equal unit weights to the client's centroids\", 8) ('\\\\section{Experimental Results}', 1)\n",
      "(\"Note that assigning weights to the client's centroids based on local clusterings not only enables a simple analysis of the scheme but also improves the empirical performance, especially for datasets with highly imbalanced cluster distributions\", 8) ('\\\\section{Experimental Results}', 1)\n",
      "('For all other datasets except Gaussian, MUFC obtained competitive clustering performance compared to K-FED/DC-KM', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('The main reason why DC-KM outperforms MUFC on Gaussian data is that all clusters are of the same size in this case', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('Also note that DC-KM runs full $K$-means++ clustering for each client while MUFC only performs initialization', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('Although running full $K$-means++ clustering at the client side can improve the empirical performance on certain datasets, it also greatly increases the computational complexity during training and the retraining probability during unlearning, which is shown in Figure~\\\\ref{fig:main_exp}', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('Nevertheless, we also compare the performance of MUFC with K-FED/DC-KM when running full $K$-means++ clustering on clients for MUFC in Appendix~\\\\ref{app:exp}', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('We also investigated the influence of $K^\\\\prime$ and $\\\\gamma$ on the clustering performance', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('Figure~\\\\ref{fig:main_exp}(a) shows that MUFC can obtain a lower loss ratio when $K^\\\\prime < K$, indicating that data is non-i.i.d. distributed across clients', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('Figure~\\\\ref{fig:main_exp}(b) shows that the choice of $\\\\gamma$ does not seem to have a strong influence on the clustering performance of Gaussian datasets, due to the fact that we use uniform sampling in Step 6 of Algorithm~\\\\ref{alg:fl_clustering} to generate the server dataset', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('Meanwhile, Figure~\\\\ref{fig:main_exp}(c) shows that $\\\\gamma$ can have a significant influence on the clustering performance of real-world datasets, which agrees with our analysis in Theorem~\\\\ref{thm:fl_performance_quantize}', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('The red vertical line in both figures indicates the default choice of $\\\\gamma={1}/{\\\\sqrt{n}}$, where $n$ stands for the number of total data points across clients.\\n\\n\\\\textbf{Unlearning performance.} Since K-FED does not support data removal, has high computational complexity, and its empirical clustering performance is worse than DC-KM (see Table~\\\\ref{tab:performance}), we only compare the unlearning performance of MUFC with that of DC-KM', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('For simplicity, we consider removing one data point from a uniformly at random chosen client $l$ at each round of unlearning', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('The second row of Table~\\\\ref{tab:performance} records the speed-up ratio w.r.t. complete retraining for one round of MUFC unlearning (Algorithm~\\\\ref{alg:mu_fl}) when the removed point does not lie in the centroid set selected at client $l$', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('Figure~\\\\ref{fig:main_exp}(e) shows the accumulated removal time on the TMI dataset for adversarial removals, which are simulated by removing the data points with the highest contribution to the current value of the objective function at each round, while Figure~\\\\ref{fig:main_exp}(f)-(l) shows the accumulated removal time on different datasets for random removals', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('The results show that MUFC maintains high unlearning efficiency compared to all other baseline approaches, and offers an average speed-up ratio of $84$x when compared to complete retraining for random removals across seven datasets', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('We also report the change in the loss ratio of MUFC during unlearning in Figure~\\\\ref{fig:main_exp}(d)', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('The loss ratio remains nearly constant after each removal, indicating that our unlearning approach does not significantly degrade clustering performance', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('Similar conclusions hold for other tested datasets, as shown in Appendix~\\\\ref{app:exp}.\\n\\n', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('\\\\begin{table}[t]\\n\\\\setlength{\\\\tabcolsep}{3pt}\\n\\\\centering\\n\\\\footnotesize\\n\\\\caption{Clustering performance of different federated clustering algorithms compared to centralized $K$-means++ clustering.}\\n\\\\vspace{0.2in}\\n\\\\label{tab:performance}\\n\\\\begin{tabular}{@{}cc|c|c|c|c|c|c|c@{}}\\n\\\\toprule\\n                                      & & TMI & Celltype & Gaussian & TCGA & Postures & FEMNIST & Covtype         \\\\\\\\ \\\\midrule\\n\\\\multirow{3}{*}{Loss ratio}  & \\\\multicolumn{1}{c|}{{MUFC}} & {$1.24\\\\pm 0.10$} & {$1.14\\\\pm 0.03$} & {$1.25\\\\pm 0.02$} & {$1.18\\\\pm 0.05$} & {$1.10\\\\pm 0.01$} & {$1.20\\\\pm 0.00$} & {$1.03\\\\pm 0.02$} \\\\\\\\\\n                                      & \\\\multicolumn{1}{c|}{{K-FED}}  & {$1.84\\\\pm 0.07$} & {$1.72\\\\pm 0.24$}  & {$1.25\\\\pm 0.01$}  & {$1.56\\\\pm 0.11$}  & {$1.13\\\\pm 0.01$}  & {$1.21\\\\pm 0.00$}  & {$1.60\\\\pm0.01$} \\\\\\\\\\n                                      & \\\\multicolumn{1}{c|}{{DC-KM}} & {$1.54\\\\pm 0.13$}  & {$1.46\\\\pm 0.01$} & {$1.02\\\\pm 0.00$} & {$1.15\\\\pm 0.02$} & {$1.03\\\\pm 0.00$} & {$1.18\\\\pm 0.00$} & {$1.03\\\\pm 0.02$} \\\\\\\\ \\\\midrule\\n\\\\multicolumn{2}{c|}{\\\\begin{tabular}[x]{@{}c@{}}Speed-up of MUFC\\\\\\\\ (if no retraining is performed) \\\\end{tabular}} & {$151$x} & {$1535$x} & {$2074$x} & {$483$x} & {$613$x} & {$53$x} & {$267$x}\\\\\\\\ \\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('\\n\\n', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('\\\\begin{figure}[htb]\\n    \\\\centering\\n   \\\\includegraphics[width=\\\\linewidth]{figs/main_text_exp.png}\\n    \\\\caption{The shaded areas represent the standard deviation of results from different trails. (a) Influence of data heterogeneity on the clustering performance of MUFC: $K^\\\\prime$ represents the maximum number of (global) clusters covered by the data at the clients, while $K^\\\\prime=10$ indicates that the data points are i.i.d. distributed across clients. (b)(c) Influence of the quantization step size $\\\\gamma$ on the clustering performance of MUFC. The red vertical line indicates the default choice of $\\\\gamma=1/\\\\sqrt{n}$, where $n$ is the total number of data points across clients. (d) The change in the loss ratio after each round of unlearning. (e) The accumulated removal time for adversarial removals. (f)-(l) The accumulated removal time for random removals.}\\n    \\\\label{fig:main_exp}\\n\\\\end{figure}', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('\\n\\n', 8) ('\\\\section{Experimental Results}', 1)\n",
      "('\\\\section{Conclusion}', 1) ('document', 0)\n",
      "('\\nWe described a novel federated clustering approach that achieves good clustering performance, and low computational and communication complexity while meeting the privacy criteria that are commonly used in classical FL literature', 8) ('\\\\section{Conclusion}', 1)\n",
      "('Furthermore, we proposed an intuitive and efficient exact unlearning algorithm to accommodate data removal requests within our federated clustering framework', 8) ('\\\\section{Conclusion}', 1)\n",
      "('Our empirical studies on seven benchmark datasets established fundamental performance-complexity trade-offs between unlearning and complete retraining.\\n\\n', 8) ('\\\\section{Conclusion}', 1)\n",
      "('\\\\section*{Acknowledgments}', 1) ('document', 0)\n",
      "('\\nThis work was funded by NSF grants 1816913 and 1956384.\\n\\n\\\\clearpage\\n\\\\bibliography{main}\\n\\\\bibliographystyle{tmlr}\\n\\n\\\\clearpage\\n\\\\appendix\\n', 8) ('\\\\section*{Acknowledgments}', 1)\n",
      "('\\\\section{$K$-means++ Initialization}', 1) ('document', 0)\n",
      "('\\nThe $K$-means problem is NP-hard even for $K=2$, and when the points lie in a two-dimensional Euclidean space~', 8) ('\\\\section{$K$-means++ Initialization}', 1)\n",
      "(\"Heuristic algorithms for solving the problem, including Lloyd's~ and Hartigan's method~, are not guaranteed to obtain the global optimal solution unless further assumptions are made on the point and cluster structures~\", 8) ('\\\\section{$K$-means++ Initialization}', 1)\n",
      "('Although obtaining the exact optimal solution for the $K$-means problem is difficult, there are many methods that can obtain quality approximations for the optimal centroids', 8) ('\\\\section{$K$-means++ Initialization}', 1)\n",
      "('For example, a randomized initialization algorithm ($K$-means++) was introduced in~ and the expected objective value after initialization is a $(\\\\log K)$-approximation to the optimal objective ($\\\\mathbb{E}(\\\\phi)\\\\leq (8\\\\ln K+16)\\\\phi^*$). $K$-means++ initialization works as follows: initially, the centroid set $\\\\mathbf{C}$ is assumed to be empty', 8) ('\\\\section{$K$-means++ Initialization}', 1)\n",
      "('Then, a point is sampled uniformly at random from $X$ for the first centroid and added to $\\\\mathbf{C}$', 8) ('\\\\section{$K$-means++ Initialization}', 1)\n",
      "('For the following $K-1$ rounds, a point $x$ from $X$ is sampled with probability $d^2(x,\\\\mathbf{C})/\\\\phi_c(X;\\\\mathbf{C})$ for the new centroid and added to $\\\\mathbf{C}$', 8) ('\\\\section{$K$-means++ Initialization}', 1)\n",
      "('Here, $d(x,\\\\mathbf{C})$ denotes the minimum $\\\\ell_2$ distance between $x$ and the centroids in $\\\\mathbf{C}$ chosen so far', 8) ('\\\\section{$K$-means++ Initialization}', 1)\n",
      "(\"After the initialization step, we arrive at $K$ initial centroids in $\\\\mathbf{C}$ used for running Lloyd's algorithm.\\n\\n\", 8) ('\\\\section{$K$-means++ Initialization}', 1)\n",
      "('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1) ('document', 0)\n",
      "('\\nAssume that the number of data points in $\\\\mathcal{X}$ is $n$, the size of $\\\\mathcal{X}_R$ is $R$, and the initial centroid set for $\\\\mathcal{X}$ is $\\\\mathbf{C}$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('We use induction to prove that $\\\\mathbf{C}^\\\\prime$ returned by Algorithm~\\\\ref{alg:mu_kpp} is probabilistically equivalent to re-running the $K$-means++ initialization on $\\\\mathcal{X}^\\\\prime=\\\\mathcal{X}\\\\backslash\\\\mathcal{X}_R$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('The base case of induction amounts to investigating the removal process for $c_1$, the first point selected by $K$-means++', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('There are two possible scenarios: $c_1\\\\in\\\\mathcal{X}_R$ and $c_1\\\\notin\\\\mathcal{X}_R$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('In the first case, we will re-run the initialization process over $\\\\mathcal{X}^\\\\prime$, which is equivalent to retraining the model', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('In the second case, since we know $c_1\\\\notin \\\\mathcal{X}_R$, the probability of choosing $c_1$ from $\\\\mathcal{X}$ as the first centroid equals the conditional probability \\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('\\\\begin{align*}\\n\\\\frac{1}{n-R}&=\\\\mathbb{P}(\\\\text{choose }c_1\\\\text{ from }\\\\mathcal{X}\\\\text{ as the first centroid}|c_1\\\\notin \\\\mathcal{X}_R)\\\\\\\\\\n&=\\\\mathbb{P}(\\\\text{choose }c_1\\\\text{ from }\\\\mathcal{X}^\\\\prime\\\\text{ as the first centroid}).\\n\\\\end{align*}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('\\n\\n\\nNext suppose that $K>1$, $i=(\\\\argmin_j c_j\\\\in \\\\mathcal{X}_R) - 1$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('The centroids $\\\\mathbf{C}_{i-1}^\\\\prime=\\\\{c_1^\\\\prime=c_1,\\\\ldots,c_{i-1}^\\\\prime=c_{i-1}\\\\}$ returned by Algorithm~\\\\ref{alg:mu_kpp} can be viewed probabilistically equivalent to the model obtained from re-running the initialization process over $\\\\mathcal{X}^\\\\prime$ for the first $i-1$ rounds', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('Then we have\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('\\\\begin{align*}\\n    \\\\mathbb{P}(\\\\text{choose }c_i\\\\text{ from }\\\\mathcal{X}\\\\text{ as $i$-th centroid}|c_i\\\\notin \\\\mathcal{X}_R) &= \\\\frac{\\\\mathbb{P}(\\\\text{choose }c_i\\\\text{ from }\\\\mathcal{X}\\\\text{ as $i$-th centroid} \\\\cap c_i\\\\notin \\\\mathcal{X}_R)}{\\\\mathbb{P}(c_i\\\\notin \\\\mathcal{X}_R)} \\\\\\\\\\n    &\\\\stackrel{(a)}{=} \\\\frac{\\\\mathbb{P}(\\\\text{choose }c_i\\\\text{ from }\\\\mathcal{X}\\\\text{ as $i$-th centroid})}{\\\\mathbb{P}(c_i\\\\notin \\\\mathcal{X}_R)}\\\\\\\\\\n    &= \\\\frac{d^2(c_i,\\\\mathbf{C}_{i-1}^\\\\prime) / \\\\phi_c(X;\\\\mathbf{C}_{i-1}^\\\\prime)}{1-\\\\sum_{x\\\\in\\\\mathcal{X}_R} d^2(x,\\\\mathbf{C}_{i-1}^\\\\prime) / \\\\phi_c(X;\\\\mathbf{C}_{i-1}^\\\\prime)}\\\\\\\\\\n    &=\\\\frac{d^2(c_i,\\\\mathbf{C}_{i-1}^\\\\prime) / \\\\phi_c(X;\\\\mathbf{C}_{i-1}^\\\\prime)}{ \\\\phi_c(X^\\\\prime;\\\\mathbf{C}_{i-1}^\\\\prime) / \\\\phi_c(X;\\\\mathbf{C}_{i-1}^\\\\prime)}\\\\\\\\\\n    &= \\\\frac{d^2(c_i,\\\\mathbf{C}_{i-1}^\\\\prime)}{\\\\phi_c(X^\\\\prime;\\\\mathbf{C}_{i-1}^\\\\prime)}\\\\\\\\\\n    &= \\\\mathbb{P}(\\\\text{choose }c_i\\\\text{ from }\\\\mathcal{X}^\\\\prime\\\\text{ as $i$-th centroid}),\\n\\\\end{align*}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('\\nwhere $(a)$ holds based on the definition of $i$, indicating that the $i$-th centroid is not in $\\\\mathcal{X}_R$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('Therefore, the centroid $c_i^\\\\prime=c_i$ returned by Algorithm~\\\\ref{alg:mu_kpp} can be seen as if obtained from rerunning the initialization process over $\\\\mathcal{X}^\\\\prime$ in the $i$-th round', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('Again based on the definition of $i$, it is clear that for $j>i$, $c_j^\\\\prime$ are the centroids chosen by the $K$-means++ procedure over $\\\\mathcal{X}^\\\\prime$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('This proves our claim that $\\\\mathbf{C}^\\\\prime$ returned by Algorithm~\\\\ref{alg:mu_kpp} is probabilistic equivalent to the result obtained by rerunning the $K$-means++ initialization on $\\\\mathcal{X}^\\\\prime$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('Theorem 1.1 of~ then establishes that\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('\\\\begin{equation}\\n    \\\\mathbb{E}(\\\\phi_c(X^\\\\prime;\\\\mathbf{C}^\\\\prime))\\\\leq 8(\\\\ln K+2)\\\\phi^*_c(X^\\\\prime),\\n\\\\end{equation}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('\\nwhich completes the proof.\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('\\n\\n\\n\\n\\n\\n\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:mu_kpp}}', 1)\n",
      "('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1) ('document', 0)\n",
      "('\\nSince outliers can be arbitrarily far from all true cluster points according to Equation~(\\\\ref{eq:outlier}), during initialization they may be sampled as centroids with very high probability', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('For simplicity of analysis, we thus assume that outliers are sampled as centroids with probability $1$ if they exist in the dataset', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('Note that for the adversarial removal scenario, we also assume that outliers will request unlearning with probability $1$, meaning that we will always need to re-run the $K$-means++ initialization when outliers exist in the complete dataset before any removals', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('For random removals, where the point requested for unlearning, $x_i$, is drawn uniformly at random from $\\\\mathcal{X}$, it is clear that $\\\\mathbb{P}(x_i\\\\in\\\\mathbf{C})=\\\\frac{K}{n}$, since $\\\\mathbf{C}$ contains $K$ distinct data points in $\\\\mathcal{X}$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('For adversarial removals, we need to analyze the probability of choosing $x_i$ as the $(k+1)$-th centroid, given that the first $k$ centroids have been determined and $x_i\\\\notin\\\\mathbf{C}_{k}=\\\\{c_1,\\\\dots,c_k\\\\}$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('For simplicity we first assume that there is no outlier in $\\\\mathcal{X}$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('Then we have\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\\\begin{equation}\\\\label{eq:app_1}\\n\\\\mathbb{P}(\\\\text{choose }x_i\\\\text{ from }\\\\mathcal{X}\\\\text{ as the $(k+1)$-th centroid}|\\\\mathbf{C}_{k})=\\\\frac{d^2(x_i,\\\\mathbf{C}_k)}{\\\\sum_{y\\\\neq x_i}d^2(y,\\\\mathbf{C}_k)+d^2(x_i,\\\\mathbf{C}_k)}\\n\\\\end{equation}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\nFor the denominator $\\\\sum_{y\\\\neq x_i}d^2(y,\\\\mathbf{C}_k)+d^2(x_i,\\\\mathbf{C}_k)$, the following three observations are in place\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\\\begin{align*}\\n    \\\\sum_{y\\\\neq x_i}d^2(y,\\\\mathbf{C}_k)+d^2(x_i,\\\\mathbf{C}_k) &\\\\geq \\\\phi_c^*(X) \\\\geq \\\\phi_c^*(\\\\mathcal{C}_i^*), x_i\\\\in\\\\mathcal{C}_i^* \\\\\\\\\\n    \\\\sum_{y\\\\neq x_i}d^2(y,\\\\mathbf{C}_k)+d^2(x_i,\\\\mathbf{C}_k) &\\\\geq \\\\sum_{y\\\\neq x_i}d^2(y,\\\\mathbf{C}^*) \\\\\\\\\\n    \\\\sum_{y\\\\neq x_i}d^2(y,\\\\mathbf{C}_k)+d^2(x_i,\\\\mathbf{C}_k) &\\\\geq \\\\sum_{y\\\\neq x_i}d^2(y,\\\\mathbf{C}_k).\\n\\\\end{align*}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\nTherefore, \\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\\\begin{align}\\\\label{eq:app_2}\\n    \\\\sum_{y\\\\neq x_i}d^2(y,\\\\mathbf{C}_k)+d^2(x_i,\\\\mathbf{C}_k) &\\\\geq\\n    \\\\frac{\\\\phi_c^*(\\\\mathcal{C}_i^*)}{5} + \\\\frac{2}{5}\\\\left(\\\\sum_{y\\\\neq x_i}d^2(y,\\\\mathbf{C}^*)+d^2(y,\\\\mathbf{C}_k)\\\\right) \\\\notag\\\\\\\\\\n    &\\\\stackrel{(a)}{\\\\geq} \\\\frac{1}{5}\\\\left(\\\\phi_c^*(\\\\mathcal{C}_i^*) + \\\\sum_{y\\\\neq x_i} \\\\|c_y-c_y^*\\\\|^2\\\\right),\\n\\\\end{align}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\nwhere $c_y, c_y^*$ are the closest centroid in $\\\\mathbf{C}_k$ and $\\\\mathbf{C}^*$ to $y$, respectively', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('Here, $(a)$ is a consequence of the fact that $\\\\|a-b\\\\|^2=\\\\|a-c+c-b\\\\|^2\\\\leq 2(\\\\|a-c\\\\|^2+\\\\|b-c\\\\|^2)$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('Since $x_i$ is not an outlier for $\\\\mathcal{C}_i^*$ based on our assumption, we have \\n$$\\n\\\\phi_c^*(\\\\mathcal{C}_i^*)\\\\geq \\\\frac{|\\\\mathcal{C}_i^*|}{\\\\epsilon_2}\\\\|x_i-c_i^*\\\\|^2 \\\\geq \\\\frac{n}{K\\\\epsilon_1\\\\epsilon_2}\\\\|x_i-c_i^*\\\\|^2.\\n$$\\nConsequently,\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\\\begin{align}\\\\label{eq:app_3}\\n    \\\\phi_c^*(\\\\mathcal{C}_i^*) + \\\\sum_{y\\\\neq x_i} \\\\|c_y-c_y^*\\\\|^2 &\\\\geq \\\\frac{|\\\\mathcal{C}_i^*|}{\\\\epsilon_2}\\\\|x_i-c_i^*\\\\|^2 + \\\\sum_{y\\\\in\\\\mathcal{C}_i^*} \\\\|c_y-c_y^*\\\\|^2 \\\\notag\\\\\\\\\\n    &= \\\\frac{|\\\\mathcal{C}_i^*|}{\\\\epsilon_2}\\\\|x_i-c_i^*\\\\|^2 + \\\\sum_{y\\\\in\\\\mathcal{C}_i^*} \\\\|c_y-c_i^*\\\\|^2.\\n\\\\end{align}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\nFor $\\\\forall y\\\\in\\\\mathcal{C}_i^*$, it hold $\\\\|x_i-c_i^*\\\\|^2+\\\\|c_y-c_i^*\\\\|^2\\\\geq \\\\frac{1}{2}\\\\|x_i-c_y\\\\|^2\\\\geq\\\\frac{1}{2}d^2(x_i,\\\\mathbf{C}_k)$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('Thus,~(\\\\ref{eq:app_3}) can be lower bounded by\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\\\begin{equation}\\\\label{eq:app_4}\\n    \\\\frac{|\\\\mathcal{C}_i^*|}{\\\\epsilon_2}\\\\|x_i-c_i^*\\\\|^2 + \\\\sum_{y\\\\in\\\\mathcal{C}_i^*} \\\\|c_y-c_i^*\\\\|^2 \\\\geq \\\\frac{|\\\\mathcal{C}_i^*|}{2\\\\epsilon_2}d^2(x_i,\\\\mathbf{C}_k)\\\\geq \\\\frac{n}{2K\\\\epsilon_1\\\\epsilon_2}d^2(x_i,\\\\mathbf{C}_k).\\n\\\\end{equation}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\nCombining~(\\\\ref{eq:app_4}) and~(\\\\ref{eq:app_2}) we obtain\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\\\begin{equation*}\\n    \\\\sum_{y\\\\neq x_i}d^2(y,\\\\mathbf{C}_k)+d^2(x_i,\\\\mathbf{C}_k) \\\\geq \\\\frac{n}{10K\\\\epsilon_1\\\\epsilon_2}d^2(x_i,\\\\mathbf{C}_k).\\n\\\\end{equation*}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\nUsing this expression in~(\\\\ref{eq:app_1}) results in\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\\\begin{equation}\\n    \\\\mathbb{P}(\\\\text{choose }x_i\\\\text{ from }\\\\mathcal{X}\\\\text{ as the $(k+1)$-th centroid}|\\\\mathbf{C}_{k}) \\\\leq \\\\frac{10K\\\\epsilon_1\\\\epsilon_2}{n},\\n\\\\end{equation}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\nwhich holds for $\\\\forall k\\\\in[K]$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('Thus, the probability $\\\\mathbb{P}(x_i\\\\in\\\\mathbf{C})$ can be computed as\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\\\begin{align}\\n    \\\\mathbb{P}(x_i\\\\in\\\\mathbf{C}) &= \\\\sum_{k=0}^{K-1}\\\\mathbb{P}(\\\\text{choose }x_i\\\\text{ from }\\\\mathcal{X}\\\\text{ as the $(k+1)$-th centroid}|\\\\mathbf{C}_{k})\\\\mathbb{P}(\\\\mathbf{C}_k)\\\\notag \\\\\\\\\\n    &\\\\leq \\\\sum_{k=0}^{K-1}\\\\mathbb{P}(\\\\text{choose }x_i\\\\text{ from }\\\\mathcal{X}\\\\text{ as the $(k+1)$-th centroid}|\\\\mathbf{C}_{k})\\\\notag \\\\\\\\\\n    &\\\\leq \\\\frac{1}{n}+\\\\frac{10K(K-1)\\\\epsilon_1\\\\epsilon_2}{n} < O\\\\left(\\\\frac{K^2\\\\epsilon_1\\\\epsilon_2}{n}\\\\right).\\n\\\\end{align}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\nHere, we assumed that $\\\\mathbf{C}_0=\\\\varnothing$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('For the case where outliers are present in the dataset, we have\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\\\begin{align*}\\n    \\\\mathbb{P}(x_i\\\\in\\\\mathbf{C}) &= \\\\mathbb{P}(x_i\\\\in\\\\mathbf{C}|x_i\\\\text{ is outlier})\\\\mathbb{P}(x_i\\\\text{ is outlier}) + \\\\mathbb{P}(x_i\\\\in\\\\mathbf{C}|x_i\\\\text{ is not outlier})\\\\mathbb{P}(x_i\\\\text{ is not outlier}) \\\\\\\\\\n    &\\\\leq 1\\\\cdot O\\\\left(\\\\frac{1}{n}\\\\right) + O\\\\left(\\\\frac{K^2\\\\epsilon_1\\\\epsilon_2}{n}\\\\right)\\\\cdot 1 < O\\\\left(\\\\frac{K^2\\\\epsilon_1\\\\epsilon_2}{n}\\\\right),\\n\\\\end{align*}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\nwhich completes the proof for the adversarial removal scenario.\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\n\\n\\n\\n\\n\\n\\n\\n\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:retrain_prob}}', 1)\n",
      "('\\\\section{Proof of Theorem~\\\\ref{thm:expected_removal_time}}', 1) ('document', 0)\n",
      "('\\nAssume that $\\\\mathcal{X}_R$ denotes the removal request set over $\\\\mathcal{X}$', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:expected_removal_time}}', 1)\n",
      "('From the union bound we have\\n', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:expected_removal_time}}', 1)\n",
      "('\\\\begin{align*}\\n    & \\\\text{Random removals: }\\\\mathbb{P}(\\\\exists j:c_j \\\\in\\\\mathcal{X}_R) < O\\\\left(\\\\frac{RK}{n}\\\\right),\\\\\\\\\\n    & \\\\text{Adversarial removals: }\\\\mathbb{P}(\\\\exists j:c_j \\\\in\\\\mathcal{X}_R) < O\\\\left(\\\\frac{RK^2\\\\epsilon_1\\\\epsilon_2}{n}\\\\right).\\n\\\\end{align*}', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:expected_removal_time}}', 1)\n",
      "('\\nTherefore, the expected removal time for random removals can be upper bounded by\\n', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:expected_removal_time}}', 1)\n",
      "('\\\\begin{align*}\\n    \\\\mathbb{E}(\\\\text{Removal time}) &= \\\\mathbb{E}(\\\\text{Removal time}|\\\\text{new initialization needed})\\\\mathbb{P}(\\\\text{new initialization needed}) + \\\\\\\\\\n    &\\\\mathbb{E}(\\\\text{Removal time}|\\\\text{new initialization not needed})\\\\mathbb{P}(\\\\text{new initialization not needed}) \\\\\\\\\\n    &\\\\leq O(nKd+RK)\\\\cdot O\\\\left(\\\\frac{RK}{n}\\\\right) + O(RK)\\\\cdot 1\\\\\\\\\\n    & < O(RK^2d).\\n\\\\end{align*}', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:expected_removal_time}}', 1)\n",
      "('\\nFollowing a similar argument, we can also show that the expected removal time for adversarial removals can be upper bounded by $O(RK^3\\\\epsilon_1\\\\epsilon_2d)$', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:expected_removal_time}}', 1)\n",
      "('This completes the proof.\\n', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:expected_removal_time}}', 1)\n",
      "('\\n\\n\\n', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:expected_removal_time}}', 1)\n",
      "('\\\\section{Comparison between Algorithm~\\\\ref{alg:mu_kpp} and Quantized $K$-means}', 1) ('document', 0)\n",
      "('\\nIn~, quantized $K$-means were proposed to solve a similar problem of machine unlearning in the centralized setting', 8) ('\\\\section{Comparison between Algorithm~\\\\ref{alg:mu_kpp} and Quantized $K$-means}', 1)\n",
      "('However, that approach substantially differs from Algorithm~\\\\ref{alg:mu_kpp}', 8) ('\\\\section{Comparison between Algorithm~\\\\ref{alg:mu_kpp} and Quantized $K$-means}', 1)\n",
      "('First, the intuition behind quantized $K$-means is that the centroids are computed by taking an average, and the effect of a small number of points is negligible when there are enough terms left in the clusters after removal', 8) ('\\\\section{Comparison between Algorithm~\\\\ref{alg:mu_kpp} and Quantized $K$-means}', 1)\n",
      "(\"Therefore, if we quantize all centroids after each Lloyd's iteration, the quantized centroids will not change with high probability when we remove a small number of points from the dataset\", 8) ('\\\\section{Comparison between Algorithm~\\\\ref{alg:mu_kpp} and Quantized $K$-means}', 1)\n",
      "('Meanwhile, the intuition behind Algorithm~\\\\ref{alg:mu_kpp} is as described in Lemma~\\\\ref{lma:retrain_prob}', 8) ('\\\\section{Comparison between Algorithm~\\\\ref{alg:mu_kpp} and Quantized $K$-means}', 1)\n",
      "(\"Second, the expected removal time complexity for quantized $K$-means equals $O\\\\left({R^2K^3T^2d^{2.5}}/{\\\\epsilon}\\\\right)$, which is high since one needs to check if all quantized centroids remain unchanged after removal at each iteration, where $T$ denotes the maximum number of Lloyd's iteration before convergence and $\\\\epsilon$ is some intrinsic parameter\", 8) ('\\\\section{Comparison between Algorithm~\\\\ref{alg:mu_kpp} and Quantized $K$-means}', 1)\n",
      "('In contrast, Algorithm~\\\\ref{alg:mu_kpp} only needs $O(RK^3\\\\epsilon_1\\\\epsilon_2d)$ even for adversarial removals', 8) ('\\\\section{Comparison between Algorithm~\\\\ref{alg:mu_kpp} and Quantized $K$-means}', 1)\n",
      "('Also note that the described quantized $K$-means algorithm does not come with performance guarantees on removal time complexity unless it is randomly initialized.\\n\\n', 8) ('\\\\section{Comparison between Algorithm~\\\\ref{alg:mu_kpp} and Quantized $K$-means}', 1)\n",
      "('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1) ('document', 0)\n",
      "('\\nLet $\\\\mathbf{C}^*$ denote the optimal set of centroids that minimize the objective~(\\\\ref{eq:central_kmeans}) for the entire dataset $X\\\\in\\\\mathbb{R}^{n\\\\times d}$, let $C^*\\\\in\\\\mathbb{R}^{n\\\\times d}$ be the matrix that records the closest centroid in $\\\\mathbf{C}^*$ to each data point, $\\\\mathbf{C}_s$ the set of centroids returned by Algorithm~\\\\ref{alg:fl_clustering}, and $C_s\\\\in\\\\mathbb{R}^{n\\\\times d}$ the matrix that records the corresponding centroid in $\\\\mathbf{C}_s$ for each data point based on the global clustering defined in Definition~\\\\ref{def:induced_cluster}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('Since we perform $K$-means++ initialization on each client dataset, for client $l$ it holds \\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\\\begin{align}\\\\label{eq:client_performance}\\n    \\\\mathbb{E}\\\\left(\\\\|X^{(l)}-C^{(l)}\\\\|_F^2\\\\right) &\\\\leq (8\\\\ln K+16) \\\\|X^{(l)}-C_*^{(l)}\\\\|_F^2,\\\\quad \\\\forall l\\\\in[L] \\\\notag\\\\\\\\\\n    &\\\\leq (8\\\\ln K+16) \\\\|X^{(l)}-C^{*,(l)}\\\\|_F^2\\n\\\\end{align}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\nwhere $C^{(l)}\\\\in\\\\mathbb{R}^{n^{(l)}\\\\times d}$ records the closest centroid in $\\\\mathbf{C}^{(l)}$ to each data point $x_i$ in $\\\\mathcal{X}^{(l)}$, $C_*^{(l)}$ is the optimal solution that can minimize the local $K$-means objective for client $l$, and $C^{*,(l)}$ denotes the row in $C^*$ that corresponds to client $l$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('Summing up~(\\\\ref{eq:client_performance}) over all clients gives\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\\\begin{align}\\\\label{eq:client_performance_sum}\\n    \\\\mathbb{E}\\\\left(\\\\sum_{l=1}^L\\\\|X^{(l)}-C^{(l)}\\\\|_F^2\\\\right) \\\\leq (8\\\\ln K+16) \\\\sum_{l=1}^L\\\\|X^{(l)}-C^{*,(l)}\\\\|_F^2.\\n\\\\end{align}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\nAt the server side the client centroids are reorganized into a matrix $X_s\\\\in\\\\mathbb{R}^{n\\\\times d}$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('The weights of the client centroids are converted to replicates of rows in $X_s$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('Since we perform full $K$-means++ clustering at the server, it follows that \\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\\\begin{align}\\\\label{eq:server_performance}\\n    \\\\mathbb{E}\\\\left(\\\\|X_s-C_s\\\\|_F^2\\\\right) &= \\\\mathbb{E}\\\\left(\\\\sum_{l=1}^L\\\\|C^{(l)}-C_s^{(l)}\\\\|_F^2\\\\right)\\\\notag \\\\\\\\\\n    &\\\\stackrel{(a)}{\\\\leq} (8\\\\ln K+16)\\\\sum_{l=1}^L\\\\mathbb{E}\\\\left(\\\\|C^{(l)}-C_{s,*}^{(l)}\\\\|_F^2\\\\right) \\\\notag \\\\\\\\\\n    &\\\\leq (8\\\\ln K+16)\\\\sum_{l=1}^L\\\\mathbb{E}\\\\left(\\\\|C^{(l)}-C^{*,(l)}\\\\|_F^2\\\\right),\\n\\\\end{align}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\nwhere $C_{s,*}\\\\in\\\\mathbb{R}^{n\\\\times d}$ is the optimal solution that minimizes the $K$-means objective at the server', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('It is worth pointing out that $C_{s,*}$ is different from $C^*$, as they are optimal solutions for different optimization objectives', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('Note that we still keep the expectation on RHS for $(a)$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('The randomness comes from the fact that $C^{(l)}$ is obtained by $K$-means++ initialization, which is a probabilistic procedure', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('Combining~(\\\\ref{eq:client_performance_sum}) and~(\\\\ref{eq:server_performance}) results in\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\\\begin{align}\\\\label{eq:lma_bound}\\n    \\\\mathbb{E}\\\\left(\\\\phi_f(X;\\\\mathbf{C}_s)\\\\right)&=\\\\mathbb{E}\\\\left(\\\\sum_{l=1}^L\\\\|X^{(l)}-C_s^{(l)}\\\\|_F^2\\\\right) \\\\notag \\\\\\\\\\n    &\\\\leq 2\\\\cdot\\\\mathbb{E}\\\\left[\\\\sum_{l=1}^L\\\\left(\\\\|X^{(l)}-C^{(l)}\\\\|_F^2+\\\\|C^{(l)}-C_s^{(l)}\\\\|_F^2\\\\right)\\\\right] \\\\notag \\\\\\\\\\n    &\\\\leq (16\\\\ln K+32)\\\\sum_{l=1}^L\\\\left[\\\\|X^{(l)}-C^{*,(l)}\\\\|_F^2+\\\\mathbb{E}\\\\left(\\\\|C^{(l)}-C^{*,(l)}\\\\|_F^2\\\\right)\\\\right].\\n\\\\end{align}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\nFor $\\\\mathbb{E}\\\\left(\\\\|C^{(l)}-C^{*,(l)}\\\\|_F^2\\\\right)$, we have\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\\\begin{align}\\\\label{eq:decompose}\\n    \\\\mathbb{E}\\\\left(\\\\|C^{(l)}-C^{*,(l)}\\\\|_F^2\\\\right) &\\\\leq 2\\\\cdot \\\\mathbb{E}\\\\left(\\\\|C^{(l)}-X^{(l)}\\\\|_F^2+\\\\|X^{(l)}-C^{*,(l)}\\\\|_F^2\\\\right) \\\\notag\\\\\\\\\\n    &=2\\\\cdot\\\\|X^{(l)}-C^{*,(l)}\\\\|_F^2 + 2\\\\cdot \\\\mathbb{E}\\\\left(\\\\|C^{(l)}-X^{(l)}\\\\|_F^2\\\\right).\\n\\\\end{align}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\nReplacing~(\\\\ref{eq:decompose}) into~(\\\\ref{eq:lma_bound}) shows that $\\\\mathbb{E}\\\\left(\\\\phi_f(X;\\\\mathbf{C}_s)\\\\right) < O(\\\\log^2 K)\\\\cdot \\\\phi_c^*(X)$, which completes the proof', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('If we are only concerned with the performance of non-outlier points over the entire dataset, we can upper bound the term $\\\\mathbb{E}\\\\left(\\\\sum_{l=1}^L\\\\|C^{(l)}-C^{*,(l)}\\\\|_F^2\\\\right)$ by\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\\\begin{equation}\\\\label{eq:valid_point_upper_bound}\\n    \\\\mathbb{E}\\\\left(\\\\sum_{l=1}^L\\\\|C^{(l)}-C^{*,(l)}\\\\|_F^2\\\\right) \\\\leq \\\\epsilon_2 \\\\phi_c^*(X).\\n\\\\end{equation}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\nHere, we used the fact that rows of $C^{(l)}$ are all real data points sampled by the $K$-means++ initialization procedure', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('For each data point $x_i$, it holds that $\\\\|x_i-c_i^*\\\\|^2|\\\\mathcal{C}_i^*|\\\\leq \\\\epsilon_2\\\\phi_c^*(\\\\mathcal{C}_i^*)$, where $x_i\\\\in\\\\mathcal{C}_i^*$', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('In this case, we arrive at $\\\\mathbb{E}\\\\left(\\\\phi_f(X_t;\\\\mathbf{C}_s)\\\\right) < O(\\\\epsilon_2\\\\log K)\\\\cdot \\\\phi_c^*(X_t)$, where $X_t$ corresponds to all non-outlier points.\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\n\\n\\n\\n\\n\\n\\n\\n\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\\\begin{remark*}\\nIn Theorem 4 of~\\\\citet{guha2003clustering} the authors show that for the distributed $K$-median problem, if we use a $O(b)$-approximation algorithm (i.e., $\\\\phi\\\\leq O(b)\\\\cdot\\\\phi^*$) for the $K$-median problem with subdatasets on distributed machines, and use a $O(c)$-approximation algorithm for the $K$-median problem on the centralized machine, the overall distributed algorithm achieves effectively a $O(bc)$-approximation of the optimal solution to the centralized $K$-median problem. This is consistent with our observation that Algorithm~\\\\ref{alg:fl_nonsecure_clustering} can offer in expectation a $O(\\\\log^2 K)$-approximation to the optimal solution of the centralized $K$-means problem, since $K$-means++ initialization achieves a $O(\\\\log K)$-approximation on both the client and server side.\\n\\nWe also point out that in~\\\\citet{dennis2021heterogeneity} the authors assume that the exact number of clusters from the global optimal clustering on client $l$ is known and equal to $K^{(l)}$, and propose the $K$-FED algorithm which performs well when $K^\\\\prime=\\\\max_{l\\\\in[L]}K^{(l)}\\\\leq\\\\sqrt{K}$. The difference between $K^\\\\prime$ and $K$ represents the data heterogeneity across different clients. With a slight modifications of the proof, we can also obtain $\\\\mathbb{E}\\\\left(\\\\phi_f(X;\\\\mathbf{C}_s)\\\\right) < O(\\\\log K\\\\cdot \\\\log K^\\\\prime)\\\\cdot \\\\phi_c^*(X)$, when $K^{(l)}$ is known for each client beforehand, and perform $K^{(l)}$-means++ on client $l$ instead of $K$-means++ in Algorithm~\\\\ref{alg:fl_clustering}. For the extreme setting where each client safeguards data of one entire cluster (w.r.t. the global optimal clustering ($L=K, K^\\\\prime=1$)), the performance guarantee for Algorithm~\\\\ref{alg:fl_clustering} becomes $\\\\mathbb{E}\\\\left(\\\\phi_f(X;\\\\mathbf{C}_s)\\\\right) < O(1)\\\\cdot \\\\phi_c^*(X)$, which is the same as seeding each optimal cluster by a data point sampled uniformly at random from that cluster. From Lemma 3.1 of~\\\\citet{vassilvitskii2006k} we see that we can indeed have $\\\\mathbb{E}\\\\left(\\\\phi_f(X;\\\\mathbf{C}_s)\\\\right) =2\\\\phi_c^*(X)$, where the approximation factor does not depend on $K$. This shows that data heterogeneity across different clients can benefit the entire federated clustering framework introduced.\\n\\\\end{remark*}', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\n\\n', 8) ('\\\\section{Proof of Lemma~\\\\ref{lma:fl_performance_analog}}', 1)\n",
      "('\\\\section{Proof of Theorem~\\\\ref{thm:fl_performance_quantize}}', 1) ('document', 0)\n",
      "('\\nFollowing the same idea as the one used in the proof of Lemma~\\\\ref{lma:fl_performance_analog}, we arrive at\\n', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:fl_performance_quantize}}', 1)\n",
      "('\\\\begin{align}\\\\label{eq:theorem_decompose}\\n    \\\\mathbb{E}\\\\left(\\\\phi_f(X;\\\\mathbf{C}_s)\\\\right) &\\\\leq 3\\\\cdot\\\\mathbb{E}\\\\left[\\\\sum_{l=1}^L\\\\left(\\\\|X^{(l)}-C^{(l)}\\\\|_F^2+\\\\|C^{(l)}-\\\\widehat{C}^{(l)}\\\\|_F^2+\\\\|\\\\widehat{C}^{(l)}-C_s^{(l)}\\\\|_F^2\\\\right)\\\\right],\\n\\\\end{align}', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:fl_performance_quantize}}', 1)\n",
      "('\\nwhere $\\\\widehat{C}^{(l)}$ is the quantized version of $C^{(l)}$', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:fl_performance_quantize}}', 1)\n",
      "('The first term can be upper bounded in the same way as in Lemma~\\\\ref{lma:fl_performance_analog}', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:fl_performance_quantize}}', 1)\n",
      "('For the second term, the distortion introduced by quantizing one point is bounded by $\\\\frac{\\\\sqrt{d}\\\\gamma}{2}$, if we choose the center of the quantization bin as the reconstruction point', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:fl_performance_quantize}}', 1)\n",
      "('Therefore, \\n', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:fl_performance_quantize}}', 1)\n",
      "('\\\\begin{equation}\\\\label{eq:quantization_distortion}\\n    \\\\mathbb{E}\\\\left(\\\\sum_{l=1}^L \\\\|C^{(l)}-\\\\widehat{C}^{(l)}\\\\|_F^2\\\\right) \\\\leq n\\\\left(\\\\frac{\\\\sqrt{d}\\\\gamma}{2}\\\\right)^2=\\\\frac{nd\\\\gamma^2}{4}.\\n\\\\end{equation}', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:fl_performance_quantize}}', 1)\n",
      "('\\nThe third term can be bounded as\\n', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:fl_performance_quantize}}', 1)\n",
      "('\\\\begin{align}\\\\label{eq:decompose_2}\\n    \\\\mathbb{E}\\\\left(\\\\sum_{l=1}^L\\\\|\\\\widehat{C}^{(l)}-C_s^{(l)}\\\\|_F^2\\\\right) \\\\leq (8\\\\ln K+16)\\\\sum_{l=1}^L\\\\mathbb{E}\\\\left(\\\\|\\\\widehat{C}^{(l)}-C^{*,(l)}\\\\|_F^2\\\\right)\\\\notag \\\\\\\\\\n    \\\\mathbb{E}\\\\left(\\\\|\\\\widehat{C}^{(l)}-C^{*,(l)}\\\\|_F^2\\\\right) \\\\leq 3\\\\cdot \\\\mathbb{E}\\\\left(\\\\|\\\\widehat{C}^{(l)}-C^{(l)}\\\\|_F^2+\\\\|C^{(l)}-X^{(l)}\\\\|_F^2+\\\\|X^{(l)}-C^{*,(l)}\\\\|_F^2\\\\right).\\n\\\\end{align}', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:fl_performance_quantize}}', 1)\n",
      "('\\nReplacing~(\\\\ref{eq:quantization_distortion}) and~(\\\\ref{eq:decompose_2}) into~(\\\\ref{eq:theorem_decompose}) leads to\\n$$\\n    \\\\mathbb{E}\\\\left(\\\\phi_f(X;\\\\mathbf{C}_s)\\\\right) < O(\\\\log^2 K)\\\\cdot \\\\phi_c^*(X) + O(nd\\\\gamma^2\\\\log K),\\n$$\\nwhich completes the proof', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:fl_performance_quantize}}', 1)\n",
      "('Similar as in Lemma~\\\\ref{lma:fl_performance_analog}, we can have that for non-outlier points $X_t$, $\\\\mathbb{E}\\\\left(\\\\phi_f(X_t;\\\\mathbf{C}_s)\\\\right) < O(\\\\epsilon_2\\\\log K)\\\\cdot \\\\phi_c^*(X_t) + O(nd\\\\gamma^2\\\\log K)$.\\n', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:fl_performance_quantize}}', 1)\n",
      "('\\n\\n\\n\\n\\n', 8) ('\\\\section{Proof of Theorem~\\\\ref{thm:fl_performance_quantize}}', 1)\n",
      "('\\\\section{Quantization}', 1) ('document', 0)\n",
      "('\\nFor uniform quantization, we set $\\\\hat{y}=\\\\gamma\\\\cdot a(y)$, where $a(y)=\\\\argmin_{j\\\\in\\\\mathbb{Z}}|y-\\\\gamma j|, y\\\\in\\\\mathbb{R}$ to make the data appear more uniformly distributed within the quantization bins.}', 8) ('\\\\section{Quantization}', 1)\n",
      "('The parameter $\\\\gamma>0$ determines the number of quantization bins in each dimension', 8) ('\\\\section{Quantization}', 1)\n",
      "('Suppose all client data lie in the unit hypercube centered at the origin, and that if needed, pre-processing is performed to meet this requirement', 8) ('\\\\section{Quantization}', 1)\n",
      "('Then the number of quantization bins in each dimension equals $B=\\\\gamma^{-1}$, while the total number of quantization bins for $d$ dimensions is $B^d=\\\\gamma^{-d}$.\\n\\n', 8) ('\\\\section{Quantization}', 1)\n",
      "('\\\\section{Simplified Federated $K$-means Clustering}', 1) ('document', 0)\n",
      "('[1]\\n    \\\\STATE \\\\textbf{input:} Dataset $\\\\mathcal{X}$ distributed on $L$ clients ($\\\\mathcal{X}^{(1)},\\\\ldots,\\\\mathcal{X}^{(L)}$), client oversampling coefficient $\\\\alpha$.\\n    \\\\STATE Run $(\\\\alpha K)$-means++ initialization on each client $l$ in parallel, obtain the initial centroid sets $\\\\mathbf{C}^{(l)}$, and record the corresponding cluster sizes $\\\\left(|\\\\mathcal{C}_1^{(l)}|, \\\\ldots, |\\\\mathcal{C}_{\\\\alpha K}^{(l)}|\\\\right),\\\\;\\\\forall l\\\\in[L]$.\\n    \\\\STATE Send $\\\\left(c_1^{(l)}, \\\\ldots, c_{\\\\alpha K}^{(l)}\\\\right)$ along with the corresponding cluster sizes $\\\\left(|\\\\mathcal{C}_1^{(l)}|, \\\\ldots, |\\\\mathcal{C}_{\\\\alpha K}^{(l)}|\\\\right)$ to the server, $\\\\forall l\\\\in[L]$.\\n    \\\\STATE Concatenate $\\\\left(c_1^{(l)}, \\\\ldots, c_{\\\\alpha K}^{(l)}\\\\right)$ as rows of $X_s$ and set $\\\\left(|\\\\mathcal{C}_1^{(l)}|, \\\\ldots, |\\\\mathcal{C}_{\\\\alpha K}^{(l)}|\\\\right)$ as the weights for the corresponding rows, $\\\\forall l\\\\in[L]$.\\n    \\\\STATE Run full weighted $K$-means++ clustering at server with $X_s$ to obtain the centroid set at server $\\\\mathbf{C}_s$.\\n    \\\\RETURN Each client retains their own centroid set $\\\\mathbf{C}^{(l)}$ while the server retains $X_s$ and $\\\\mathbf{C}_s$.\\n  ', 8) ('\\\\section{Simplified Federated $K$-means Clustering}', 1)\n",
      "('\\n   \\\\caption{Simplified Federated $K$-means Clustering}\\n   \\n   \\n', 8) ('\\\\section{Simplified Federated $K$-means Clustering}', 1)\n",
      "('\\nWhen privacy criteria like the one stated in Section~\\\\ref{sec:prelim} are not enforced, and as done in the framework of ~, one can skip Steps 3-6 in Algorithm~\\\\ref{alg:fl_clustering} and send the centroid set $\\\\mathbf{C}^{(l)}$ obtained by client $l$ along with the cluster sizes $(|\\\\mathcal{C}_1^{(l)}|, \\\\ldots, |\\\\mathcal{C}_{\\\\alpha K}^{(l)}|)$ directly to the server', 8) ('\\\\section{Simplified Federated $K$-means Clustering}', 1)\n",
      "('Then, one can run the weighted $K$-means++ algorithm at the server on the aggregated centroid set to obtain $\\\\mathbf{C}_s$', 8) ('\\\\section{Simplified Federated $K$-means Clustering}', 1)\n",
      "('The pseudocode for this simplified case is shown in Algorithm~\\\\ref{alg:fl_nonsecure_clustering}', 8) ('\\\\section{Simplified Federated $K$-means Clustering}', 1)\n",
      "('It follows a similar idea as the divide-and-conquer schemes of~, developed for distributed clustering', 8) ('\\\\section{Simplified Federated $K$-means Clustering}', 1)\n",
      "(\"In Step 5 of Algorithm~\\\\ref{alg:fl_nonsecure_clustering}, weighted $K$-means++ would assign weights to data points when computing the sampling probability during the initialization procedure and when computing the average of clusters during the Lloyd's iterations\", 8) ('\\\\section{Simplified Federated $K$-means Clustering}', 1)\n",
      "('Since the weights we are considering here are always positive integers, a weighted data point can also be viewed as there exist identical data points in the dataset with multiplicity equals to the weight.\\n\\n', 8) ('\\\\section{Simplified Federated $K$-means Clustering}', 1)\n",
      "('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1) ('document', 0)\n",
      "('\\nTo demonstrate that the messages generated by Algorithm~\\\\ref{alg:fl_secure} can be uniquely decoded, we prove that there exists a unique\\n$q$ that produces the aggregated values $\\\\{S_i\\\\}_{i\\\\in[2KL]}$ at the server', 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "('The proof is by contradiction', 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "(\"Assume that there exist two different vectors $q$ and $q'$ that result in the same $\\\\{S_i\\\\}_{i\\\\in[2KL]}$\", 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "(\"In this case, we have the following set of linear equations $\\\\sum_{j:q_j\\\\ne 0}q_j \\\\cdot j^{i-1}-\\\\sum_{j:q'_j\\\\ne 0}q'_j \\\\cdot j^{i-1}=0,$ $i\\\\in[2KL]$\", 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "(\"Given that $\\\\{q_j:q_j\\\\ne 0\\\\}$ and $\\\\{q'_j:q'_j\\\\ne 0\\\\}$ represent at most $2KL$ unknowns and $j^{i-1}$ coefficients, the linear equations can be described using a square Vandermonde matrix for the coefficients, with the columns of the generated by the indices of the nonzero entries in $q$\", 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "('This leads to a contradiction since a square Vandermonde matrix with different column generators is invertible, which we show below', 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "('Hence, the aggregated values $\\\\{S_i\\\\}$ must be different for different $q$', 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "('Similarly, the sums $\\\\sum_{j:q^{(l)}_j\\\\ne 0}q^{(l)}_j \\\\cdot j^{i-1}$ are distinct for different choices of vectors $q^{(l)}$, $i\\\\in[2KL]$, $l\\\\in[L]$', 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "(\"If two vectors $q$ and $q'$ result in the same $\\\\{S_i\\\\}_{i\\\\in[2KL]}$, then $\\\\sum_{j:q_j\\\\ne 0}q_j\\\\cdot j^{i-1}-\\\\sum_{j:q'_j\\\\ne 0}q'_j\\\\cdot j^{i-1}=0,$ for all $i\\\\in[2KL]$\", 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "(\"Let $\\\\{i_1,\\\\ldots,i_u\\\\}=(\\\\{j:q_j\\\\ne 0\\\\}\\\\cup\\\\{ j:q'_j= 0\\\\})$ be the set of integers such that at least one of $q_{i_m}$ and $q'_{i_m}$ is nonzero for $m\\\\in[u]$\", 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "('Note that $u\\\\le 2KL$', 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "('Rewrite this equation as\\n', 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "(\"\\\\begin{align}\\\\label{eq:vandermonde}\\n    \\\\begin{bmatrix}\\n1 & \\\\cdots & 1\\\\\\\\\\ni_1 & \\\\cdots & i_u\\\\\\\\\\n\\\\vdots& \\\\vdots &\\\\vdots \\\\\\\\\\ni^{2KL-1}_1 & \\\\cdots & i^{2KL-1}_u\\n\\\\end{bmatrix}\\\\begin{bmatrix}\\nq_{i_1}-q'_{i_1}\\\\\\\\\\n\\\\vdots\\\\\\\\\\nq_{i_u}-q'_{i_u}\\n\\\\end{bmatrix}=\\\\boldsymbol{0}.\\n\\\\end{align}\", 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "('\\nSince $u\\\\le 2KL$, we take the first $u$ equations in (\\\\ref{eq:vandermonde}) and rewrite them as\\n', 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "('\\\\begin{align*}\\n Bv=\\\\boldsymbol{0},\\n\\\\end{align*}', 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "('\\nwhere \\n', 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "('\\\\begin{align*}\\n    B=\\\\begin{bmatrix}\\n1 & \\\\cdots & 1\\\\\\\\\\ni_1 & \\\\cdots & i_u\\\\\\\\\\n\\\\vdots& \\\\vdots &\\\\vdots \\\\\\\\\\ni^{2KL-1}_1 & \\\\cdots & i^{2KL-1}_u\\n\\\\end{bmatrix}\\n\\\\end{align*}', 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "('\\nis a square Vandermonde matrix and \\n', 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "(\"\\\\begin{align*}\\n    v=\\\\begin{bmatrix}\\nq_{i_1}-q'_{i_1}\\\\\\\\\\n\\\\vdots\\\\\\\\\\nq_{i_u}-q'_{i_u}\\n\\\\end{bmatrix}\\n\\\\end{align*}\", 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "(\"\\nis a nonzero vector since $q\\\\ne q'$\", 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "('It is known that the determinant of a square Vandermonde matrix $B$ is given by $\\\\prod_{m_1<m_2,m_1,m_2\\\\in[u]}(i_{m_2}-i_{m_1})$, which in our case is nonzero since all the $i_1,\\\\ldots,i_u$ are different', 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "('Therefore, $B$ is invertible and does not admit a non-zero solution, which contradicts the equation $Bv=\\\\boldsymbol{0}$.\\n\\n', 8) ('\\\\section{The Uniqueness of the Vector $q$ given $\\\\{S_i\\\\}_{i\\\\in[2KL]}$}', 1)\n",
      "('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1) ('document', 0)\n",
      "('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn the SCMA scheme we described in Algorithm \\\\ref{alg:fl_clustering}, the goal of the server is to reconstruct the vector $q$, given values $S_i=\\\\sum_{j:q_j\\\\ne 0}q_j\\\\cdot j^{i-1} \\\\text{ mod } p$ for $i\\\\in[2KL]$', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('To this end, we first use the  Berlekamp-Massey algorithm to compute the polynomial $g(x)=\\\\prod_{j:q_j\\\\ne 0}(1-j\\\\cdot x)$', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('Then, we factorize $g(x)$ over the finite field $\\\\mathbb{F}_p$ using the algorithm described in~', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('The complexity $O((KL)^{1.5}\\\\log p+ KL\\\\log^2 p)$ referred to in Section~\\\\ref{sec:fl_complexity} corresponds to the average complexity (finding a deterministic algorithm that factorizes a polynomial over finite fields with $poly(\\\\log p)$ worst-case complexity is an open problem)', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('The complexity $\\\\max\\\\{O(K^2L^2), O((KL)^{1.5}\\\\log p+ KL\\\\log^2 p)\\\\}$ referred to in Appendix~\\\\ref{sec:fl_complexity} for the SCMA scheme represents an average complexity', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('We show next that the SCMA scheme has small worst-case complexity under a deterministic decoding algorithm at the server as well', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"To this end, we replace the integer $p$ in Algorithm \\\\ref{alg:fl_secure} with a large number $p'\\\\ge \\\\max\\\\{KLB^{2dKL},n\\\\}+1$ such that $p'$ is larger than the largest possible $S_i$ and there is no overflow when applying the modulo $p'$ operation on $S_i$\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"It is known (Bertrand's postulate) that there exists a prime number between any integer $n>3$ and $2n-2$, and hence there must be a prime number lower-bounded by $\\\\max\\\\{KLB^{2dKL},n\\\\}+1$ and twice the lower bound $2(\\\\max\\\\{KLB^{2dKL},n\\\\}+1)$\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"However, since searching for a prime number of this size can be computationally intractable, we remove the requirement that $p'$ is prime\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"Correspondingly, $\\\\mathbb{F}_{p'}$ is not necessarily a finite field\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"Then, instead of sending $S^{(l)}_i=(\\\\sum_{j:q^{(l)}_j\\\\ne 0}q^{(l)}_j\\\\cdot j^{i-1}+z^{(l)}_i) \\\\text{ mod } p$, client $l$, $l\\\\in[L],$ will send $S^{(l)}_i=(\\\\sum_{j:q^{(l)}_j\\\\ne 0}q^{(l)}_j\\\\cdot j^{i-1}+z^{(l)}_i) \\\\text{ mod } p'$ to the server, $i\\\\in[2KL]$, where random keys $z^{(l)}_i$ are independently and uniformly distributed over $\\\\{0,\\\\ldots,p'-1\\\\}$ and hidden from the server\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"After obtaining $S_i$, $i\\\\in[2KL]$, the server can continue performing operations over the field of reals since there is no overflow in computing $S_i\\\\mod p'$\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"We note that though $p'$ is exponentially large, the computation of $S^{(l)}_i$ and $S_i$, $l\\\\in[L]$ and $i\\\\in[2KL]$ is still manageable, and achieved by computing and storing $S^{(l)}_i$ and $S_i$ using $O(KL)$ floating point numbers, instead of computing and storing $S^{(l)}_i$ in a single floating point number\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('Note that $j^{i}$ can be computed using $O(i)$ floating point numbers with complexity almost linear in $i$ (i.e., $O(i\\\\log^c i)$ for some constant $c$)', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('We now present a low complexity secure aggregation algorithm at the server', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('After reconstructing $S_i$, we have $S_i=\\\\sum_{j:q_j\\\\ne 0}q_j\\\\cdot j^{i-1}$', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('The server switches to computations over the real field', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('First, it uses the Berlekamp-Massey algorithm to find the polynomial $g(x)=\\\\prod_{j:q_j\\\\ne 0}(1-j\\\\cdot x)$ (the algorithm was originally proposed for decoding of BCH codes over finite fields, but it applies to arbitrary fields)', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('Let $m$ be the degree of $g(x)$', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('Then $h(x)=x^mg(1/x)=\\\\prod_{j:q_j\\\\ne 0}(x-j)$', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('The goal is to factorize $h(x)$ over the field of reals, where the roots are known to be integers in $[B^d]$ and the multiplicity of each root is one', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('If the degree of $h(x)$ is odd, then $h(0)<0$ and $h(B^d)>0$', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"Then we can use bisection search to find a root of $h(x)$, which requires $O(\\\\log B^d)$ polynomial evaluations of $h(x)$, and thus $O(MK\\\\log B^d)$ multiplication and addition operations of integers of size at most $\\\\log p'$\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('After finding one root $j$, we can divide $h(x)$ by $x-j$ and start the next root-finding iteration', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"If the degree of $h(x)$ is even, then the degree of $h'(x)$ is odd, and the roots of $h'(x)$ are different and confined to $[B^d]$\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"We use bisection search to find a root $j'$ of $h'(x)$\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"If $h(j')<0$, then we use bisection search on $[0,j']=\\\\{0,1,\\\\ldots,j'\\\\}$ to find a root of $h(x)$ and start a new iteration as described above when the degree of $h(x)$ is odd\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"If $h(j')>0$, then $h'(j'-1)>0$ and $h'(0)<0$\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"We use bisection search to find another root of $h'(x)$ in $[j'-1]$\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"Note that for every two roots $j'_1$ and $j'_2$ ($j'_1<j'_2$) of $h'(x)$ satisfying $h(j'_1)>0$ and $h(j'_2)>0$ we can always find another root $j'_3$ of $h'(x)$ in $[j'_1+1,j'_2-1]$\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"We keep iterating the search for every two such roots $j'_1,j'_2$ until we find a list of roots $r_1,\\\\ldots,r_{2R+1}$ of $h'(x)$ such that $h(r_{i})<0$ for odd $i$ in $[2R+1]$ and $h(r_i)>0$ for even $i\\\\in[2R+1]$\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('Then we can run bisection search on the sets $[0,r_1], [r_1,r_2],\\\\ldots,[r_{2R},r_{2R+1}],[r_{2R+1},B^d]$, to find $2R+2$ roots of $h(x)$', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"Note that during the iteration we need $2R+1$ bisection search iterations to find the roots $r_1,\\\\ldots,r_{2R+1}$ for $h'(x)$ and $2R+2$ bisection search iterations to find $2R+2$ roots for $h(x)$\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "(\"The total computations complexity is hence at most $O(MK\\\\log B^d)$ evaluations of polynomials with degree at most $O(MK)$ and at most $O(MK)$ polynomial divisions, which requires at most $O((MK)^2\\\\log B^d)$ multiplications and additions for integers of size at most $\\\\log p'$\", 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('This results in an overall complexity of $O((MK)^3d^2\\\\log^c (MK)\\\\log B),$ for some constant $c<2$. \\n\\n', 8) ('\\\\section{A Deterministic Low-Complexity Decoding Algorithm for SCMA at the Server}', 1)\n",
      "('\\\\section{Comparison among SCMA, Private Set Union and Sparse Secure Model Aggregation}', 1) ('document', 0)\n",
      "('\\nOur SCMA scheme is different from a related problem of private set union~, in which multiple parties communicate with each other to securely compute the union of their sets', 8) ('\\\\section{Comparison among SCMA, Private Set Union and Sparse Secure Model Aggregation}', 1)\n",
      "('In SCMA we aggregate multisets, which include the frequency of each element that is not revealed in the private set union problem', 8) ('\\\\section{Comparison among SCMA, Private Set Union and Sparse Secure Model Aggregation}', 1)\n",
      "('In addition, our scheme includes only one round of communication from clients to the server, while there is no server in the private set union problem and multi-round client to client communication is needed', 8) ('\\\\section{Comparison among SCMA, Private Set Union and Sparse Secure Model Aggregation}', 1)\n",
      "('Note that SCMA may also be adapted to solve problems related with sparse secure model aggregation~, which is another interesting but orthogonal research direction to this work.\\n\\n', 8) ('\\\\section{Comparison among SCMA, Private Set Union and Sparse Secure Model Aggregation}', 1)\n",
      "('\\\\section{Experimental Setup and Additional Results}', 1) ('document', 0)\n",
      "('\\n', 8) ('\\\\section{Experimental Setup and Additional Results}', 1)\n",
      "('\\\\subsection{Datasets}', 2) ('\\\\section{Experimental Setup and Additional Results}', 1)\n",
      "('\\nIn what follows, we describe the datasets used in our numerical experiments', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('Note that we preprocessed all datasets such that the absolute value of each element in the data matrix is smaller than $1$', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('Each dataset has an intrinsic parameter $K$ for the number of optimal clusters, and these are used in the centralized $K$-means++ algorithm to compute the approximation of the optimal objective value', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('We use $\\\\phi_c^*(X)$ in subsequent derivation to denote the objective value returned by the $K$-means++ algorithm', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('Besides $K$, we set an additional parameter $K^\\\\prime\\\\sim\\\\sqrt{K}$ for each client data so that the number of true clusters at the client level is not larger than $K^\\\\prime$', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('This non-i.i.d. data distribution across clients is discussed in~', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('For small datasets (e.g., TCGA, TMI), we consider the number of clients $L$ as $10$, and set $L=100$ for all other datasets.\\n\\n\\n\\n\\\\textbf{Celltype} [$n=12009,d=10,K=4$]~ comprises single cell RNA sequences belonging to a mixture of four cell types: fibroblasts, microglial cells, endothelial cells and mesenchymal stem cells', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('The data, retrieved\\nfrom the Mouse Cell Atlas, consists of $12009$ data points and each sample has $10$ feature dimensions, reduced from an original dimension of $23,433$ using Principal Component Analysis (PCA)', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('The sizes of the four clusters are $6506, 2328, 2201, 974$.\\n\\n\\\\textbf{Postures} [$n=74975,d=15,K=5$]~ comprises images obtained via a motion capture system and a glove for $12$ different users performing five hand postures -- fist, pointing with one finger,\\npointing with two fingers, stop (hand flat), and grab (fingers curled)', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('For establishing a rotation and translation invariant local coordinate system, a rigid unlabeled pattern on the back of the glove was utilized', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('There are a total of $74975$ samples in the dataset and the data dimension is $15$', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('The sizes of the given clusters are $19772, 17340, 15141, 12225, 10497$.\\n\\n\\\\textbf{Covtype} [$n=15120,d=52,K=7$]~ comprises digital spatial data for seven forest cover types obtained from the US Forest Service (USFS) and the US Geological Survey (USGS)', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('There are $52$ cartographic variables including slope, elevation, and aspect', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('The dataset has $15120$ samples', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('The sizes of the seven clusters are $3742, 3105, 2873, 2307, 1482, 886, 725$.\\n\\n\\\\textbf{Gaussian} [$n=30000,d=10,K=10$] comprises ten clusters, each generated from a $10$-variate Gaussian distribution centered at uniformly at random chosen locations in the unit hypercube', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('From each cluster, $3000$ samples are taken, for a total of $30000$ samples', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('Each Gaussian cluster is spherical with variance $0.5$.\\n\\n\\\\textbf{FEMNIST} [$n=36725,d=784,K=62$]~ is a popular FL benchmark dataset comprising images of digits (0-9) and letters from the English alphabet (both upper and lower cases) from over $3500$ users', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('It dataset is essentially built from the Extended MNIST repository~ by partitioning it on the basis of the writer of the digit/character', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('We extract data corresponding to $100$ different clients, each of which contributed at least $350$ data points', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('Each image has dimension $784$', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('The size of the largest cluster is $1234$, and that of the smallest cluster is $282$.   \\n\\n\\\\textbf{TCGA} [$n=1904,d=57,K=4$] methylation consists of methylation microarray data for $1904$ samples from The Cancer Genome Atlas (TCGA)~ corresponding to four different cancer types: Low Grade Glioma (LGG), Lung Adenocarcinoma (LUAD), Lung Squamous Cell Carcinoma (LUSC) and Stomach Adenocarcinoma (STAD)', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('The observed features correspond to a subset of $\\\\beta$ values, representing the coverage of the methylated sites, at $57$ locations on the promoters of $11$ different genes (ATM, BRCA1, CASP8, CDH1, IGF2, KRAS, MGMT, MLH1, PTEN, SFRP5 and TP53)', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('This subset of genes was chosen for its relevance in carcinogenesis', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('The sizes of the four clusters are $735, 503, 390, 276$.\\n\\n\\\\textbf{TMI} [$n=1126,d=984,K=4$] contains samples from human gut microbiomes', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('We retrieved $1126$ human gut microbiome samples from the NIH Human Gut Microbiome~', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('Each data point is of dimension $983$, capturing the frequency (concentration) of  identified bacterial species or genera in the sample', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('The dataset can be roughly divided into four classes based on gender and age', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('The sizes of the four clusters are $934, 125, 46, 21$.\\n\\n', 8) ('\\\\subsection{Datasets}', 2)\n",
      "('\\\\subsection{Baseline Setups}', 2) ('\\\\section{Experimental Setup and Additional Results}', 1)\n",
      "(' We use the publicly available implementation of K-FED and DC-KM as our baseline methods', 8) ('\\\\subsection{Baseline Setups}', 2)\n",
      "('For DC-KM, we set the height of the computation tree to $2$, and observe that the leaves represent the clients', 8) ('\\\\subsection{Baseline Setups}', 2)\n",
      "('Since K-FED does not originally support data removal, has high computational complexity, and its clustering performance is not comparable with that of DC-KM (see Table~\\\\ref{tab:performance}), we thus only compare the unlearning performance of MUFC with DC-KM', 8) ('\\\\subsection{Baseline Setups}', 2)\n",
      "('During training, the clustering parameter $K$ is set to be the same in both clients and server for all methods, no matter how the data was distributed across the clients', 8) ('\\\\subsection{Baseline Setups}', 2)\n",
      "('Experiments on all datasets except FEMNIST were repeated $5$ times to obtain the mean and standard deviations, and experiments on FEMNIST were repeated $3$ times due to the high complexity of training', 8) ('\\\\subsection{Baseline Setups}', 2)\n",
      "('Note that we used the same number of repeated experiments as in~.\\n\\n', 8) ('\\\\subsection{Baseline Setups}', 2)\n",
      "('\\\\subsection{Enabling Complete Client Training for MUFC}', 2) ('\\\\section{Experimental Setup and Additional Results}', 1)\n",
      "('\\nNote that both K-FED and DC-KM allow clients to perform full $K$-means++ clustering to improve the clustering performance at the server', 8) ('\\\\subsection{Enabling Complete Client Training for MUFC}', 2)\n",
      "('Thus it is reasonable to enable complete client training for MUFC as well to compare the clustering performance on the full datasets', 8) ('\\\\subsection{Enabling Complete Client Training for MUFC}', 2)\n",
      "('Although in this case we need to retrain affected clients and the server for MUFC upon each removal request, leading to a similar unlearning complexity as DC-KM, the clustering performance of MUFC is consistently better than that of the other two baseline approaches (see Table~\\\\ref{tab:app_exp})', 8) ('\\\\subsection{Enabling Complete Client Training for MUFC}', 2)\n",
      "('This is due to the fact that we utilize information about the aggregated weights of client centroids.\\n\\n', 8) ('\\\\subsection{Enabling Complete Client Training for MUFC}', 2)\n",
      "('\\\\begin{table}[ht]\\n\\\\setlength{\\\\tabcolsep}{3pt}\\n\\\\centering\\n\\\\footnotesize\\n\\\\caption{Clustering performance of different federated clustering algorithms compared to centralized $K$-means++ clustering. Boldface numbers are used to mark the best results.}\\n\\\\vspace{0.2in}\\n\\\\label{tab:app_exp}\\n\\\\begin{tabular}{@{}cc|c|c|c|c|c|c|c@{}}\\n\\\\toprule\\n                                      & & TMI & Celltype & Gaussian & TCGA & Postures & FEMNIST & Covtype         \\\\\\\\ \\\\midrule\\n\\\\multirow{3}{*}{Loss ratio}  & \\\\multicolumn{1}{c|}{{MUFC}} & {$\\\\mathbf{1.05\\\\pm 0.01}$} & {$\\\\mathbf{1.03\\\\pm 0.00}$} & {$\\\\mathbf{1.02\\\\pm 0.00}$} & {$\\\\mathbf{1.02\\\\pm 0.01}$} & {$\\\\mathbf{1.02\\\\pm 0.00}$} & {$\\\\mathbf{1.12\\\\pm 0.00}$} & {$\\\\mathbf{1.02\\\\pm 0.00}$} \\\\\\\\\\n                                      & \\\\multicolumn{1}{c|}{{K-FED}}  & {$1.84\\\\pm 0.07$} & {$1.72\\\\pm 0.24$}  & {$1.25\\\\pm 0.01$}  & {$1.56\\\\pm 0.11$}  & {$1.13\\\\pm 0.01$}  & {$1.21\\\\pm 0.00$}  & {$1.60\\\\pm0.01$} \\\\\\\\\\n                                      & \\\\multicolumn{1}{c|}{{DC-KM}} & {$1.54\\\\pm 0.13$}  & {$1.46\\\\pm 0.01$} & {$\\\\mathbf{1.02\\\\pm 0.00}$} & {$1.15\\\\pm 0.02$} & {$1.03\\\\pm 0.00$} & {$1.18\\\\pm 0.00$} & {$1.03\\\\pm 0.02$} \\\\\\\\ \\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}', 8) ('\\\\subsection{Enabling Complete Client Training for MUFC}', 2)\n",
      "('\\n\\n', 8) ('\\\\subsection{Enabling Complete Client Training for MUFC}', 2)\n",
      "('\\\\subsection{Loss Ratio and Unlearning Efficiency}', 2) ('\\\\section{Experimental Setup and Additional Results}', 1)\n",
      "('\\nIn Figure~\\\\ref{fig:app_exp} we plot results pertaining to the change of loss ratio after each removal request and the accumulated removal time when the removal requests are adversarial', 8) ('\\\\subsection{Loss Ratio and Unlearning Efficiency}', 2)\n",
      "('The conclusion is consistent with the results in Section~\\\\ref{sec:exp}.\\n\\n', 8) ('\\\\subsection{Loss Ratio and Unlearning Efficiency}', 2)\n",
      "('\\\\begin{figure}[htb]\\n    \\\\centering\\n    \\\\includegraphics[width=\\\\linewidth]{figs/app_exp.png}\\n    \\\\caption{The shaded areas represent the standard deviation of results from different trails for all subplots. (a)-(d) The change of loss ratio ${\\\\phi_f(X;\\\\mathbf{C}_s)}/{\\\\phi_c^*(X)}$ after each round of unlearning procedure. (e)-(h) The accumulated removal time for adversarial removals.}\\n    \\\\label{fig:app_exp}\\n\\\\end{figure}', 8) ('\\\\subsection{Loss Ratio and Unlearning Efficiency}', 2)\n",
      "('\\n\\n', 8) ('\\\\subsection{Loss Ratio and Unlearning Efficiency}', 2)\n"
     ]
    }
   ],
   "source": [
    "nodes, node_hierarchy, refs = hierarchy_version(\"../../23127247_milestone1/2210.16424/tex/2210.16424v1/\")\n",
    "print(\"Refs:\", refs)\n",
    "for node, parent in node_hierarchy.items():\n",
    "    print(nodes[node], nodes[parent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fbdbaf-e44d-444e-ba50-b925537a30bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
